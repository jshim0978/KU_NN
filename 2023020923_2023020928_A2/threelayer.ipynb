{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Part1 Three layer network (60 points):\n",
    "\n",
    "<p>\n",
    "Create a notebook called threelayer.ipynb.\n",
    "Extend the fully-connected two layer perceptron shown in class for the regression\n",
    "problem by one more layer to have two hidden layers.\n",
    "For this, take a long look at the derivation of the backpropagation. As you can see,\n",
    "the derivation of the internal derivatives is always the same, no matter the number\n",
    "of hidden layers.\n",
    "Armed with this knowledge, it should be easy to extend the different functions in\n",
    "the notebook.\n",
    "The create_model function should now of course receive a list of values for the\n",
    "number of hidden_nodes in each layer - in addition, please extend the functionality,\n",
    "so that I can pass the activation function type as a string already here:\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "```python\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function = 'relu') :\n",
    "    return\n",
    "```\n",
    "\n",
    "<p>\n",
    "forward, calculate_loss, backprop should of course be extended\n",
    "accordingly to deal with the added number of layers and the activation function\n",
    "choice.\n",
    "Next, do a series of tests for the x^2+y^2+1 function using a learning rate of 0.001, a\n",
    "tolerance threshold of 0.0001, maximum iterations of 100,000, NO sgd, and NO\n",
    "regularization, and a “relu” function, comparing the “old” two-layer version with\n",
    "your “new” three-layer version as follows:\n",
    "- take 8 neurons for the two-layer version and 4 + 4 neurons for the three-layer\n",
    "version, and run each network 20 times, recording the loss and the number\n",
    "of iterations it needs\n",
    "- repeat this with 16 neurons for the two-layer version and 8+8 neurons for the\n",
    "three-layer version and 20 runs each\n",
    "- which network architecture converges “better” (earlier? lower error?). Plot\n",
    "the results nicely in one graph for errors and in another for number of\n",
    "iterations and comment on the results.\n",
    "</p>"
   ],
   "metadata": {
    "id": "89fqwYoOQ-kY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# numpy, matplotlib imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T04:40:50.526542500Z",
     "start_time": "2023-10-27T04:40:50.493609700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1. * (X > 0)\n",
    "\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return 1.-tanh(X)**2\n",
    "\n",
    "def logistic(X):\n",
    "    return 1./(1. + np.exp(-X))\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return logistic(X)*(1. - logistic(X))\n",
    "\n",
    "# Activation functions mapping\n",
    "activation_functions = {\n",
    "    'relu': relu,\n",
    "    'tanh': tanh,\n",
    "    'logistic': logistic\n",
    "}\n",
    "\n",
    "# Activation functions derivatives mapping\n",
    "activation_derivatives = {\n",
    "    'relu': relu_derivative,\n",
    "    'tanh': tanh_derivative,\n",
    "    'logistic': logistic_derivative\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T04:40:51.229678500Z",
     "start_time": "2023-10-27T04:40:51.196595700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def old_create_model(X, hidden_nodes, output_dim = 2):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b1'] = np.zeros((1, hidden_nodes))\n",
    "\n",
    "    # second set of weights from hidden layer 1 to output\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b2'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def old_feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "\n",
    "    # activation function\n",
    "    #a1 = logistic(z1)\n",
    "    #a1 = tanh(z1)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "\n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = z2\n",
    "    return z1, a1, z2, out\n",
    "\n",
    "# define the regression loss\n",
    "def old_calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out = old_feed_forward(model, X)\n",
    "\n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def old_backprop(X,y,model,z1,a1,z2,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # derivative of loss function\n",
    "    delta3 = (output-y)/num_examples\n",
    "    \n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    \n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    #delta2 = delta3.dot(model['W2'].T) * logistic_derivative(a1) #if logistic\n",
    "    #delta2 = delta3.dot(model['W2'].T) * tanh_derivative(a1) #if tanh\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1) #if ReLU\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = np.dot(X.T, delta2)\n",
    "    \n",
    "    # and sum over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop\n",
    "def old_train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1,a1,z2,output = old_feed_forward(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW1, dW2, db1, db2 = old_backprop(X, y, model, z1, a1, z2, output)\n",
    "\n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "\n",
    "        loss = old_calculate_loss(model, X, y)\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"two-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n",
    "                print(np.abs((previous_loss - loss) / previous_loss), previous_loss, loss)\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "            \n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T04:40:52.386661Z",
     "start_time": "2023-10-27T04:40:52.349015100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create a three-layer neural network\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function='relu'):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    #save activation function to model\n",
    "    model['activation_function'] = activation_function\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # [i -> 1]weights and biases from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes[0]) / np.sqrt(input_dim)\n",
    "    model['b1'] = np.zeros((1, hidden_nodes[0]))\n",
    "    \n",
    "    # [1 -> 2]weights and biases  from  hidden layer 1 to hidden layer 2\n",
    "    model['W2'] = np.random.randn(hidden_nodes[0], hidden_nodes[1]) / np.sqrt(hidden_nodes[0])\n",
    "    model['b2'] = np.zeros((1, hidden_nodes[1]))\n",
    "\n",
    "    # [2 -> o]weights and biases from hidden layer 2 to output\n",
    "    model['W3'] = np.random.randn(hidden_nodes[1], output_dim) / np.sqrt(hidden_nodes[1])\n",
    "    model['b3'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = relu(z2)\n",
    "    \n",
    "    # third layer\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    out = z3\n",
    "    \n",
    "    return z1, a1, z2, a2, z3, out\n",
    "    \n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, a2, z3, out = feed_forward(model, X)\n",
    "\n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,a2,z3,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "\n",
    "    # Derivative of loss function for output layer\n",
    "    delta4 = (output - y) / num_examples\n",
    "\n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW3 = a2.T.dot(delta4)\n",
    "\n",
    "    # and over all neurons\n",
    "    db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta3 = delta4.dot(model['W3'].T) * relu_derivative(a2)\n",
    "\n",
    "    # multiply this by hidden layer outputs\n",
    "    dW2 = a1.T.dot(delta3)\n",
    "\n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1)\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = X.T.dot(delta2)\n",
    "\n",
    "    # and over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, dW3, db1, db2, db3\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance = 0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1, a1, z2, a2, z3, output = feed_forward(model, X)\n",
    "\n",
    "        # feed this into backprop\n",
    "        dW1, dW2, dW3, db1, db2, db3 = backprop(X, y, model, z1, a1, z2, a2, z3, output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        model['W3'] -= learning_rate * dW3\n",
    "        model['b3'] -= learning_rate * db3\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"three-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "\n",
    "        # Check for convergence\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n",
    "                print(np.abs((previous_loss - loss) / previous_loss), previous_loss, loss)\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T04:40:53.319838800Z",
     "start_time": "2023-10-27T04:40:53.295002600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-layer Loss after iteration 0: 1581.9608489308757\n",
      "two-layer Loss after iteration 1000: 23.233829936524586\n",
      "two-layer Loss after iteration 2000: 9.317535963241921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_20564\\3437914713.py:112: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-layer Loss after iteration 3000: 7.466601878626524\n",
      "two-layer Loss after iteration 4000: 6.49344740222007\n",
      "two-layer Loss after iteration 5000: 5.738455708844847\n",
      "two-layer Loss after iteration 6000: 5.387939530574615\n",
      "two-layer Loss after iteration 7000: 5.083224230531048\n",
      "two-layer Loss after iteration 8000: 4.917302245558028\n",
      "two-layer Loss after iteration 9000: 4.786824583758771\n",
      "two-layer Loss after iteration 10000: 4.688087399905922\n",
      "two-layer Loss after iteration 11000: 4.607625751267616\n",
      "two-layer Loss after iteration 12000: 4.541125821034007\n",
      "two-layer Loss after iteration 13000: 4.4845509855619605\n",
      "two-layer Loss after iteration 14000: 4.435625958653285\n",
      "two-layer Loss after iteration 15000: 4.392706416900018\n",
      "two-layer Loss after iteration 16000: 4.354838914990365\n",
      "two-layer Loss after iteration 17000: 4.321287575210507\n",
      "two-layer Loss after iteration 18000: 4.289881546011533\n",
      "two-layer Loss after iteration 19000: 4.259783559575826\n",
      "two-layer Loss after iteration 20000: 4.233936480246311\n",
      "two-layer Loss after iteration 21000: 4.1869583790818306\n",
      "two-layer Loss after iteration 22000: 4.164390756963462\n",
      "two-layer Loss after iteration 23000: 4.145486105790919\n",
      "two-layer Loss after iteration 24000: 4.129107907764575\n",
      "two-layer Loss after iteration 25000: 4.1147661882307105\n",
      "two-layer Loss after iteration 26000: 4.102135052308827\n",
      "two-layer Loss after iteration 27000: 4.090971928949173\n",
      "two-layer Loss after iteration 28000: 4.078891707485582\n",
      "two-layer Loss after iteration 29000: 4.069375960333005\n",
      "two-layer Loss after iteration 30000: 4.0614132387943895\n",
      "two-layer Loss after iteration 31000: 4.054513239306922\n",
      "two-layer Loss after iteration 32000: 4.047016802896551\n",
      "two-layer Loss after iteration 33000: 4.041746906511738\n",
      "two-layer Loss after iteration 34000: 4.037199088662189\n",
      "two-layer Loss after iteration 35000: 4.033234660009638\n",
      "two-layer Loss after iteration 36000: 4.0297681137957015\n",
      "two-layer Loss after iteration 37000: 4.026732258066459\n",
      "two-layer Loss after iteration 38000: 4.0240819032755315\n",
      "two-layer Loss after iteration 39000: 4.001702930729668\n",
      "two-layer Loss after iteration 40000: 3.9955679097413346\n",
      "two-layer Loss after iteration 41000: 3.9919946778350472\n",
      "two-layer Loss after iteration 42000: 3.9894698513813567\n",
      "two-layer Loss after iteration 43000: 3.985053646217206\n",
      "two-layer Loss after iteration 44000: 3.9818017715345584\n",
      "two-layer Loss after iteration 45000: 3.9793209752675285\n",
      "two-layer Loss after iteration 46000: 3.977306079334877\n",
      "two-layer Loss after iteration 47000: 3.9748698950230916\n",
      "two-layer Loss after iteration 48000: 3.9738115587491376\n",
      "two-layer Loss after iteration 49000: 3.9729846552096215\n",
      "two-layer Loss after iteration 50000: 3.9723185277847763\n",
      "two-layer Loss after iteration 51000: 3.971774229057641\n",
      "two-layer Loss after iteration 52000: 3.9713231506417905\n",
      "two-layer Loss after iteration 53000: 3.97093550866198\n",
      "9.761028380375312e-05 3.9713231506417905 3.97093550866198\n",
      "two-layer Loss after iteration 0: 1647.721222025801\n",
      "two-layer Loss after iteration 1000: 23.7850539095627\n",
      "two-layer Loss after iteration 2000: 9.642833870318569\n",
      "two-layer Loss after iteration 3000: 7.197808085525734\n",
      "two-layer Loss after iteration 4000: 6.346490763163574\n",
      "two-layer Loss after iteration 5000: 5.554328115429041\n",
      "two-layer Loss after iteration 6000: 5.163937359786537\n",
      "two-layer Loss after iteration 7000: 4.949895742711508\n",
      "two-layer Loss after iteration 8000: 4.80185252159899\n",
      "two-layer Loss after iteration 9000: 4.567046774441709\n",
      "two-layer Loss after iteration 10000: 4.456673508159124\n",
      "two-layer Loss after iteration 11000: 4.375432133961814\n",
      "two-layer Loss after iteration 12000: 4.314802667661686\n",
      "two-layer Loss after iteration 13000: 4.265134282991628\n",
      "two-layer Loss after iteration 14000: 4.222945078444407\n",
      "two-layer Loss after iteration 15000: 4.1871903351888555\n",
      "two-layer Loss after iteration 16000: 4.156188820173968\n",
      "two-layer Loss after iteration 17000: 4.1270706174450655\n",
      "two-layer Loss after iteration 18000: 4.099760429954929\n",
      "two-layer Loss after iteration 19000: 4.076196424157974\n",
      "two-layer Loss after iteration 20000: 4.040038032312806\n",
      "two-layer Loss after iteration 21000: 4.005120388622602\n",
      "two-layer Loss after iteration 22000: 3.959378835432325\n",
      "two-layer Loss after iteration 23000: 3.936842916962301\n",
      "two-layer Loss after iteration 24000: 3.916509290794684\n",
      "two-layer Loss after iteration 25000: 3.8987566292194162\n",
      "two-layer Loss after iteration 26000: 3.8841643482894535\n",
      "two-layer Loss after iteration 27000: 3.871789815678454\n",
      "two-layer Loss after iteration 28000: 3.8611782275618305\n",
      "two-layer Loss after iteration 29000: 3.852028445262535\n",
      "two-layer Loss after iteration 30000: 3.8441106808941146\n",
      "two-layer Loss after iteration 31000: 3.8334411827214163\n",
      "two-layer Loss after iteration 32000: 3.806102831895217\n",
      "two-layer Loss after iteration 33000: 3.791573944843581\n",
      "two-layer Loss after iteration 34000: 3.781803917958992\n",
      "two-layer Loss after iteration 35000: 3.7737959517536805\n",
      "two-layer Loss after iteration 36000: 3.7656810991869567\n",
      "two-layer Loss after iteration 37000: 3.7591107074955445\n",
      "two-layer Loss after iteration 38000: 3.7535751602426095\n",
      "two-layer Loss after iteration 39000: 3.746720355791536\n",
      "two-layer Loss after iteration 40000: 3.741571993705326\n",
      "two-layer Loss after iteration 41000: 3.737202462330485\n",
      "two-layer Loss after iteration 42000: 3.7334565067109313\n",
      "two-layer Loss after iteration 43000: 3.7302146695827094\n",
      "two-layer Loss after iteration 44000: 3.727383493279274\n",
      "two-layer Loss after iteration 45000: 3.724889323874765\n",
      "two-layer Loss after iteration 46000: 3.722673730456502\n",
      "two-layer Loss after iteration 47000: 3.7206900603739834\n",
      "two-layer Loss after iteration 48000: 3.7189008325687074\n",
      "two-layer Loss after iteration 49000: 3.717275754915371\n",
      "two-layer Loss after iteration 50000: 3.715790207542285\n",
      "two-layer Loss after iteration 51000: 3.7144240751598945\n",
      "two-layer Loss after iteration 52000: 3.71316084188245\n",
      "two-layer Loss after iteration 53000: 3.7119868845394994\n",
      "two-layer Loss after iteration 54000: 3.7108909170366142\n",
      "two-layer Loss after iteration 55000: 3.709863550476227\n",
      "two-layer Loss after iteration 56000: 3.7088969426566463\n",
      "two-layer Loss after iteration 57000: 3.707984517105928\n",
      "two-layer Loss after iteration 58000: 3.7071207366200514\n",
      "two-layer Loss after iteration 59000: 3.7063027563586712\n",
      "two-layer Loss after iteration 60000: 3.7029867136610854\n",
      "two-layer Loss after iteration 61000: 3.6998433501203576\n",
      "two-layer Loss after iteration 62000: 3.6969648176018866\n",
      "two-layer Loss after iteration 63000: 3.6943139933304394\n",
      "two-layer Loss after iteration 64000: 3.6918664892948367\n",
      "two-layer Loss after iteration 65000: 3.6896019721594575\n",
      "two-layer Loss after iteration 66000: 3.6875030729129312\n",
      "two-layer Loss after iteration 67000: 3.6855547577112238\n",
      "two-layer Loss after iteration 68000: 3.6837438719865694\n",
      "two-layer Loss after iteration 69000: 3.6820587945519483\n",
      "two-layer Loss after iteration 70000: 3.6804891723282354\n",
      "two-layer Loss after iteration 71000: 3.679025715690973\n",
      "two-layer Loss after iteration 72000: 3.677660039452038\n",
      "two-layer Loss after iteration 73000: 3.676384538115433\n",
      "two-layer Loss after iteration 74000: 3.675192286830709\n",
      "two-layer Loss after iteration 75000: 3.674076961612236\n",
      "two-layer Loss after iteration 76000: 3.673032774031919\n",
      "two-layer Loss after iteration 77000: 3.6720544168353637\n",
      "two-layer Loss after iteration 78000: 3.6711370178649125\n",
      "two-layer Loss after iteration 79000: 3.6702761003688154\n",
      "two-layer Loss after iteration 80000: 3.6694675482907395\n",
      "two-layer Loss after iteration 81000: 3.668707575512163\n",
      "two-layer Loss after iteration 82000: 3.667992698295865\n",
      "two-layer Loss after iteration 83000: 3.6673197103789668\n",
      "two-layer Loss after iteration 84000: 3.6666856603074858\n",
      "two-layer Loss after iteration 85000: 3.6660878307075024\n",
      "two-layer Loss after iteration 86000: 3.66552371926093\n",
      "two-layer Loss after iteration 87000: 3.664991021205734\n",
      "two-layer Loss after iteration 88000: 3.664487613216765\n",
      "two-layer Loss after iteration 89000: 3.6640115385489946\n",
      "two-layer Loss after iteration 90000: 3.6635609933431574\n",
      "two-layer Loss after iteration 91000: 3.663134314006572\n",
      "two-layer Loss after iteration 92000: 3.6627299655912546\n",
      "two-layer Loss after iteration 93000: 3.6623465310985592\n",
      "two-layer Loss after iteration 94000: 3.6619827016446536\n",
      "9.934326280056806e-05 3.6623465310985592 3.6619827016446536\n",
      "two-layer Loss after iteration 0: 1551.6630479431253\n",
      "two-layer Loss after iteration 1000: 24.19784729350416\n",
      "two-layer Loss after iteration 2000: 10.100732005841245\n",
      "two-layer Loss after iteration 3000: 7.681096116007132\n",
      "two-layer Loss after iteration 4000: 6.557912332917019\n",
      "two-layer Loss after iteration 5000: 5.610276198559903\n",
      "two-layer Loss after iteration 6000: 5.320051044597096\n",
      "two-layer Loss after iteration 7000: 5.14337344238735\n",
      "two-layer Loss after iteration 8000: 4.981346803777643\n",
      "two-layer Loss after iteration 9000: 4.846480072850366\n",
      "two-layer Loss after iteration 10000: 4.717417680314944\n",
      "two-layer Loss after iteration 11000: 4.598918552778012\n",
      "two-layer Loss after iteration 12000: 4.495959649254216\n",
      "two-layer Loss after iteration 13000: 4.398294997931447\n",
      "two-layer Loss after iteration 14000: 4.328376675126468\n",
      "two-layer Loss after iteration 15000: 4.26656939368709\n",
      "two-layer Loss after iteration 16000: 4.1787042133904215\n",
      "two-layer Loss after iteration 17000: 4.1211443898597\n",
      "two-layer Loss after iteration 18000: 4.086984951549232\n",
      "two-layer Loss after iteration 19000: 4.029746291555983\n",
      "two-layer Loss after iteration 20000: 3.9939485054540627\n",
      "two-layer Loss after iteration 21000: 3.9668106248715818\n",
      "two-layer Loss after iteration 22000: 3.9503018956183475\n",
      "two-layer Loss after iteration 23000: 3.9419368062070075\n",
      "two-layer Loss after iteration 24000: 3.935338100123776\n",
      "two-layer Loss after iteration 25000: 3.929799550989794\n",
      "two-layer Loss after iteration 26000: 3.925063879959421\n",
      "two-layer Loss after iteration 27000: 3.9211587200042546\n",
      "two-layer Loss after iteration 28000: 3.904930897282303\n",
      "two-layer Loss after iteration 29000: 3.8993857050352068\n",
      "two-layer Loss after iteration 30000: 3.8955323371470274\n",
      "two-layer Loss after iteration 31000: 3.8838599216634315\n",
      "two-layer Loss after iteration 32000: 3.8756256383909236\n",
      "two-layer Loss after iteration 33000: 3.869938281350561\n",
      "two-layer Loss after iteration 34000: 3.864980788036306\n",
      "two-layer Loss after iteration 35000: 3.8606446195625135\n",
      "two-layer Loss after iteration 36000: 3.84674782890905\n",
      "two-layer Loss after iteration 37000: 3.840520885550481\n",
      "two-layer Loss after iteration 38000: 3.83562078775837\n",
      "two-layer Loss after iteration 39000: 3.831451306268653\n",
      "two-layer Loss after iteration 40000: 3.8278139648597467\n",
      "two-layer Loss after iteration 41000: 3.824588143835219\n",
      "two-layer Loss after iteration 42000: 3.8216997861257744\n",
      "two-layer Loss after iteration 43000: 3.8190726816707987\n",
      "two-layer Loss after iteration 44000: 3.8166936872264783\n",
      "two-layer Loss after iteration 45000: 3.8145118244356704\n",
      "two-layer Loss after iteration 46000: 3.804939064861424\n",
      "two-layer Loss after iteration 47000: 3.795723311803961\n",
      "two-layer Loss after iteration 48000: 3.779146494617289\n",
      "two-layer Loss after iteration 49000: 3.755575637373165\n",
      "two-layer Loss after iteration 50000: 3.738585510237355\n",
      "two-layer Loss after iteration 51000: 3.442313113881134\n",
      "two-layer Loss after iteration 52000: 3.243774691470827\n",
      "two-layer Loss after iteration 53000: 3.236840612002721\n",
      "two-layer Loss after iteration 54000: 3.232393250308918\n",
      "two-layer Loss after iteration 55000: 3.2297536037345678\n",
      "two-layer Loss after iteration 56000: 3.227488873701584\n",
      "two-layer Loss after iteration 57000: 3.2254120375640167\n",
      "two-layer Loss after iteration 58000: 3.223468113461307\n",
      "two-layer Loss after iteration 59000: 3.221624662781168\n",
      "two-layer Loss after iteration 60000: 3.219886665130855\n",
      "two-layer Loss after iteration 61000: 3.218233233084243\n",
      "two-layer Loss after iteration 62000: 3.216643150641229\n",
      "two-layer Loss after iteration 63000: 3.2151340174447056\n",
      "two-layer Loss after iteration 64000: 3.213675465046849\n",
      "two-layer Loss after iteration 65000: 3.2122781940533294\n",
      "two-layer Loss after iteration 66000: 3.210924728672585\n",
      "two-layer Loss after iteration 67000: 3.2096345969699964\n",
      "two-layer Loss after iteration 68000: 3.208371633084407\n",
      "two-layer Loss after iteration 69000: 3.207157955530433\n",
      "two-layer Loss after iteration 70000: 3.2059617135580183\n",
      "two-layer Loss after iteration 71000: 3.2048182426154903\n",
      "two-layer Loss after iteration 72000: 3.2036948464757313\n",
      "two-layer Loss after iteration 73000: 3.202604679875293\n",
      "two-layer Loss after iteration 74000: 3.2015269648521847\n",
      "two-layer Loss after iteration 75000: 3.200498815682494\n",
      "two-layer Loss after iteration 76000: 3.1994720558395\n",
      "two-layer Loss after iteration 77000: 3.1984901807769646\n",
      "two-layer Loss after iteration 78000: 3.1975240737335207\n",
      "two-layer Loss after iteration 79000: 3.1965743558324076\n",
      "two-layer Loss after iteration 80000: 3.1956516043301755\n",
      "two-layer Loss after iteration 81000: 3.1947534557245953\n",
      "two-layer Loss after iteration 82000: 3.1938738776693656\n",
      "two-layer Loss after iteration 83000: 3.193024670252284\n",
      "two-layer Loss after iteration 84000: 3.1798834778220084\n",
      "two-layer Loss after iteration 85000: 3.177987512683578\n",
      "two-layer Loss after iteration 86000: 3.176555003408996\n",
      "two-layer Loss after iteration 87000: 3.1752455736396556\n",
      "two-layer Loss after iteration 88000: 3.174002709528524\n",
      "two-layer Loss after iteration 89000: 3.1728206250738227\n",
      "two-layer Loss after iteration 90000: 3.1716922758542583\n",
      "two-layer Loss after iteration 91000: 3.170605415868967\n",
      "two-layer Loss after iteration 92000: 3.169567816123232\n",
      "two-layer Loss after iteration 93000: 3.1685641943431415\n",
      "two-layer Loss after iteration 94000: 3.167583232389851\n",
      "two-layer Loss after iteration 95000: 3.166631849104972\n",
      "two-layer Loss after iteration 96000: 3.165691515779653\n",
      "two-layer Loss after iteration 97000: 3.164788887236102\n",
      "two-layer Loss after iteration 98000: 3.163891957343749\n",
      "two-layer Loss after iteration 99000: 3.163023130423085\n",
      "two-layer Loss after iteration 100000: 3.1621634472414026\n",
      "two-layer Loss after iteration 101000: 3.1613406998161815\n",
      "two-layer Loss after iteration 102000: 3.1605189839091143\n",
      "two-layer Loss after iteration 103000: 3.156723440322874\n",
      "two-layer Loss after iteration 104000: 3.1526586536406174\n",
      "two-layer Loss after iteration 105000: 3.1508529742110296\n",
      "two-layer Loss after iteration 106000: 3.1493432473593463\n",
      "two-layer Loss after iteration 107000: 3.1479597961982693\n",
      "two-layer Loss after iteration 108000: 3.14665114487745\n",
      "two-layer Loss after iteration 109000: 3.1453993256506974\n",
      "two-layer Loss after iteration 110000: 3.1441948346216972\n",
      "two-layer Loss after iteration 111000: 3.143033531804552\n",
      "two-layer Loss after iteration 112000: 3.141911597821054\n",
      "two-layer Loss after iteration 113000: 3.1408243511928697\n",
      "two-layer Loss after iteration 114000: 3.139767679010942\n",
      "two-layer Loss after iteration 115000: 3.1387391310349764\n",
      "two-layer Loss after iteration 116000: 3.1377366919700314\n",
      "two-layer Loss after iteration 117000: 3.1367588177060814\n",
      "two-layer Loss after iteration 118000: 3.135807154497586\n",
      "two-layer Loss after iteration 119000: 3.1348791938699563\n",
      "two-layer Loss after iteration 120000: 3.1339759324785583\n",
      "two-layer Loss after iteration 121000: 3.1330936904015045\n",
      "two-layer Loss after iteration 122000: 3.1322327434912443\n",
      "two-layer Loss after iteration 123000: 3.1313941426356258\n",
      "two-layer Loss after iteration 124000: 3.1305781861000215\n",
      "two-layer Loss after iteration 125000: 3.1297784838741984\n",
      "two-layer Loss after iteration 126000: 3.1289939872074926\n",
      "two-layer Loss after iteration 127000: 3.128232966612761\n",
      "two-layer Loss after iteration 128000: 3.127487703153902\n",
      "two-layer Loss after iteration 129000: 3.1267657240765727\n",
      "two-layer Loss after iteration 130000: 3.126072577111284\n",
      "two-layer Loss after iteration 131000: 3.125378750409713\n",
      "two-layer Loss after iteration 132000: 3.124714555787368\n",
      "two-layer Loss after iteration 133000: 3.1240883020197474\n",
      "two-layer Loss after iteration 134000: 3.123467841939443\n",
      "two-layer Loss after iteration 135000: 3.122860180686613\n",
      "two-layer Loss after iteration 136000: 3.12228139912199\n",
      "two-layer Loss after iteration 137000: 3.121726107558912\n",
      "two-layer Loss after iteration 138000: 3.1202235242945284\n",
      "two-layer Loss after iteration 139000: 3.1193713659489783\n",
      "two-layer Loss after iteration 140000: 3.118597933979736\n",
      "two-layer Loss after iteration 141000: 3.117879149185172\n",
      "two-layer Loss after iteration 142000: 3.1171904847726166\n",
      "two-layer Loss after iteration 143000: 3.1165489946321405\n",
      "two-layer Loss after iteration 144000: 3.115929868824421\n",
      "two-layer Loss after iteration 145000: 3.115334634971125\n",
      "two-layer Loss after iteration 146000: 3.1147862056272158\n",
      "two-layer Loss after iteration 147000: 3.1142161403064437\n",
      "two-layer Loss after iteration 148000: 3.113679383966528\n",
      "two-layer Loss after iteration 149000: 3.1131653344822845\n",
      "two-layer Loss after iteration 150000: 3.112669969031536\n",
      "two-layer Loss after iteration 151000: 3.1121977228301927\n",
      "two-layer Loss after iteration 152000: 3.1117312546578515\n",
      "two-layer Loss after iteration 153000: 3.111295871541304\n",
      "two-layer Loss after iteration 154000: 3.1108683226686678\n",
      "two-layer Loss after iteration 155000: 3.1104457053727845\n",
      "two-layer Loss after iteration 156000: 3.110062512466112\n",
      "two-layer Loss after iteration 157000: 3.1096748578472204\n",
      "two-layer Loss after iteration 158000: 3.1092802370066903\n",
      "two-layer Loss after iteration 159000: 3.1089343328315753\n",
      "two-layer Loss after iteration 160000: 3.108602271175712\n",
      "two-layer Loss after iteration 161000: 3.1050185693903987\n",
      "two-layer Loss after iteration 162000: 3.103637921603238\n",
      "two-layer Loss after iteration 163000: 3.10271848513219\n",
      "two-layer Loss after iteration 164000: 3.1018983686407453\n",
      "two-layer Loss after iteration 165000: 3.1011298154861118\n",
      "two-layer Loss after iteration 166000: 3.100399015522942\n",
      "two-layer Loss after iteration 167000: 3.099698703325713\n",
      "two-layer Loss after iteration 168000: 3.0990290146832304\n",
      "two-layer Loss after iteration 169000: 3.098375410478206\n",
      "two-layer Loss after iteration 170000: 3.0977478784238155\n",
      "two-layer Loss after iteration 171000: 3.0971392011006684\n",
      "two-layer Loss after iteration 172000: 3.0965505735326677\n",
      "two-layer Loss after iteration 173000: 3.0959143947124357\n",
      "two-layer Loss after iteration 174000: 3.095310752683296\n",
      "two-layer Loss after iteration 175000: 3.094728706085278\n",
      "two-layer Loss after iteration 176000: 3.0941704097447005\n",
      "two-layer Loss after iteration 177000: 3.0936225663515926\n",
      "two-layer Loss after iteration 178000: 3.09308923809782\n",
      "two-layer Loss after iteration 179000: 3.0925753598082544\n",
      "two-layer Loss after iteration 180000: 3.0920731496809215\n",
      "two-layer Loss after iteration 181000: 3.0915816261026583\n",
      "two-layer Loss after iteration 182000: 3.091101503647568\n",
      "two-layer Loss after iteration 183000: 3.0906348412284763\n",
      "two-layer Loss after iteration 184000: 3.090183969667403\n",
      "two-layer Loss after iteration 185000: 3.089753139558459\n",
      "two-layer Loss after iteration 186000: 3.0893307075697565\n",
      "two-layer Loss after iteration 187000: 3.088903799464298\n",
      "two-layer Loss after iteration 188000: 3.088508937462367\n",
      "two-layer Loss after iteration 189000: 3.088132287012568\n",
      "two-layer Loss after iteration 190000: 3.087742204296209\n",
      "two-layer Loss after iteration 191000: 3.087396075046688\n",
      "two-layer Loss after iteration 192000: 3.0870394296803765\n",
      "two-layer Loss after iteration 193000: 3.0867069604749866\n",
      "two-layer Loss after iteration 194000: 3.0863807595317656\n",
      "two-layer Loss after iteration 195000: 3.086057166952873\n",
      "two-layer Loss after iteration 196000: 3.085765480887662\n",
      "9.451738883344601e-05 3.086057166952873 3.085765480887662\n",
      "two-layer Loss after iteration 0: 1783.1866977683867\n",
      "two-layer Loss after iteration 1000: 25.61387181319538\n",
      "two-layer Loss after iteration 2000: 9.980440998533531\n",
      "two-layer Loss after iteration 3000: 8.111686574733792\n",
      "two-layer Loss after iteration 4000: 7.421663476343608\n",
      "two-layer Loss after iteration 5000: 6.986928880812528\n",
      "two-layer Loss after iteration 6000: 6.636202425742541\n",
      "two-layer Loss after iteration 7000: 6.399390338914728\n",
      "two-layer Loss after iteration 8000: 6.178312998633187\n",
      "two-layer Loss after iteration 9000: 5.698743222936593\n",
      "two-layer Loss after iteration 10000: 4.853864163858824\n",
      "two-layer Loss after iteration 11000: 4.776729024264498\n",
      "two-layer Loss after iteration 12000: 4.735786621984\n",
      "two-layer Loss after iteration 13000: 4.709074080339277\n",
      "two-layer Loss after iteration 14000: 4.6894526764274085\n",
      "two-layer Loss after iteration 15000: 4.673508509306413\n",
      "two-layer Loss after iteration 16000: 4.658167219586393\n",
      "two-layer Loss after iteration 17000: 4.645523229668046\n",
      "two-layer Loss after iteration 18000: 4.634475806995986\n",
      "two-layer Loss after iteration 19000: 4.624464923280211\n",
      "two-layer Loss after iteration 20000: 4.615249679204058\n",
      "two-layer Loss after iteration 21000: 4.6067414486892\n",
      "two-layer Loss after iteration 22000: 4.596320576007699\n",
      "two-layer Loss after iteration 23000: 4.580747124038532\n",
      "two-layer Loss after iteration 24000: 4.5639626007925775\n",
      "two-layer Loss after iteration 25000: 4.552657951586358\n",
      "two-layer Loss after iteration 26000: 4.54527155995331\n",
      "two-layer Loss after iteration 27000: 4.53957219135641\n",
      "two-layer Loss after iteration 28000: 4.534628617869926\n",
      "two-layer Loss after iteration 29000: 4.530699529798828\n",
      "two-layer Loss after iteration 30000: 4.5275020269068715\n",
      "two-layer Loss after iteration 31000: 4.524821205231847\n",
      "two-layer Loss after iteration 32000: 4.5225057692309685\n",
      "two-layer Loss after iteration 33000: 4.5204774732753386\n",
      "two-layer Loss after iteration 34000: 4.518659004685888\n",
      "two-layer Loss after iteration 35000: 4.517025893370961\n",
      "two-layer Loss after iteration 36000: 4.250085061683928\n",
      "two-layer Loss after iteration 37000: 4.068722249706338\n",
      "two-layer Loss after iteration 38000: 4.0226113152512735\n",
      "two-layer Loss after iteration 39000: 4.009193373142039\n",
      "two-layer Loss after iteration 40000: 4.003986059498946\n",
      "two-layer Loss after iteration 41000: 4.0009985683472395\n",
      "two-layer Loss after iteration 42000: 3.9988458398398334\n",
      "two-layer Loss after iteration 43000: 3.9971387905201006\n",
      "two-layer Loss after iteration 44000: 3.995719407209408\n",
      "two-layer Loss after iteration 45000: 3.994516708025933\n",
      "two-layer Loss after iteration 46000: 3.9934633501237213\n",
      "two-layer Loss after iteration 47000: 3.9925395039179365\n",
      "two-layer Loss after iteration 48000: 3.99170302505028\n",
      "two-layer Loss after iteration 49000: 3.9909504976702594\n",
      "two-layer Loss after iteration 50000: 3.990271389776549\n",
      "two-layer Loss after iteration 51000: 3.98964420962135\n",
      "two-layer Loss after iteration 52000: 3.9890793291192312\n",
      "two-layer Loss after iteration 53000: 3.988550640932216\n",
      "two-layer Loss after iteration 54000: 3.988105865655454\n",
      "two-layer Loss after iteration 55000: 3.9877045702772067\n",
      "two-layer Loss after iteration 56000: 3.9873337400913584\n",
      "9.299339489998666e-05 3.9877045702772067 3.9873337400913584\n",
      "two-layer Loss after iteration 0: 1717.182621558765\n",
      "two-layer Loss after iteration 1000: 24.929731006222976\n",
      "two-layer Loss after iteration 2000: 10.220587684704858\n",
      "two-layer Loss after iteration 3000: 8.07940784366603\n",
      "two-layer Loss after iteration 4000: 6.93423020200815\n",
      "two-layer Loss after iteration 5000: 6.305455473654027\n",
      "two-layer Loss after iteration 6000: 5.973069336000764\n",
      "two-layer Loss after iteration 7000: 5.423259853410578\n",
      "two-layer Loss after iteration 8000: 5.201532601916774\n",
      "two-layer Loss after iteration 9000: 5.096945457760788\n",
      "two-layer Loss after iteration 10000: 4.985623236313441\n",
      "two-layer Loss after iteration 11000: 4.891069395889919\n",
      "two-layer Loss after iteration 12000: 4.814041728642423\n",
      "two-layer Loss after iteration 13000: 4.751552694551649\n",
      "two-layer Loss after iteration 14000: 4.701800129439636\n",
      "two-layer Loss after iteration 15000: 4.663909184236308\n",
      "two-layer Loss after iteration 16000: 4.63657496656763\n",
      "two-layer Loss after iteration 17000: 4.615074893407503\n",
      "two-layer Loss after iteration 18000: 4.592218605526588\n",
      "two-layer Loss after iteration 19000: 4.577167637811946\n",
      "two-layer Loss after iteration 20000: 4.557891811242249\n",
      "two-layer Loss after iteration 21000: 4.547952285269615\n",
      "two-layer Loss after iteration 22000: 4.524542994313121\n",
      "two-layer Loss after iteration 23000: 4.514201079696088\n",
      "two-layer Loss after iteration 24000: 4.507208977168244\n",
      "two-layer Loss after iteration 25000: 4.501865248231133\n",
      "two-layer Loss after iteration 26000: 4.497636236734843\n",
      "two-layer Loss after iteration 27000: 4.494199132022209\n",
      "two-layer Loss after iteration 28000: 4.491376372009274\n",
      "two-layer Loss after iteration 29000: 4.4890468008810105\n",
      "two-layer Loss after iteration 30000: 4.487118728533835\n",
      "two-layer Loss after iteration 31000: 4.485477912414596\n",
      "two-layer Loss after iteration 32000: 4.484155361518788\n",
      "two-layer Loss after iteration 33000: 4.48306236267315\n",
      "two-layer Loss after iteration 34000: 4.482152470447638\n",
      "two-layer Loss after iteration 35000: 4.4813912015259465\n",
      "two-layer Loss after iteration 36000: 4.480751497359874\n",
      "two-layer Loss after iteration 37000: 4.480211727963759\n",
      "two-layer Loss after iteration 38000: 4.479754435845599\n",
      "two-layer Loss after iteration 39000: 4.479365450180403\n",
      "8.683191696493143e-05 4.479754435845599 4.479365450180403\n",
      "two-layer Loss after iteration 0: 1579.786517800788\n",
      "two-layer Loss after iteration 1000: 24.416236349346477\n",
      "two-layer Loss after iteration 2000: 9.838464405647612\n",
      "two-layer Loss after iteration 3000: 7.440243710716119\n",
      "two-layer Loss after iteration 4000: 6.055942264666278\n",
      "two-layer Loss after iteration 5000: 4.850955992907735\n",
      "two-layer Loss after iteration 6000: 4.448939526922531\n",
      "two-layer Loss after iteration 7000: 4.216257777363376\n",
      "two-layer Loss after iteration 8000: 4.009641907496399\n",
      "two-layer Loss after iteration 9000: 3.8547487110731367\n",
      "two-layer Loss after iteration 10000: 3.720418162101721\n",
      "two-layer Loss after iteration 11000: 3.6198922018825295\n",
      "two-layer Loss after iteration 12000: 3.5229331379616666\n",
      "two-layer Loss after iteration 13000: 3.4435467725651936\n",
      "two-layer Loss after iteration 14000: 3.378438010641888\n",
      "two-layer Loss after iteration 15000: 3.3257283103744966\n",
      "two-layer Loss after iteration 16000: 3.285150044479878\n",
      "two-layer Loss after iteration 17000: 3.2448199485237166\n",
      "two-layer Loss after iteration 18000: 3.2072033413470544\n",
      "two-layer Loss after iteration 19000: 3.171850774483005\n",
      "two-layer Loss after iteration 20000: 3.1432339168961487\n",
      "two-layer Loss after iteration 21000: 3.124718732699069\n",
      "two-layer Loss after iteration 22000: 3.1113433613623234\n",
      "two-layer Loss after iteration 23000: 3.097930495979715\n",
      "two-layer Loss after iteration 24000: 3.0879763424228632\n",
      "two-layer Loss after iteration 25000: 3.077871210362154\n",
      "two-layer Loss after iteration 26000: 3.0688320871292922\n",
      "two-layer Loss after iteration 27000: 3.0633863709156053\n",
      "two-layer Loss after iteration 28000: 3.0557131631727126\n",
      "two-layer Loss after iteration 29000: 3.0495278622149034\n",
      "two-layer Loss after iteration 30000: 3.044423435621953\n",
      "two-layer Loss after iteration 31000: 3.0411683754698253\n",
      "two-layer Loss after iteration 32000: 3.037469445274926\n",
      "two-layer Loss after iteration 33000: 3.035806072616238\n",
      "two-layer Loss after iteration 34000: 3.0350310209691043\n",
      "two-layer Loss after iteration 35000: 3.034404779529416\n",
      "two-layer Loss after iteration 36000: 3.0338632854982275\n",
      "two-layer Loss after iteration 37000: 3.0333945389613666\n",
      "two-layer Loss after iteration 38000: 3.0329709499879756\n",
      "two-layer Loss after iteration 39000: 3.0325964709534925\n",
      "two-layer Loss after iteration 40000: 3.0322587104871057\n",
      "two-layer Loss after iteration 41000: 3.0319564252377442\n",
      "9.968979504157469e-05 3.0322587104871057 3.0319564252377442\n",
      "two-layer Loss after iteration 0: 1345.9807322791632\n",
      "two-layer Loss after iteration 1000: 24.376528436958342\n",
      "two-layer Loss after iteration 2000: 9.95038496900221\n",
      "two-layer Loss after iteration 3000: 7.842023890456666\n",
      "two-layer Loss after iteration 4000: 6.2507171501144425\n",
      "two-layer Loss after iteration 5000: 5.19138384852001\n",
      "two-layer Loss after iteration 6000: 4.783402272934958\n",
      "two-layer Loss after iteration 7000: 4.582142117427959\n",
      "two-layer Loss after iteration 8000: 4.439687870944424\n",
      "two-layer Loss after iteration 9000: 4.364152016381322\n",
      "two-layer Loss after iteration 10000: 4.2997210139804185\n",
      "two-layer Loss after iteration 11000: 4.2545985139218585\n",
      "two-layer Loss after iteration 12000: 4.193881758090256\n",
      "two-layer Loss after iteration 13000: 4.1642648791974795\n",
      "two-layer Loss after iteration 14000: 4.127074261494132\n",
      "two-layer Loss after iteration 15000: 4.102688882093831\n",
      "two-layer Loss after iteration 16000: 4.08376683187264\n",
      "two-layer Loss after iteration 17000: 4.069525157230629\n",
      "two-layer Loss after iteration 18000: 4.055329412043934\n",
      "two-layer Loss after iteration 19000: 4.0449142537289315\n",
      "two-layer Loss after iteration 20000: 4.0370173545801595\n",
      "two-layer Loss after iteration 21000: 4.030852360311389\n",
      "two-layer Loss after iteration 22000: 4.025925155432435\n",
      "two-layer Loss after iteration 23000: 4.0218725358957395\n",
      "two-layer Loss after iteration 24000: 4.0184272932112695\n",
      "two-layer Loss after iteration 25000: 4.015448247417922\n",
      "two-layer Loss after iteration 26000: 4.012804977535151\n",
      "two-layer Loss after iteration 27000: 4.010454462730983\n",
      "two-layer Loss after iteration 28000: 4.008349294168713\n",
      "two-layer Loss after iteration 29000: 4.006430978239148\n",
      "two-layer Loss after iteration 30000: 3.999253083341667\n",
      "two-layer Loss after iteration 31000: 3.9957868125229603\n",
      "two-layer Loss after iteration 32000: 3.9930907625807484\n",
      "two-layer Loss after iteration 33000: 3.9906808376157246\n",
      "two-layer Loss after iteration 34000: 3.9885251596638973\n",
      "two-layer Loss after iteration 35000: 3.986575409512278\n",
      "two-layer Loss after iteration 36000: 3.984816152477545\n",
      "two-layer Loss after iteration 37000: 3.9832111802834262\n",
      "two-layer Loss after iteration 38000: 3.9817464368745927\n",
      "two-layer Loss after iteration 39000: 3.9804173624835637\n",
      "two-layer Loss after iteration 40000: 3.979202949070954\n",
      "two-layer Loss after iteration 41000: 3.9780869529536944\n",
      "two-layer Loss after iteration 42000: 3.973337692925009\n",
      "two-layer Loss after iteration 43000: 3.9705153969095237\n",
      "two-layer Loss after iteration 44000: 3.969170930700469\n",
      "two-layer Loss after iteration 45000: 3.9680413983197145\n",
      "two-layer Loss after iteration 46000: 3.967012675320049\n",
      "two-layer Loss after iteration 47000: 3.9660626604416365\n",
      "two-layer Loss after iteration 48000: 3.965197823818496\n",
      "two-layer Loss after iteration 49000: 3.964386905110956\n",
      "two-layer Loss after iteration 50000: 3.963651548551724\n",
      "two-layer Loss after iteration 51000: 3.962971331246458\n",
      "two-layer Loss after iteration 52000: 3.962354534278596\n",
      "two-layer Loss after iteration 53000: 3.9617704876152575\n",
      "two-layer Loss after iteration 54000: 3.961239329542744\n",
      "two-layer Loss after iteration 55000: 3.960751982574962\n",
      "two-layer Loss after iteration 56000: 3.96029985742952\n",
      "two-layer Loss after iteration 57000: 3.959881690794083\n",
      "two-layer Loss after iteration 58000: 3.959494433512119\n",
      "9.77951646546378e-05 3.959881690794083 3.959494433512119\n",
      "two-layer Loss after iteration 0: 1313.0797289574189\n",
      "two-layer Loss after iteration 1000: 23.757562018624213\n",
      "two-layer Loss after iteration 2000: 9.044320004765039\n",
      "two-layer Loss after iteration 3000: 6.948186916668639\n",
      "two-layer Loss after iteration 4000: 6.0687508542452635\n",
      "two-layer Loss after iteration 5000: 5.543960145005127\n",
      "two-layer Loss after iteration 6000: 5.154557852729316\n",
      "two-layer Loss after iteration 7000: 4.972408578318739\n",
      "two-layer Loss after iteration 8000: 4.81196212473224\n",
      "two-layer Loss after iteration 9000: 4.690847589429563\n",
      "two-layer Loss after iteration 10000: 4.579516121518286\n",
      "two-layer Loss after iteration 11000: 4.188431289097724\n",
      "two-layer Loss after iteration 12000: 4.023397124481708\n",
      "two-layer Loss after iteration 13000: 3.92288805044844\n",
      "two-layer Loss after iteration 14000: 3.8482636898999014\n",
      "two-layer Loss after iteration 15000: 3.797712649659743\n",
      "two-layer Loss after iteration 16000: 3.760704413761728\n",
      "two-layer Loss after iteration 17000: 3.7315381931880807\n",
      "two-layer Loss after iteration 18000: 3.703702303652246\n",
      "two-layer Loss after iteration 19000: 3.6818271493597767\n",
      "two-layer Loss after iteration 20000: 3.663215352668925\n",
      "two-layer Loss after iteration 21000: 3.6471015389576342\n",
      "two-layer Loss after iteration 22000: 3.633009629166438\n",
      "two-layer Loss after iteration 23000: 3.6206031666056018\n",
      "two-layer Loss after iteration 24000: 3.6085858035576375\n",
      "two-layer Loss after iteration 25000: 3.597954089200368\n",
      "two-layer Loss after iteration 26000: 3.5885400029989563\n",
      "two-layer Loss after iteration 27000: 3.5802015487075383\n",
      "two-layer Loss after iteration 28000: 3.572829297570256\n",
      "two-layer Loss after iteration 29000: 3.5663266872836568\n",
      "two-layer Loss after iteration 30000: 3.5606272312219724\n",
      "two-layer Loss after iteration 31000: 3.524577892735044\n",
      "two-layer Loss after iteration 32000: 3.513565284445861\n",
      "two-layer Loss after iteration 33000: 3.5063115334313455\n",
      "two-layer Loss after iteration 34000: 3.4990855768159252\n",
      "two-layer Loss after iteration 35000: 3.4933920743386175\n",
      "two-layer Loss after iteration 36000: 3.488663833707044\n",
      "two-layer Loss after iteration 37000: 3.4846919391713107\n",
      "two-layer Loss after iteration 38000: 3.4813136577948316\n",
      "two-layer Loss after iteration 39000: 3.478499501769453\n",
      "two-layer Loss after iteration 40000: 3.4760898481856066\n",
      "two-layer Loss after iteration 41000: 3.4740342419619874\n",
      "two-layer Loss after iteration 42000: 3.4722572340657587\n",
      "two-layer Loss after iteration 43000: 3.470705530616194\n",
      "two-layer Loss after iteration 44000: 3.469372162387659\n",
      "two-layer Loss after iteration 45000: 3.4681890469011893\n",
      "two-layer Loss after iteration 46000: 3.467162155663927\n",
      "two-layer Loss after iteration 47000: 3.4662606335220008\n",
      "two-layer Loss after iteration 48000: 3.4654671510581445\n",
      "two-layer Loss after iteration 49000: 3.4647648031866476\n",
      "two-layer Loss after iteration 50000: 3.4640781817756885\n",
      "two-layer Loss after iteration 51000: 3.4634126976304436\n",
      "two-layer Loss after iteration 52000: 3.462875683402487\n",
      "two-layer Loss after iteration 53000: 3.4624084489602995\n",
      "two-layer Loss after iteration 54000: 3.462014075609131\n",
      "two-layer Loss after iteration 55000: 3.461657850562447\n",
      "two-layer Loss after iteration 56000: 3.4613459166536718\n",
      "9.011113236522493e-05 3.461657850562447 3.4613459166536718\n",
      "two-layer Loss after iteration 0: 1418.1894958006776\n",
      "two-layer Loss after iteration 1000: 24.454960840670424\n",
      "two-layer Loss after iteration 2000: 9.640246496287805\n",
      "two-layer Loss after iteration 3000: 7.306607415506541\n",
      "two-layer Loss after iteration 4000: 6.661285554552847\n",
      "two-layer Loss after iteration 5000: 6.479054503997244\n",
      "two-layer Loss after iteration 6000: 6.361178600042816\n",
      "two-layer Loss after iteration 7000: 5.76858463869513\n",
      "two-layer Loss after iteration 8000: 5.048947285997507\n",
      "two-layer Loss after iteration 9000: 4.364636285249367\n",
      "two-layer Loss after iteration 10000: 4.185917989441069\n",
      "two-layer Loss after iteration 11000: 4.109660077863248\n",
      "two-layer Loss after iteration 12000: 4.083438621757004\n",
      "two-layer Loss after iteration 13000: 4.068349483647704\n",
      "two-layer Loss after iteration 14000: 4.058041690589223\n",
      "two-layer Loss after iteration 15000: 4.050416261882744\n",
      "two-layer Loss after iteration 16000: 4.041440264033966\n",
      "two-layer Loss after iteration 17000: 4.034940306763521\n",
      "two-layer Loss after iteration 18000: 4.029588481821541\n",
      "two-layer Loss after iteration 19000: 4.0251209289671435\n",
      "two-layer Loss after iteration 20000: 4.02130589531108\n",
      "two-layer Loss after iteration 21000: 4.017429238657084\n",
      "two-layer Loss after iteration 22000: 4.011937418327392\n",
      "two-layer Loss after iteration 23000: 4.004694203267825\n",
      "two-layer Loss after iteration 24000: 3.9994759488590765\n",
      "two-layer Loss after iteration 25000: 3.995483339424233\n",
      "two-layer Loss after iteration 26000: 3.9920548944520022\n",
      "two-layer Loss after iteration 27000: 3.9803201887333475\n",
      "two-layer Loss after iteration 28000: 3.9685857070972763\n",
      "two-layer Loss after iteration 29000: 3.957202225641797\n",
      "two-layer Loss after iteration 30000: 3.9475868721991243\n",
      "two-layer Loss after iteration 31000: 3.939190565332792\n",
      "two-layer Loss after iteration 32000: 3.924208423731834\n",
      "two-layer Loss after iteration 33000: 3.918294886694987\n",
      "two-layer Loss after iteration 34000: 3.9119043217502876\n",
      "two-layer Loss after iteration 35000: 3.9030146384014452\n",
      "two-layer Loss after iteration 36000: 3.8972648374410013\n",
      "two-layer Loss after iteration 37000: 3.8933151374240142\n",
      "two-layer Loss after iteration 38000: 3.889324693841347\n",
      "two-layer Loss after iteration 39000: 3.885471059304227\n",
      "two-layer Loss after iteration 40000: 3.882242225882333\n",
      "two-layer Loss after iteration 41000: 3.879338689718071\n",
      "two-layer Loss after iteration 42000: 3.86766750186271\n",
      "two-layer Loss after iteration 43000: 3.7173812141540363\n",
      "two-layer Loss after iteration 44000: 3.697918498998487\n",
      "two-layer Loss after iteration 45000: 3.682121620064107\n",
      "two-layer Loss after iteration 46000: 3.668329528449264\n",
      "two-layer Loss after iteration 47000: 3.6559373658636156\n",
      "two-layer Loss after iteration 48000: 3.6442270287031424\n",
      "two-layer Loss after iteration 49000: 3.6339054125533505\n",
      "two-layer Loss after iteration 50000: 3.6239410100853418\n",
      "two-layer Loss after iteration 51000: 3.6150578929300123\n",
      "two-layer Loss after iteration 52000: 3.606084366039194\n",
      "two-layer Loss after iteration 53000: 3.5978220749297356\n",
      "two-layer Loss after iteration 54000: 3.5897357581342986\n",
      "two-layer Loss after iteration 55000: 3.581888642561535\n",
      "two-layer Loss after iteration 56000: 3.5746659003278007\n",
      "two-layer Loss after iteration 57000: 3.16090705478837\n",
      "two-layer Loss after iteration 58000: 3.06813756346082\n",
      "two-layer Loss after iteration 59000: 3.0403053615411086\n",
      "two-layer Loss after iteration 60000: 3.027358277202817\n",
      "two-layer Loss after iteration 61000: 3.018434126723623\n",
      "two-layer Loss after iteration 62000: 3.010984614906613\n",
      "two-layer Loss after iteration 63000: 3.0043198645856957\n",
      "two-layer Loss after iteration 64000: 2.997912730263822\n",
      "two-layer Loss after iteration 65000: 2.9919234528809566\n",
      "two-layer Loss after iteration 66000: 2.9862884982961817\n",
      "two-layer Loss after iteration 67000: 2.981023371690217\n",
      "two-layer Loss after iteration 68000: 2.9760146770602725\n",
      "two-layer Loss after iteration 69000: 2.971036474406066\n",
      "two-layer Loss after iteration 70000: 2.966189448439122\n",
      "two-layer Loss after iteration 71000: 2.96069652774842\n",
      "two-layer Loss after iteration 72000: 2.9550658209118508\n",
      "two-layer Loss after iteration 73000: 2.949485534323079\n",
      "two-layer Loss after iteration 74000: 2.941530703137617\n",
      "two-layer Loss after iteration 75000: 2.935152738312351\n",
      "two-layer Loss after iteration 76000: 2.929542289117034\n",
      "two-layer Loss after iteration 77000: 2.9247400429317896\n",
      "two-layer Loss after iteration 78000: 2.9201744189606447\n",
      "two-layer Loss after iteration 79000: 2.9159031642289523\n",
      "two-layer Loss after iteration 80000: 2.911859123657704\n",
      "two-layer Loss after iteration 81000: 2.907934592422461\n",
      "two-layer Loss after iteration 82000: 2.9045116567097122\n",
      "two-layer Loss after iteration 83000: 2.9009998003082065\n",
      "two-layer Loss after iteration 84000: 2.8972507219457295\n",
      "two-layer Loss after iteration 85000: 2.8937348090671287\n",
      "two-layer Loss after iteration 86000: 2.8907419890151167\n",
      "two-layer Loss after iteration 87000: 2.8876531006207204\n",
      "two-layer Loss after iteration 88000: 2.8845456007607186\n",
      "two-layer Loss after iteration 89000: 2.8816999372841434\n",
      "two-layer Loss after iteration 90000: 2.8787539498803394\n",
      "two-layer Loss after iteration 91000: 2.876049801585332\n",
      "two-layer Loss after iteration 92000: 2.8731961158111394\n",
      "two-layer Loss after iteration 93000: 2.8701258353667285\n",
      "two-layer Loss after iteration 94000: 2.8677612005367235\n",
      "two-layer Loss after iteration 95000: 2.865157393791676\n",
      "two-layer Loss after iteration 96000: 2.8623455405087386\n",
      "two-layer Loss after iteration 97000: 2.860025754116191\n",
      "two-layer Loss after iteration 98000: 2.8580775414837323\n",
      "two-layer Loss after iteration 99000: 2.8559052093167883\n",
      "two-layer Loss after iteration 100000: 2.8536380624231463\n",
      "two-layer Loss after iteration 101000: 2.851618473204809\n",
      "two-layer Loss after iteration 102000: 2.849694650793476\n",
      "two-layer Loss after iteration 103000: 2.8477702300748478\n",
      "two-layer Loss after iteration 104000: 2.8463466247495965\n",
      "two-layer Loss after iteration 105000: 2.844298217503777\n",
      "two-layer Loss after iteration 106000: 2.842808206803765\n",
      "two-layer Loss after iteration 107000: 2.841053315586531\n",
      "two-layer Loss after iteration 108000: 2.8395236073163463\n",
      "two-layer Loss after iteration 109000: 2.837853173338053\n",
      "two-layer Loss after iteration 110000: 2.8365169643585117\n",
      "two-layer Loss after iteration 111000: 2.834875615703094\n",
      "two-layer Loss after iteration 112000: 2.8334832602103535\n",
      "two-layer Loss after iteration 113000: 2.8321926857443986\n",
      "two-layer Loss after iteration 114000: 2.83085542613861\n",
      "two-layer Loss after iteration 115000: 2.8295412807693303\n",
      "two-layer Loss after iteration 116000: 2.828224319820826\n",
      "two-layer Loss after iteration 117000: 2.827065284583787\n",
      "two-layer Loss after iteration 118000: 2.8259389336438483\n",
      "two-layer Loss after iteration 119000: 2.8248642252099065\n",
      "two-layer Loss after iteration 120000: 2.823968444296688\n",
      "two-layer Loss after iteration 121000: 2.8227421204138956\n",
      "two-layer Loss after iteration 122000: 2.8215388157517656\n",
      "two-layer Loss after iteration 123000: 2.8205842436219246\n",
      "two-layer Loss after iteration 124000: 2.8196194246669783\n",
      "two-layer Loss after iteration 125000: 2.8187537644082297\n",
      "two-layer Loss after iteration 126000: 2.8177237107280964\n",
      "two-layer Loss after iteration 127000: 2.816805180148662\n",
      "two-layer Loss after iteration 128000: 2.8159542714329624\n",
      "two-layer Loss after iteration 129000: 2.8151316942719755\n",
      "two-layer Loss after iteration 130000: 2.8142908542110017\n",
      "two-layer Loss after iteration 131000: 2.813525275968013\n",
      "two-layer Loss after iteration 132000: 2.8127727920955454\n",
      "two-layer Loss after iteration 133000: 2.8119894286758527\n",
      "two-layer Loss after iteration 134000: 2.811279391581073\n",
      "two-layer Loss after iteration 135000: 2.8105991838077675\n",
      "two-layer Loss after iteration 136000: 2.809904406955208\n",
      "two-layer Loss after iteration 137000: 2.8092318355020462\n",
      "two-layer Loss after iteration 138000: 2.808588744835108\n",
      "two-layer Loss after iteration 139000: 2.807958835424609\n",
      "two-layer Loss after iteration 140000: 2.807341788688585\n",
      "two-layer Loss after iteration 141000: 2.8067399529463772\n",
      "two-layer Loss after iteration 142000: 2.8061490270909135\n",
      "two-layer Loss after iteration 143000: 2.8055704795787313\n",
      "two-layer Loss after iteration 144000: 2.805006225598661\n",
      "two-layer Loss after iteration 145000: 2.804452457343703\n",
      "two-layer Loss after iteration 146000: 2.8039090657397376\n",
      "two-layer Loss after iteration 147000: 2.8033807157745008\n",
      "two-layer Loss after iteration 148000: 2.8028606136274075\n",
      "two-layer Loss after iteration 149000: 2.802351257597334\n",
      "two-layer Loss after iteration 150000: 2.8018518893200812\n",
      "two-layer Loss after iteration 151000: 2.801357186502634\n",
      "two-layer Loss after iteration 152000: 2.800871772850673\n",
      "two-layer Loss after iteration 153000: 2.8003941388255003\n",
      "two-layer Loss after iteration 154000: 2.7999241238320267\n",
      "two-layer Loss after iteration 155000: 2.799460360732055\n",
      "two-layer Loss after iteration 156000: 2.7990050724386353\n",
      "two-layer Loss after iteration 157000: 2.7985568061273325\n",
      "two-layer Loss after iteration 158000: 2.798115377152802\n",
      "two-layer Loss after iteration 159000: 2.797681864411318\n",
      "two-layer Loss after iteration 160000: 2.7972525925593508\n",
      "two-layer Loss after iteration 161000: 2.796831217335715\n",
      "two-layer Loss after iteration 162000: 2.7964155590609043\n",
      "two-layer Loss after iteration 163000: 2.796007342231483\n",
      "two-layer Loss after iteration 164000: 2.795605651729115\n",
      "two-layer Loss after iteration 165000: 2.7952100750509667\n",
      "two-layer Loss after iteration 166000: 2.7948201861417727\n",
      "two-layer Loss after iteration 167000: 2.794437800101047\n",
      "two-layer Loss after iteration 168000: 2.794061617179463\n",
      "two-layer Loss after iteration 169000: 2.793688456408753\n",
      "two-layer Loss after iteration 170000: 2.793321696546652\n",
      "two-layer Loss after iteration 171000: 2.7929628792605277\n",
      "two-layer Loss after iteration 172000: 2.7926069519500145\n",
      "two-layer Loss after iteration 173000: 2.792260308688506\n",
      "two-layer Loss after iteration 174000: 2.791915812013967\n",
      "two-layer Loss after iteration 175000: 2.791577732632574\n",
      "two-layer Loss after iteration 176000: 2.791245666670031\n",
      "two-layer Loss after iteration 177000: 2.7909170393224545\n",
      "two-layer Loss after iteration 178000: 2.790594394139309\n",
      "two-layer Loss after iteration 179000: 2.7902778317706622\n",
      "two-layer Loss after iteration 180000: 2.7899640182699303\n",
      "two-layer Loss after iteration 181000: 2.7896548299989132\n",
      "two-layer Loss after iteration 182000: 2.7893532976833133\n",
      "two-layer Loss after iteration 183000: 2.7890539083546657\n",
      "two-layer Loss after iteration 184000: 2.788759773280853\n",
      "two-layer Loss after iteration 185000: 2.7884710064345524\n",
      "two-layer Loss after iteration 186000: 2.7881876391496774\n",
      "two-layer Loss after iteration 187000: 2.7879060841962717\n",
      "two-layer Loss after iteration 188000: 2.787631159277667\n",
      "9.861340744703832e-05 2.7879060841962717 2.787631159277667\n",
      "two-layer Loss after iteration 0: 1598.0558351836003\n",
      "two-layer Loss after iteration 1000: 22.835152812757055\n",
      "two-layer Loss after iteration 2000: 9.79761178066367\n",
      "two-layer Loss after iteration 3000: 7.947341129263877\n",
      "two-layer Loss after iteration 4000: 7.2585532770369285\n",
      "two-layer Loss after iteration 5000: 6.706667459865393\n",
      "two-layer Loss after iteration 6000: 5.916745848596769\n",
      "two-layer Loss after iteration 7000: 5.456887405603985\n",
      "two-layer Loss after iteration 8000: 5.184021161547567\n",
      "two-layer Loss after iteration 9000: 4.946919649310221\n",
      "two-layer Loss after iteration 10000: 4.810027341723541\n",
      "two-layer Loss after iteration 11000: 4.6926784105708474\n",
      "two-layer Loss after iteration 12000: 4.586386465205727\n",
      "two-layer Loss after iteration 13000: 4.442082105956688\n",
      "two-layer Loss after iteration 14000: 4.309810090045834\n",
      "two-layer Loss after iteration 15000: 4.220959961622571\n",
      "two-layer Loss after iteration 16000: 4.053909038187041\n",
      "two-layer Loss after iteration 17000: 3.9514601871749386\n",
      "two-layer Loss after iteration 18000: 3.8817768744316616\n",
      "two-layer Loss after iteration 19000: 3.8299313767091108\n",
      "two-layer Loss after iteration 20000: 3.7601825891505136\n",
      "two-layer Loss after iteration 21000: 3.6664038231559255\n",
      "two-layer Loss after iteration 22000: 3.3855554576650295\n",
      "two-layer Loss after iteration 23000: 3.0893642236620296\n",
      "two-layer Loss after iteration 24000: 2.8921982280771217\n",
      "two-layer Loss after iteration 25000: 2.804662547706853\n",
      "two-layer Loss after iteration 26000: 2.7816764518811112\n",
      "two-layer Loss after iteration 27000: 2.766837628468594\n",
      "two-layer Loss after iteration 28000: 2.754579889186889\n",
      "two-layer Loss after iteration 29000: 2.743602588538472\n",
      "two-layer Loss after iteration 30000: 2.7331764686564775\n",
      "two-layer Loss after iteration 31000: 2.7234934000595756\n",
      "two-layer Loss after iteration 32000: 2.7149382788571232\n",
      "two-layer Loss after iteration 33000: 2.707231170596121\n",
      "two-layer Loss after iteration 34000: 2.7001228497175567\n",
      "two-layer Loss after iteration 35000: 2.693067743524679\n",
      "two-layer Loss after iteration 36000: 2.6864215801247293\n",
      "two-layer Loss after iteration 37000: 2.6799923528149594\n",
      "two-layer Loss after iteration 38000: 2.6738086700639663\n",
      "two-layer Loss after iteration 39000: 2.6678312769613672\n",
      "two-layer Loss after iteration 40000: 2.6602001130014012\n",
      "two-layer Loss after iteration 41000: 2.6538562008172537\n",
      "two-layer Loss after iteration 42000: 2.6479528899277924\n",
      "two-layer Loss after iteration 43000: 2.642230216931093\n",
      "two-layer Loss after iteration 44000: 2.636657628473502\n",
      "two-layer Loss after iteration 45000: 2.631205988453637\n",
      "two-layer Loss after iteration 46000: 2.6258693846285657\n",
      "two-layer Loss after iteration 47000: 2.6206549761279927\n",
      "two-layer Loss after iteration 48000: 2.6155261044607863\n",
      "two-layer Loss after iteration 49000: 2.6092119418032804\n",
      "two-layer Loss after iteration 50000: 2.6033191892401657\n",
      "two-layer Loss after iteration 51000: 2.5979834665221717\n",
      "two-layer Loss after iteration 52000: 2.5928020426146245\n",
      "two-layer Loss after iteration 53000: 2.587715613814829\n",
      "two-layer Loss after iteration 54000: 2.582714238330089\n",
      "two-layer Loss after iteration 55000: 2.5777700768468526\n",
      "two-layer Loss after iteration 56000: 2.572906255471821\n",
      "two-layer Loss after iteration 57000: 2.568100182089102\n",
      "two-layer Loss after iteration 58000: 2.5633557225185064\n",
      "two-layer Loss after iteration 59000: 2.558675071022175\n",
      "two-layer Loss after iteration 60000: 2.554065033598729\n",
      "two-layer Loss after iteration 61000: 2.5494921507449786\n",
      "two-layer Loss after iteration 62000: 2.544982595161427\n",
      "two-layer Loss after iteration 63000: 2.540533576006643\n",
      "two-layer Loss after iteration 64000: 2.5360593822776907\n",
      "two-layer Loss after iteration 65000: 2.531668692392258\n",
      "two-layer Loss after iteration 66000: 2.527329954447782\n",
      "two-layer Loss after iteration 67000: 2.523035939477593\n",
      "two-layer Loss after iteration 68000: 2.518798558423885\n",
      "two-layer Loss after iteration 69000: 2.5145985628808285\n",
      "two-layer Loss after iteration 70000: 2.5104676335654994\n",
      "two-layer Loss after iteration 71000: 2.5063451759134803\n",
      "two-layer Loss after iteration 72000: 2.5022850362795004\n",
      "two-layer Loss after iteration 73000: 2.498251353206174\n",
      "two-layer Loss after iteration 74000: 2.494265051520933\n",
      "two-layer Loss after iteration 75000: 2.490321971851949\n",
      "two-layer Loss after iteration 76000: 2.4864119345062905\n",
      "two-layer Loss after iteration 77000: 2.4825518242759386\n",
      "two-layer Loss after iteration 78000: 2.4786962661723084\n",
      "two-layer Loss after iteration 79000: 2.474925380428147\n",
      "two-layer Loss after iteration 80000: 2.4711542788370737\n",
      "two-layer Loss after iteration 81000: 2.467450368998246\n",
      "two-layer Loss after iteration 82000: 2.463771877988637\n",
      "two-layer Loss after iteration 83000: 2.460126497297119\n",
      "two-layer Loss after iteration 84000: 2.456532086206371\n",
      "two-layer Loss after iteration 85000: 2.4529699776310103\n",
      "two-layer Loss after iteration 86000: 2.441767778899082\n",
      "two-layer Loss after iteration 87000: 2.435728768382935\n",
      "two-layer Loss after iteration 88000: 2.4311478016951\n",
      "two-layer Loss after iteration 89000: 2.4267166679464167\n",
      "two-layer Loss after iteration 90000: 2.4223906100066515\n",
      "two-layer Loss after iteration 91000: 2.418260183551027\n",
      "two-layer Loss after iteration 92000: 2.414220106413386\n",
      "two-layer Loss after iteration 93000: 2.4102839764495734\n",
      "two-layer Loss after iteration 94000: 2.4064719466238036\n",
      "two-layer Loss after iteration 95000: 2.402798795726318\n",
      "two-layer Loss after iteration 96000: 2.3991399487262304\n",
      "two-layer Loss after iteration 97000: 2.395644856533299\n",
      "two-layer Loss after iteration 98000: 2.3921556773585526\n",
      "two-layer Loss after iteration 99000: 2.3887548591157732\n",
      "two-layer Loss after iteration 100000: 2.385465792570848\n",
      "two-layer Loss after iteration 101000: 2.3821616174000035\n",
      "two-layer Loss after iteration 102000: 2.3789737392016255\n",
      "two-layer Loss after iteration 103000: 2.3758412371374567\n",
      "two-layer Loss after iteration 104000: 2.3727122806165233\n",
      "two-layer Loss after iteration 105000: 2.369656961900695\n",
      "two-layer Loss after iteration 106000: 2.3666921654602966\n",
      "two-layer Loss after iteration 107000: 2.3637189450842375\n",
      "two-layer Loss after iteration 108000: 2.360762991128943\n",
      "two-layer Loss after iteration 109000: 2.3578837710300813\n",
      "two-layer Loss after iteration 110000: 2.3550371255112834\n",
      "two-layer Loss after iteration 111000: 2.352228154770648\n",
      "two-layer Loss after iteration 112000: 2.349479767681394\n",
      "two-layer Loss after iteration 113000: 2.3467098857334183\n",
      "two-layer Loss after iteration 114000: 2.344002024894053\n",
      "two-layer Loss after iteration 115000: 2.341299414261873\n",
      "two-layer Loss after iteration 116000: 2.338645996102817\n",
      "two-layer Loss after iteration 117000: 2.336020460457759\n",
      "two-layer Loss after iteration 118000: 2.3334233305901897\n",
      "two-layer Loss after iteration 119000: 2.3308608833510482\n",
      "two-layer Loss after iteration 120000: 2.3283310705705738\n",
      "two-layer Loss after iteration 121000: 2.3258026358890405\n",
      "two-layer Loss after iteration 122000: 2.3233186339735834\n",
      "two-layer Loss after iteration 123000: 2.3208300260548644\n",
      "two-layer Loss after iteration 124000: 2.318297380917233\n",
      "two-layer Loss after iteration 125000: 2.29291878123469\n",
      "two-layer Loss after iteration 126000: 2.283088343246908\n",
      "two-layer Loss after iteration 127000: 2.2759524649921685\n",
      "two-layer Loss after iteration 128000: 2.269874600630165\n",
      "two-layer Loss after iteration 129000: 2.264341952112454\n",
      "two-layer Loss after iteration 130000: 2.2591798612044\n",
      "two-layer Loss after iteration 131000: 2.2542650077719673\n",
      "two-layer Loss after iteration 132000: 2.2495757969867936\n",
      "two-layer Loss after iteration 133000: 2.2451299068823785\n",
      "two-layer Loss after iteration 134000: 2.2408385761834255\n",
      "two-layer Loss after iteration 135000: 2.236817356348067\n",
      "two-layer Loss after iteration 136000: 2.2329804530035906\n",
      "two-layer Loss after iteration 137000: 2.2292926532083097\n",
      "two-layer Loss after iteration 138000: 2.2257415904427806\n",
      "two-layer Loss after iteration 139000: 2.2223354656872885\n",
      "two-layer Loss after iteration 140000: 2.217396049167373\n",
      "two-layer Loss after iteration 141000: 2.213337025504081\n",
      "two-layer Loss after iteration 142000: 2.2097913331716326\n",
      "two-layer Loss after iteration 143000: 2.206395170335618\n",
      "two-layer Loss after iteration 144000: 2.20311647510824\n",
      "two-layer Loss after iteration 145000: 2.1999273832497503\n",
      "two-layer Loss after iteration 146000: 2.1967702271230882\n",
      "two-layer Loss after iteration 147000: 2.193694528693392\n",
      "two-layer Loss after iteration 148000: 2.1906709822540615\n",
      "two-layer Loss after iteration 149000: 2.187693748839308\n",
      "two-layer Loss after iteration 150000: 2.1847550111833156\n",
      "two-layer Loss after iteration 151000: 2.181878759996874\n",
      "two-layer Loss after iteration 152000: 2.178980691950606\n",
      "two-layer Loss after iteration 153000: 2.1761718253262554\n",
      "two-layer Loss after iteration 154000: 2.173390445860369\n",
      "two-layer Loss after iteration 155000: 2.170666617718058\n",
      "two-layer Loss after iteration 156000: 2.1679998451426696\n",
      "two-layer Loss after iteration 157000: 2.165412248170186\n",
      "two-layer Loss after iteration 158000: 2.162857914201734\n",
      "two-layer Loss after iteration 159000: 2.160287898043492\n",
      "two-layer Loss after iteration 160000: 2.1578083809856374\n",
      "two-layer Loss after iteration 161000: 2.1553657105704733\n",
      "two-layer Loss after iteration 162000: 2.1529467891383254\n",
      "two-layer Loss after iteration 163000: 2.1505270646866834\n",
      "two-layer Loss after iteration 164000: 2.1482015184341354\n",
      "two-layer Loss after iteration 165000: 2.145862691019887\n",
      "two-layer Loss after iteration 166000: 2.143543936255822\n",
      "two-layer Loss after iteration 167000: 2.141282036688689\n",
      "two-layer Loss after iteration 168000: 2.1390150285630196\n",
      "two-layer Loss after iteration 169000: 2.1368252140854387\n",
      "two-layer Loss after iteration 170000: 2.1346520143104875\n",
      "two-layer Loss after iteration 171000: 2.1325195241245045\n",
      "two-layer Loss after iteration 172000: 2.1303618710834322\n",
      "two-layer Loss after iteration 173000: 2.1282452712317417\n",
      "two-layer Loss after iteration 174000: 2.126187103512329\n",
      "two-layer Loss after iteration 175000: 2.124127402672083\n",
      "two-layer Loss after iteration 176000: 2.1220509733224793\n",
      "two-layer Loss after iteration 177000: 2.1200650003160244\n",
      "two-layer Loss after iteration 178000: 2.1180691716716584\n",
      "two-layer Loss after iteration 179000: 2.1160913236159598\n",
      "two-layer Loss after iteration 180000: 2.110251474994746\n",
      "two-layer Loss after iteration 181000: 2.107409573557756\n",
      "two-layer Loss after iteration 182000: 2.104866974815126\n",
      "two-layer Loss after iteration 183000: 2.102438958932573\n",
      "two-layer Loss after iteration 184000: 2.1000369700233295\n",
      "two-layer Loss after iteration 185000: 2.097739958608437\n",
      "two-layer Loss after iteration 186000: 2.095484938706718\n",
      "two-layer Loss after iteration 187000: 2.093234917783563\n",
      "two-layer Loss after iteration 188000: 2.0910305524101274\n",
      "two-layer Loss after iteration 189000: 2.0888279643329146\n",
      "two-layer Loss after iteration 190000: 2.0866438133871337\n",
      "two-layer Loss after iteration 191000: 2.0845279874753273\n",
      "two-layer Loss after iteration 192000: 2.0824624179549662\n",
      "two-layer Loss after iteration 193000: 2.080389708733305\n",
      "two-layer Loss after iteration 194000: 2.078342456169593\n",
      "two-layer Loss after iteration 195000: 2.076335249485479\n",
      "two-layer Loss after iteration 196000: 2.0743573998809737\n",
      "two-layer Loss after iteration 197000: 2.072373797941411\n",
      "two-layer Loss after iteration 198000: 2.070389575211813\n",
      "two-layer Loss after iteration 199000: 2.068477066828554\n",
      "two-layer Loss after iteration 200000: 2.0665848562915956\n",
      "two-layer Loss after iteration 201000: 2.0646955150850106\n",
      "two-layer Loss after iteration 202000: 2.0628442916357113\n",
      "two-layer Loss after iteration 203000: 2.061030775253441\n",
      "two-layer Loss after iteration 204000: 2.059227300288433\n",
      "two-layer Loss after iteration 205000: 2.0574012133232\n",
      "two-layer Loss after iteration 206000: 2.055631096868937\n",
      "two-layer Loss after iteration 207000: 2.0538753487255876\n",
      "two-layer Loss after iteration 208000: 2.0521720524993308\n",
      "two-layer Loss after iteration 209000: 2.050426751256609\n",
      "two-layer Loss after iteration 210000: 2.0487749968976217\n",
      "two-layer Loss after iteration 211000: 2.0470726046435135\n",
      "two-layer Loss after iteration 212000: 2.0454388957021643\n",
      "two-layer Loss after iteration 213000: 2.043761184176735\n",
      "two-layer Loss after iteration 214000: 2.041701256048949\n",
      "two-layer Loss after iteration 215000: 2.039928920244417\n",
      "two-layer Loss after iteration 216000: 2.0382962301333625\n",
      "two-layer Loss after iteration 217000: 2.0366655823517092\n",
      "two-layer Loss after iteration 218000: 2.0350006172129076\n",
      "two-layer Loss after iteration 219000: 2.033414254774192\n",
      "two-layer Loss after iteration 220000: 2.0318233811441018\n",
      "two-layer Loss after iteration 221000: 2.029021517421132\n",
      "two-layer Loss after iteration 222000: 2.027101762301216\n",
      "two-layer Loss after iteration 223000: 2.025435849255754\n",
      "two-layer Loss after iteration 224000: 2.0237564805088364\n",
      "two-layer Loss after iteration 225000: 2.0221586010342523\n",
      "two-layer Loss after iteration 226000: 2.020601796467824\n",
      "two-layer Loss after iteration 227000: 2.0190354756153184\n",
      "two-layer Loss after iteration 228000: 2.017521066114484\n",
      "two-layer Loss after iteration 229000: 2.0159901279987205\n",
      "two-layer Loss after iteration 230000: 2.0145096548115506\n",
      "two-layer Loss after iteration 231000: 2.01305675872148\n",
      "two-layer Loss after iteration 232000: 2.011573145938125\n",
      "two-layer Loss after iteration 233000: 2.010155653662415\n",
      "two-layer Loss after iteration 234000: 2.0087278703313065\n",
      "two-layer Loss after iteration 235000: 2.0072854268057134\n",
      "two-layer Loss after iteration 236000: 2.0058877246009974\n",
      "two-layer Loss after iteration 237000: 2.004511997877583\n",
      "two-layer Loss after iteration 238000: 2.0031559182348118\n",
      "two-layer Loss after iteration 239000: 2.0017937008339834\n",
      "two-layer Loss after iteration 240000: 2.000465934959225\n",
      "two-layer Loss after iteration 241000: 1.999155917190312\n",
      "two-layer Loss after iteration 242000: 1.9945545962126299\n",
      "two-layer Loss after iteration 243000: 1.992698699416961\n",
      "two-layer Loss after iteration 244000: 1.9910291294233922\n",
      "two-layer Loss after iteration 245000: 1.9894185903747672\n",
      "two-layer Loss after iteration 246000: 1.9878659811239883\n",
      "two-layer Loss after iteration 247000: 1.9863185995247412\n",
      "two-layer Loss after iteration 248000: 1.9848160638621928\n",
      "two-layer Loss after iteration 249000: 1.9833252076307524\n",
      "two-layer Loss after iteration 250000: 1.9818694686325948\n",
      "two-layer Loss after iteration 251000: 1.9804198162206976\n",
      "two-layer Loss after iteration 252000: 1.9790418213270564\n",
      "two-layer Loss after iteration 253000: 1.9776461434757602\n",
      "two-layer Loss after iteration 254000: 1.9762230059437258\n",
      "two-layer Loss after iteration 255000: 1.9748631064038882\n",
      "two-layer Loss after iteration 256000: 1.9735139219591307\n",
      "two-layer Loss after iteration 257000: 1.9721978410948198\n",
      "two-layer Loss after iteration 258000: 1.9708765071975467\n",
      "two-layer Loss after iteration 259000: 1.969622169152432\n",
      "two-layer Loss after iteration 260000: 1.9683181844246895\n",
      "two-layer Loss after iteration 261000: 1.9670815329250684\n",
      "two-layer Loss after iteration 262000: 1.965800735242694\n",
      "two-layer Loss after iteration 263000: 1.9645678375895912\n",
      "two-layer Loss after iteration 264000: 1.9633440101435438\n",
      "two-layer Loss after iteration 265000: 1.9621456856325048\n",
      "two-layer Loss after iteration 266000: 1.9609538766512746\n",
      "two-layer Loss after iteration 267000: 1.9597860927999136\n",
      "two-layer Loss after iteration 268000: 1.9586269191841388\n",
      "two-layer Loss after iteration 269000: 1.9574789386420661\n",
      "two-layer Loss after iteration 270000: 1.9563410146439097\n",
      "two-layer Loss after iteration 271000: 1.9552261705030285\n",
      "two-layer Loss after iteration 272000: 1.9541150719951303\n",
      "two-layer Loss after iteration 273000: 1.9530229382794735\n",
      "two-layer Loss after iteration 274000: 1.9519442498493143\n",
      "two-layer Loss after iteration 275000: 1.9508760486568073\n",
      "two-layer Loss after iteration 276000: 1.949821759837551\n",
      "two-layer Loss after iteration 277000: 1.948779705809269\n",
      "two-layer Loss after iteration 278000: 1.9477498788439087\n",
      "two-layer Loss after iteration 279000: 1.946731841696645\n",
      "two-layer Loss after iteration 280000: 1.9457248655283943\n",
      "two-layer Loss after iteration 281000: 1.9447286166920432\n",
      "two-layer Loss after iteration 282000: 1.9437428553151574\n",
      "two-layer Loss after iteration 283000: 1.9427673916569872\n",
      "two-layer Loss after iteration 284000: 1.9418020638996996\n",
      "two-layer Loss after iteration 285000: 1.9408467266571192\n",
      "two-layer Loss after iteration 286000: 1.939901244937692\n",
      "two-layer Loss after iteration 287000: 1.9389654909038012\n",
      "two-layer Loss after iteration 288000: 1.9380393420746174\n",
      "two-layer Loss after iteration 289000: 1.9371226802808337\n",
      "two-layer Loss after iteration 290000: 1.9362153910162365\n",
      "two-layer Loss after iteration 291000: 1.9353173630032385\n",
      "two-layer Loss after iteration 292000: 1.9344284878776292\n",
      "two-layer Loss after iteration 293000: 1.9335486599430876\n",
      "two-layer Loss after iteration 294000: 1.9326777759691773\n",
      "two-layer Loss after iteration 295000: 1.931815735018704\n",
      "two-layer Loss after iteration 296000: 1.9309624382964556\n",
      "two-layer Loss after iteration 297000: 1.9301177890146188\n",
      "two-layer Loss after iteration 298000: 1.929281692271941\n",
      "two-layer Loss after iteration 299000: 1.9284540549445814\n",
      "two-layer Loss after iteration 300000: 1.9276347855872105\n",
      "two-layer Loss after iteration 301000: 1.9268237943431354\n",
      "two-layer Loss after iteration 302000: 1.9260209928625487\n",
      "two-layer Loss after iteration 303000: 1.9252262942279983\n",
      "two-layer Loss after iteration 304000: 1.9244396128864243\n",
      "two-layer Loss after iteration 305000: 1.923660864587086\n",
      "two-layer Loss after iteration 306000: 1.9228899663247858\n",
      "two-layer Loss after iteration 307000: 1.9221268362879396\n",
      "two-layer Loss after iteration 308000: 1.9205700766074456\n",
      "two-layer Loss after iteration 309000: 1.9196121357605265\n",
      "two-layer Loss after iteration 310000: 1.9187416641148078\n",
      "two-layer Loss after iteration 311000: 1.917899692318955\n",
      "two-layer Loss after iteration 312000: 1.9170739489532127\n",
      "two-layer Loss after iteration 313000: 1.9162604014028064\n",
      "two-layer Loss after iteration 314000: 1.9154575797913886\n",
      "two-layer Loss after iteration 315000: 1.914664830620442\n",
      "two-layer Loss after iteration 316000: 1.9138817735413476\n",
      "two-layer Loss after iteration 317000: 1.9131081292880736\n",
      "two-layer Loss after iteration 318000: 1.9123436632360677\n",
      "two-layer Loss after iteration 319000: 1.911588165610749\n",
      "two-layer Loss after iteration 320000: 1.9108414436780852\n",
      "two-layer Loss after iteration 321000: 1.910103318070257\n",
      "two-layer Loss after iteration 322000: 1.9093736206715692\n",
      "two-layer Loss after iteration 323000: 1.908652193176237\n",
      "two-layer Loss after iteration 324000: 1.907938885986711\n",
      "two-layer Loss after iteration 325000: 1.9072335573150114\n",
      "two-layer Loss after iteration 326000: 1.906536072422052\n",
      "two-layer Loss after iteration 327000: 1.905846302959619\n",
      "two-layer Loss after iteration 328000: 1.9051641263935193\n",
      "two-layer Loss after iteration 329000: 1.9044914295590663\n",
      "two-layer Loss after iteration 330000: 1.9038251608414554\n",
      "two-layer Loss after iteration 331000: 1.9031680841032166\n",
      "two-layer Loss after iteration 332000: 1.9025186658332107\n",
      "two-layer Loss after iteration 333000: 1.9018778684519708\n",
      "two-layer Loss after iteration 334000: 1.901243058778286\n",
      "two-layer Loss after iteration 335000: 1.9006175450898455\n",
      "two-layer Loss after iteration 336000: 1.8999983588384481\n",
      "two-layer Loss after iteration 337000: 1.8993897842043037\n",
      "two-layer Loss after iteration 338000: 1.898797335097026\n",
      "two-layer Loss after iteration 339000: 1.898188835003969\n",
      "two-layer Loss after iteration 340000: 1.8975989322768714\n",
      "two-layer Loss after iteration 341000: 1.8970190335289667\n",
      "two-layer Loss after iteration 342000: 1.8964383479298825\n",
      "two-layer Loss after iteration 343000: 1.8958723554132337\n",
      "two-layer Loss after iteration 344000: 1.8953080050922906\n",
      "two-layer Loss after iteration 345000: 1.8947507993604202\n",
      "two-layer Loss after iteration 346000: 1.8942049207482792\n",
      "two-layer Loss after iteration 347000: 1.8936647911865985\n",
      "two-layer Loss after iteration 348000: 1.893124555623919\n",
      "two-layer Loss after iteration 349000: 1.8926118120192164\n",
      "two-layer Loss after iteration 350000: 1.8920877423090054\n",
      "two-layer Loss after iteration 351000: 1.8915548061294951\n",
      "two-layer Loss after iteration 352000: 1.8910423384915853\n",
      "two-layer Loss after iteration 353000: 1.8905350291463956\n",
      "two-layer Loss after iteration 354000: 1.8900357391210438\n",
      "two-layer Loss after iteration 355000: 1.8895425498256562\n",
      "two-layer Loss after iteration 356000: 1.889062179153922\n",
      "two-layer Loss after iteration 357000: 1.8885943010796922\n",
      "two-layer Loss after iteration 358000: 1.8880976042165847\n",
      "two-layer Loss after iteration 359000: 1.8876310166252692\n",
      "two-layer Loss after iteration 360000: 1.887182540174683\n",
      "two-layer Loss after iteration 361000: 1.8867036094349852\n",
      "two-layer Loss after iteration 362000: 1.8862542356315892\n",
      "two-layer Loss after iteration 363000: 1.8858106608891398\n",
      "two-layer Loss after iteration 364000: 1.8853752236314014\n",
      "two-layer Loss after iteration 365000: 1.8849394426093642\n",
      "two-layer Loss after iteration 366000: 1.8844844270466694\n",
      "two-layer Loss after iteration 367000: 1.8840775668241685\n",
      "two-layer Loss after iteration 368000: 1.8836318142858206\n",
      "two-layer Loss after iteration 369000: 1.8832178580482084\n",
      "two-layer Loss after iteration 370000: 1.8828007689994661\n",
      "two-layer Loss after iteration 371000: 1.8823926407743248\n",
      "two-layer Loss after iteration 372000: 1.8820031187533093\n",
      "two-layer Loss after iteration 373000: 1.8816032372138563\n",
      "two-layer Loss after iteration 374000: 1.8811965315191428\n",
      "two-layer Loss after iteration 375000: 1.8808296940249574\n",
      "two-layer Loss after iteration 376000: 1.8797452483743584\n",
      "two-layer Loss after iteration 377000: 1.879277138435477\n",
      "two-layer Loss after iteration 378000: 1.8788455595585793\n",
      "two-layer Loss after iteration 379000: 1.8784172835302384\n",
      "two-layer Loss after iteration 380000: 1.8779983097550395\n",
      "two-layer Loss after iteration 381000: 1.8776094291366943\n",
      "two-layer Loss after iteration 382000: 1.8771996534418218\n",
      "two-layer Loss after iteration 383000: 1.876806682584348\n",
      "two-layer Loss after iteration 384000: 1.8763913044802332\n",
      "two-layer Loss after iteration 385000: 1.8760121897372073\n",
      "two-layer Loss after iteration 386000: 1.8756242799532665\n",
      "two-layer Loss after iteration 387000: 1.8752647539928946\n",
      "two-layer Loss after iteration 388000: 1.8748894195207173\n",
      "two-layer Loss after iteration 389000: 1.8745085587533534\n",
      "two-layer Loss after iteration 390000: 1.8741483631380749\n",
      "two-layer Loss after iteration 391000: 1.873795518287588\n",
      "two-layer Loss after iteration 392000: 1.8734378443025765\n",
      "two-layer Loss after iteration 393000: 1.8728802600116135\n",
      "two-layer Loss after iteration 394000: 1.8724714553248425\n",
      "two-layer Loss after iteration 395000: 1.872123883152322\n",
      "two-layer Loss after iteration 396000: 1.871752078537153\n",
      "two-layer Loss after iteration 397000: 1.8714191018300512\n",
      "two-layer Loss after iteration 398000: 1.871073657105008\n",
      "two-layer Loss after iteration 399000: 1.8707303585648638\n",
      "two-layer Loss after iteration 400000: 1.8704146706030527\n",
      "two-layer Loss after iteration 401000: 1.8700742506413663\n",
      "two-layer Loss after iteration 402000: 1.8697466235421143\n",
      "two-layer Loss after iteration 403000: 1.8694327359074838\n",
      "two-layer Loss after iteration 404000: 1.8691270239023865\n",
      "two-layer Loss after iteration 405000: 1.868739618117879\n",
      "two-layer Loss after iteration 406000: 1.8684087191431942\n",
      "two-layer Loss after iteration 407000: 1.868078979855308\n",
      "two-layer Loss after iteration 408000: 1.8677741636915284\n",
      "two-layer Loss after iteration 409000: 1.8674842407457022\n",
      "two-layer Loss after iteration 410000: 1.8671915224340088\n",
      "two-layer Loss after iteration 411000: 1.8668802229439108\n",
      "two-layer Loss after iteration 412000: 1.8666058658885551\n",
      "two-layer Loss after iteration 413000: 1.8663059607303238\n",
      "two-layer Loss after iteration 414000: 1.8660246800866467\n",
      "two-layer Loss after iteration 415000: 1.8657520082532497\n",
      "two-layer Loss after iteration 416000: 1.8654862449451435\n",
      "two-layer Loss after iteration 417000: 1.865202535732433\n",
      "two-layer Loss after iteration 418000: 1.8649487616837375\n",
      "two-layer Loss after iteration 419000: 1.8646680061947094\n",
      "two-layer Loss after iteration 420000: 1.864425255106986\n",
      "two-layer Loss after iteration 421000: 1.8641546515983607\n",
      "two-layer Loss after iteration 422000: 1.863913356613726\n",
      "two-layer Loss after iteration 423000: 1.86364450981313\n",
      "two-layer Loss after iteration 424000: 1.8633975888240006\n",
      "two-layer Loss after iteration 425000: 1.8631750477233577\n",
      "two-layer Loss after iteration 426000: 1.862920523122402\n",
      "two-layer Loss after iteration 427000: 1.8626836662052955\n",
      "two-layer Loss after iteration 428000: 1.8624575630631421\n",
      "two-layer Loss after iteration 429000: 1.8622210447730985\n",
      "two-layer Loss after iteration 430000: 1.8619755132598828\n",
      "two-layer Loss after iteration 431000: 1.8617514368045511\n",
      "two-layer Loss after iteration 432000: 1.8615274186556252\n",
      "two-layer Loss after iteration 433000: 1.8613200200347815\n",
      "two-layer Loss after iteration 434000: 1.8610874435981444\n",
      "two-layer Loss after iteration 435000: 1.860868798564978\n",
      "two-layer Loss after iteration 436000: 1.8606690547146958\n",
      "two-layer Loss after iteration 437000: 1.8604596632790493\n",
      "two-layer Loss after iteration 438000: 1.8602540137044021\n",
      "two-layer Loss after iteration 439000: 1.8600269309669268\n",
      "two-layer Loss after iteration 440000: 1.8598413793523605\n",
      "9.975748817243742e-05 1.8600269309669268 1.8598413793523605\n",
      "two-layer Loss after iteration 0: 1616.625705807937\n",
      "two-layer Loss after iteration 1000: 25.602688755346207\n",
      "two-layer Loss after iteration 2000: 10.269637248443075\n",
      "two-layer Loss after iteration 3000: 7.949721086800097\n",
      "two-layer Loss after iteration 4000: 6.968758043173766\n",
      "two-layer Loss after iteration 5000: 6.401287467787913\n",
      "two-layer Loss after iteration 6000: 5.886771098490137\n",
      "two-layer Loss after iteration 7000: 5.061336506574793\n",
      "two-layer Loss after iteration 8000: 4.577616360189107\n",
      "two-layer Loss after iteration 9000: 4.343891557943062\n",
      "two-layer Loss after iteration 10000: 4.205390554102265\n",
      "two-layer Loss after iteration 11000: 4.093867022945696\n",
      "two-layer Loss after iteration 12000: 4.003010007435556\n",
      "two-layer Loss after iteration 13000: 3.9254668549103897\n",
      "two-layer Loss after iteration 14000: 3.819819593427038\n",
      "two-layer Loss after iteration 15000: 3.686922698083373\n",
      "two-layer Loss after iteration 16000: 3.596840064661159\n",
      "two-layer Loss after iteration 17000: 3.5274183420487253\n",
      "two-layer Loss after iteration 18000: 3.481702121774718\n",
      "two-layer Loss after iteration 19000: 3.4486504587836184\n",
      "two-layer Loss after iteration 20000: 3.4226460705040425\n",
      "two-layer Loss after iteration 21000: 3.391121050445974\n",
      "two-layer Loss after iteration 22000: 3.363840528661071\n",
      "two-layer Loss after iteration 23000: 3.339786489686367\n",
      "two-layer Loss after iteration 24000: 3.3191005157583287\n",
      "two-layer Loss after iteration 25000: 3.302225634300398\n",
      "two-layer Loss after iteration 26000: 3.288576259448156\n",
      "two-layer Loss after iteration 27000: 3.2773905869608684\n",
      "two-layer Loss after iteration 28000: 3.26749589686916\n",
      "two-layer Loss after iteration 29000: 3.259890027101358\n",
      "two-layer Loss after iteration 30000: 3.2534679911791238\n",
      "two-layer Loss after iteration 31000: 3.246119546120543\n",
      "two-layer Loss after iteration 32000: 3.241034714885717\n",
      "two-layer Loss after iteration 33000: 3.2373133829668\n",
      "two-layer Loss after iteration 34000: 3.2329800819873116\n",
      "two-layer Loss after iteration 35000: 3.216936757708745\n",
      "two-layer Loss after iteration 36000: 3.2096678620034886\n",
      "two-layer Loss after iteration 37000: 3.1968942318000657\n",
      "two-layer Loss after iteration 38000: 3.1906989592317596\n",
      "two-layer Loss after iteration 39000: 3.185802196171088\n",
      "two-layer Loss after iteration 40000: 3.180604087863858\n",
      "two-layer Loss after iteration 41000: 3.1754326433125515\n",
      "two-layer Loss after iteration 42000: 3.171502904917462\n",
      "two-layer Loss after iteration 43000: 3.167418023901118\n",
      "two-layer Loss after iteration 44000: 3.164282188791829\n",
      "two-layer Loss after iteration 45000: 3.16197531873437\n",
      "two-layer Loss after iteration 46000: 3.160204948052341\n",
      "two-layer Loss after iteration 47000: 3.158526848130775\n",
      "two-layer Loss after iteration 48000: 3.1573471219928866\n",
      "two-layer Loss after iteration 49000: 3.156553233265513\n",
      "two-layer Loss after iteration 50000: 3.155915964560167\n",
      "two-layer Loss after iteration 51000: 3.1553881632197136\n",
      "two-layer Loss after iteration 52000: 3.154939214573749\n",
      "two-layer Loss after iteration 53000: 3.154224115020786\n",
      "two-layer Loss after iteration 54000: 3.153490691145731\n",
      "two-layer Loss after iteration 55000: 3.152978674602649\n",
      "two-layer Loss after iteration 56000: 3.152412845425023\n",
      "two-layer Loss after iteration 57000: 3.1398020448189072\n",
      "two-layer Loss after iteration 58000: 3.1338589915784785\n",
      "two-layer Loss after iteration 59000: 3.132583270628146\n",
      "two-layer Loss after iteration 60000: 3.131719850164052\n",
      "two-layer Loss after iteration 61000: 3.131140876339926\n",
      "two-layer Loss after iteration 62000: 3.0928195054398913\n",
      "two-layer Loss after iteration 63000: 3.077697301456414\n",
      "two-layer Loss after iteration 64000: 2.9484162359602837\n",
      "two-layer Loss after iteration 65000: 2.4735840127155355\n",
      "two-layer Loss after iteration 66000: 2.387465813931066\n",
      "two-layer Loss after iteration 67000: 2.361933707989634\n",
      "two-layer Loss after iteration 68000: 2.3444827246289335\n",
      "two-layer Loss after iteration 69000: 2.324996525270489\n",
      "two-layer Loss after iteration 70000: 2.3080015008340533\n",
      "two-layer Loss after iteration 71000: 2.2970559842771663\n",
      "two-layer Loss after iteration 72000: 2.2894057728995034\n",
      "two-layer Loss after iteration 73000: 2.2831769852239234\n",
      "two-layer Loss after iteration 74000: 2.277989164792449\n",
      "two-layer Loss after iteration 75000: 2.2696140606988995\n",
      "two-layer Loss after iteration 76000: 2.26452786069872\n",
      "two-layer Loss after iteration 77000: 2.2601642259711685\n",
      "two-layer Loss after iteration 78000: 2.256403554651396\n",
      "two-layer Loss after iteration 79000: 2.2529960551457506\n",
      "two-layer Loss after iteration 80000: 2.2493470462759317\n",
      "two-layer Loss after iteration 81000: 2.2462114715340213\n",
      "two-layer Loss after iteration 82000: 2.2434705348710056\n",
      "two-layer Loss after iteration 83000: 2.2410342396192604\n",
      "two-layer Loss after iteration 84000: 2.2389567774221666\n",
      "two-layer Loss after iteration 85000: 2.237297824548254\n",
      "two-layer Loss after iteration 86000: 2.2359138480746044\n",
      "two-layer Loss after iteration 87000: 2.234720081831167\n",
      "two-layer Loss after iteration 88000: 2.233660023983937\n",
      "two-layer Loss after iteration 89000: 2.232702516869899\n",
      "two-layer Loss after iteration 90000: 2.231809660759558\n",
      "two-layer Loss after iteration 91000: 2.230966250867531\n",
      "two-layer Loss after iteration 92000: 2.230167436475883\n",
      "two-layer Loss after iteration 93000: 2.2294093939069493\n",
      "two-layer Loss after iteration 94000: 2.2286889750970658\n",
      "two-layer Loss after iteration 95000: 2.2280035144567942\n",
      "two-layer Loss after iteration 96000: 2.227350707503376\n",
      "two-layer Loss after iteration 97000: 2.2267285296380095\n",
      "two-layer Loss after iteration 98000: 2.2261351789540007\n",
      "two-layer Loss after iteration 99000: 2.225569034351654\n",
      "two-layer Loss after iteration 100000: 2.2239848883327866\n",
      "two-layer Loss after iteration 101000: 2.2231596233671636\n",
      "two-layer Loss after iteration 102000: 2.2224028107866816\n",
      "two-layer Loss after iteration 103000: 2.2217053313829576\n",
      "two-layer Loss after iteration 104000: 2.2210574387539967\n",
      "two-layer Loss after iteration 105000: 2.2204503216801204\n",
      "two-layer Loss after iteration 106000: 2.2198813173278675\n",
      "two-layer Loss after iteration 107000: 2.2193442910181855\n",
      "two-layer Loss after iteration 108000: 2.2180226319255723\n",
      "two-layer Loss after iteration 109000: 2.2167180015701424\n",
      "two-layer Loss after iteration 110000: 2.2156314473676155\n",
      "two-layer Loss after iteration 111000: 2.214696225204684\n",
      "two-layer Loss after iteration 112000: 2.2138668701512136\n",
      "two-layer Loss after iteration 113000: 2.2131170279197816\n",
      "two-layer Loss after iteration 114000: 2.212427871163191\n",
      "two-layer Loss after iteration 115000: 2.211786152375964\n",
      "two-layer Loss after iteration 116000: 2.2111774851089847\n",
      "two-layer Loss after iteration 117000: 2.210594855105675\n",
      "two-layer Loss after iteration 118000: 2.2100363381797457\n",
      "two-layer Loss after iteration 119000: 2.209500431617045\n",
      "two-layer Loss after iteration 120000: 2.2089866379471426\n",
      "two-layer Loss after iteration 121000: 2.2084988603829028\n",
      "two-layer Loss after iteration 122000: 2.20803487056048\n",
      "two-layer Loss after iteration 123000: 2.207594700842234\n",
      "two-layer Loss after iteration 124000: 2.207186797222496\n",
      "two-layer Loss after iteration 125000: 2.2068064945742596\n",
      "two-layer Loss after iteration 126000: 2.206448915313334\n",
      "two-layer Loss after iteration 127000: 2.2061119001439113\n",
      "two-layer Loss after iteration 128000: 2.2057926056943264\n",
      "two-layer Loss after iteration 129000: 2.205491701617149\n",
      "two-layer Loss after iteration 130000: 2.2052067888544835\n",
      "two-layer Loss after iteration 131000: 2.204937119375749\n",
      "two-layer Loss after iteration 132000: 2.2046825646790076\n",
      "two-layer Loss after iteration 133000: 2.2044409993197616\n",
      "two-layer Loss after iteration 134000: 2.2042125005112605\n",
      "two-layer Loss after iteration 135000: 2.2039962032249174\n",
      "9.812905347960907e-05 2.2042125005112605 2.2039962032249174\n",
      "two-layer Loss after iteration 0: 1435.6524411596197\n",
      "two-layer Loss after iteration 1000: 22.992826092320826\n",
      "two-layer Loss after iteration 2000: 8.943273079374134\n",
      "two-layer Loss after iteration 3000: 6.240425052921712\n",
      "two-layer Loss after iteration 4000: 5.579517095190638\n",
      "two-layer Loss after iteration 5000: 5.156236208559862\n",
      "two-layer Loss after iteration 6000: 4.679473201188762\n",
      "two-layer Loss after iteration 7000: 4.424729963658661\n",
      "two-layer Loss after iteration 8000: 4.247123811084403\n",
      "two-layer Loss after iteration 9000: 4.121538087224963\n",
      "two-layer Loss after iteration 10000: 4.018647720732446\n",
      "two-layer Loss after iteration 11000: 3.9446839380417567\n",
      "two-layer Loss after iteration 12000: 3.8866793776234068\n",
      "two-layer Loss after iteration 13000: 3.8082604937721083\n",
      "two-layer Loss after iteration 14000: 3.7319478761240954\n",
      "two-layer Loss after iteration 15000: 3.683544534618544\n",
      "two-layer Loss after iteration 16000: 3.6260130412418534\n",
      "two-layer Loss after iteration 17000: 3.5870698597336124\n",
      "two-layer Loss after iteration 18000: 3.5504462875785223\n",
      "two-layer Loss after iteration 19000: 3.5211424794507202\n",
      "two-layer Loss after iteration 20000: 3.496508510054531\n",
      "two-layer Loss after iteration 21000: 3.475309318539421\n",
      "two-layer Loss after iteration 22000: 3.4568173828676088\n",
      "two-layer Loss after iteration 23000: 3.434541439624412\n",
      "two-layer Loss after iteration 24000: 3.4136694841975066\n",
      "two-layer Loss after iteration 25000: 3.392820322202823\n",
      "two-layer Loss after iteration 26000: 3.3748299450508106\n",
      "two-layer Loss after iteration 27000: 3.3596781049343227\n",
      "two-layer Loss after iteration 28000: 3.339956936467241\n",
      "two-layer Loss after iteration 29000: 3.324904663529758\n",
      "two-layer Loss after iteration 30000: 3.312413880811549\n",
      "two-layer Loss after iteration 31000: 3.300135256318775\n",
      "two-layer Loss after iteration 32000: 3.2851858081999166\n",
      "two-layer Loss after iteration 33000: 3.2732683877789794\n",
      "two-layer Loss after iteration 34000: 3.2628224270789885\n",
      "two-layer Loss after iteration 35000: 3.2495743198092684\n",
      "two-layer Loss after iteration 36000: 3.2232462345938893\n",
      "two-layer Loss after iteration 37000: 3.2117015001312135\n",
      "two-layer Loss after iteration 38000: 3.2037886484937195\n",
      "two-layer Loss after iteration 39000: 3.1972405118535603\n",
      "two-layer Loss after iteration 40000: 3.1915468055037852\n",
      "two-layer Loss after iteration 41000: 3.1865349870848676\n",
      "two-layer Loss after iteration 42000: 3.1821058070649895\n",
      "two-layer Loss after iteration 43000: 3.15232081782591\n",
      "two-layer Loss after iteration 44000: 3.141626754053076\n",
      "two-layer Loss after iteration 45000: 3.1325883123438607\n",
      "two-layer Loss after iteration 46000: 3.1242250656475665\n",
      "two-layer Loss after iteration 47000: 3.1147638615285778\n",
      "two-layer Loss after iteration 48000: 3.0913314231933335\n",
      "two-layer Loss after iteration 49000: 3.0788927949061904\n",
      "two-layer Loss after iteration 50000: 3.0695285961713874\n",
      "two-layer Loss after iteration 51000: 3.0594861777123037\n",
      "two-layer Loss after iteration 52000: 3.049923209346169\n",
      "two-layer Loss after iteration 53000: 3.0216816346908613\n",
      "two-layer Loss after iteration 54000: 3.0071642378429138\n",
      "two-layer Loss after iteration 55000: 2.995618469274239\n",
      "two-layer Loss after iteration 56000: 2.985973447532059\n",
      "two-layer Loss after iteration 57000: 2.978669281318623\n",
      "two-layer Loss after iteration 58000: 2.972905700205956\n",
      "two-layer Loss after iteration 59000: 2.9680748699833446\n",
      "two-layer Loss after iteration 60000: 2.962908788787408\n",
      "two-layer Loss after iteration 61000: 2.9574232797116515\n",
      "two-layer Loss after iteration 62000: 2.9520664952417968\n",
      "two-layer Loss after iteration 63000: 2.948099704252105\n",
      "two-layer Loss after iteration 64000: 2.940644762134445\n",
      "two-layer Loss after iteration 65000: 2.930331535069975\n",
      "two-layer Loss after iteration 66000: 2.921963542302806\n",
      "two-layer Loss after iteration 67000: 2.9150647185394574\n",
      "two-layer Loss after iteration 68000: 2.909200374158345\n",
      "two-layer Loss after iteration 69000: 2.9043650178917284\n",
      "two-layer Loss after iteration 70000: 2.9001715390707052\n",
      "two-layer Loss after iteration 71000: 2.896407489082707\n",
      "two-layer Loss after iteration 72000: 2.8925215038611616\n",
      "two-layer Loss after iteration 73000: 2.8891072455252034\n",
      "two-layer Loss after iteration 74000: 2.8820433771900924\n",
      "two-layer Loss after iteration 75000: 2.877033605026917\n",
      "two-layer Loss after iteration 76000: 2.873056540343244\n",
      "two-layer Loss after iteration 77000: 2.8618164216543716\n",
      "two-layer Loss after iteration 78000: 2.8558889660882585\n",
      "two-layer Loss after iteration 79000: 2.8519582243793278\n",
      "two-layer Loss after iteration 80000: 2.848961703874597\n",
      "two-layer Loss after iteration 81000: 2.8465759210350674\n",
      "two-layer Loss after iteration 82000: 2.8448849287245923\n",
      "two-layer Loss after iteration 83000: 2.843717176630098\n",
      "two-layer Loss after iteration 84000: 2.839853246202644\n",
      "two-layer Loss after iteration 85000: 2.8378811812755376\n",
      "two-layer Loss after iteration 86000: 2.8361370412708284\n",
      "two-layer Loss after iteration 87000: 2.8345755504949164\n",
      "two-layer Loss after iteration 88000: 2.833173879508921\n",
      "two-layer Loss after iteration 89000: 2.831917999644179\n",
      "two-layer Loss after iteration 90000: 2.823738755019377\n",
      "two-layer Loss after iteration 91000: 2.8200110792981468\n",
      "two-layer Loss after iteration 92000: 2.8184018952942966\n",
      "two-layer Loss after iteration 93000: 2.8171192070674635\n",
      "two-layer Loss after iteration 94000: 2.8160192333133747\n",
      "two-layer Loss after iteration 95000: 2.815050890930245\n",
      "two-layer Loss after iteration 96000: 2.814210433102708\n",
      "two-layer Loss after iteration 97000: 2.813471358405662\n",
      "two-layer Loss after iteration 98000: 2.812820834080294\n",
      "two-layer Loss after iteration 99000: 2.812258397845786\n",
      "two-layer Loss after iteration 100000: 2.8117618904284907\n",
      "two-layer Loss after iteration 101000: 2.8113285601808244\n",
      "two-layer Loss after iteration 102000: 2.8109434017135166\n",
      "two-layer Loss after iteration 103000: 2.8106158605100484\n",
      "two-layer Loss after iteration 104000: 2.8103201097787323\n",
      "two-layer Loss after iteration 105000: 2.8100643652317117\n",
      "9.100192754936143e-05 2.8103201097787323 2.8100643652317117\n",
      "two-layer Loss after iteration 0: 1608.4074316431174\n",
      "two-layer Loss after iteration 1000: 23.775860756798778\n",
      "two-layer Loss after iteration 2000: 9.889233991935184\n",
      "two-layer Loss after iteration 3000: 8.169731778723207\n",
      "two-layer Loss after iteration 4000: 7.31431570629516\n",
      "two-layer Loss after iteration 5000: 6.615468575220544\n",
      "two-layer Loss after iteration 6000: 5.969441146382475\n",
      "two-layer Loss after iteration 7000: 5.564961271636721\n",
      "two-layer Loss after iteration 8000: 5.286469094876748\n",
      "two-layer Loss after iteration 9000: 5.066484656156808\n",
      "two-layer Loss after iteration 10000: 4.882676413839155\n",
      "two-layer Loss after iteration 11000: 4.750626979628208\n",
      "two-layer Loss after iteration 12000: 4.643205320351036\n",
      "two-layer Loss after iteration 13000: 4.540843503353404\n",
      "two-layer Loss after iteration 14000: 4.456123152100283\n",
      "two-layer Loss after iteration 15000: 4.391407482067639\n",
      "two-layer Loss after iteration 16000: 4.263734915225659\n",
      "two-layer Loss after iteration 17000: 4.186995654792437\n",
      "two-layer Loss after iteration 18000: 4.123408472374805\n",
      "two-layer Loss after iteration 19000: 4.070438732202691\n",
      "two-layer Loss after iteration 20000: 4.027367006682363\n",
      "two-layer Loss after iteration 21000: 3.994236093855113\n",
      "two-layer Loss after iteration 22000: 3.968519736352875\n",
      "two-layer Loss after iteration 23000: 3.9384358578942353\n",
      "two-layer Loss after iteration 24000: 3.915287803372041\n",
      "two-layer Loss after iteration 25000: 3.8973207092998017\n",
      "two-layer Loss after iteration 26000: 3.8809026299986504\n",
      "two-layer Loss after iteration 27000: 3.8671561787522064\n",
      "two-layer Loss after iteration 28000: 3.853409383779808\n",
      "two-layer Loss after iteration 29000: 3.8428993192925636\n",
      "two-layer Loss after iteration 30000: 3.807537541060426\n",
      "two-layer Loss after iteration 31000: 3.771418060560837\n",
      "two-layer Loss after iteration 32000: 3.731301593113339\n",
      "two-layer Loss after iteration 33000: 3.671088970144655\n",
      "two-layer Loss after iteration 34000: 3.612754924150129\n",
      "two-layer Loss after iteration 35000: 3.5822413752686835\n",
      "two-layer Loss after iteration 36000: 3.560354025825332\n",
      "two-layer Loss after iteration 37000: 3.529286561550216\n",
      "two-layer Loss after iteration 38000: 3.51123421124766\n",
      "two-layer Loss after iteration 39000: 3.493601418764204\n",
      "two-layer Loss after iteration 40000: 3.4787503497697605\n",
      "two-layer Loss after iteration 41000: 3.457219234320525\n",
      "two-layer Loss after iteration 42000: 3.333434572850338\n",
      "two-layer Loss after iteration 43000: 3.2466984832520893\n",
      "two-layer Loss after iteration 44000: 3.2220726415203407\n",
      "two-layer Loss after iteration 45000: 3.1905547053800345\n",
      "two-layer Loss after iteration 46000: 3.147153257172685\n",
      "two-layer Loss after iteration 47000: 3.12380068834178\n",
      "two-layer Loss after iteration 48000: 3.109223658588755\n",
      "two-layer Loss after iteration 49000: 3.0740437702901637\n",
      "two-layer Loss after iteration 50000: 3.057465996826192\n",
      "two-layer Loss after iteration 51000: 3.04727515656652\n",
      "two-layer Loss after iteration 52000: 3.038109165138214\n",
      "two-layer Loss after iteration 53000: 3.027774046062643\n",
      "two-layer Loss after iteration 54000: 3.0193226096474612\n",
      "two-layer Loss after iteration 55000: 3.0122163265095887\n",
      "two-layer Loss after iteration 56000: 3.00609797598149\n",
      "two-layer Loss after iteration 57000: 3.0007946762079034\n",
      "two-layer Loss after iteration 58000: 2.9961903488145354\n",
      "two-layer Loss after iteration 59000: 2.992987919167837\n",
      "two-layer Loss after iteration 60000: 2.9905823541667975\n",
      "two-layer Loss after iteration 61000: 2.9884878403095207\n",
      "two-layer Loss after iteration 62000: 2.9867003899107663\n",
      "two-layer Loss after iteration 63000: 2.985094327582545\n",
      "two-layer Loss after iteration 64000: 2.983696142125088\n",
      "two-layer Loss after iteration 65000: 2.982529044271987\n",
      "two-layer Loss after iteration 66000: 2.981539910233079\n",
      "two-layer Loss after iteration 67000: 2.980658293867283\n",
      "two-layer Loss after iteration 68000: 2.9798680063920497\n",
      "two-layer Loss after iteration 69000: 2.979341293539025\n",
      "two-layer Loss after iteration 70000: 2.9790051736960583\n",
      "two-layer Loss after iteration 71000: 2.9787000272944533\n",
      "two-layer Loss after iteration 72000: 2.978429878400804\n",
      "9.069355462919102e-05 2.9787000272944533 2.978429878400804\n",
      "two-layer Loss after iteration 0: 1603.8304800855176\n",
      "two-layer Loss after iteration 1000: 25.167675858332675\n",
      "two-layer Loss after iteration 2000: 10.605854264905808\n",
      "two-layer Loss after iteration 3000: 8.39549987202847\n",
      "two-layer Loss after iteration 4000: 7.717540443960845\n",
      "two-layer Loss after iteration 5000: 7.24265884375038\n",
      "two-layer Loss after iteration 6000: 6.053534540906021\n",
      "two-layer Loss after iteration 7000: 5.130048319873594\n",
      "two-layer Loss after iteration 8000: 4.966440655687121\n",
      "two-layer Loss after iteration 9000: 4.836860786513334\n",
      "two-layer Loss after iteration 10000: 4.735746210086996\n",
      "two-layer Loss after iteration 11000: 4.627994367983405\n",
      "two-layer Loss after iteration 12000: 4.482382717901721\n",
      "two-layer Loss after iteration 13000: 4.318547953971066\n",
      "two-layer Loss after iteration 14000: 4.270001451810919\n",
      "two-layer Loss after iteration 15000: 4.233075583289852\n",
      "two-layer Loss after iteration 16000: 4.204745671953416\n",
      "two-layer Loss after iteration 17000: 4.181378242716325\n",
      "two-layer Loss after iteration 18000: 4.161331935676888\n",
      "two-layer Loss after iteration 19000: 4.143688619999281\n",
      "two-layer Loss after iteration 20000: 4.12778566383602\n",
      "two-layer Loss after iteration 21000: 4.113266724062301\n",
      "two-layer Loss after iteration 22000: 4.0999321365400725\n",
      "two-layer Loss after iteration 23000: 4.08610291605424\n",
      "two-layer Loss after iteration 24000: 4.073307721131694\n",
      "two-layer Loss after iteration 25000: 4.055069435896206\n",
      "two-layer Loss after iteration 26000: 4.044325477559812\n",
      "two-layer Loss after iteration 27000: 4.03504360136393\n",
      "two-layer Loss after iteration 28000: 4.026741192710735\n",
      "two-layer Loss after iteration 29000: 4.019209559456607\n",
      "two-layer Loss after iteration 30000: 4.012348164600539\n",
      "two-layer Loss after iteration 31000: 4.006078340406107\n",
      "two-layer Loss after iteration 32000: 4.000335601911639\n",
      "two-layer Loss after iteration 33000: 3.994012756000862\n",
      "two-layer Loss after iteration 34000: 3.987845606345826\n",
      "two-layer Loss after iteration 35000: 3.9798696883274256\n",
      "two-layer Loss after iteration 36000: 3.972655007488731\n",
      "two-layer Loss after iteration 37000: 3.967006153744269\n",
      "two-layer Loss after iteration 38000: 3.9624193510262837\n",
      "two-layer Loss after iteration 39000: 3.9583165164570824\n",
      "two-layer Loss after iteration 40000: 3.954618393582174\n",
      "two-layer Loss after iteration 41000: 3.9278670681065138\n",
      "two-layer Loss after iteration 42000: 3.9110256270018717\n",
      "two-layer Loss after iteration 43000: 3.9015663972917336\n",
      "two-layer Loss after iteration 44000: 3.892769373648416\n",
      "two-layer Loss after iteration 45000: 3.883803648674874\n",
      "two-layer Loss after iteration 46000: 3.8756933289914195\n",
      "two-layer Loss after iteration 47000: 3.868321075340479\n",
      "two-layer Loss after iteration 48000: 3.8625938013666654\n",
      "two-layer Loss after iteration 49000: 3.858420424800677\n",
      "two-layer Loss after iteration 50000: 3.854728906723762\n",
      "two-layer Loss after iteration 51000: 3.851486522287861\n",
      "two-layer Loss after iteration 52000: 3.8484028522540066\n",
      "two-layer Loss after iteration 53000: 3.8024408557124896\n",
      "two-layer Loss after iteration 54000: 3.7981399214138447\n",
      "two-layer Loss after iteration 55000: 3.789558686673578\n",
      "two-layer Loss after iteration 56000: 3.7854442053713404\n",
      "two-layer Loss after iteration 57000: 3.782056666700124\n",
      "two-layer Loss after iteration 58000: 3.777000846247458\n",
      "two-layer Loss after iteration 59000: 3.7667855964843002\n",
      "two-layer Loss after iteration 60000: 3.763145764446723\n",
      "two-layer Loss after iteration 61000: 3.7600425910240047\n",
      "two-layer Loss after iteration 62000: 3.757247789029901\n",
      "two-layer Loss after iteration 63000: 3.754334220553309\n",
      "two-layer Loss after iteration 64000: 3.7509242071638105\n",
      "two-layer Loss after iteration 65000: 3.7478019533912255\n",
      "two-layer Loss after iteration 66000: 3.7449106644234154\n",
      "two-layer Loss after iteration 67000: 3.74129843268269\n",
      "two-layer Loss after iteration 68000: 3.7370924965297783\n",
      "two-layer Loss after iteration 69000: 3.7332714494660766\n",
      "two-layer Loss after iteration 70000: 3.7304009050196987\n",
      "two-layer Loss after iteration 71000: 3.727979543078988\n",
      "two-layer Loss after iteration 72000: 3.725852950226693\n",
      "two-layer Loss after iteration 73000: 3.724558653793836\n",
      "two-layer Loss after iteration 74000: 3.724027721142689\n",
      "two-layer Loss after iteration 75000: 3.72359462919748\n",
      "two-layer Loss after iteration 76000: 3.7232280456909344\n",
      "9.844882245542498e-05 3.72359462919748 3.7232280456909344\n",
      "two-layer Loss after iteration 0: 1652.7772546416638\n",
      "two-layer Loss after iteration 1000: 23.1589186224879\n",
      "two-layer Loss after iteration 2000: 9.348231525202532\n",
      "two-layer Loss after iteration 3000: 7.048213478322217\n",
      "two-layer Loss after iteration 4000: 6.1030217921384775\n",
      "two-layer Loss after iteration 5000: 5.754363375005687\n",
      "two-layer Loss after iteration 6000: 5.517956432738917\n",
      "two-layer Loss after iteration 7000: 5.061718935655858\n",
      "two-layer Loss after iteration 8000: 4.688195411147154\n",
      "two-layer Loss after iteration 9000: 4.517742163812531\n",
      "two-layer Loss after iteration 10000: 4.381999417878663\n",
      "two-layer Loss after iteration 11000: 4.230203926508716\n",
      "two-layer Loss after iteration 12000: 4.119752661565675\n",
      "two-layer Loss after iteration 13000: 3.9892264574613443\n",
      "two-layer Loss after iteration 14000: 3.8648971124013722\n",
      "two-layer Loss after iteration 15000: 3.8107455182501817\n",
      "two-layer Loss after iteration 16000: 3.7746807154810216\n",
      "two-layer Loss after iteration 17000: 3.7545778369738896\n",
      "two-layer Loss after iteration 18000: 3.7186605123532503\n",
      "two-layer Loss after iteration 19000: 3.68805871494615\n",
      "two-layer Loss after iteration 20000: 3.6690152113877925\n",
      "two-layer Loss after iteration 21000: 3.6555986343649187\n",
      "two-layer Loss after iteration 22000: 3.646041787245502\n",
      "two-layer Loss after iteration 23000: 3.636073087717662\n",
      "two-layer Loss after iteration 24000: 3.6173267470091246\n",
      "two-layer Loss after iteration 25000: 3.6095961193178643\n",
      "two-layer Loss after iteration 26000: 3.6026955254431807\n",
      "two-layer Loss after iteration 27000: 3.5786397985083105\n",
      "two-layer Loss after iteration 28000: 3.5683896313052816\n",
      "two-layer Loss after iteration 29000: 3.5601038983798023\n",
      "two-layer Loss after iteration 30000: 3.5529914043314443\n",
      "two-layer Loss after iteration 31000: 3.5467387639251715\n",
      "two-layer Loss after iteration 32000: 3.5411663579259423\n",
      "two-layer Loss after iteration 33000: 3.5345671113722736\n",
      "two-layer Loss after iteration 34000: 3.5275732770351977\n",
      "two-layer Loss after iteration 35000: 3.521049453794142\n",
      "two-layer Loss after iteration 36000: 3.51500277582327\n",
      "two-layer Loss after iteration 37000: 3.5096865924249188\n",
      "two-layer Loss after iteration 38000: 3.5049548498360763\n",
      "two-layer Loss after iteration 39000: 3.500178989757648\n",
      "two-layer Loss after iteration 40000: 3.495451855490354\n",
      "two-layer Loss after iteration 41000: 3.491563132347367\n",
      "two-layer Loss after iteration 42000: 3.488282834306874\n",
      "two-layer Loss after iteration 43000: 3.485453478455144\n",
      "two-layer Loss after iteration 44000: 3.48294575913793\n",
      "two-layer Loss after iteration 45000: 3.48073395700506\n",
      "two-layer Loss after iteration 46000: 3.478767069810577\n",
      "two-layer Loss after iteration 47000: 3.4769006700145795\n",
      "two-layer Loss after iteration 48000: 3.4737691858903337\n",
      "two-layer Loss after iteration 49000: 3.4713671144484133\n",
      "two-layer Loss after iteration 50000: 3.4694063505611785\n",
      "two-layer Loss after iteration 51000: 3.467758937357385\n",
      "two-layer Loss after iteration 52000: 3.4663347573751575\n",
      "two-layer Loss after iteration 53000: 3.4651052840815475\n",
      "two-layer Loss after iteration 54000: 3.464020353224941\n",
      "two-layer Loss after iteration 55000: 3.463079107947557\n",
      "two-layer Loss after iteration 56000: 3.4622685493298975\n",
      "two-layer Loss after iteration 57000: 3.4594750898119355\n",
      "two-layer Loss after iteration 58000: 3.456186967486882\n",
      "two-layer Loss after iteration 59000: 3.4539683886868877\n",
      "two-layer Loss after iteration 60000: 3.4522544721019526\n",
      "two-layer Loss after iteration 61000: 3.4508049974146786\n",
      "two-layer Loss after iteration 62000: 3.449600883867013\n",
      "two-layer Loss after iteration 63000: 3.448541895958937\n",
      "two-layer Loss after iteration 64000: 3.447656025034985\n",
      "two-layer Loss after iteration 65000: 3.4468282297554964\n",
      "two-layer Loss after iteration 66000: 3.4461217853442627\n",
      "two-layer Loss after iteration 67000: 3.4455011447396253\n",
      "two-layer Loss after iteration 68000: 3.4449436093867356\n",
      "two-layer Loss after iteration 69000: 3.4427846982114376\n",
      "two-layer Loss after iteration 70000: 3.437373431120726\n",
      "two-layer Loss after iteration 71000: 3.4363973661259695\n",
      "two-layer Loss after iteration 72000: 3.435566755413321\n",
      "two-layer Loss after iteration 73000: 3.4348919995341607\n",
      "two-layer Loss after iteration 74000: 3.434337982935397\n",
      "two-layer Loss after iteration 75000: 3.4321779803469883\n",
      "two-layer Loss after iteration 76000: 3.4312929773606267\n",
      "two-layer Loss after iteration 77000: 3.4306384926623954\n",
      "two-layer Loss after iteration 78000: 3.4300921860515086\n",
      "two-layer Loss after iteration 79000: 3.4296334145792833\n",
      "two-layer Loss after iteration 80000: 3.4286448397775926\n",
      "two-layer Loss after iteration 81000: 3.4250043822580767\n",
      "two-layer Loss after iteration 82000: 3.4225703643114955\n",
      "two-layer Loss after iteration 83000: 3.420720426304163\n",
      "two-layer Loss after iteration 84000: 3.4191927168033565\n",
      "two-layer Loss after iteration 85000: 3.417649427632687\n",
      "two-layer Loss after iteration 86000: 3.415928975531451\n",
      "two-layer Loss after iteration 87000: 3.414618778960045\n",
      "two-layer Loss after iteration 88000: 3.4134562980749372\n",
      "two-layer Loss after iteration 89000: 3.4086102100169446\n",
      "two-layer Loss after iteration 90000: 3.403586786603419\n",
      "two-layer Loss after iteration 91000: 3.3994518438920256\n",
      "two-layer Loss after iteration 92000: 3.395835655719732\n",
      "two-layer Loss after iteration 93000: 3.392820937076115\n",
      "two-layer Loss after iteration 94000: 3.390372099370428\n",
      "two-layer Loss after iteration 95000: 3.3882869800615114\n",
      "two-layer Loss after iteration 96000: 3.3835797235611937\n",
      "two-layer Loss after iteration 97000: 3.380702693643743\n",
      "two-layer Loss after iteration 98000: 3.3783843183305917\n",
      "two-layer Loss after iteration 99000: 3.375938124680258\n",
      "two-layer Loss after iteration 100000: 3.3734474666002985\n",
      "two-layer Loss after iteration 101000: 3.371417014305808\n",
      "two-layer Loss after iteration 102000: 3.369286889766888\n",
      "two-layer Loss after iteration 103000: 3.36753342695908\n",
      "two-layer Loss after iteration 104000: 3.3660288218002252\n",
      "two-layer Loss after iteration 105000: 3.3647882344538464\n",
      "two-layer Loss after iteration 106000: 3.3637226274000054\n",
      "two-layer Loss after iteration 107000: 3.3628124256050898\n",
      "two-layer Loss after iteration 108000: 3.3621084778954993\n",
      "two-layer Loss after iteration 109000: 3.361555370996874\n",
      "two-layer Loss after iteration 110000: 3.3610682948245354\n",
      "two-layer Loss after iteration 111000: 3.3606247638655824\n",
      "two-layer Loss after iteration 112000: 3.3602932439289983\n",
      "9.864830496659117e-05 3.3606247638655824 3.3602932439289983\n",
      "two-layer Loss after iteration 0: 1552.5300782311367\n",
      "two-layer Loss after iteration 1000: 23.646214406939073\n",
      "two-layer Loss after iteration 2000: 9.419564369865466\n",
      "two-layer Loss after iteration 3000: 7.569603795440206\n",
      "two-layer Loss after iteration 4000: 6.3466460021127675\n",
      "two-layer Loss after iteration 5000: 5.6943760046267515\n",
      "two-layer Loss after iteration 6000: 5.225574867521288\n",
      "two-layer Loss after iteration 7000: 5.028711760030097\n",
      "two-layer Loss after iteration 8000: 4.613508726815408\n",
      "two-layer Loss after iteration 9000: 4.3749196301158\n",
      "two-layer Loss after iteration 10000: 4.249696924196389\n",
      "two-layer Loss after iteration 11000: 4.125544490224331\n",
      "two-layer Loss after iteration 12000: 4.0401975151383605\n",
      "two-layer Loss after iteration 13000: 3.965448316317866\n",
      "two-layer Loss after iteration 14000: 3.881218080319412\n",
      "two-layer Loss after iteration 15000: 3.8158061674024957\n",
      "two-layer Loss after iteration 16000: 3.759243169609879\n",
      "two-layer Loss after iteration 17000: 3.7051069365211604\n",
      "two-layer Loss after iteration 18000: 3.644512414804373\n",
      "two-layer Loss after iteration 19000: 3.5971060012985694\n",
      "two-layer Loss after iteration 20000: 3.555225689464672\n",
      "two-layer Loss after iteration 21000: 3.5181247492676793\n",
      "two-layer Loss after iteration 22000: 3.4854951249345545\n",
      "two-layer Loss after iteration 23000: 3.4504100736239383\n",
      "two-layer Loss after iteration 24000: 3.413466290338879\n",
      "two-layer Loss after iteration 25000: 3.3805158795024055\n",
      "two-layer Loss after iteration 26000: 3.3543808592661835\n",
      "two-layer Loss after iteration 27000: 3.3306189738267684\n",
      "two-layer Loss after iteration 28000: 3.308713812870941\n",
      "two-layer Loss after iteration 29000: 3.289897103482598\n",
      "two-layer Loss after iteration 30000: 3.2745336749614\n",
      "two-layer Loss after iteration 31000: 3.261402948061363\n",
      "two-layer Loss after iteration 32000: 3.2500561006431656\n",
      "two-layer Loss after iteration 33000: 3.2397743336246934\n",
      "two-layer Loss after iteration 34000: 3.228578076716357\n",
      "two-layer Loss after iteration 35000: 3.2207374197684455\n",
      "two-layer Loss after iteration 36000: 3.2140935630060143\n",
      "two-layer Loss after iteration 37000: 3.1967046052489594\n",
      "two-layer Loss after iteration 38000: 3.1862501632062594\n",
      "two-layer Loss after iteration 39000: 3.177829174518359\n",
      "two-layer Loss after iteration 40000: 3.170491237152197\n",
      "two-layer Loss after iteration 41000: 3.1641347606576384\n",
      "two-layer Loss after iteration 42000: 3.158662583192824\n",
      "two-layer Loss after iteration 43000: 3.1538551689352845\n",
      "two-layer Loss after iteration 44000: 3.144711476015298\n",
      "two-layer Loss after iteration 45000: 3.1333430204680575\n",
      "two-layer Loss after iteration 46000: 3.121320621639163\n",
      "two-layer Loss after iteration 47000: 3.113911648852608\n",
      "two-layer Loss after iteration 48000: 3.107877933506856\n",
      "two-layer Loss after iteration 49000: 3.1026417059583333\n",
      "two-layer Loss after iteration 50000: 3.0981449413540405\n",
      "two-layer Loss after iteration 51000: 3.0943300609328173\n",
      "two-layer Loss after iteration 52000: 3.0676762704447214\n",
      "two-layer Loss after iteration 53000: 3.06118195354936\n",
      "two-layer Loss after iteration 54000: 3.0514774388200876\n",
      "two-layer Loss after iteration 55000: 3.0378800991775328\n",
      "two-layer Loss after iteration 56000: 3.031401205383787\n",
      "two-layer Loss after iteration 57000: 3.0245478814544153\n",
      "two-layer Loss after iteration 58000: 3.01965708769505\n",
      "two-layer Loss after iteration 59000: 3.0155785330001303\n",
      "two-layer Loss after iteration 60000: 3.0120140000191604\n",
      "two-layer Loss after iteration 61000: 3.008863141647629\n",
      "two-layer Loss after iteration 62000: 3.0060087175488843\n",
      "two-layer Loss after iteration 63000: 3.003473807543288\n",
      "two-layer Loss after iteration 64000: 3.0012214190617903\n",
      "two-layer Loss after iteration 65000: 2.9992168665678522\n",
      "two-layer Loss after iteration 66000: 2.997440891575366\n",
      "two-layer Loss after iteration 67000: 2.995863639697521\n",
      "two-layer Loss after iteration 68000: 2.9931229699932445\n",
      "two-layer Loss after iteration 69000: 2.9910606481836184\n",
      "two-layer Loss after iteration 70000: 2.9869969869253343\n",
      "two-layer Loss after iteration 71000: 2.9845932752916626\n",
      "two-layer Loss after iteration 72000: 2.982797498785466\n",
      "two-layer Loss after iteration 73000: 2.9814670009278172\n",
      "two-layer Loss after iteration 74000: 2.9804409699913483\n",
      "two-layer Loss after iteration 75000: 2.979638265885346\n",
      "two-layer Loss after iteration 76000: 2.978953610859705\n",
      "two-layer Loss after iteration 77000: 2.9783764422895738\n",
      "two-layer Loss after iteration 78000: 2.965889473768278\n",
      "two-layer Loss after iteration 79000: 2.9625633011742654\n",
      "two-layer Loss after iteration 80000: 2.9601701042237782\n",
      "two-layer Loss after iteration 81000: 2.958295021271964\n",
      "two-layer Loss after iteration 82000: 2.956790258362019\n",
      "two-layer Loss after iteration 83000: 2.9555399843850836\n",
      "two-layer Loss after iteration 84000: 2.9544970347896506\n",
      "two-layer Loss after iteration 85000: 2.9536184718200906\n",
      "two-layer Loss after iteration 86000: 2.952874481281153\n",
      "two-layer Loss after iteration 87000: 2.9522422526629404\n",
      "two-layer Loss after iteration 88000: 2.95170327329861\n",
      "two-layer Loss after iteration 89000: 2.951242402128359\n",
      "two-layer Loss after iteration 90000: 2.950847201931544\n",
      "two-layer Loss after iteration 91000: 2.950507416544598\n",
      "two-layer Loss after iteration 92000: 2.950214555469808\n",
      "9.925786769689669e-05 2.950507416544598 2.950214555469808\n",
      "two-layer Loss after iteration 0: 1458.4104752617764\n",
      "two-layer Loss after iteration 1000: 24.112999667014076\n",
      "two-layer Loss after iteration 2000: 9.51910511175928\n",
      "two-layer Loss after iteration 3000: 7.220750027503202\n",
      "two-layer Loss after iteration 4000: 6.624261198355939\n",
      "two-layer Loss after iteration 5000: 6.252586182757326\n",
      "two-layer Loss after iteration 6000: 5.842725600814749\n",
      "two-layer Loss after iteration 7000: 5.666981318534796\n",
      "two-layer Loss after iteration 8000: 5.534587293120724\n",
      "two-layer Loss after iteration 9000: 5.375661801983672\n",
      "two-layer Loss after iteration 10000: 4.563704484400909\n",
      "two-layer Loss after iteration 11000: 4.3142488526387055\n",
      "two-layer Loss after iteration 12000: 3.9996942131598088\n",
      "two-layer Loss after iteration 13000: 3.889071110101594\n",
      "two-layer Loss after iteration 14000: 3.7996058022413406\n",
      "two-layer Loss after iteration 15000: 3.650674614837265\n",
      "two-layer Loss after iteration 16000: 3.549585707327357\n",
      "two-layer Loss after iteration 17000: 3.4760318284342064\n",
      "two-layer Loss after iteration 18000: 3.4070347714941978\n",
      "two-layer Loss after iteration 19000: 3.3516357516941646\n",
      "two-layer Loss after iteration 20000: 3.3047117485273896\n",
      "two-layer Loss after iteration 21000: 3.261491358018342\n",
      "two-layer Loss after iteration 22000: 3.2117228315306505\n",
      "two-layer Loss after iteration 23000: 3.149332023915866\n",
      "two-layer Loss after iteration 24000: 3.1063342295974645\n",
      "two-layer Loss after iteration 25000: 3.070846889247685\n",
      "two-layer Loss after iteration 26000: 3.0393682322100677\n",
      "two-layer Loss after iteration 27000: 3.013590191382377\n",
      "two-layer Loss after iteration 28000: 2.9967719833609268\n",
      "two-layer Loss after iteration 29000: 2.98388786539269\n",
      "two-layer Loss after iteration 30000: 2.9620283672304315\n",
      "two-layer Loss after iteration 31000: 2.9457819607939433\n",
      "two-layer Loss after iteration 32000: 2.925724034919924\n",
      "two-layer Loss after iteration 33000: 2.9152447085633875\n",
      "two-layer Loss after iteration 34000: 2.903418046869757\n",
      "two-layer Loss after iteration 35000: 2.893151719683538\n",
      "two-layer Loss after iteration 36000: 2.8810399331522802\n",
      "two-layer Loss after iteration 37000: 2.858260626534246\n",
      "two-layer Loss after iteration 38000: 2.813220803582984\n",
      "two-layer Loss after iteration 39000: 2.783545273509273\n",
      "two-layer Loss after iteration 40000: 2.7482561263507543\n",
      "two-layer Loss after iteration 41000: 2.7085854763937274\n",
      "two-layer Loss after iteration 42000: 2.6637125245289814\n",
      "two-layer Loss after iteration 43000: 2.6254952153163558\n",
      "two-layer Loss after iteration 44000: 2.581209681350245\n",
      "two-layer Loss after iteration 45000: 2.518570013915606\n",
      "two-layer Loss after iteration 46000: 2.461600525847684\n",
      "two-layer Loss after iteration 47000: 2.422436634295965\n",
      "two-layer Loss after iteration 48000: 2.3762030837748385\n",
      "two-layer Loss after iteration 49000: 2.3323508019502843\n",
      "two-layer Loss after iteration 50000: 2.2946616587983333\n",
      "two-layer Loss after iteration 51000: 2.253655881122391\n",
      "two-layer Loss after iteration 52000: 2.2239395336242187\n",
      "two-layer Loss after iteration 53000: 2.1960638002133233\n",
      "two-layer Loss after iteration 54000: 2.1691695113171705\n",
      "two-layer Loss after iteration 55000: 2.1425834356527513\n",
      "two-layer Loss after iteration 56000: 2.1179067200573254\n",
      "two-layer Loss after iteration 57000: 2.0957112863811536\n",
      "two-layer Loss after iteration 58000: 2.080481110163279\n",
      "two-layer Loss after iteration 59000: 2.0677221585503287\n",
      "two-layer Loss after iteration 60000: 2.057515812222015\n",
      "two-layer Loss after iteration 61000: 2.0494732809277667\n",
      "two-layer Loss after iteration 62000: 2.0430850037345447\n",
      "two-layer Loss after iteration 63000: 2.020190071660811\n",
      "two-layer Loss after iteration 64000: 2.005525365803865\n",
      "two-layer Loss after iteration 65000: 1.9921120622602067\n",
      "two-layer Loss after iteration 66000: 1.982317501821106\n",
      "two-layer Loss after iteration 67000: 1.9744698007370804\n",
      "two-layer Loss after iteration 68000: 1.9683479102668384\n",
      "two-layer Loss after iteration 69000: 1.963517571276717\n",
      "two-layer Loss after iteration 70000: 1.959736190889369\n",
      "two-layer Loss after iteration 71000: 1.9565796369905222\n",
      "two-layer Loss after iteration 72000: 1.9538885609099197\n",
      "two-layer Loss after iteration 73000: 1.9515940553480686\n",
      "two-layer Loss after iteration 74000: 1.9495942811044855\n",
      "two-layer Loss after iteration 75000: 1.9426291480840403\n",
      "two-layer Loss after iteration 76000: 1.939826055400981\n",
      "two-layer Loss after iteration 77000: 1.937448908258091\n",
      "two-layer Loss after iteration 78000: 1.935441609407667\n",
      "two-layer Loss after iteration 79000: 1.933943206702914\n",
      "two-layer Loss after iteration 80000: 1.9267068690932112\n",
      "two-layer Loss after iteration 81000: 1.9169917469586264\n",
      "two-layer Loss after iteration 82000: 1.9141884515893506\n",
      "two-layer Loss after iteration 83000: 1.912452382958317\n",
      "two-layer Loss after iteration 84000: 1.9113137233903499\n",
      "two-layer Loss after iteration 85000: 1.9105706915490015\n",
      "two-layer Loss after iteration 86000: 1.9099669922509446\n",
      "two-layer Loss after iteration 87000: 1.9095159897273728\n",
      "two-layer Loss after iteration 88000: 1.9091530313587024\n",
      "two-layer Loss after iteration 89000: 1.9088738737734279\n",
      "two-layer Loss after iteration 90000: 1.9086271663004095\n",
      "two-layer Loss after iteration 91000: 1.9084188875033692\n",
      "two-layer Loss after iteration 92000: 1.9082264774677682\n",
      "two-layer Loss after iteration 93000: 1.9080539186148608\n",
      "9.042891655938668e-05 1.9082264774677682 1.9080539186148608\n",
      "two-layer Loss after iteration 0: 1414.9982840159728\n",
      "two-layer Loss after iteration 1000: 23.704267638074146\n",
      "two-layer Loss after iteration 2000: 9.536065328282609\n",
      "two-layer Loss after iteration 3000: 7.729706950450323\n",
      "two-layer Loss after iteration 4000: 6.880968676062142\n",
      "two-layer Loss after iteration 5000: 6.389157148234439\n",
      "two-layer Loss after iteration 6000: 5.961354400330864\n",
      "two-layer Loss after iteration 7000: 5.671280581654151\n",
      "two-layer Loss after iteration 8000: 5.47773914911172\n",
      "two-layer Loss after iteration 9000: 5.34420429242636\n",
      "two-layer Loss after iteration 10000: 5.24643315819699\n",
      "two-layer Loss after iteration 11000: 5.166713890437576\n",
      "two-layer Loss after iteration 12000: 5.102272385088327\n",
      "two-layer Loss after iteration 13000: 5.040031676746478\n",
      "two-layer Loss after iteration 14000: 4.9843727439958405\n",
      "two-layer Loss after iteration 15000: 4.931124272920498\n",
      "two-layer Loss after iteration 16000: 4.8794904947135675\n",
      "two-layer Loss after iteration 17000: 4.79459955554145\n",
      "two-layer Loss after iteration 18000: 4.707239694355128\n",
      "two-layer Loss after iteration 19000: 4.622139522907256\n",
      "two-layer Loss after iteration 20000: 4.530269510762171\n",
      "two-layer Loss after iteration 21000: 4.446563093052968\n",
      "two-layer Loss after iteration 22000: 4.2431366343627195\n",
      "two-layer Loss after iteration 23000: 4.150182962612424\n",
      "two-layer Loss after iteration 24000: 4.110325429237894\n",
      "two-layer Loss after iteration 25000: 4.089447165779806\n",
      "two-layer Loss after iteration 26000: 4.070201617440172\n",
      "two-layer Loss after iteration 27000: 4.052675402287097\n",
      "two-layer Loss after iteration 28000: 4.038134402933349\n",
      "two-layer Loss after iteration 29000: 4.0252588973785475\n",
      "two-layer Loss after iteration 30000: 4.014124550488094\n",
      "two-layer Loss after iteration 31000: 4.004765493133655\n",
      "two-layer Loss after iteration 32000: 3.996807902445104\n",
      "two-layer Loss after iteration 33000: 3.9888524070312386\n",
      "two-layer Loss after iteration 34000: 3.982388652643577\n",
      "two-layer Loss after iteration 35000: 3.9759170970894955\n",
      "two-layer Loss after iteration 36000: 3.9703098754069823\n",
      "two-layer Loss after iteration 37000: 3.94865487252555\n",
      "two-layer Loss after iteration 38000: 3.927820164403709\n",
      "two-layer Loss after iteration 39000: 3.91752520725713\n",
      "two-layer Loss after iteration 40000: 3.9033415837619336\n",
      "two-layer Loss after iteration 41000: 3.87416200903853\n",
      "two-layer Loss after iteration 42000: 3.8555980790959676\n",
      "two-layer Loss after iteration 43000: 3.8419534187717836\n",
      "two-layer Loss after iteration 44000: 3.832231433114148\n",
      "two-layer Loss after iteration 45000: 3.8255089822347896\n",
      "two-layer Loss after iteration 46000: 3.8200598609188354\n",
      "two-layer Loss after iteration 47000: 3.8151585722289925\n",
      "two-layer Loss after iteration 48000: 3.8106518026837017\n",
      "two-layer Loss after iteration 49000: 3.806614813246872\n",
      "two-layer Loss after iteration 50000: 3.8027314792068903\n",
      "two-layer Loss after iteration 51000: 3.7993720439914562\n",
      "two-layer Loss after iteration 52000: 3.793854151423877\n",
      "two-layer Loss after iteration 53000: 3.7902956484903685\n",
      "two-layer Loss after iteration 54000: 3.7872778819946507\n",
      "two-layer Loss after iteration 55000: 3.784350855071653\n",
      "two-layer Loss after iteration 56000: 3.780695207406443\n",
      "two-layer Loss after iteration 57000: 3.77764640812396\n",
      "two-layer Loss after iteration 58000: 3.7746430459487663\n",
      "two-layer Loss after iteration 59000: 3.7721115079517102\n",
      "two-layer Loss after iteration 60000: 3.7694458023841473\n",
      "two-layer Loss after iteration 61000: 3.7670383723491905\n",
      "two-layer Loss after iteration 62000: 3.764559116710521\n",
      "two-layer Loss after iteration 63000: 3.7623357847144816\n",
      "two-layer Loss after iteration 64000: 3.760128529886156\n",
      "two-layer Loss after iteration 65000: 3.757998270282728\n",
      "two-layer Loss after iteration 66000: 3.755652440618508\n",
      "two-layer Loss after iteration 67000: 3.7535506715102054\n",
      "two-layer Loss after iteration 68000: 3.751558418938078\n",
      "two-layer Loss after iteration 69000: 3.7495048158948703\n",
      "two-layer Loss after iteration 70000: 3.7476293244634276\n",
      "two-layer Loss after iteration 71000: 3.7457623388261747\n",
      "two-layer Loss after iteration 72000: 3.7440062532903475\n",
      "two-layer Loss after iteration 73000: 3.7422592041437253\n",
      "two-layer Loss after iteration 74000: 3.740555886356722\n",
      "two-layer Loss after iteration 75000: 3.7388874453409717\n",
      "two-layer Loss after iteration 76000: 3.7372702750666043\n",
      "two-layer Loss after iteration 77000: 3.7356892002691917\n",
      "two-layer Loss after iteration 78000: 3.734148011436255\n",
      "two-layer Loss after iteration 79000: 3.7326407385788563\n",
      "two-layer Loss after iteration 80000: 3.7311678068145264\n",
      "two-layer Loss after iteration 81000: 3.729726741687775\n",
      "two-layer Loss after iteration 82000: 3.72831224906691\n",
      "two-layer Loss after iteration 83000: 3.726939285674575\n",
      "two-layer Loss after iteration 84000: 3.725574534378256\n",
      "two-layer Loss after iteration 85000: 3.724251572829456\n",
      "two-layer Loss after iteration 86000: 3.722953506304201\n",
      "two-layer Loss after iteration 87000: 3.7216810640675573\n",
      "two-layer Loss after iteration 88000: 3.7204572239781677\n",
      "two-layer Loss after iteration 89000: 3.7192219237635284\n",
      "two-layer Loss after iteration 90000: 3.718043665617921\n",
      "two-layer Loss after iteration 91000: 3.7168896597120833\n",
      "two-layer Loss after iteration 92000: 3.7157388843817563\n",
      "two-layer Loss after iteration 93000: 3.7146178723032066\n",
      "two-layer Loss after iteration 94000: 3.713522612340397\n",
      "two-layer Loss after iteration 95000: 3.7124548558907677\n",
      "two-layer Loss after iteration 96000: 3.7114088754710832\n",
      "two-layer Loss after iteration 97000: 3.7103831770354505\n",
      "two-layer Loss after iteration 98000: 3.709390456405687\n",
      "two-layer Loss after iteration 99000: 3.7084022575552646\n",
      "two-layer Loss after iteration 100000: 3.7074640486912864\n",
      "two-layer Loss after iteration 101000: 3.7065281281122506\n",
      "two-layer Loss after iteration 102000: 3.705602745773734\n",
      "two-layer Loss after iteration 103000: 3.7046922136639355\n",
      "two-layer Loss after iteration 104000: 3.703818732175607\n",
      "two-layer Loss after iteration 105000: 3.70294633953212\n",
      "two-layer Loss after iteration 106000: 3.7021272827717304\n",
      "two-layer Loss after iteration 107000: 3.701298583090373\n",
      "two-layer Loss after iteration 108000: 3.7005053454930454\n",
      "two-layer Loss after iteration 109000: 3.6996833645482434\n",
      "two-layer Loss after iteration 110000: 3.6989137397398926\n",
      "two-layer Loss after iteration 111000: 3.6981711334205425\n",
      "two-layer Loss after iteration 112000: 3.6974335569165007\n",
      "two-layer Loss after iteration 113000: 3.696723102939663\n",
      "two-layer Loss after iteration 114000: 3.6959833007252\n",
      "two-layer Loss after iteration 115000: 3.695316503636877\n",
      "two-layer Loss after iteration 116000: 3.6946237721354005\n",
      "two-layer Loss after iteration 117000: 3.693943118450256\n",
      "two-layer Loss after iteration 118000: 3.693321677622984\n",
      "two-layer Loss after iteration 119000: 3.692682824678642\n",
      "two-layer Loss after iteration 120000: 3.6920646350811612\n",
      "two-layer Loss after iteration 121000: 3.691454564051944\n",
      "two-layer Loss after iteration 122000: 3.690853763069711\n",
      "two-layer Loss after iteration 123000: 3.690278859524317\n",
      "two-layer Loss after iteration 124000: 3.6896758090046458\n",
      "two-layer Loss after iteration 125000: 3.6891280280194194\n",
      "two-layer Loss after iteration 126000: 3.688593479177406\n",
      "two-layer Loss after iteration 127000: 3.6880397855180833\n",
      "two-layer Loss after iteration 128000: 3.6875199530847693\n",
      "two-layer Loss after iteration 129000: 3.6870291290819557\n",
      "two-layer Loss after iteration 130000: 3.686499627755377\n",
      "two-layer Loss after iteration 131000: 3.686019636981383\n",
      "two-layer Loss after iteration 132000: 3.6855395388558896\n",
      "two-layer Loss after iteration 133000: 3.685072216072978\n",
      "two-layer Loss after iteration 134000: 3.6846372047263745\n",
      "two-layer Loss after iteration 135000: 3.68417019172979\n",
      "two-layer Loss after iteration 136000: 3.6837409091643694\n",
      "two-layer Loss after iteration 137000: 3.6833132143405147\n",
      "two-layer Loss after iteration 138000: 3.682871344198831\n",
      "two-layer Loss after iteration 139000: 3.6824562447868554\n",
      "two-layer Loss after iteration 140000: 3.6820618833921936\n",
      "two-layer Loss after iteration 141000: 3.681667421201829\n",
      "two-layer Loss after iteration 142000: 3.6812720267566186\n",
      "two-layer Loss after iteration 143000: 3.6808957085745595\n",
      "two-layer Loss after iteration 144000: 3.6805388471412335\n",
      "9.694961813091513e-05 3.6808957085745595 3.6805388471412335\n",
      "two-layer Loss after iteration 0: 1547.035524536947\n",
      "two-layer Loss after iteration 1000: 22.898462186240955\n",
      "two-layer Loss after iteration 2000: 10.090590871901075\n",
      "two-layer Loss after iteration 3000: 8.176864891656683\n",
      "two-layer Loss after iteration 4000: 7.59733116056616\n",
      "two-layer Loss after iteration 5000: 6.976093713821018\n",
      "two-layer Loss after iteration 6000: 6.668647129711095\n",
      "two-layer Loss after iteration 7000: 6.429414012226618\n",
      "two-layer Loss after iteration 8000: 6.2374245841633345\n",
      "two-layer Loss after iteration 9000: 6.077560391518427\n",
      "two-layer Loss after iteration 10000: 5.945648725901952\n",
      "two-layer Loss after iteration 11000: 5.841292921218387\n",
      "two-layer Loss after iteration 12000: 5.749212840684146\n",
      "two-layer Loss after iteration 13000: 5.67355905075071\n",
      "two-layer Loss after iteration 14000: 5.611496040845124\n",
      "two-layer Loss after iteration 15000: 5.333493229313297\n",
      "two-layer Loss after iteration 16000: 4.847225523288721\n",
      "two-layer Loss after iteration 17000: 4.512864375242775\n",
      "two-layer Loss after iteration 18000: 4.35538802303687\n",
      "two-layer Loss after iteration 19000: 4.241265270315885\n",
      "two-layer Loss after iteration 20000: 4.110807248560609\n",
      "two-layer Loss after iteration 21000: 3.867830972229629\n",
      "two-layer Loss after iteration 22000: 3.751473525979108\n",
      "two-layer Loss after iteration 23000: 3.6656587518794015\n",
      "two-layer Loss after iteration 24000: 3.6010060170438325\n",
      "two-layer Loss after iteration 25000: 3.5536028211575226\n",
      "two-layer Loss after iteration 26000: 3.5195404760903544\n",
      "two-layer Loss after iteration 27000: 3.4927822842756817\n",
      "two-layer Loss after iteration 28000: 3.454153899000415\n",
      "two-layer Loss after iteration 29000: 3.4260719014817096\n",
      "two-layer Loss after iteration 30000: 3.4066067585749513\n",
      "two-layer Loss after iteration 31000: 3.3947970984664253\n",
      "two-layer Loss after iteration 32000: 3.3863927137832364\n",
      "two-layer Loss after iteration 33000: 3.380199200482381\n",
      "two-layer Loss after iteration 34000: 3.375584858038698\n",
      "two-layer Loss after iteration 35000: 3.3720627618933423\n",
      "two-layer Loss after iteration 36000: 3.3607159771450954\n",
      "two-layer Loss after iteration 37000: 3.3578519723590574\n",
      "two-layer Loss after iteration 38000: 3.3556622981374344\n",
      "two-layer Loss after iteration 39000: 3.353931740895024\n",
      "two-layer Loss after iteration 40000: 3.3525432954652494\n",
      "two-layer Loss after iteration 41000: 3.351410993123069\n",
      "two-layer Loss after iteration 42000: 3.350470540668549\n",
      "two-layer Loss after iteration 43000: 3.3496801655005717\n",
      "two-layer Loss after iteration 44000: 3.3490089453159397\n",
      "two-layer Loss after iteration 45000: 3.3484308520677493\n",
      "two-layer Loss after iteration 46000: 3.3479287554200505\n",
      "two-layer Loss after iteration 47000: 3.3474867004419924\n",
      "two-layer Loss after iteration 48000: 3.34709383733139\n",
      "two-layer Loss after iteration 49000: 3.346745141444481\n",
      "two-layer Loss after iteration 50000: 3.3464298792705462\n",
      "9.419963594794605e-05 3.346745141444481 3.3464298792705462\n",
      "two-layer Loss after iteration 0: 1587.1401848898063\n",
      "two-layer Loss after iteration 1000: 23.769921547716265\n",
      "two-layer Loss after iteration 2000: 9.308860701355052\n",
      "two-layer Loss after iteration 3000: 7.012078160645092\n",
      "two-layer Loss after iteration 4000: 5.595317607473333\n",
      "two-layer Loss after iteration 5000: 5.178870624553722\n",
      "two-layer Loss after iteration 6000: 4.9582011601871665\n",
      "two-layer Loss after iteration 7000: 4.514855097696813\n",
      "two-layer Loss after iteration 8000: 4.195774946231541\n",
      "two-layer Loss after iteration 9000: 3.997792830158587\n",
      "two-layer Loss after iteration 10000: 3.888590481782307\n",
      "two-layer Loss after iteration 11000: 3.809275074469213\n",
      "two-layer Loss after iteration 12000: 3.7583912137119366\n",
      "two-layer Loss after iteration 13000: 3.2851319431463355\n",
      "two-layer Loss after iteration 14000: 3.1182109852514452\n",
      "two-layer Loss after iteration 15000: 2.974914281501941\n",
      "two-layer Loss after iteration 16000: 2.905859884835658\n",
      "two-layer Loss after iteration 17000: 2.8541930628513748\n",
      "two-layer Loss after iteration 18000: 2.8114840792908478\n",
      "two-layer Loss after iteration 19000: 2.7798432290818424\n",
      "two-layer Loss after iteration 20000: 2.7317238524055947\n",
      "two-layer Loss after iteration 21000: 2.7109141496146245\n",
      "two-layer Loss after iteration 22000: 2.69735700725962\n",
      "two-layer Loss after iteration 23000: 2.687830347373017\n",
      "two-layer Loss after iteration 24000: 2.6809098193384107\n",
      "two-layer Loss after iteration 25000: 2.6763232131164214\n",
      "two-layer Loss after iteration 26000: 2.673525998749029\n",
      "two-layer Loss after iteration 27000: 2.671587405823679\n",
      "two-layer Loss after iteration 28000: 2.670067019000946\n",
      "two-layer Loss after iteration 29000: 2.668848206635239\n",
      "two-layer Loss after iteration 30000: 2.667856886699503\n",
      "two-layer Loss after iteration 31000: 2.667039894682616\n",
      "two-layer Loss after iteration 32000: 2.6663582637861722\n",
      "two-layer Loss after iteration 33000: 2.6657829760373755\n",
      "two-layer Loss after iteration 34000: 2.665292140633966\n",
      "two-layer Loss after iteration 35000: 2.664869060642967\n",
      "two-layer Loss after iteration 36000: 2.6620161455162012\n",
      "two-layer Loss after iteration 37000: 2.6604353791036814\n",
      "two-layer Loss after iteration 38000: 2.659394461267955\n",
      "two-layer Loss after iteration 39000: 2.658706822736474\n",
      "two-layer Loss after iteration 40000: 2.6582330191967007\n",
      "two-layer Loss after iteration 41000: 2.657878070994456\n",
      "two-layer Loss after iteration 42000: 2.657608750702875\n",
      "two-layer Loss after iteration 43000: 2.6574032589954517\n",
      "7.732203145745494e-05 2.657608750702875 2.6574032589954517\n",
      "three-layer Loss after iteration 0: 1589.049740355202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_20564\\2957246329.py:131: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three-layer Loss after iteration 1000: 32.38670983116243\n",
      "three-layer Loss after iteration 2000: 25.697860526653717\n",
      "three-layer Loss after iteration 3000: 19.068586828818404\n",
      "three-layer Loss after iteration 4000: 16.52403029705557\n",
      "three-layer Loss after iteration 5000: 15.588816642782776\n",
      "three-layer Loss after iteration 6000: 13.776926972332099\n",
      "three-layer Loss after iteration 7000: 12.294897314229425\n",
      "three-layer Loss after iteration 8000: 11.030194811758887\n",
      "three-layer Loss after iteration 9000: 12.160830222360406\n",
      "three-layer Loss after iteration 10000: 11.567142729281137\n",
      "three-layer Loss after iteration 11000: 12.25708801577535\n",
      "three-layer Loss after iteration 12000: 11.026018500539518\n",
      "three-layer Loss after iteration 13000: 11.820875760780764\n",
      "three-layer Loss after iteration 14000: 10.419347232596051\n",
      "three-layer Loss after iteration 15000: 13.320099643021365\n",
      "three-layer Loss after iteration 16000: 12.699503452822254\n",
      "three-layer Loss after iteration 17000: 11.625131531078784\n",
      "three-layer Loss after iteration 18000: 10.99633082069915\n",
      "three-layer Loss after iteration 19000: 10.403001401786115\n",
      "three-layer Loss after iteration 20000: 10.571338171768154\n",
      "three-layer Loss after iteration 21000: 12.099442997862948\n",
      "three-layer Loss after iteration 22000: 12.430823677149258\n",
      "three-layer Loss after iteration 23000: 8.022895487387554\n",
      "three-layer Loss after iteration 24000: 8.952645742709977\n",
      "three-layer Loss after iteration 25000: 9.697063700834182\n",
      "three-layer Loss after iteration 26000: 9.092602782633428\n",
      "three-layer Loss after iteration 27000: 8.984334483781614\n",
      "three-layer Loss after iteration 28000: 8.781938069260777\n",
      "three-layer Loss after iteration 29000: 8.561029554139578\n",
      "three-layer Loss after iteration 30000: 8.199263732012524\n",
      "three-layer Loss after iteration 31000: 9.642151593963131\n",
      "three-layer Loss after iteration 32000: 8.170659034996273\n",
      "three-layer Loss after iteration 33000: 9.74184854544791\n",
      "three-layer Loss after iteration 34000: 8.622835011475456\n",
      "three-layer Loss after iteration 35000: 8.23878877050965\n",
      "three-layer Loss after iteration 36000: 8.409799297727409\n",
      "three-layer Loss after iteration 37000: 8.078918036584295\n",
      "three-layer Loss after iteration 38000: 7.960796253926045\n",
      "three-layer Loss after iteration 39000: 7.8696186731560305\n",
      "three-layer Loss after iteration 40000: 7.7875451243237\n",
      "three-layer Loss after iteration 41000: 7.712890447549373\n",
      "three-layer Loss after iteration 42000: 7.644565790976305\n",
      "three-layer Loss after iteration 43000: 7.582296271316968\n",
      "three-layer Loss after iteration 44000: 7.524244173455098\n",
      "three-layer Loss after iteration 45000: 7.4843755797576845\n",
      "three-layer Loss after iteration 46000: 7.525963917066029\n",
      "three-layer Loss after iteration 47000: 7.5260866386460075\n",
      "1.6306426835223453e-05 7.525963917066029 7.5260866386460075\n",
      "three-layer Loss after iteration 0: 1676.4519917940238\n",
      "three-layer Loss after iteration 1000: 23.990998864484094\n",
      "three-layer Loss after iteration 2000: 18.97130257931331\n",
      "three-layer Loss after iteration 3000: 17.459361750550833\n",
      "three-layer Loss after iteration 4000: 16.29472385707314\n",
      "three-layer Loss after iteration 5000: 16.503795519813327\n",
      "three-layer Loss after iteration 6000: 15.867557075275638\n",
      "three-layer Loss after iteration 7000: 14.306652817579616\n",
      "three-layer Loss after iteration 8000: 13.661763116824712\n",
      "three-layer Loss after iteration 9000: 13.175452692847147\n",
      "three-layer Loss after iteration 10000: 12.617611769619485\n",
      "three-layer Loss after iteration 11000: 12.38968691067592\n",
      "three-layer Loss after iteration 12000: 11.715295576062328\n",
      "three-layer Loss after iteration 13000: 11.708175125432865\n",
      "three-layer Loss after iteration 14000: 11.490298820615712\n",
      "three-layer Loss after iteration 15000: 12.287544190105475\n",
      "three-layer Loss after iteration 16000: 11.884310295929042\n",
      "three-layer Loss after iteration 17000: 11.229605687918944\n",
      "three-layer Loss after iteration 18000: 10.957730517999384\n",
      "three-layer Loss after iteration 19000: 10.718630300432775\n",
      "three-layer Loss after iteration 20000: 10.483251255904559\n",
      "three-layer Loss after iteration 21000: 10.12633368830568\n",
      "three-layer Loss after iteration 22000: 9.846923732746957\n",
      "three-layer Loss after iteration 23000: 10.073807465526269\n",
      "three-layer Loss after iteration 24000: 9.695999462970835\n",
      "three-layer Loss after iteration 25000: 9.249507597362978\n",
      "three-layer Loss after iteration 26000: 9.567475060428542\n",
      "three-layer Loss after iteration 27000: 9.3664159998837\n",
      "three-layer Loss after iteration 28000: 9.175755261318981\n",
      "three-layer Loss after iteration 29000: 9.013205712131391\n",
      "three-layer Loss after iteration 30000: 8.862473538625755\n",
      "three-layer Loss after iteration 31000: 8.681496945804616\n",
      "three-layer Loss after iteration 32000: 8.898052792351788\n",
      "three-layer Loss after iteration 33000: 8.826213645798076\n",
      "three-layer Loss after iteration 34000: 8.710005889559943\n",
      "three-layer Loss after iteration 35000: 8.192510262743648\n",
      "three-layer Loss after iteration 36000: 8.793136725835414\n",
      "three-layer Loss after iteration 37000: 8.109891632376454\n",
      "three-layer Loss after iteration 38000: 8.287912908245811\n",
      "three-layer Loss after iteration 39000: 7.915055201745129\n",
      "three-layer Loss after iteration 40000: 8.634764670852604\n",
      "three-layer Loss after iteration 41000: 8.230523987221758\n",
      "three-layer Loss after iteration 42000: 8.080910055470435\n",
      "three-layer Loss after iteration 43000: 8.215728756671357\n",
      "three-layer Loss after iteration 44000: 7.4873081507339645\n",
      "three-layer Loss after iteration 45000: 8.365686795845152\n",
      "three-layer Loss after iteration 46000: 7.56304928121067\n",
      "three-layer Loss after iteration 47000: 7.849698501168387\n",
      "three-layer Loss after iteration 48000: 10.109670203039657\n",
      "three-layer Loss after iteration 49000: 7.701514084036283\n",
      "three-layer Loss after iteration 50000: 7.8543624701142845\n",
      "three-layer Loss after iteration 51000: 7.878764024836082\n",
      "three-layer Loss after iteration 52000: 8.326684535824539\n",
      "three-layer Loss after iteration 53000: 7.937641869300334\n",
      "three-layer Loss after iteration 54000: 7.959344768738804\n",
      "three-layer Loss after iteration 55000: 8.05448006585415\n",
      "three-layer Loss after iteration 56000: 8.252704298027792\n",
      "three-layer Loss after iteration 57000: 8.27775428666339\n",
      "three-layer Loss after iteration 58000: 8.16585569800162\n",
      "three-layer Loss after iteration 59000: 8.077129707614654\n",
      "three-layer Loss after iteration 60000: 8.004251844255515\n",
      "three-layer Loss after iteration 61000: 7.943443027948419\n",
      "three-layer Loss after iteration 62000: 7.893305132501092\n",
      "three-layer Loss after iteration 63000: 7.848899954593714\n",
      "three-layer Loss after iteration 64000: 7.812139119444736\n",
      "three-layer Loss after iteration 65000: 7.77872031071465\n",
      "three-layer Loss after iteration 66000: 7.7481432991966\n",
      "three-layer Loss after iteration 67000: 7.7111226345734325\n",
      "three-layer Loss after iteration 68000: 8.196610118305397\n",
      "three-layer Loss after iteration 69000: 8.140042959981114\n",
      "three-layer Loss after iteration 70000: 8.121477728464892\n",
      "three-layer Loss after iteration 71000: 8.069434217020243\n",
      "three-layer Loss after iteration 72000: 8.012394624430945\n",
      "three-layer Loss after iteration 73000: 7.9601438920754\n",
      "three-layer Loss after iteration 74000: 7.912710477607651\n",
      "three-layer Loss after iteration 75000: 7.869502806347299\n",
      "three-layer Loss after iteration 76000: 7.830281914930422\n",
      "three-layer Loss after iteration 77000: 7.792364885741335\n",
      "three-layer Loss after iteration 78000: 7.757502653879526\n",
      "three-layer Loss after iteration 79000: 7.726086563656904\n",
      "three-layer Loss after iteration 80000: 7.697307785934467\n",
      "three-layer Loss after iteration 81000: 7.672106289644936\n",
      "three-layer Loss after iteration 82000: 7.6491018548667515\n",
      "three-layer Loss after iteration 83000: 7.6240979538241165\n",
      "three-layer Loss after iteration 84000: 7.602870322995459\n",
      "three-layer Loss after iteration 85000: 7.5866227568564994\n",
      "three-layer Loss after iteration 86000: 7.5720053930487365\n",
      "three-layer Loss after iteration 87000: 7.558653899916108\n",
      "three-layer Loss after iteration 88000: 7.545979001800468\n",
      "three-layer Loss after iteration 89000: 7.534577584496623\n",
      "three-layer Loss after iteration 90000: 7.523934526254228\n",
      "three-layer Loss after iteration 91000: 7.514294888532521\n",
      "three-layer Loss after iteration 92000: 7.505345771140311\n",
      "three-layer Loss after iteration 93000: 7.497255756534194\n",
      "three-layer Loss after iteration 94000: 7.489759202805478\n",
      "three-layer Loss after iteration 95000: 7.483085833751744\n",
      "three-layer Loss after iteration 96000: 7.476873963630154\n",
      "three-layer Loss after iteration 97000: 7.471126457678486\n",
      "three-layer Loss after iteration 98000: 7.465936566460372\n",
      "three-layer Loss after iteration 99000: 7.461201376150973\n",
      "three-layer Loss after iteration 100000: 7.456955510562042\n",
      "three-layer Loss after iteration 101000: 7.45294177108233\n",
      "three-layer Loss after iteration 102000: 7.449374072627185\n",
      "three-layer Loss after iteration 103000: 7.446014522071684\n",
      "three-layer Loss after iteration 104000: 7.442932713492971\n",
      "three-layer Loss after iteration 105000: 7.440196149071827\n",
      "three-layer Loss after iteration 106000: 7.437687260919137\n",
      "three-layer Loss after iteration 107000: 7.435367700271375\n",
      "three-layer Loss after iteration 108000: 7.433292268701389\n",
      "three-layer Loss after iteration 109000: 7.431277321643143\n",
      "three-layer Loss after iteration 110000: 7.428993583700264\n",
      "three-layer Loss after iteration 111000: 7.427658174612163\n",
      "three-layer Loss after iteration 112000: 7.426560463658322\n",
      "three-layer Loss after iteration 113000: 7.4253074995452755\n",
      "three-layer Loss after iteration 114000: 7.4241744375478325\n",
      "three-layer Loss after iteration 115000: 7.423072867486981\n",
      "three-layer Loss after iteration 116000: 7.421696864331239\n",
      "three-layer Loss after iteration 117000: 7.420583500437743\n",
      "three-layer Loss after iteration 118000: 7.4195850951966715\n",
      "three-layer Loss after iteration 119000: 7.418766740585366\n",
      "three-layer Loss after iteration 120000: 7.41797266363106\n",
      "three-layer Loss after iteration 121000: 7.417241967082176\n",
      "9.850353755908481e-05 7.41797266363106 7.417241967082176\n",
      "three-layer Loss after iteration 0: 1562.460547435864\n",
      "three-layer Loss after iteration 1000: 28.754062850497867\n",
      "three-layer Loss after iteration 2000: 27.222494337541825\n",
      "three-layer Loss after iteration 3000: 19.491870443755293\n",
      "three-layer Loss after iteration 4000: 17.143407911174233\n",
      "three-layer Loss after iteration 5000: 14.705586240033881\n",
      "three-layer Loss after iteration 6000: 14.03709474391538\n",
      "three-layer Loss after iteration 7000: 12.533761035612187\n",
      "three-layer Loss after iteration 8000: 12.32125476569822\n",
      "three-layer Loss after iteration 9000: 11.716634469898572\n",
      "three-layer Loss after iteration 10000: 11.178417332683713\n",
      "three-layer Loss after iteration 11000: 10.22091975007781\n",
      "three-layer Loss after iteration 12000: 9.870686923051927\n",
      "three-layer Loss after iteration 13000: 9.934228320772013\n",
      "three-layer Loss after iteration 14000: 9.342576545417018\n",
      "three-layer Loss after iteration 15000: 9.079116290637366\n",
      "three-layer Loss after iteration 16000: 9.300769864999829\n",
      "three-layer Loss after iteration 17000: 8.761727751313417\n",
      "three-layer Loss after iteration 18000: 8.419589357607048\n",
      "three-layer Loss after iteration 19000: 8.440862902410382\n",
      "three-layer Loss after iteration 20000: 8.578078476592676\n",
      "three-layer Loss after iteration 21000: 7.9445660823271105\n",
      "three-layer Loss after iteration 22000: 8.045937277218973\n",
      "three-layer Loss after iteration 23000: 7.350564113699073\n",
      "three-layer Loss after iteration 24000: 6.96013670350734\n",
      "three-layer Loss after iteration 25000: 7.222114825992632\n",
      "three-layer Loss after iteration 26000: 7.756724428180913\n",
      "three-layer Loss after iteration 27000: 7.502405901101781\n",
      "three-layer Loss after iteration 28000: 11.119611550996522\n",
      "three-layer Loss after iteration 29000: 5.354001662655976\n",
      "three-layer Loss after iteration 30000: 6.24301846686154\n",
      "three-layer Loss after iteration 31000: 9.041667113118157\n",
      "three-layer Loss after iteration 32000: 7.6290271069023134\n",
      "three-layer Loss after iteration 33000: 5.93787126415069\n",
      "three-layer Loss after iteration 34000: 6.086252060621081\n",
      "three-layer Loss after iteration 35000: 6.233324854300329\n",
      "three-layer Loss after iteration 36000: 5.676332422388981\n",
      "three-layer Loss after iteration 37000: 4.047542351795493\n",
      "three-layer Loss after iteration 38000: 4.855650249351841\n",
      "three-layer Loss after iteration 39000: 5.201279133309962\n",
      "three-layer Loss after iteration 40000: 5.985956352668082\n",
      "three-layer Loss after iteration 41000: 5.512045460452393\n",
      "three-layer Loss after iteration 42000: 4.003491633835036\n",
      "three-layer Loss after iteration 43000: 5.183479132615141\n",
      "three-layer Loss after iteration 44000: 5.332560492907912\n",
      "three-layer Loss after iteration 45000: 4.675027979778482\n",
      "three-layer Loss after iteration 46000: 4.942173048389091\n",
      "three-layer Loss after iteration 47000: 4.930516303783206\n",
      "three-layer Loss after iteration 48000: 3.827844668195448\n",
      "three-layer Loss after iteration 49000: 4.890471895255362\n",
      "three-layer Loss after iteration 50000: 3.8757934115673236\n",
      "three-layer Loss after iteration 51000: 4.089750721702945\n",
      "three-layer Loss after iteration 52000: 3.5811715922013807\n",
      "three-layer Loss after iteration 53000: 3.5707408405523737\n",
      "three-layer Loss after iteration 54000: 3.222128340928951\n",
      "three-layer Loss after iteration 55000: 4.323196501019259\n",
      "three-layer Loss after iteration 56000: 3.3377437448649543\n",
      "three-layer Loss after iteration 57000: 4.237260988616725\n",
      "three-layer Loss after iteration 58000: 3.866950723752525\n",
      "three-layer Loss after iteration 59000: 3.752855213625348\n",
      "three-layer Loss after iteration 60000: 3.5786716937064527\n",
      "three-layer Loss after iteration 61000: 3.871344294116338\n",
      "three-layer Loss after iteration 62000: 3.996621539985035\n",
      "three-layer Loss after iteration 63000: 3.988658812990849\n",
      "three-layer Loss after iteration 64000: 3.840389707523326\n",
      "three-layer Loss after iteration 65000: 3.7955927359259536\n",
      "three-layer Loss after iteration 66000: 3.513709754961763\n",
      "three-layer Loss after iteration 67000: 3.3498287718760227\n",
      "three-layer Loss after iteration 68000: 3.498386683735123\n",
      "three-layer Loss after iteration 69000: 3.57738964421895\n",
      "three-layer Loss after iteration 70000: 3.3402583772550822\n",
      "three-layer Loss after iteration 71000: 3.7110473434002023\n",
      "three-layer Loss after iteration 72000: 3.4826229315040322\n",
      "three-layer Loss after iteration 73000: 3.341498008615881\n",
      "three-layer Loss after iteration 74000: 3.573932407380609\n",
      "three-layer Loss after iteration 75000: 3.6593043990880827\n",
      "three-layer Loss after iteration 76000: 3.893264443097428\n",
      "three-layer Loss after iteration 77000: 3.2216531079148574\n",
      "three-layer Loss after iteration 78000: 3.249484239818203\n",
      "three-layer Loss after iteration 79000: 4.433162290883774\n",
      "three-layer Loss after iteration 80000: 3.2216243647244696\n",
      "three-layer Loss after iteration 81000: 3.1914913828351787\n",
      "three-layer Loss after iteration 82000: 3.1833024785272612\n",
      "three-layer Loss after iteration 83000: 3.1801395209997088\n",
      "three-layer Loss after iteration 84000: 3.175717313867542\n",
      "three-layer Loss after iteration 85000: 3.1742105295569836\n",
      "three-layer Loss after iteration 86000: 3.170246964297587\n",
      "three-layer Loss after iteration 87000: 3.1674683259778926\n",
      "three-layer Loss after iteration 88000: 3.1654904055118784\n",
      "three-layer Loss after iteration 89000: 3.162600001597596\n",
      "three-layer Loss after iteration 90000: 3.1625328078748556\n",
      "2.1246355121296376e-05 3.162600001597596 3.1625328078748556\n",
      "three-layer Loss after iteration 0: 1734.1946605336889\n",
      "three-layer Loss after iteration 1000: 12.530081707471595\n",
      "three-layer Loss after iteration 2000: 7.167423333839685\n",
      "three-layer Loss after iteration 3000: 7.219981023818694\n",
      "three-layer Loss after iteration 4000: 6.230344890723039\n",
      "three-layer Loss after iteration 5000: 5.515809609644522\n",
      "three-layer Loss after iteration 6000: 5.370632317502285\n",
      "three-layer Loss after iteration 7000: 5.046862473058521\n",
      "three-layer Loss after iteration 8000: 4.754451835924018\n",
      "three-layer Loss after iteration 9000: 4.523513550858205\n",
      "three-layer Loss after iteration 10000: 4.406951159416758\n",
      "three-layer Loss after iteration 11000: 4.253028049670388\n",
      "three-layer Loss after iteration 12000: 3.8202681227986486\n",
      "three-layer Loss after iteration 13000: 3.5773727379781666\n",
      "three-layer Loss after iteration 14000: 3.5560773950644604\n",
      "three-layer Loss after iteration 15000: 3.2717307793162287\n",
      "three-layer Loss after iteration 16000: 3.3164198616690403\n",
      "three-layer Loss after iteration 17000: 3.044007931592753\n",
      "three-layer Loss after iteration 18000: 2.981007979860889\n",
      "three-layer Loss after iteration 19000: 2.772420665783032\n",
      "three-layer Loss after iteration 20000: 2.6360358619565423\n",
      "three-layer Loss after iteration 21000: 3.1283755136101568\n",
      "three-layer Loss after iteration 22000: 3.0369855305634337\n",
      "three-layer Loss after iteration 23000: 2.4801684996939346\n",
      "three-layer Loss after iteration 24000: 2.241933490457233\n",
      "three-layer Loss after iteration 25000: 2.0866571699300844\n",
      "three-layer Loss after iteration 26000: 2.232715193748405\n",
      "three-layer Loss after iteration 27000: 2.160850904032859\n",
      "three-layer Loss after iteration 28000: 2.132014898207067\n",
      "three-layer Loss after iteration 29000: 2.0631607288437888\n",
      "three-layer Loss after iteration 30000: 2.012479360854139\n",
      "three-layer Loss after iteration 31000: 1.9612739018480507\n",
      "three-layer Loss after iteration 32000: 1.9281368217265906\n",
      "three-layer Loss after iteration 33000: 1.8926022130918119\n",
      "three-layer Loss after iteration 34000: 1.8590642228361804\n",
      "three-layer Loss after iteration 35000: 1.8260424159791506\n",
      "three-layer Loss after iteration 36000: 1.8065829905576531\n",
      "three-layer Loss after iteration 37000: 1.785310090992253\n",
      "three-layer Loss after iteration 38000: 1.741238606272533\n",
      "three-layer Loss after iteration 39000: 1.6070598352428402\n",
      "three-layer Loss after iteration 40000: 1.574595955224509\n",
      "three-layer Loss after iteration 41000: 1.559470308950245\n",
      "three-layer Loss after iteration 42000: 1.545604739474943\n",
      "three-layer Loss after iteration 43000: 1.542831265972695\n",
      "three-layer Loss after iteration 44000: 1.626651885086414\n",
      "three-layer Loss after iteration 45000: 1.6908907042862458\n",
      "three-layer Loss after iteration 46000: 1.66349490465003\n",
      "three-layer Loss after iteration 47000: 1.6395240620619513\n",
      "three-layer Loss after iteration 48000: 1.6144289049233826\n",
      "three-layer Loss after iteration 49000: 1.5875275803500477\n",
      "three-layer Loss after iteration 50000: 1.5758868100431362\n",
      "three-layer Loss after iteration 51000: 1.5590639078286108\n",
      "three-layer Loss after iteration 52000: 1.5423053444704802\n",
      "three-layer Loss after iteration 53000: 1.5276688942514995\n",
      "three-layer Loss after iteration 54000: 1.5135313403834831\n",
      "three-layer Loss after iteration 55000: 1.4996495900136952\n",
      "three-layer Loss after iteration 56000: 1.4873572737511496\n",
      "three-layer Loss after iteration 57000: 1.4753161196186009\n",
      "three-layer Loss after iteration 58000: 1.4636453700366499\n",
      "three-layer Loss after iteration 59000: 1.4314267580090827\n",
      "three-layer Loss after iteration 60000: 1.4439071986538137\n",
      "three-layer Loss after iteration 61000: 1.4315297319065239\n",
      "three-layer Loss after iteration 62000: 1.422680393364076\n",
      "three-layer Loss after iteration 63000: 1.4134089994993781\n",
      "three-layer Loss after iteration 64000: 1.4046194217551782\n",
      "three-layer Loss after iteration 65000: 1.3996479868374743\n",
      "three-layer Loss after iteration 66000: 1.3535455794409128\n",
      "three-layer Loss after iteration 67000: 1.35535211787938\n",
      "three-layer Loss after iteration 68000: 1.3505422509602252\n",
      "three-layer Loss after iteration 69000: 1.3467384645199043\n",
      "three-layer Loss after iteration 70000: 1.3431695894691085\n",
      "three-layer Loss after iteration 71000: 1.340536627152168\n",
      "three-layer Loss after iteration 72000: 1.3367532269391944\n",
      "three-layer Loss after iteration 73000: 1.3334362338558523\n",
      "three-layer Loss after iteration 74000: 1.3290671540550771\n",
      "three-layer Loss after iteration 75000: 1.3246647708268644\n",
      "three-layer Loss after iteration 76000: 1.3192398803584091\n",
      "three-layer Loss after iteration 77000: 1.3110521164471385\n",
      "three-layer Loss after iteration 78000: 1.303934689040016\n",
      "three-layer Loss after iteration 79000: 1.2973223521000454\n",
      "three-layer Loss after iteration 80000: 1.291000108944643\n",
      "three-layer Loss after iteration 81000: 1.2849011091658364\n",
      "three-layer Loss after iteration 82000: 1.2789609774158206\n",
      "three-layer Loss after iteration 83000: 1.280295635857181\n",
      "three-layer Loss after iteration 84000: 1.2732966356774007\n",
      "three-layer Loss after iteration 85000: 1.26802011861794\n",
      "three-layer Loss after iteration 86000: 1.2636369662027593\n",
      "three-layer Loss after iteration 87000: 1.2588418257547123\n",
      "three-layer Loss after iteration 88000: 1.2541317103036946\n",
      "three-layer Loss after iteration 89000: 1.249496287379651\n",
      "three-layer Loss after iteration 90000: 1.244160419066586\n",
      "three-layer Loss after iteration 91000: 1.2402610035835506\n",
      "three-layer Loss after iteration 92000: 1.2369315919992712\n",
      "three-layer Loss after iteration 93000: 1.2314805534882438\n",
      "three-layer Loss after iteration 94000: 1.227175464889916\n",
      "three-layer Loss after iteration 95000: 1.2241436778625947\n",
      "three-layer Loss after iteration 96000: 1.218438196652162\n",
      "three-layer Loss after iteration 97000: 1.2153046064383832\n",
      "three-layer Loss after iteration 98000: 1.2118443956527831\n",
      "three-layer Loss after iteration 99000: 1.2070534935127109\n",
      "three-layer Loss after iteration 100000: 1.2040480067929424\n",
      "three-layer Loss after iteration 101000: 1.1993083928302313\n",
      "three-layer Loss after iteration 102000: 1.1961746977726817\n",
      "three-layer Loss after iteration 103000: 1.194206325246645\n",
      "three-layer Loss after iteration 104000: 1.1883411702267146\n",
      "three-layer Loss after iteration 105000: 1.1857932036303898\n",
      "three-layer Loss after iteration 106000: 1.1832833091752029\n",
      "three-layer Loss after iteration 107000: 1.17882444581993\n",
      "three-layer Loss after iteration 108000: 1.1765409071434387\n",
      "three-layer Loss after iteration 109000: 1.173540328376444\n",
      "three-layer Loss after iteration 110000: 1.1671901954752832\n",
      "three-layer Loss after iteration 111000: 1.1636647089393026\n",
      "three-layer Loss after iteration 112000: 1.1624392225733684\n",
      "three-layer Loss after iteration 113000: 1.1585693004187068\n",
      "three-layer Loss after iteration 114000: 1.15584764779712\n",
      "three-layer Loss after iteration 115000: 1.1528027583729967\n",
      "three-layer Loss after iteration 116000: 1.1491533989277054\n",
      "three-layer Loss after iteration 117000: 1.22174510814591\n",
      "three-layer Loss after iteration 118000: 1.1199756883719645\n",
      "three-layer Loss after iteration 119000: 1.1119685261408752\n",
      "three-layer Loss after iteration 120000: 1.2467812298061467\n",
      "three-layer Loss after iteration 121000: 1.1298896307243196\n",
      "three-layer Loss after iteration 122000: 1.09597364324023\n",
      "three-layer Loss after iteration 123000: 1.2265400528316976\n",
      "three-layer Loss after iteration 124000: 1.1684283172562449\n",
      "three-layer Loss after iteration 125000: 1.1470882116398793\n",
      "three-layer Loss after iteration 126000: 1.1389109710234717\n",
      "three-layer Loss after iteration 127000: 1.1335497496346698\n",
      "three-layer Loss after iteration 128000: 1.1292788765148776\n",
      "three-layer Loss after iteration 129000: 1.1253114735999044\n",
      "three-layer Loss after iteration 130000: 1.1216770223125248\n",
      "three-layer Loss after iteration 131000: 1.1188076427141163\n",
      "three-layer Loss after iteration 132000: 1.115348091435284\n",
      "three-layer Loss after iteration 133000: 1.1124108829179082\n",
      "three-layer Loss after iteration 134000: 1.1096751799717088\n",
      "three-layer Loss after iteration 135000: 1.1070175967818285\n",
      "three-layer Loss after iteration 136000: 1.104139601348355\n",
      "three-layer Loss after iteration 137000: 1.1012809942019102\n",
      "three-layer Loss after iteration 138000: 1.0991048976668256\n",
      "three-layer Loss after iteration 139000: 1.0967579907315965\n",
      "three-layer Loss after iteration 140000: 1.0937158948147263\n",
      "three-layer Loss after iteration 141000: 1.0886840096986203\n",
      "three-layer Loss after iteration 142000: 1.0860153061200597\n",
      "three-layer Loss after iteration 143000: 1.0845617150619016\n",
      "three-layer Loss after iteration 144000: 1.0804983280772535\n",
      "three-layer Loss after iteration 145000: 1.0781503388446954\n",
      "three-layer Loss after iteration 146000: 1.076920400567925\n",
      "three-layer Loss after iteration 147000: 1.074317315284084\n",
      "three-layer Loss after iteration 148000: 1.0716670478247214\n",
      "three-layer Loss after iteration 149000: 1.0370151739182696\n",
      "three-layer Loss after iteration 150000: 1.0297635880786216\n",
      "three-layer Loss after iteration 151000: 1.0264166697579065\n",
      "three-layer Loss after iteration 152000: 1.0235078221536544\n",
      "three-layer Loss after iteration 153000: 1.0207899913081544\n",
      "three-layer Loss after iteration 154000: 1.0159286447860267\n",
      "three-layer Loss after iteration 155000: 1.0121998294825143\n",
      "three-layer Loss after iteration 156000: 1.008326022468542\n",
      "three-layer Loss after iteration 157000: 1.0031764370888052\n",
      "three-layer Loss after iteration 158000: 1.0475827920679415\n",
      "three-layer Loss after iteration 159000: 0.9860348423888755\n",
      "three-layer Loss after iteration 160000: 0.982798437620967\n",
      "three-layer Loss after iteration 161000: 0.9738702589472549\n",
      "three-layer Loss after iteration 162000: 0.9654322873785213\n",
      "three-layer Loss after iteration 163000: 0.961485638513818\n",
      "three-layer Loss after iteration 164000: 0.9591075812800494\n",
      "three-layer Loss after iteration 165000: 0.9563672313888626\n",
      "three-layer Loss after iteration 166000: 0.9534408336222688\n",
      "three-layer Loss after iteration 167000: 0.9510742132441942\n",
      "three-layer Loss after iteration 168000: 0.9430607673420147\n",
      "three-layer Loss after iteration 169000: 0.9279429727677022\n",
      "three-layer Loss after iteration 170000: 0.9146119332973538\n",
      "three-layer Loss after iteration 171000: 0.9112942376255733\n",
      "three-layer Loss after iteration 172000: 0.9081263171844481\n",
      "three-layer Loss after iteration 173000: 0.9050499776316909\n",
      "three-layer Loss after iteration 174000: 0.9020314398321407\n",
      "three-layer Loss after iteration 175000: 0.8990777158050818\n",
      "three-layer Loss after iteration 176000: 0.8961781919525387\n",
      "three-layer Loss after iteration 177000: 0.8933288954144865\n",
      "three-layer Loss after iteration 178000: 0.8905241497727877\n",
      "three-layer Loss after iteration 179000: 0.88775864357759\n",
      "three-layer Loss after iteration 180000: 0.8850360559077983\n",
      "three-layer Loss after iteration 181000: 0.882353325205957\n",
      "three-layer Loss after iteration 182000: 0.8701931166397409\n",
      "three-layer Loss after iteration 183000: 0.9278896156721246\n",
      "three-layer Loss after iteration 184000: 0.8032230488350792\n",
      "three-layer Loss after iteration 185000: 0.798311755277383\n",
      "three-layer Loss after iteration 186000: 0.8066644864933609\n",
      "three-layer Loss after iteration 187000: 0.8210828396993075\n",
      "three-layer Loss after iteration 188000: 0.8178429239758\n",
      "three-layer Loss after iteration 189000: 0.8116691821930253\n",
      "three-layer Loss after iteration 190000: 0.8073206628940757\n",
      "three-layer Loss after iteration 191000: 0.8027568125247477\n",
      "three-layer Loss after iteration 192000: 0.7979663033899566\n",
      "three-layer Loss after iteration 193000: 0.7933687384883897\n",
      "three-layer Loss after iteration 194000: 0.7890507714014272\n",
      "three-layer Loss after iteration 195000: 0.7844398972344634\n",
      "three-layer Loss after iteration 196000: 0.7804829809199426\n",
      "three-layer Loss after iteration 197000: 0.7770239813265221\n",
      "three-layer Loss after iteration 198000: 0.7736273479025032\n",
      "three-layer Loss after iteration 199000: 0.7703020395830719\n",
      "three-layer Loss after iteration 200000: 0.7638723840950826\n",
      "three-layer Loss after iteration 201000: 0.7606542910487292\n",
      "three-layer Loss after iteration 202000: 0.7572594731489573\n",
      "three-layer Loss after iteration 203000: 0.753644334178773\n",
      "three-layer Loss after iteration 204000: 0.750232206942705\n",
      "three-layer Loss after iteration 205000: 0.7468142427321522\n",
      "three-layer Loss after iteration 206000: 0.7432626777703119\n",
      "three-layer Loss after iteration 207000: 0.7401970074247497\n",
      "three-layer Loss after iteration 208000: 0.7367996277332197\n",
      "three-layer Loss after iteration 209000: 0.73371409438811\n",
      "three-layer Loss after iteration 210000: 0.7306148175810381\n",
      "three-layer Loss after iteration 211000: 0.7274394674631546\n",
      "three-layer Loss after iteration 212000: 0.7244009643192691\n",
      "three-layer Loss after iteration 213000: 0.7213936860014339\n",
      "three-layer Loss after iteration 214000: 0.7184245440000827\n",
      "three-layer Loss after iteration 215000: 0.7154014299210635\n",
      "three-layer Loss after iteration 216000: 0.7125835624779635\n",
      "three-layer Loss after iteration 217000: 0.7096257974747543\n",
      "three-layer Loss after iteration 218000: 0.7068177606122943\n",
      "three-layer Loss after iteration 219000: 0.704109469004791\n",
      "three-layer Loss after iteration 220000: 0.7012233838551769\n",
      "three-layer Loss after iteration 221000: 0.6983979252995954\n",
      "three-layer Loss after iteration 222000: 0.6956092229306402\n",
      "three-layer Loss after iteration 223000: 0.6928479142834155\n",
      "three-layer Loss after iteration 224000: 0.6900969038208773\n",
      "three-layer Loss after iteration 225000: 0.6873410153758276\n",
      "three-layer Loss after iteration 226000: 0.684833854947089\n",
      "three-layer Loss after iteration 227000: 0.6820424007109687\n",
      "three-layer Loss after iteration 228000: 0.6794514787607793\n",
      "three-layer Loss after iteration 229000: 0.6768404519472916\n",
      "three-layer Loss after iteration 230000: 0.6752457296615213\n",
      "three-layer Loss after iteration 231000: 0.6738908591526149\n",
      "three-layer Loss after iteration 232000: 0.6715385138287784\n",
      "three-layer Loss after iteration 233000: 0.6690383281103075\n",
      "three-layer Loss after iteration 234000: 0.6670907060020318\n",
      "three-layer Loss after iteration 235000: 0.6647970576620649\n",
      "three-layer Loss after iteration 236000: 0.6623114683907427\n",
      "three-layer Loss after iteration 237000: 0.6600515311932827\n",
      "three-layer Loss after iteration 238000: 0.6575320189101465\n",
      "three-layer Loss after iteration 239000: 0.6552785861722534\n",
      "three-layer Loss after iteration 240000: 0.6531927720799275\n",
      "three-layer Loss after iteration 241000: 0.6507467644362258\n",
      "three-layer Loss after iteration 242000: 0.648817963856854\n",
      "three-layer Loss after iteration 243000: 0.6464124758056088\n",
      "three-layer Loss after iteration 244000: 0.6439474208736318\n",
      "three-layer Loss after iteration 245000: 0.6418320593604716\n",
      "three-layer Loss after iteration 246000: 0.6395812851552041\n",
      "three-layer Loss after iteration 247000: 0.6374703146082635\n",
      "three-layer Loss after iteration 248000: 0.6353169430915078\n",
      "three-layer Loss after iteration 249000: 0.6332793884057806\n",
      "three-layer Loss after iteration 250000: 0.6309709630517962\n",
      "three-layer Loss after iteration 251000: 0.6288526895619865\n",
      "three-layer Loss after iteration 252000: 0.626791332728481\n",
      "three-layer Loss after iteration 253000: 0.6246727210936618\n",
      "three-layer Loss after iteration 254000: 0.62263838859489\n",
      "three-layer Loss after iteration 255000: 0.6205358269063066\n",
      "three-layer Loss after iteration 256000: 0.6184421345135753\n",
      "three-layer Loss after iteration 257000: 0.6164959888625556\n",
      "three-layer Loss after iteration 258000: 0.6145757009585752\n",
      "three-layer Loss after iteration 259000: 0.6123299139643199\n",
      "three-layer Loss after iteration 260000: 0.6104960156184881\n",
      "three-layer Loss after iteration 261000: 0.6084852680148961\n",
      "three-layer Loss after iteration 262000: 0.6065957264018453\n",
      "three-layer Loss after iteration 263000: 0.6047865329914366\n",
      "three-layer Loss after iteration 264000: 0.6028493105197686\n",
      "three-layer Loss after iteration 265000: 0.6008374362051033\n",
      "three-layer Loss after iteration 266000: 0.5989595394107593\n",
      "three-layer Loss after iteration 267000: 0.5971420790814169\n",
      "three-layer Loss after iteration 268000: 0.5955247577380457\n",
      "three-layer Loss after iteration 269000: 0.5934957128926777\n",
      "three-layer Loss after iteration 270000: 0.5914965441684328\n",
      "three-layer Loss after iteration 271000: 0.5899585192299812\n",
      "three-layer Loss after iteration 272000: 0.5883769408205545\n",
      "three-layer Loss after iteration 273000: 0.5882349895512786\n",
      "three-layer Loss after iteration 274000: 0.5878538152248032\n",
      "three-layer Loss after iteration 275000: 0.5867125773929933\n",
      "three-layer Loss after iteration 276000: 0.5857833301534233\n",
      "three-layer Loss after iteration 277000: 0.5845357702914833\n",
      "three-layer Loss after iteration 278000: 0.5805393852036813\n",
      "three-layer Loss after iteration 279000: 0.5770962379747395\n",
      "three-layer Loss after iteration 280000: 0.5750983059989643\n",
      "three-layer Loss after iteration 281000: 0.5735367637988406\n",
      "three-layer Loss after iteration 282000: 0.5712396842833227\n",
      "three-layer Loss after iteration 283000: 0.5693378598026804\n",
      "three-layer Loss after iteration 284000: 0.568071385966153\n",
      "three-layer Loss after iteration 285000: 0.5664400654686966\n",
      "three-layer Loss after iteration 286000: 0.5644168163271653\n",
      "three-layer Loss after iteration 287000: 0.5630245168789357\n",
      "three-layer Loss after iteration 288000: 0.5608357034499707\n",
      "three-layer Loss after iteration 289000: 0.5595982143700123\n",
      "three-layer Loss after iteration 290000: 0.5596305310910482\n",
      "5.774986446705527e-05 0.5595982143700123 0.5596305310910482\n",
      "three-layer Loss after iteration 0: 1572.1674761937895\n",
      "three-layer Loss after iteration 1000: 14.453277125116557\n",
      "three-layer Loss after iteration 2000: 10.982492193012922\n",
      "three-layer Loss after iteration 3000: 11.355920225058096\n",
      "three-layer Loss after iteration 4000: 10.948973736520587\n",
      "three-layer Loss after iteration 5000: 9.951602566055186\n",
      "three-layer Loss after iteration 6000: 9.288511829912462\n",
      "three-layer Loss after iteration 7000: 8.88266783371406\n",
      "three-layer Loss after iteration 8000: 8.838476374884578\n",
      "three-layer Loss after iteration 9000: 8.681491582814798\n",
      "three-layer Loss after iteration 10000: 8.43617988874473\n",
      "three-layer Loss after iteration 11000: 7.925285607082149\n",
      "three-layer Loss after iteration 12000: 7.7674461439245635\n",
      "three-layer Loss after iteration 13000: 7.7913141712167295\n",
      "three-layer Loss after iteration 14000: 7.880986909667672\n",
      "three-layer Loss after iteration 15000: 7.740929542874374\n",
      "three-layer Loss after iteration 16000: 7.253810375513763\n",
      "three-layer Loss after iteration 17000: 7.744751167255961\n",
      "three-layer Loss after iteration 18000: 7.693166176761721\n",
      "three-layer Loss after iteration 19000: 7.824263885491277\n",
      "three-layer Loss after iteration 20000: 7.950817733886797\n",
      "three-layer Loss after iteration 21000: 8.074704431633753\n",
      "three-layer Loss after iteration 22000: 8.022308120434806\n",
      "three-layer Loss after iteration 23000: 7.968404766237141\n",
      "three-layer Loss after iteration 24000: 7.762654271778533\n",
      "three-layer Loss after iteration 25000: 7.657782141474068\n",
      "three-layer Loss after iteration 26000: 7.487181334912536\n",
      "three-layer Loss after iteration 27000: 7.3608848074340765\n",
      "three-layer Loss after iteration 28000: 7.162018713659701\n",
      "three-layer Loss after iteration 29000: 7.201137882278423\n",
      "three-layer Loss after iteration 30000: 6.94888871275285\n",
      "three-layer Loss after iteration 31000: 6.859430991315623\n",
      "three-layer Loss after iteration 32000: 6.905248827460424\n",
      "three-layer Loss after iteration 33000: 6.790120072273082\n",
      "three-layer Loss after iteration 34000: 6.766243967461788\n",
      "three-layer Loss after iteration 35000: 6.661280605340019\n",
      "three-layer Loss after iteration 36000: 6.593436456597754\n",
      "three-layer Loss after iteration 37000: 6.513385037560163\n",
      "three-layer Loss after iteration 38000: 6.579881310098579\n",
      "three-layer Loss after iteration 39000: 6.89923849132891\n",
      "three-layer Loss after iteration 40000: 6.760169192402706\n",
      "three-layer Loss after iteration 41000: 5.792004836977038\n",
      "three-layer Loss after iteration 42000: 6.259624063747632\n",
      "three-layer Loss after iteration 43000: 6.092625935017467\n",
      "three-layer Loss after iteration 44000: 5.618161564632545\n",
      "three-layer Loss after iteration 45000: 6.346548509280124\n",
      "three-layer Loss after iteration 46000: 6.330077516310101\n",
      "three-layer Loss after iteration 47000: 6.263068909017912\n",
      "three-layer Loss after iteration 48000: 6.142754364839188\n",
      "three-layer Loss after iteration 49000: 5.5979964435992144\n",
      "three-layer Loss after iteration 50000: 5.655567209010217\n",
      "three-layer Loss after iteration 51000: 6.536134459410957\n",
      "three-layer Loss after iteration 52000: 6.26522182541754\n",
      "three-layer Loss after iteration 53000: 6.089481503498189\n",
      "three-layer Loss after iteration 54000: 6.03688667725182\n",
      "three-layer Loss after iteration 55000: 5.997225433607814\n",
      "three-layer Loss after iteration 56000: 5.969288407667318\n",
      "three-layer Loss after iteration 57000: 5.9324094426415\n",
      "three-layer Loss after iteration 58000: 5.903714799338794\n",
      "three-layer Loss after iteration 59000: 5.882199030120487\n",
      "three-layer Loss after iteration 60000: 5.834218953258255\n",
      "three-layer Loss after iteration 61000: 5.808936836996777\n",
      "three-layer Loss after iteration 62000: 5.790104662052259\n",
      "three-layer Loss after iteration 63000: 5.772939032138875\n",
      "three-layer Loss after iteration 64000: 5.7569761781571405\n",
      "three-layer Loss after iteration 65000: 5.7417690032792486\n",
      "three-layer Loss after iteration 66000: 5.748546801856755\n",
      "three-layer Loss after iteration 67000: 5.7288426829149675\n",
      "three-layer Loss after iteration 68000: 5.715530870093938\n",
      "three-layer Loss after iteration 69000: 5.703039293787588\n",
      "three-layer Loss after iteration 70000: 5.691282532122754\n",
      "three-layer Loss after iteration 71000: 5.680257293933648\n",
      "three-layer Loss after iteration 72000: 5.669932418013344\n",
      "three-layer Loss after iteration 73000: 5.660263660773389\n",
      "three-layer Loss after iteration 74000: 5.651204210106575\n",
      "three-layer Loss after iteration 75000: 5.642990847816684\n",
      "three-layer Loss after iteration 76000: 5.633741976841306\n",
      "three-layer Loss after iteration 77000: 5.624989207093814\n",
      "three-layer Loss after iteration 78000: 5.6115599080933825\n",
      "three-layer Loss after iteration 79000: 5.590174955912629\n",
      "three-layer Loss after iteration 80000: 5.587805112952543\n",
      "three-layer Loss after iteration 81000: 5.589037693995073\n",
      "three-layer Loss after iteration 82000: 5.583935886999847\n",
      "three-layer Loss after iteration 83000: 5.573122676846326\n",
      "three-layer Loss after iteration 84000: 5.560375339796063\n",
      "three-layer Loss after iteration 85000: 5.550844865417695\n",
      "three-layer Loss after iteration 86000: 5.54253502643843\n",
      "three-layer Loss after iteration 87000: 5.534086233667196\n",
      "three-layer Loss after iteration 88000: 5.52922969350086\n",
      "three-layer Loss after iteration 89000: 5.52112848019221\n",
      "three-layer Loss after iteration 90000: 5.519502316096461\n",
      "three-layer Loss after iteration 91000: 5.511351863506875\n",
      "three-layer Loss after iteration 92000: 5.507237574884548\n",
      "three-layer Loss after iteration 93000: 5.505783033282233\n",
      "three-layer Loss after iteration 94000: 5.501653437117318\n",
      "three-layer Loss after iteration 95000: 5.500664343079417\n",
      "three-layer Loss after iteration 96000: 5.502421110617337\n",
      "three-layer Loss after iteration 97000: 5.498483249481382\n",
      "three-layer Loss after iteration 98000: 5.498785209476185\n",
      "5.491696184244491e-05 5.498483249481382 5.498785209476185\n",
      "three-layer Loss after iteration 0: 1701.8468145505417\n",
      "three-layer Loss after iteration 1000: 26.61209533160761\n",
      "three-layer Loss after iteration 2000: 27.314667090077183\n",
      "three-layer Loss after iteration 3000: 23.084664305731025\n",
      "three-layer Loss after iteration 4000: 20.19018726407243\n",
      "three-layer Loss after iteration 5000: 20.618438869808266\n",
      "three-layer Loss after iteration 6000: 19.098559231655862\n",
      "three-layer Loss after iteration 7000: 18.19654204304242\n",
      "three-layer Loss after iteration 8000: 17.028131998487314\n",
      "three-layer Loss after iteration 9000: 16.024507942528917\n",
      "three-layer Loss after iteration 10000: 15.172601061374525\n",
      "three-layer Loss after iteration 11000: 14.389340351916825\n",
      "three-layer Loss after iteration 12000: 13.422414517415334\n",
      "three-layer Loss after iteration 13000: 14.013164952317364\n",
      "three-layer Loss after iteration 14000: 13.535457150118058\n",
      "three-layer Loss after iteration 15000: 13.342126427410761\n",
      "three-layer Loss after iteration 16000: 11.932832510618407\n",
      "three-layer Loss after iteration 17000: 13.717478082035738\n",
      "three-layer Loss after iteration 18000: 12.0367043509483\n",
      "three-layer Loss after iteration 19000: 12.530295535555956\n",
      "three-layer Loss after iteration 20000: 12.295456142212513\n",
      "three-layer Loss after iteration 21000: 12.030246524606632\n",
      "three-layer Loss after iteration 22000: 11.723034034169057\n",
      "three-layer Loss after iteration 23000: 11.404719745996093\n",
      "three-layer Loss after iteration 24000: 11.488796328808123\n",
      "three-layer Loss after iteration 25000: 11.338735198160503\n",
      "three-layer Loss after iteration 26000: 11.344435774126085\n",
      "three-layer Loss after iteration 27000: 11.048138361638198\n",
      "three-layer Loss after iteration 28000: 10.979934256575994\n",
      "three-layer Loss after iteration 29000: 11.107966368083074\n",
      "three-layer Loss after iteration 30000: 10.948471864055854\n",
      "three-layer Loss after iteration 31000: 11.456932176428596\n",
      "three-layer Loss after iteration 32000: 11.202315997596232\n",
      "three-layer Loss after iteration 33000: 11.300112188412177\n",
      "three-layer Loss after iteration 34000: 11.092264628378986\n",
      "three-layer Loss after iteration 35000: 11.852685829848006\n",
      "three-layer Loss after iteration 36000: 10.81758721211712\n",
      "three-layer Loss after iteration 37000: 10.786926170072151\n",
      "three-layer Loss after iteration 38000: 10.427502648773082\n",
      "three-layer Loss after iteration 39000: 10.333087582481463\n",
      "three-layer Loss after iteration 40000: 10.164970036643766\n",
      "three-layer Loss after iteration 41000: 10.403360206982269\n",
      "three-layer Loss after iteration 42000: 9.610608063366712\n",
      "three-layer Loss after iteration 43000: 9.087885246988964\n",
      "three-layer Loss after iteration 44000: 9.039805558766085\n",
      "three-layer Loss after iteration 45000: 9.44815777423222\n",
      "three-layer Loss after iteration 46000: 10.761717230355854\n",
      "three-layer Loss after iteration 47000: 9.663939503561613\n",
      "three-layer Loss after iteration 48000: 9.009175446604806\n",
      "three-layer Loss after iteration 49000: 9.037723729377833\n",
      "three-layer Loss after iteration 50000: 9.874902751022088\n",
      "three-layer Loss after iteration 51000: 8.8977396741465\n",
      "three-layer Loss after iteration 52000: 10.745026281693857\n",
      "three-layer Loss after iteration 53000: 8.938441737364132\n",
      "three-layer Loss after iteration 54000: 10.392661225777523\n",
      "three-layer Loss after iteration 55000: 8.90566579936846\n",
      "three-layer Loss after iteration 56000: 9.366826506659804\n",
      "three-layer Loss after iteration 57000: 9.112871898651333\n",
      "three-layer Loss after iteration 58000: 8.844240495564819\n",
      "three-layer Loss after iteration 59000: 9.583581705763155\n",
      "three-layer Loss after iteration 60000: 8.862391494212757\n",
      "three-layer Loss after iteration 61000: 9.718113893519181\n",
      "three-layer Loss after iteration 62000: 8.913061379764034\n",
      "three-layer Loss after iteration 63000: 9.516705665846295\n",
      "three-layer Loss after iteration 64000: 8.857062807097488\n",
      "three-layer Loss after iteration 65000: 8.974351299157172\n",
      "three-layer Loss after iteration 66000: 9.021258311472906\n",
      "three-layer Loss after iteration 67000: 9.054743394683523\n",
      "three-layer Loss after iteration 68000: 9.10736878732343\n",
      "three-layer Loss after iteration 69000: 9.49885889885067\n",
      "three-layer Loss after iteration 70000: 9.012108813440111\n",
      "three-layer Loss after iteration 71000: 8.860496703607723\n",
      "three-layer Loss after iteration 72000: 8.968616445062336\n",
      "three-layer Loss after iteration 73000: 9.432467547359359\n",
      "three-layer Loss after iteration 74000: 8.852988043218508\n",
      "three-layer Loss after iteration 75000: 8.900779795884977\n",
      "three-layer Loss after iteration 76000: 9.270041370798747\n",
      "three-layer Loss after iteration 77000: 8.89481890754348\n",
      "three-layer Loss after iteration 78000: 8.878547590019016\n",
      "three-layer Loss after iteration 79000: 9.197049571653952\n",
      "three-layer Loss after iteration 80000: 8.904291459338998\n",
      "three-layer Loss after iteration 81000: 8.851185846164087\n",
      "three-layer Loss after iteration 82000: 9.109109113413483\n",
      "three-layer Loss after iteration 83000: 9.072499608033063\n",
      "three-layer Loss after iteration 84000: 8.84369200097677\n",
      "three-layer Loss after iteration 85000: 8.857581650611952\n",
      "three-layer Loss after iteration 86000: 9.027467644986961\n",
      "three-layer Loss after iteration 87000: 9.064087298637109\n",
      "three-layer Loss after iteration 88000: 9.018684045476581\n",
      "three-layer Loss after iteration 89000: 8.969724627543727\n",
      "three-layer Loss after iteration 90000: 8.90436767934754\n",
      "three-layer Loss after iteration 91000: 9.072912035711312\n",
      "three-layer Loss after iteration 92000: 9.052816140976697\n",
      "three-layer Loss after iteration 93000: 9.022498357201743\n",
      "three-layer Loss after iteration 94000: 9.020245636893574\n",
      "three-layer Loss after iteration 95000: 9.000334711826552\n",
      "three-layer Loss after iteration 96000: 8.985089662492943\n",
      "three-layer Loss after iteration 97000: 8.974926621422116\n",
      "three-layer Loss after iteration 98000: 8.978616235663218\n",
      "three-layer Loss after iteration 99000: 8.95828931708564\n",
      "three-layer Loss after iteration 100000: 8.952007768323089\n",
      "three-layer Loss after iteration 101000: 8.94049962792631\n",
      "three-layer Loss after iteration 102000: 8.933150638761125\n",
      "three-layer Loss after iteration 103000: 8.931359445297247\n",
      "three-layer Loss after iteration 104000: 8.941294817951158\n",
      "three-layer Loss after iteration 105000: 8.922187120473772\n",
      "three-layer Loss after iteration 106000: 8.91788326880052\n",
      "three-layer Loss after iteration 107000: 8.908558393932374\n",
      "three-layer Loss after iteration 108000: 8.908815656921707\n",
      "2.8878184096334965e-05 8.908558393932374 8.908815656921707\n",
      "three-layer Loss after iteration 0: 1733.3780650419787\n",
      "three-layer Loss after iteration 1000: 27.141253371554406\n",
      "three-layer Loss after iteration 2000: 29.239186142986405\n",
      "three-layer Loss after iteration 3000: 24.860533580981517\n",
      "three-layer Loss after iteration 4000: 22.129630494937\n",
      "three-layer Loss after iteration 5000: 20.118754221606043\n",
      "three-layer Loss after iteration 6000: 19.936129979057124\n",
      "three-layer Loss after iteration 7000: 18.241196634506448\n",
      "three-layer Loss after iteration 8000: 17.45143941274245\n",
      "three-layer Loss after iteration 9000: 17.18492728782654\n",
      "three-layer Loss after iteration 10000: 16.03075083979526\n",
      "three-layer Loss after iteration 11000: 16.460962425409033\n",
      "three-layer Loss after iteration 12000: 15.310090956109242\n",
      "three-layer Loss after iteration 13000: 14.764172249246746\n",
      "three-layer Loss after iteration 14000: 13.776424801795567\n",
      "three-layer Loss after iteration 15000: 13.667712639621945\n",
      "three-layer Loss after iteration 16000: 13.779255677169797\n",
      "three-layer Loss after iteration 17000: 14.432373948035176\n",
      "three-layer Loss after iteration 18000: 13.97977976239854\n",
      "three-layer Loss after iteration 19000: 13.4491830586276\n",
      "three-layer Loss after iteration 20000: 13.022051036707019\n",
      "three-layer Loss after iteration 21000: 12.527492956488633\n",
      "three-layer Loss after iteration 22000: 12.505518946941123\n",
      "three-layer Loss after iteration 23000: 12.372120727478805\n",
      "three-layer Loss after iteration 24000: 12.124080945292803\n",
      "three-layer Loss after iteration 25000: 11.927757408131413\n",
      "three-layer Loss after iteration 26000: 11.785964130937476\n",
      "three-layer Loss after iteration 27000: 11.650333570502648\n",
      "three-layer Loss after iteration 28000: 11.584771832262287\n",
      "three-layer Loss after iteration 29000: 11.347783647849736\n",
      "three-layer Loss after iteration 30000: 11.389378967322852\n",
      "three-layer Loss after iteration 31000: 11.356564741984464\n",
      "three-layer Loss after iteration 32000: 10.906778440475529\n",
      "three-layer Loss after iteration 33000: 10.751766322080169\n",
      "three-layer Loss after iteration 34000: 10.739290114720037\n",
      "three-layer Loss after iteration 35000: 10.615659211423246\n",
      "three-layer Loss after iteration 36000: 10.511245855135789\n",
      "three-layer Loss after iteration 37000: 10.413921528036514\n",
      "three-layer Loss after iteration 38000: 10.323389123148608\n",
      "three-layer Loss after iteration 39000: 10.239438568708614\n",
      "three-layer Loss after iteration 40000: 10.117021713919458\n",
      "three-layer Loss after iteration 41000: 10.035734906282846\n",
      "three-layer Loss after iteration 42000: 9.97543727646243\n",
      "three-layer Loss after iteration 43000: 9.920171315028986\n",
      "three-layer Loss after iteration 44000: 9.868876283881944\n",
      "three-layer Loss after iteration 45000: 9.774515843980799\n",
      "three-layer Loss after iteration 46000: 9.721700933991245\n",
      "three-layer Loss after iteration 47000: 9.60568285298106\n",
      "three-layer Loss after iteration 48000: 9.685139197802833\n",
      "three-layer Loss after iteration 49000: 9.489934060372399\n",
      "three-layer Loss after iteration 50000: 10.045576464994634\n",
      "three-layer Loss after iteration 51000: 9.959293883047883\n",
      "three-layer Loss after iteration 52000: 9.88788595518569\n",
      "three-layer Loss after iteration 53000: 9.825961527768198\n",
      "three-layer Loss after iteration 54000: 9.755837212310594\n",
      "three-layer Loss after iteration 55000: 9.671735217306118\n",
      "three-layer Loss after iteration 56000: 9.456485848832866\n",
      "three-layer Loss after iteration 57000: 9.166022666264668\n",
      "three-layer Loss after iteration 58000: 11.057437480019273\n",
      "three-layer Loss after iteration 59000: 9.367902375945363\n",
      "three-layer Loss after iteration 60000: 9.479687519086125\n",
      "three-layer Loss after iteration 61000: 9.539575241738437\n",
      "three-layer Loss after iteration 62000: 9.46833239014824\n",
      "three-layer Loss after iteration 63000: 9.425885376581752\n",
      "three-layer Loss after iteration 64000: 9.388090741859388\n",
      "three-layer Loss after iteration 65000: 9.355144598628014\n",
      "three-layer Loss after iteration 66000: 9.332230760452282\n",
      "three-layer Loss after iteration 67000: 9.311047664184176\n",
      "three-layer Loss after iteration 68000: 9.295233679869213\n",
      "three-layer Loss after iteration 69000: 9.27779732107798\n",
      "three-layer Loss after iteration 70000: 9.261019691558213\n",
      "three-layer Loss after iteration 71000: 9.693970816434804\n",
      "three-layer Loss after iteration 72000: 9.65760816864968\n",
      "three-layer Loss after iteration 73000: 9.448302708271655\n",
      "three-layer Loss after iteration 74000: 9.354913397885575\n",
      "three-layer Loss after iteration 75000: 9.337787210424985\n",
      "three-layer Loss after iteration 76000: 9.346101908550281\n",
      "three-layer Loss after iteration 77000: 9.138039690579667\n",
      "three-layer Loss after iteration 78000: 9.224350919123243\n",
      "three-layer Loss after iteration 79000: 9.136188237057524\n",
      "three-layer Loss after iteration 80000: 9.15601949283297\n",
      "three-layer Loss after iteration 81000: 9.188560521330825\n",
      "three-layer Loss after iteration 82000: 9.221097246637934\n",
      "three-layer Loss after iteration 83000: 9.531443828369419\n",
      "three-layer Loss after iteration 84000: 9.321284305496903\n",
      "three-layer Loss after iteration 85000: 9.264867819586094\n",
      "three-layer Loss after iteration 86000: 9.21982998249701\n",
      "three-layer Loss after iteration 87000: 9.273569296448493\n",
      "three-layer Loss after iteration 88000: 9.199075037714698\n",
      "three-layer Loss after iteration 89000: 9.160809766923387\n",
      "three-layer Loss after iteration 90000: 9.12263649196726\n",
      "three-layer Loss after iteration 91000: 9.086714425319972\n",
      "three-layer Loss after iteration 92000: 8.442736717191138\n",
      "three-layer Loss after iteration 93000: 8.604700939297414\n",
      "three-layer Loss after iteration 94000: 9.192193738110493\n",
      "three-layer Loss after iteration 95000: 8.615886519766747\n",
      "three-layer Loss after iteration 96000: 8.823311294867858\n",
      "three-layer Loss after iteration 97000: 8.885592548015884\n",
      "three-layer Loss after iteration 98000: 8.796211229264406\n",
      "three-layer Loss after iteration 99000: 8.739435294488434\n",
      "three-layer Loss after iteration 100000: 8.738083706843403\n",
      "three-layer Loss after iteration 101000: 9.00200862844155\n",
      "three-layer Loss after iteration 102000: 8.873813869930434\n",
      "three-layer Loss after iteration 103000: 8.817756456736914\n",
      "three-layer Loss after iteration 104000: 8.782725775884378\n",
      "three-layer Loss after iteration 105000: 9.038421290783758\n",
      "three-layer Loss after iteration 106000: 8.965303538138588\n",
      "three-layer Loss after iteration 107000: 8.881316261854057\n",
      "three-layer Loss after iteration 108000: 8.84180792919305\n",
      "three-layer Loss after iteration 109000: 8.810503976877763\n",
      "three-layer Loss after iteration 110000: 8.753804810708907\n",
      "three-layer Loss after iteration 111000: 8.70279029575684\n",
      "three-layer Loss after iteration 112000: 8.674367849411594\n",
      "three-layer Loss after iteration 113000: 8.64674199496805\n",
      "three-layer Loss after iteration 114000: 8.62034610086986\n",
      "three-layer Loss after iteration 115000: 8.59616694568954\n",
      "three-layer Loss after iteration 116000: 8.572057133755004\n",
      "three-layer Loss after iteration 117000: 8.538940134172249\n",
      "three-layer Loss after iteration 118000: 8.516313671136874\n",
      "three-layer Loss after iteration 119000: 8.489771922097804\n",
      "three-layer Loss after iteration 120000: 8.465888137632238\n",
      "three-layer Loss after iteration 121000: 8.304263446625104\n",
      "three-layer Loss after iteration 122000: 8.41301791689742\n",
      "three-layer Loss after iteration 123000: 8.08228200508389\n",
      "three-layer Loss after iteration 124000: 8.86710756880079\n",
      "three-layer Loss after iteration 125000: 8.03104904820511\n",
      "three-layer Loss after iteration 126000: 8.870053926071368\n",
      "three-layer Loss after iteration 127000: 8.016397622505027\n",
      "three-layer Loss after iteration 128000: 8.805151540843184\n",
      "three-layer Loss after iteration 129000: 7.992530359147174\n",
      "three-layer Loss after iteration 130000: 8.270720446133145\n",
      "three-layer Loss after iteration 131000: 8.131961308113453\n",
      "three-layer Loss after iteration 132000: 7.998941019756164\n",
      "three-layer Loss after iteration 133000: 8.542323620435273\n",
      "three-layer Loss after iteration 134000: 8.277391264039926\n",
      "three-layer Loss after iteration 135000: 8.063163485069476\n",
      "three-layer Loss after iteration 136000: 8.384737659492872\n",
      "three-layer Loss after iteration 137000: 8.230473631889772\n",
      "three-layer Loss after iteration 138000: 8.435309839042931\n",
      "three-layer Loss after iteration 139000: 8.029313866987938\n",
      "three-layer Loss after iteration 140000: 8.504915129796592\n",
      "three-layer Loss after iteration 141000: 7.973921286349162\n",
      "three-layer Loss after iteration 142000: 7.907246384030258\n",
      "three-layer Loss after iteration 143000: 7.972394742080002\n",
      "three-layer Loss after iteration 144000: 8.33969407842435\n",
      "three-layer Loss after iteration 145000: 8.22040211927757\n",
      "three-layer Loss after iteration 146000: 8.19608441096247\n",
      "three-layer Loss after iteration 147000: 8.220618002297432\n",
      "three-layer Loss after iteration 148000: 8.21880225708809\n",
      "three-layer Loss after iteration 149000: 8.206196849424279\n",
      "three-layer Loss after iteration 150000: 8.17180811875913\n",
      "three-layer Loss after iteration 151000: 7.937627261819683\n",
      "three-layer Loss after iteration 152000: 8.111197579977976\n",
      "three-layer Loss after iteration 153000: 8.1965469788351\n",
      "three-layer Loss after iteration 154000: 7.922106000203322\n",
      "three-layer Loss after iteration 155000: 8.291871016799037\n",
      "three-layer Loss after iteration 156000: 8.00270334237611\n",
      "three-layer Loss after iteration 157000: 7.9727956105283555\n",
      "three-layer Loss after iteration 158000: 8.069399114972118\n",
      "three-layer Loss after iteration 159000: 8.044756074998066\n",
      "three-layer Loss after iteration 160000: 7.993168429095674\n",
      "three-layer Loss after iteration 161000: 7.888419895446165\n",
      "three-layer Loss after iteration 162000: 7.852746681567357\n",
      "three-layer Loss after iteration 163000: 7.841031698218029\n",
      "three-layer Loss after iteration 164000: 7.836906987588468\n",
      "three-layer Loss after iteration 165000: 7.834856010857715\n",
      "three-layer Loss after iteration 166000: 7.8322604268763465\n",
      "three-layer Loss after iteration 167000: 7.83011280267676\n",
      "three-layer Loss after iteration 168000: 7.828410964460547\n",
      "three-layer Loss after iteration 169000: 8.108226402540406\n",
      "three-layer Loss after iteration 170000: 7.94430094805502\n",
      "three-layer Loss after iteration 171000: 8.026125722560193\n",
      "three-layer Loss after iteration 172000: 8.026483177248004\n",
      "4.453639279602646e-05 8.026125722560193 8.026483177248004\n",
      "three-layer Loss after iteration 0: 1695.36241748534\n",
      "three-layer Loss after iteration 1000: 23.779646687847993\n",
      "three-layer Loss after iteration 2000: 23.775743584209717\n",
      "three-layer Loss after iteration 3000: 22.929158954992538\n",
      "three-layer Loss after iteration 4000: 20.3399795207809\n",
      "three-layer Loss after iteration 5000: 19.282356392896368\n",
      "three-layer Loss after iteration 6000: 18.7348855254062\n",
      "three-layer Loss after iteration 7000: 17.410101777951642\n",
      "three-layer Loss after iteration 8000: 16.393070647130973\n",
      "three-layer Loss after iteration 9000: 15.7644429364321\n",
      "three-layer Loss after iteration 10000: 15.23136593957898\n",
      "three-layer Loss after iteration 11000: 14.818374753491456\n",
      "three-layer Loss after iteration 12000: 14.797858850313904\n",
      "three-layer Loss after iteration 13000: 14.227168282225733\n",
      "three-layer Loss after iteration 14000: 14.085982383638733\n",
      "three-layer Loss after iteration 15000: 14.17992760969547\n",
      "three-layer Loss after iteration 16000: 14.194371271675706\n",
      "three-layer Loss after iteration 17000: 14.99086820257359\n",
      "three-layer Loss after iteration 18000: 14.561880507443071\n",
      "three-layer Loss after iteration 19000: 14.538371556322188\n",
      "three-layer Loss after iteration 20000: 12.73498994152891\n",
      "three-layer Loss after iteration 21000: 13.689733269024659\n",
      "three-layer Loss after iteration 22000: 14.054767901769745\n",
      "three-layer Loss after iteration 23000: 14.36190267671229\n",
      "three-layer Loss after iteration 24000: 14.133368509102684\n",
      "three-layer Loss after iteration 25000: 14.375796704873927\n",
      "three-layer Loss after iteration 26000: 13.99996591505346\n",
      "three-layer Loss after iteration 27000: 14.59252972705664\n",
      "three-layer Loss after iteration 28000: 13.918900135199198\n",
      "three-layer Loss after iteration 29000: 13.722255167853517\n",
      "three-layer Loss after iteration 30000: 13.705206187773271\n",
      "three-layer Loss after iteration 31000: 13.514756282277606\n",
      "three-layer Loss after iteration 32000: 13.363484093068154\n",
      "three-layer Loss after iteration 33000: 13.24654762561612\n",
      "three-layer Loss after iteration 34000: 13.158843829168354\n",
      "three-layer Loss after iteration 35000: 13.183048919937452\n",
      "three-layer Loss after iteration 36000: 13.116014970800384\n",
      "three-layer Loss after iteration 37000: 13.038019075193874\n",
      "three-layer Loss after iteration 38000: 12.981108448902196\n",
      "three-layer Loss after iteration 39000: 12.930650704181728\n",
      "three-layer Loss after iteration 40000: 12.88773170469857\n",
      "three-layer Loss after iteration 41000: 12.397868845121094\n",
      "three-layer Loss after iteration 42000: 12.153067974836441\n",
      "three-layer Loss after iteration 43000: 12.070087222087022\n",
      "three-layer Loss after iteration 44000: 12.033114372062066\n",
      "three-layer Loss after iteration 45000: 12.010318220812394\n",
      "three-layer Loss after iteration 46000: 12.00037030629017\n",
      "three-layer Loss after iteration 47000: 11.97515389354016\n",
      "three-layer Loss after iteration 48000: 11.964169230675486\n",
      "three-layer Loss after iteration 49000: 11.955447932332076\n",
      "three-layer Loss after iteration 50000: 21.866543751105745\n",
      "three-layer Loss after iteration 51000: 13.862032505152007\n",
      "three-layer Loss after iteration 52000: 13.334962227394676\n",
      "three-layer Loss after iteration 53000: 13.047270216477886\n",
      "three-layer Loss after iteration 54000: 13.282440059850185\n",
      "three-layer Loss after iteration 55000: 13.62882965386234\n",
      "three-layer Loss after iteration 56000: 13.118429753244998\n",
      "three-layer Loss after iteration 57000: 13.370995228721181\n",
      "three-layer Loss after iteration 58000: 13.355083073526597\n",
      "three-layer Loss after iteration 59000: 13.110730023128013\n",
      "three-layer Loss after iteration 60000: 12.91778709908919\n",
      "three-layer Loss after iteration 61000: 12.762045211396982\n",
      "three-layer Loss after iteration 62000: 12.621378515042124\n",
      "three-layer Loss after iteration 63000: 12.523917260925955\n",
      "three-layer Loss after iteration 64000: 12.412387628973075\n",
      "three-layer Loss after iteration 65000: 12.372762670493978\n",
      "three-layer Loss after iteration 66000: 12.434611286535954\n",
      "three-layer Loss after iteration 67000: 12.162678158102969\n",
      "three-layer Loss after iteration 68000: 12.303031107300344\n",
      "three-layer Loss after iteration 69000: 12.265408574912831\n",
      "three-layer Loss after iteration 70000: 11.921469360665562\n",
      "three-layer Loss after iteration 71000: 11.694409352846883\n",
      "three-layer Loss after iteration 72000: 11.517564833216412\n",
      "three-layer Loss after iteration 73000: 11.348881949402125\n",
      "three-layer Loss after iteration 74000: 11.214894684037995\n",
      "three-layer Loss after iteration 75000: 11.210196544820663\n",
      "three-layer Loss after iteration 76000: 11.108525517190941\n",
      "three-layer Loss after iteration 77000: 11.05873685029782\n",
      "three-layer Loss after iteration 78000: 10.992991532740456\n",
      "three-layer Loss after iteration 79000: 11.295655170720327\n",
      "three-layer Loss after iteration 80000: 11.059237559161037\n",
      "three-layer Loss after iteration 81000: 10.962289602659084\n",
      "three-layer Loss after iteration 82000: 10.819184356069586\n",
      "three-layer Loss after iteration 83000: 10.743329349751454\n",
      "three-layer Loss after iteration 84000: 10.633997831388434\n",
      "three-layer Loss after iteration 85000: 10.568441446314852\n",
      "three-layer Loss after iteration 86000: 10.502421198130396\n",
      "three-layer Loss after iteration 87000: 10.417429277585196\n",
      "three-layer Loss after iteration 88000: 10.193476401075964\n",
      "three-layer Loss after iteration 89000: 10.432001438325921\n",
      "three-layer Loss after iteration 90000: 10.376414337714774\n",
      "three-layer Loss after iteration 91000: 10.313001860761077\n",
      "three-layer Loss after iteration 92000: 10.250968644083926\n",
      "three-layer Loss after iteration 93000: 10.20431735110393\n",
      "three-layer Loss after iteration 94000: 10.348951952985807\n",
      "three-layer Loss after iteration 95000: 10.203952373964647\n",
      "three-layer Loss after iteration 96000: 10.176377329935633\n",
      "three-layer Loss after iteration 97000: 10.13476635135891\n",
      "three-layer Loss after iteration 98000: 10.07983838239133\n",
      "three-layer Loss after iteration 99000: 10.02967974374075\n",
      "three-layer Loss after iteration 100000: 10.012107417929009\n",
      "three-layer Loss after iteration 101000: 9.983574344579546\n",
      "three-layer Loss after iteration 102000: 9.970229233941197\n",
      "three-layer Loss after iteration 103000: 9.861042092681213\n",
      "three-layer Loss after iteration 104000: 9.751850116712612\n",
      "three-layer Loss after iteration 105000: 9.770514441171509\n",
      "three-layer Loss after iteration 106000: 9.690490765501613\n",
      "three-layer Loss after iteration 107000: 9.740724800556636\n",
      "three-layer Loss after iteration 108000: 9.736347210728415\n",
      "three-layer Loss after iteration 109000: 9.833963007368405\n",
      "three-layer Loss after iteration 110000: 10.037459731662638\n",
      "three-layer Loss after iteration 111000: 9.99558364047334\n",
      "three-layer Loss after iteration 112000: 9.932110894298388\n",
      "three-layer Loss after iteration 113000: 9.880522004835607\n",
      "three-layer Loss after iteration 114000: 9.856793996457737\n",
      "three-layer Loss after iteration 115000: 9.818504214101102\n",
      "three-layer Loss after iteration 116000: 9.805818671840163\n",
      "three-layer Loss after iteration 117000: 9.76626038758363\n",
      "three-layer Loss after iteration 118000: 9.772757710980107\n",
      "three-layer Loss after iteration 119000: 9.750296490928275\n",
      "three-layer Loss after iteration 120000: 9.645027222493404\n",
      "three-layer Loss after iteration 121000: 9.763703021742035\n",
      "three-layer Loss after iteration 122000: 9.737081889753401\n",
      "three-layer Loss after iteration 123000: 9.758449765211095\n",
      "three-layer Loss after iteration 124000: 9.925800615924075\n",
      "three-layer Loss after iteration 125000: 9.862555218884157\n",
      "three-layer Loss after iteration 126000: 9.79957022744608\n",
      "three-layer Loss after iteration 127000: 9.738581833373479\n",
      "three-layer Loss after iteration 128000: 9.738102428890357\n",
      "4.922734042016546e-05 9.738581833373479 9.738102428890357\n",
      "three-layer Loss after iteration 0: 1704.4617277842683\n",
      "three-layer Loss after iteration 1000: 23.508875724860868\n",
      "three-layer Loss after iteration 2000: 28.110334406271875\n",
      "three-layer Loss after iteration 3000: 26.901208415997946\n",
      "three-layer Loss after iteration 4000: 23.898818864455333\n",
      "three-layer Loss after iteration 5000: 21.31437147899167\n",
      "three-layer Loss after iteration 6000: 19.96807033440863\n",
      "three-layer Loss after iteration 7000: 19.292259770215388\n",
      "three-layer Loss after iteration 8000: 18.141606104111514\n",
      "three-layer Loss after iteration 9000: 17.89911286923549\n",
      "three-layer Loss after iteration 10000: 17.78799196242419\n",
      "three-layer Loss after iteration 11000: 16.910793827784182\n",
      "three-layer Loss after iteration 12000: 17.071036292851254\n",
      "three-layer Loss after iteration 13000: 17.623713602542555\n",
      "three-layer Loss after iteration 14000: 14.711149409397002\n",
      "three-layer Loss after iteration 15000: 17.799790408924306\n",
      "three-layer Loss after iteration 16000: 14.966620167675973\n",
      "three-layer Loss after iteration 17000: 14.522123683171671\n",
      "three-layer Loss after iteration 18000: 13.63929054068704\n",
      "three-layer Loss after iteration 19000: 13.929120012288944\n",
      "three-layer Loss after iteration 20000: 13.643829027760237\n",
      "three-layer Loss after iteration 21000: 13.220193032953109\n",
      "three-layer Loss after iteration 22000: 13.003973631713857\n",
      "three-layer Loss after iteration 23000: 12.806201659506984\n",
      "three-layer Loss after iteration 24000: 12.649406027759918\n",
      "three-layer Loss after iteration 25000: 13.351218781630976\n",
      "three-layer Loss after iteration 26000: 13.218973171324345\n",
      "three-layer Loss after iteration 27000: 12.76613604117131\n",
      "three-layer Loss after iteration 28000: 11.85916806700126\n",
      "three-layer Loss after iteration 29000: 12.001232740785943\n",
      "three-layer Loss after iteration 30000: 11.64315826553359\n",
      "three-layer Loss after iteration 31000: 11.545717475860705\n",
      "three-layer Loss after iteration 32000: 11.283795996147896\n",
      "three-layer Loss after iteration 33000: 10.986375761205128\n",
      "three-layer Loss after iteration 34000: 11.202605473275376\n",
      "three-layer Loss after iteration 35000: 10.44855731382919\n",
      "three-layer Loss after iteration 36000: 10.310043019249582\n",
      "three-layer Loss after iteration 37000: 10.187417884963919\n",
      "three-layer Loss after iteration 38000: 11.008495081937198\n",
      "three-layer Loss after iteration 39000: 11.027228801982462\n",
      "three-layer Loss after iteration 40000: 10.412329971395387\n",
      "three-layer Loss after iteration 41000: 9.906550193890014\n",
      "three-layer Loss after iteration 42000: 9.688893452144017\n",
      "three-layer Loss after iteration 43000: 9.513989687719901\n",
      "three-layer Loss after iteration 44000: 9.248920413124198\n",
      "three-layer Loss after iteration 45000: 9.26352696849148\n",
      "three-layer Loss after iteration 46000: 9.048130543157606\n",
      "three-layer Loss after iteration 47000: 8.783717124020873\n",
      "three-layer Loss after iteration 48000: 8.7548781717759\n",
      "three-layer Loss after iteration 49000: 8.583961424056723\n",
      "three-layer Loss after iteration 50000: 8.417664403024297\n",
      "three-layer Loss after iteration 51000: 8.246653360995108\n",
      "three-layer Loss after iteration 52000: 8.08874397552685\n",
      "three-layer Loss after iteration 53000: 7.99375356248519\n",
      "three-layer Loss after iteration 54000: 8.373378815600082\n",
      "three-layer Loss after iteration 55000: 8.246488094037247\n",
      "three-layer Loss after iteration 56000: 8.107690838272696\n",
      "three-layer Loss after iteration 57000: 7.970505644600483\n",
      "three-layer Loss after iteration 58000: 7.874393569618265\n",
      "three-layer Loss after iteration 59000: 7.933405161340805\n",
      "three-layer Loss after iteration 60000: 7.902024332104504\n",
      "three-layer Loss after iteration 61000: 7.0560814029416035\n",
      "three-layer Loss after iteration 62000: 7.664660500909516\n",
      "three-layer Loss after iteration 63000: 7.298948669656531\n",
      "three-layer Loss after iteration 64000: 7.016991997059318\n",
      "three-layer Loss after iteration 65000: 6.417928212892107\n",
      "three-layer Loss after iteration 66000: 6.331227477597525\n",
      "three-layer Loss after iteration 67000: 7.045326659760768\n",
      "three-layer Loss after iteration 68000: 7.031365372006678\n",
      "three-layer Loss after iteration 69000: 6.139182028226155\n",
      "three-layer Loss after iteration 70000: 6.871122890293778\n",
      "three-layer Loss after iteration 71000: 6.72924902398645\n",
      "three-layer Loss after iteration 72000: 6.5523900959306145\n",
      "three-layer Loss after iteration 73000: 6.175772723735361\n",
      "three-layer Loss after iteration 74000: 6.602166878326866\n",
      "three-layer Loss after iteration 75000: 6.570888409338073\n",
      "three-layer Loss after iteration 76000: 6.19337039027624\n",
      "three-layer Loss after iteration 77000: 6.256236094713236\n",
      "three-layer Loss after iteration 78000: 6.459822862640877\n",
      "three-layer Loss after iteration 79000: 6.164841306762949\n",
      "three-layer Loss after iteration 80000: 6.056936692214376\n",
      "three-layer Loss after iteration 81000: 6.0062956583445155\n",
      "three-layer Loss after iteration 82000: 5.934649482970173\n",
      "three-layer Loss after iteration 83000: 5.890290003053194\n",
      "three-layer Loss after iteration 84000: 5.832309256647673\n",
      "three-layer Loss after iteration 85000: 5.777475555153517\n",
      "three-layer Loss after iteration 86000: 5.725490902114812\n",
      "three-layer Loss after iteration 87000: 7.360309654585983\n",
      "three-layer Loss after iteration 88000: 7.681012323554244\n",
      "three-layer Loss after iteration 89000: 6.362135536302883\n",
      "three-layer Loss after iteration 90000: 6.348111202652856\n",
      "three-layer Loss after iteration 91000: 6.268493104705946\n",
      "three-layer Loss after iteration 92000: 6.230512084776687\n",
      "three-layer Loss after iteration 93000: 6.184665623528599\n",
      "three-layer Loss after iteration 94000: 6.225746432474889\n",
      "three-layer Loss after iteration 95000: 6.213823375072203\n",
      "three-layer Loss after iteration 96000: 6.143517029168391\n",
      "three-layer Loss after iteration 97000: 6.084668405939614\n",
      "three-layer Loss after iteration 98000: 6.011419108267045\n",
      "three-layer Loss after iteration 99000: 5.949388034099684\n",
      "three-layer Loss after iteration 100000: 5.831547924426745\n",
      "three-layer Loss after iteration 101000: 5.928818040675443\n",
      "three-layer Loss after iteration 102000: 6.06080833734964\n",
      "three-layer Loss after iteration 103000: 6.252867131138822\n",
      "three-layer Loss after iteration 104000: 5.830568319917626\n",
      "three-layer Loss after iteration 105000: 5.034252994594863\n",
      "three-layer Loss after iteration 106000: 5.153096118680829\n",
      "three-layer Loss after iteration 107000: 6.442152122267968\n",
      "three-layer Loss after iteration 108000: 5.261016307614086\n",
      "three-layer Loss after iteration 109000: 5.253173729387383\n",
      "three-layer Loss after iteration 110000: 4.859277566805065\n",
      "three-layer Loss after iteration 111000: 5.266318278936687\n",
      "three-layer Loss after iteration 112000: 5.133776717981267\n",
      "three-layer Loss after iteration 113000: 4.958348412389155\n",
      "three-layer Loss after iteration 114000: 6.43870158563079\n",
      "three-layer Loss after iteration 115000: 4.807002428797237\n",
      "three-layer Loss after iteration 116000: 4.722806248953011\n",
      "three-layer Loss after iteration 117000: 4.748700145179381\n",
      "three-layer Loss after iteration 118000: 4.947750457388854\n",
      "three-layer Loss after iteration 119000: 5.145796356098608\n",
      "three-layer Loss after iteration 120000: 4.768298941869159\n",
      "three-layer Loss after iteration 121000: 5.131439132829283\n",
      "three-layer Loss after iteration 122000: 5.0288532497586065\n",
      "three-layer Loss after iteration 123000: 4.836863633157815\n",
      "three-layer Loss after iteration 124000: 5.012616512607335\n",
      "three-layer Loss after iteration 125000: 5.418804738258279\n",
      "three-layer Loss after iteration 126000: 4.502303464836278\n",
      "three-layer Loss after iteration 127000: 5.038946834734217\n",
      "three-layer Loss after iteration 128000: 4.887696964258898\n",
      "three-layer Loss after iteration 129000: 4.653788085101157\n",
      "three-layer Loss after iteration 130000: 4.881011357836801\n",
      "three-layer Loss after iteration 131000: 4.769469438692682\n",
      "three-layer Loss after iteration 132000: 4.756933239640411\n",
      "three-layer Loss after iteration 133000: 4.731242995808865\n",
      "three-layer Loss after iteration 134000: 4.70940443366762\n",
      "three-layer Loss after iteration 135000: 4.69082796891576\n",
      "three-layer Loss after iteration 136000: 4.672187518704883\n",
      "three-layer Loss after iteration 137000: 4.654300769655038\n",
      "three-layer Loss after iteration 138000: 4.636773402752626\n",
      "three-layer Loss after iteration 139000: 4.619577403012069\n",
      "three-layer Loss after iteration 140000: 4.602706499771718\n",
      "three-layer Loss after iteration 141000: 4.5861538787985285\n",
      "three-layer Loss after iteration 142000: 4.570509217055848\n",
      "three-layer Loss after iteration 143000: 4.5571072312173815\n",
      "three-layer Loss after iteration 144000: 4.389545467010276\n",
      "three-layer Loss after iteration 145000: 4.998707131953115\n",
      "three-layer Loss after iteration 146000: 4.679362508965643\n",
      "three-layer Loss after iteration 147000: 4.615799653796299\n",
      "three-layer Loss after iteration 148000: 5.1952811243196875\n",
      "three-layer Loss after iteration 149000: 4.63233090393936\n",
      "three-layer Loss after iteration 150000: 5.173203655824401\n",
      "three-layer Loss after iteration 151000: 4.742423649064149\n",
      "three-layer Loss after iteration 152000: 4.981743830962516\n",
      "three-layer Loss after iteration 153000: 4.946464887008526\n",
      "three-layer Loss after iteration 154000: 5.145178168009457\n",
      "three-layer Loss after iteration 155000: 4.689689158704554\n",
      "three-layer Loss after iteration 156000: 4.709553594860866\n",
      "three-layer Loss after iteration 157000: 4.685983373956568\n",
      "three-layer Loss after iteration 158000: 4.6677694905556075\n",
      "three-layer Loss after iteration 159000: 5.332044615358338\n",
      "three-layer Loss after iteration 160000: 4.625059129430169\n",
      "three-layer Loss after iteration 161000: 5.075519375029223\n",
      "three-layer Loss after iteration 162000: 4.562919415878888\n",
      "three-layer Loss after iteration 163000: 4.752102176965234\n",
      "three-layer Loss after iteration 164000: 4.856371550932185\n",
      "three-layer Loss after iteration 165000: 4.582532306526743\n",
      "three-layer Loss after iteration 166000: 4.9160565367226186\n",
      "three-layer Loss after iteration 167000: 5.249820568447012\n",
      "three-layer Loss after iteration 168000: 5.58243825959856\n",
      "three-layer Loss after iteration 169000: 5.418901624129891\n",
      "three-layer Loss after iteration 170000: 5.494549655583568\n",
      "three-layer Loss after iteration 171000: 4.8223042166682735\n",
      "three-layer Loss after iteration 172000: 5.264211041402136\n",
      "three-layer Loss after iteration 173000: 4.322728661879291\n",
      "three-layer Loss after iteration 174000: 4.470371271125604\n",
      "three-layer Loss after iteration 175000: 4.960602952008056\n",
      "three-layer Loss after iteration 176000: 4.909351685327244\n",
      "three-layer Loss after iteration 177000: 5.0545072733452265\n",
      "three-layer Loss after iteration 178000: 4.961274165453613\n",
      "three-layer Loss after iteration 179000: 4.996638047733548\n",
      "three-layer Loss after iteration 180000: 4.829116560534084\n",
      "three-layer Loss after iteration 181000: 5.787846160704944\n",
      "three-layer Loss after iteration 182000: 5.093196962819832\n",
      "three-layer Loss after iteration 183000: 5.879515705576676\n",
      "three-layer Loss after iteration 184000: 5.050849261061609\n",
      "three-layer Loss after iteration 185000: 5.273433832280385\n",
      "three-layer Loss after iteration 186000: 5.333687870554512\n",
      "three-layer Loss after iteration 187000: 5.136527537439743\n",
      "three-layer Loss after iteration 188000: 5.121984427240762\n",
      "three-layer Loss after iteration 189000: 5.156433356110511\n",
      "three-layer Loss after iteration 190000: 7.35989562393998\n",
      "three-layer Loss after iteration 191000: 4.148929773145\n",
      "three-layer Loss after iteration 192000: 4.4846955294768405\n",
      "three-layer Loss after iteration 193000: 5.961248301366475\n",
      "three-layer Loss after iteration 194000: 4.166511963536124\n",
      "three-layer Loss after iteration 195000: 5.475451594360439\n",
      "three-layer Loss after iteration 196000: 5.705453259804909\n",
      "three-layer Loss after iteration 197000: 4.48062656854823\n",
      "three-layer Loss after iteration 198000: 5.000263105036136\n",
      "three-layer Loss after iteration 199000: 5.893303071125524\n",
      "three-layer Loss after iteration 200000: 4.186858000288762\n",
      "three-layer Loss after iteration 201000: 4.836247312411493\n",
      "three-layer Loss after iteration 202000: 6.878239929030944\n",
      "three-layer Loss after iteration 203000: 5.0269037054358945\n",
      "three-layer Loss after iteration 204000: 4.1378636574695244\n",
      "three-layer Loss after iteration 205000: 4.396292554366483\n",
      "three-layer Loss after iteration 206000: 5.352010028697033\n",
      "three-layer Loss after iteration 207000: 6.698177280374757\n",
      "three-layer Loss after iteration 208000: 4.962844509053803\n",
      "three-layer Loss after iteration 209000: 4.2956664862509015\n",
      "three-layer Loss after iteration 210000: 4.078228031553033\n",
      "three-layer Loss after iteration 211000: 4.0801941218455156\n",
      "three-layer Loss after iteration 212000: 4.061872842273262\n",
      "three-layer Loss after iteration 213000: 4.119799018945504\n",
      "three-layer Loss after iteration 214000: 4.398059563046976\n",
      "three-layer Loss after iteration 215000: 4.904459753816423\n",
      "three-layer Loss after iteration 216000: 5.989093430021257\n",
      "three-layer Loss after iteration 217000: 6.110331139808938\n",
      "three-layer Loss after iteration 218000: 4.830563323580441\n",
      "three-layer Loss after iteration 219000: 4.056811000386689\n",
      "three-layer Loss after iteration 220000: 4.313865325708466\n",
      "three-layer Loss after iteration 221000: 5.944290981055952\n",
      "three-layer Loss after iteration 222000: 4.013285014480441\n",
      "three-layer Loss after iteration 223000: 5.613941529490288\n",
      "three-layer Loss after iteration 224000: 4.518526417233682\n",
      "three-layer Loss after iteration 225000: 4.736318774854018\n",
      "three-layer Loss after iteration 226000: 4.5891132339832845\n",
      "three-layer Loss after iteration 227000: 4.8479351831251565\n",
      "three-layer Loss after iteration 228000: 4.6527259689066485\n",
      "three-layer Loss after iteration 229000: 4.667557151055625\n",
      "three-layer Loss after iteration 230000: 3.83916709335628\n",
      "three-layer Loss after iteration 231000: 3.9649410843376196\n",
      "three-layer Loss after iteration 232000: 4.499102048535865\n",
      "three-layer Loss after iteration 233000: 5.109474663721782\n",
      "three-layer Loss after iteration 234000: 3.9678834189565997\n",
      "three-layer Loss after iteration 235000: 6.104324323192376\n",
      "three-layer Loss after iteration 236000: 3.9459733005414757\n",
      "three-layer Loss after iteration 237000: 4.998857541916223\n",
      "three-layer Loss after iteration 238000: 5.033406180094012\n",
      "three-layer Loss after iteration 239000: 4.634010704120873\n",
      "three-layer Loss after iteration 240000: 7.2634539377739635\n",
      "three-layer Loss after iteration 241000: 4.872565299877661\n",
      "three-layer Loss after iteration 242000: 4.449772790620095\n",
      "three-layer Loss after iteration 243000: 4.477385065367188\n",
      "three-layer Loss after iteration 244000: 3.8365921616630034\n",
      "three-layer Loss after iteration 245000: 3.8300747781701725\n",
      "three-layer Loss after iteration 246000: 3.775624608130759\n",
      "three-layer Loss after iteration 247000: 3.8565127534120682\n",
      "three-layer Loss after iteration 248000: 4.218903378870727\n",
      "three-layer Loss after iteration 249000: 5.051988512593087\n",
      "three-layer Loss after iteration 250000: 6.619397697613735\n",
      "three-layer Loss after iteration 251000: 3.736243486085735\n",
      "three-layer Loss after iteration 252000: 3.8125376434859817\n",
      "three-layer Loss after iteration 253000: 3.949357861901316\n",
      "three-layer Loss after iteration 254000: 4.102206031208907\n",
      "three-layer Loss after iteration 255000: 4.105472639104015\n",
      "three-layer Loss after iteration 256000: 4.160094903137497\n",
      "three-layer Loss after iteration 257000: 4.129628644044189\n",
      "three-layer Loss after iteration 258000: 4.048151822758809\n",
      "three-layer Loss after iteration 259000: 3.9830066959610693\n",
      "three-layer Loss after iteration 260000: 4.3711653396865895\n",
      "three-layer Loss after iteration 261000: 5.901030642896636\n",
      "three-layer Loss after iteration 262000: 4.238345614588242\n",
      "three-layer Loss after iteration 263000: 4.2923130018366935\n",
      "three-layer Loss after iteration 264000: 4.991918660009331\n",
      "three-layer Loss after iteration 265000: 4.606237772921654\n",
      "three-layer Loss after iteration 266000: 4.332278692551549\n",
      "three-layer Loss after iteration 267000: 4.262576645400428\n",
      "three-layer Loss after iteration 268000: 4.1007432574785145\n",
      "three-layer Loss after iteration 269000: 4.300116453030434\n",
      "three-layer Loss after iteration 270000: 4.217646883957787\n",
      "three-layer Loss after iteration 271000: 4.433977105617846\n",
      "three-layer Loss after iteration 272000: 5.916932570467501\n",
      "three-layer Loss after iteration 273000: 4.309266218851946\n",
      "three-layer Loss after iteration 274000: 4.498923054528611\n",
      "three-layer Loss after iteration 275000: 4.335020713540306\n",
      "three-layer Loss after iteration 276000: 4.649385643814384\n",
      "three-layer Loss after iteration 277000: 4.5756570914263515\n",
      "three-layer Loss after iteration 278000: 4.314454621969865\n",
      "three-layer Loss after iteration 279000: 4.4691245978060055\n",
      "three-layer Loss after iteration 280000: 4.639045854384808\n",
      "three-layer Loss after iteration 281000: 4.15029997814281\n",
      "three-layer Loss after iteration 282000: 4.381050849008422\n",
      "three-layer Loss after iteration 283000: 4.48323701398278\n",
      "three-layer Loss after iteration 284000: 4.438688139634317\n",
      "three-layer Loss after iteration 285000: 4.631063308185336\n",
      "three-layer Loss after iteration 286000: 4.7738594772863205\n",
      "three-layer Loss after iteration 287000: 3.9982241575773796\n",
      "three-layer Loss after iteration 288000: 5.48497548765809\n",
      "three-layer Loss after iteration 289000: 4.315518573340678\n",
      "three-layer Loss after iteration 290000: 4.593519602602988\n",
      "three-layer Loss after iteration 291000: 4.9974406549857004\n",
      "three-layer Loss after iteration 292000: 4.889108571529213\n",
      "three-layer Loss after iteration 293000: 4.528867644023752\n",
      "three-layer Loss after iteration 294000: 5.054784120608248\n",
      "three-layer Loss after iteration 295000: 4.767986500205944\n",
      "three-layer Loss after iteration 296000: 4.78272333675929\n",
      "three-layer Loss after iteration 297000: 4.764224409420998\n",
      "three-layer Loss after iteration 298000: 4.7447877623494925\n",
      "three-layer Loss after iteration 299000: 4.727345614541953\n",
      "three-layer Loss after iteration 300000: 4.711481969418754\n",
      "three-layer Loss after iteration 301000: 4.6981197084297674\n",
      "three-layer Loss after iteration 302000: 4.682661885199944\n",
      "three-layer Loss after iteration 303000: 4.669331684160368\n",
      "three-layer Loss after iteration 304000: 4.656386424314179\n",
      "three-layer Loss after iteration 305000: 4.642370536584874\n",
      "three-layer Loss after iteration 306000: 4.629195186063519\n",
      "three-layer Loss after iteration 307000: 4.61796832179318\n",
      "three-layer Loss after iteration 308000: 4.605008421252906\n",
      "three-layer Loss after iteration 309000: 4.5934788727677125\n",
      "three-layer Loss after iteration 310000: 4.582582888193097\n",
      "three-layer Loss after iteration 311000: 4.569900221502521\n",
      "three-layer Loss after iteration 312000: 4.559531194056942\n",
      "three-layer Loss after iteration 313000: 4.55006915741597\n",
      "three-layer Loss after iteration 314000: 4.541333716941092\n",
      "three-layer Loss after iteration 315000: 4.609039362812677\n",
      "three-layer Loss after iteration 316000: 4.573811296040478\n",
      "three-layer Loss after iteration 317000: 4.559413137447764\n",
      "three-layer Loss after iteration 318000: 4.548191463024491\n",
      "three-layer Loss after iteration 319000: 4.538433941641797\n",
      "three-layer Loss after iteration 320000: 4.529572697684696\n",
      "three-layer Loss after iteration 321000: 4.521292763627238\n",
      "three-layer Loss after iteration 322000: 4.509963938527721\n",
      "three-layer Loss after iteration 323000: 4.501373701297924\n",
      "three-layer Loss after iteration 324000: 4.4914650319242595\n",
      "three-layer Loss after iteration 325000: 4.48148179894382\n",
      "three-layer Loss after iteration 326000: 4.472354380577494\n",
      "three-layer Loss after iteration 327000: 4.463103901709651\n",
      "three-layer Loss after iteration 328000: 4.454364564690288\n",
      "three-layer Loss after iteration 329000: 4.445691170634446\n",
      "three-layer Loss after iteration 330000: 4.437357544777387\n",
      "three-layer Loss after iteration 331000: 4.429583721959046\n",
      "three-layer Loss after iteration 332000: 4.420394178032641\n",
      "three-layer Loss after iteration 333000: 4.413337072443011\n",
      "three-layer Loss after iteration 334000: 4.404823145740635\n",
      "three-layer Loss after iteration 335000: 4.396855693318114\n",
      "three-layer Loss after iteration 336000: 4.389737638046403\n",
      "three-layer Loss after iteration 337000: 4.381854550423116\n",
      "three-layer Loss after iteration 338000: 4.374624107621022\n",
      "three-layer Loss after iteration 339000: 4.367290730383611\n",
      "three-layer Loss after iteration 340000: 4.360457190613105\n",
      "three-layer Loss after iteration 341000: 4.35212939386568\n",
      "three-layer Loss after iteration 342000: 4.345274200272312\n",
      "three-layer Loss after iteration 343000: 4.3388984516406515\n",
      "three-layer Loss after iteration 344000: 4.330541300454484\n",
      "three-layer Loss after iteration 345000: 4.325963403690455\n",
      "three-layer Loss after iteration 346000: 4.322203198376994\n",
      "three-layer Loss after iteration 347000: 4.316260366949757\n",
      "three-layer Loss after iteration 348000: 4.309442198621646\n",
      "three-layer Loss after iteration 349000: 4.303498537721465\n",
      "three-layer Loss after iteration 350000: 4.297369752919437\n",
      "three-layer Loss after iteration 351000: 4.291085524951461\n",
      "three-layer Loss after iteration 352000: 4.336161927607325\n",
      "three-layer Loss after iteration 353000: 4.437640746082023\n",
      "three-layer Loss after iteration 354000: 4.4482994805005625\n",
      "three-layer Loss after iteration 355000: 4.445829083046289\n",
      "three-layer Loss after iteration 356000: 4.4387878027204035\n",
      "three-layer Loss after iteration 357000: 4.4782921051484745\n",
      "three-layer Loss after iteration 358000: 4.483751306594031\n",
      "three-layer Loss after iteration 359000: 4.5177232485246765\n",
      "three-layer Loss after iteration 360000: 4.525623810117888\n",
      "three-layer Loss after iteration 361000: 4.5214407263187955\n",
      "three-layer Loss after iteration 362000: 4.513927668267581\n",
      "three-layer Loss after iteration 363000: 4.507818443149112\n",
      "three-layer Loss after iteration 364000: 4.498071265417302\n",
      "three-layer Loss after iteration 365000: 4.488418887825737\n",
      "three-layer Loss after iteration 366000: 4.480138582020908\n",
      "three-layer Loss after iteration 367000: 4.47101435044823\n",
      "three-layer Loss after iteration 368000: 4.461518361058528\n",
      "three-layer Loss after iteration 369000: 4.453176672183226\n",
      "three-layer Loss after iteration 370000: 4.444480173282024\n",
      "three-layer Loss after iteration 371000: 4.436642129673588\n",
      "three-layer Loss after iteration 372000: 4.427058986068039\n",
      "three-layer Loss after iteration 373000: 4.419661711059168\n",
      "three-layer Loss after iteration 374000: 4.411088460851165\n",
      "three-layer Loss after iteration 375000: 4.402520295603382\n",
      "three-layer Loss after iteration 376000: 4.395039158237567\n",
      "three-layer Loss after iteration 377000: 4.38752121587532\n",
      "three-layer Loss after iteration 378000: 4.379795715610681\n",
      "three-layer Loss after iteration 379000: 4.371868742508674\n",
      "three-layer Loss after iteration 380000: 4.365067052752944\n",
      "three-layer Loss after iteration 381000: 4.357272640289297\n",
      "three-layer Loss after iteration 382000: 4.349943254724839\n",
      "three-layer Loss after iteration 383000: 4.342639101739405\n",
      "three-layer Loss after iteration 384000: 4.3361759538348\n",
      "three-layer Loss after iteration 385000: 4.329440246573541\n",
      "three-layer Loss after iteration 386000: 4.3218976040557555\n",
      "three-layer Loss after iteration 387000: 4.315751625181818\n",
      "three-layer Loss after iteration 388000: 4.3086926604296085\n",
      "three-layer Loss after iteration 389000: 4.3008398423509195\n",
      "three-layer Loss after iteration 390000: 4.295124308586214\n",
      "three-layer Loss after iteration 391000: 4.289709916620963\n",
      "three-layer Loss after iteration 392000: 4.2845355970049965\n",
      "three-layer Loss after iteration 393000: 4.279392971706988\n",
      "three-layer Loss after iteration 394000: 4.274345567693021\n",
      "three-layer Loss after iteration 395000: 4.269357487275425\n",
      "three-layer Loss after iteration 396000: 4.264420901573834\n",
      "three-layer Loss after iteration 397000: 4.2595293235221625\n",
      "three-layer Loss after iteration 398000: 4.254678497049111\n",
      "three-layer Loss after iteration 399000: 4.249866031984502\n",
      "three-layer Loss after iteration 400000: 4.245090478501177\n",
      "three-layer Loss after iteration 401000: 4.633630038536562\n",
      "three-layer Loss after iteration 402000: 4.757874051106528\n",
      "three-layer Loss after iteration 403000: 4.715511446936212\n",
      "three-layer Loss after iteration 404000: 4.6883801995467165\n",
      "three-layer Loss after iteration 405000: 4.665669204515129\n",
      "three-layer Loss after iteration 406000: 4.647402563554842\n",
      "three-layer Loss after iteration 407000: 4.628527149900162\n",
      "three-layer Loss after iteration 408000: 4.61179236499651\n",
      "three-layer Loss after iteration 409000: 4.5964857039271525\n",
      "three-layer Loss after iteration 410000: 4.581214123191477\n",
      "three-layer Loss after iteration 411000: 4.570071670824648\n",
      "three-layer Loss after iteration 412000: 4.559813390798682\n",
      "three-layer Loss after iteration 413000: 4.5501611677315665\n",
      "three-layer Loss after iteration 414000: 4.540939838911498\n",
      "three-layer Loss after iteration 415000: 4.5320649205170085\n",
      "three-layer Loss after iteration 416000: 4.523478243895085\n",
      "three-layer Loss after iteration 417000: 4.515137714096939\n",
      "three-layer Loss after iteration 418000: 4.507012768989432\n",
      "three-layer Loss after iteration 419000: 4.499080739888509\n",
      "three-layer Loss after iteration 420000: 4.491324222864973\n",
      "three-layer Loss after iteration 421000: 4.483729342211173\n",
      "three-layer Loss after iteration 422000: 4.476284645431942\n",
      "three-layer Loss after iteration 423000: 4.468980404435524\n",
      "three-layer Loss after iteration 424000: 4.461808168155822\n",
      "three-layer Loss after iteration 425000: 4.454760469331952\n",
      "three-layer Loss after iteration 426000: 4.447830626372719\n",
      "three-layer Loss after iteration 427000: 4.44101260479546\n",
      "three-layer Loss after iteration 428000: 4.434300916774083\n",
      "three-layer Loss after iteration 429000: 4.4276905456736495\n",
      "three-layer Loss after iteration 430000: 4.421176887405295\n",
      "three-layer Loss after iteration 431000: 4.414755703433889\n",
      "three-layer Loss after iteration 432000: 4.4084230820981904\n",
      "three-layer Loss after iteration 433000: 4.402175406036472\n",
      "three-layer Loss after iteration 434000: 4.396009324241544\n",
      "three-layer Loss after iteration 435000: 4.38104047527551\n",
      "three-layer Loss after iteration 436000: 4.396603840350504\n",
      "three-layer Loss after iteration 437000: 4.396210529374679\n",
      "8.945790662666505e-05 4.396603840350504 4.396210529374679\n",
      "three-layer Loss after iteration 0: 1790.6744991065518\n",
      "three-layer Loss after iteration 1000: 32.24623131954933\n",
      "three-layer Loss after iteration 2000: 21.995701211133223\n",
      "three-layer Loss after iteration 3000: 18.16087587215166\n",
      "three-layer Loss after iteration 4000: 16.262455169487424\n",
      "three-layer Loss after iteration 5000: 14.52375037282436\n",
      "three-layer Loss after iteration 6000: 12.910478938369186\n",
      "three-layer Loss after iteration 7000: 12.323796437409445\n",
      "three-layer Loss after iteration 8000: 12.504349463366744\n",
      "three-layer Loss after iteration 9000: 12.283007714310799\n",
      "three-layer Loss after iteration 10000: 11.227109630432922\n",
      "three-layer Loss after iteration 11000: 11.379544482096104\n",
      "three-layer Loss after iteration 12000: 11.249398263167228\n",
      "three-layer Loss after iteration 13000: 11.146286442018168\n",
      "three-layer Loss after iteration 14000: 11.066069838231147\n",
      "three-layer Loss after iteration 15000: 10.999766698828147\n",
      "three-layer Loss after iteration 16000: 10.943695279680362\n",
      "three-layer Loss after iteration 17000: 10.84791953244217\n",
      "three-layer Loss after iteration 18000: 10.877219816894117\n",
      "three-layer Loss after iteration 19000: 10.975804391205838\n",
      "three-layer Loss after iteration 20000: 10.880292404344898\n",
      "three-layer Loss after iteration 21000: 10.809558839689283\n",
      "three-layer Loss after iteration 22000: 10.751605783436744\n",
      "three-layer Loss after iteration 23000: 10.382803503488125\n",
      "three-layer Loss after iteration 24000: 10.999228450101885\n",
      "three-layer Loss after iteration 25000: 10.776399069105446\n",
      "three-layer Loss after iteration 26000: 10.690069347390272\n",
      "three-layer Loss after iteration 27000: 10.633771565546354\n",
      "three-layer Loss after iteration 28000: 10.509769167672651\n",
      "three-layer Loss after iteration 29000: 10.499298568048504\n",
      "three-layer Loss after iteration 30000: 10.514397611024005\n",
      "three-layer Loss after iteration 31000: 10.474687074218\n",
      "three-layer Loss after iteration 32000: 10.29592216817945\n",
      "three-layer Loss after iteration 33000: 10.203099856020549\n",
      "three-layer Loss after iteration 34000: 10.163779427020678\n",
      "three-layer Loss after iteration 35000: 10.135303475845856\n",
      "three-layer Loss after iteration 36000: 10.132011067432064\n",
      "three-layer Loss after iteration 37000: 10.120118801010799\n",
      "three-layer Loss after iteration 38000: 10.265384490364097\n",
      "three-layer Loss after iteration 39000: 10.250913755981852\n",
      "three-layer Loss after iteration 40000: 10.38511915131274\n",
      "three-layer Loss after iteration 41000: 10.427886525619693\n",
      "three-layer Loss after iteration 42000: 10.487969811374212\n",
      "three-layer Loss after iteration 43000: 10.527055548657028\n",
      "three-layer Loss after iteration 44000: 10.57698933968396\n",
      "three-layer Loss after iteration 45000: 10.317094644177097\n",
      "three-layer Loss after iteration 46000: 10.416520647522141\n",
      "three-layer Loss after iteration 47000: 10.282385292919393\n",
      "three-layer Loss after iteration 48000: 10.283723439922909\n",
      "three-layer Loss after iteration 49000: 10.254656777560596\n",
      "three-layer Loss after iteration 50000: 10.229826123552552\n",
      "three-layer Loss after iteration 51000: 10.213542495522132\n",
      "three-layer Loss after iteration 52000: 10.199948218391182\n",
      "three-layer Loss after iteration 53000: 10.184576209505684\n",
      "three-layer Loss after iteration 54000: 10.173133867748016\n",
      "three-layer Loss after iteration 55000: 10.163385194819773\n",
      "three-layer Loss after iteration 56000: 10.155542058714092\n",
      "three-layer Loss after iteration 57000: 10.149775855334127\n",
      "three-layer Loss after iteration 58000: 10.142170690674618\n",
      "three-layer Loss after iteration 59000: 10.137624810674364\n",
      "three-layer Loss after iteration 60000: 10.1311824590527\n",
      "three-layer Loss after iteration 61000: 10.127815563918716\n",
      "three-layer Loss after iteration 62000: 10.123112289869377\n",
      "three-layer Loss after iteration 63000: 10.119705296164039\n",
      "three-layer Loss after iteration 64000: 10.11663038651298\n",
      "three-layer Loss after iteration 65000: 10.114409551644608\n",
      "three-layer Loss after iteration 66000: 10.111823583643767\n",
      "three-layer Loss after iteration 67000: 10.109683271779701\n",
      "three-layer Loss after iteration 68000: 10.107263074468117\n",
      "three-layer Loss after iteration 69000: 10.105367858743216\n",
      "three-layer Loss after iteration 70000: 10.102487363536477\n",
      "three-layer Loss after iteration 71000: 10.101217022048909\n",
      "three-layer Loss after iteration 72000: 10.09973845614619\n",
      "three-layer Loss after iteration 73000: 10.098453095851287\n",
      "three-layer Loss after iteration 74000: 10.097862291784363\n",
      "5.8504412637886114e-05 10.098453095851287 10.097862291784363\n",
      "three-layer Loss after iteration 0: 1701.5055685046532\n",
      "three-layer Loss after iteration 1000: 19.404883366379806\n",
      "three-layer Loss after iteration 2000: 9.393858045172994\n",
      "three-layer Loss after iteration 3000: 6.730503602415663\n",
      "three-layer Loss after iteration 4000: 5.953746979746417\n",
      "three-layer Loss after iteration 5000: 6.564421148342384\n",
      "three-layer Loss after iteration 6000: 5.895687303779595\n",
      "three-layer Loss after iteration 7000: 5.711133887209835\n",
      "three-layer Loss after iteration 8000: 5.564067258273693\n",
      "three-layer Loss after iteration 9000: 4.8175703597654005\n",
      "three-layer Loss after iteration 10000: 4.909218067721365\n",
      "three-layer Loss after iteration 11000: 4.656096249884379\n",
      "three-layer Loss after iteration 12000: 4.772920761057007\n",
      "three-layer Loss after iteration 13000: 4.575608308665638\n",
      "three-layer Loss after iteration 14000: 4.451769630858077\n",
      "three-layer Loss after iteration 15000: 4.386008745943934\n",
      "three-layer Loss after iteration 16000: 4.34998262918173\n",
      "three-layer Loss after iteration 17000: 4.329180355166137\n",
      "three-layer Loss after iteration 18000: 4.316418997180209\n",
      "three-layer Loss after iteration 19000: 4.308038473154376\n",
      "three-layer Loss after iteration 20000: 4.302114134764597\n",
      "three-layer Loss after iteration 21000: 4.297608862667063\n",
      "three-layer Loss after iteration 22000: 4.293951876877019\n",
      "three-layer Loss after iteration 23000: 4.2908193826340355\n",
      "three-layer Loss after iteration 24000: 4.288016256400595\n",
      "three-layer Loss after iteration 25000: 4.285413052315576\n",
      "three-layer Loss after iteration 26000: 4.282917815189656\n",
      "three-layer Loss after iteration 27000: 4.280469757694705\n",
      "three-layer Loss after iteration 28000: 4.2780396833634455\n",
      "three-layer Loss after iteration 29000: 4.275624101849355\n",
      "three-layer Loss after iteration 30000: 4.273231917361904\n",
      "three-layer Loss after iteration 31000: 4.270871810868356\n",
      "three-layer Loss after iteration 32000: 4.268546333093983\n",
      "three-layer Loss after iteration 33000: 4.2662523855374115\n",
      "three-layer Loss after iteration 34000: 4.263984681386627\n",
      "three-layer Loss after iteration 35000: 4.261739037529584\n",
      "three-layer Loss after iteration 36000: 4.2595138473052945\n",
      "three-layer Loss after iteration 37000: 4.257309670765197\n",
      "three-layer Loss after iteration 38000: 4.2551278957999985\n",
      "three-layer Loss after iteration 39000: 4.2529695396681415\n",
      "three-layer Loss after iteration 40000: 4.250834742807487\n",
      "three-layer Loss after iteration 41000: 4.248722916743375\n",
      "three-layer Loss after iteration 42000: 4.246633198337951\n",
      "three-layer Loss after iteration 43000: 4.2445648600895725\n",
      "three-layer Loss after iteration 44000: 4.24251749295434\n",
      "three-layer Loss after iteration 45000: 4.240490963541996\n",
      "three-layer Loss after iteration 46000: 4.238485256427192\n",
      "three-layer Loss after iteration 47000: 4.236500323237171\n",
      "three-layer Loss after iteration 48000: 4.234536007765692\n",
      "three-layer Loss after iteration 49000: 4.232592052290891\n",
      "three-layer Loss after iteration 50000: 4.230668150072631\n",
      "three-layer Loss after iteration 51000: 4.2287640015484085\n",
      "three-layer Loss after iteration 52000: 4.2268793468931545\n",
      "three-layer Loss after iteration 53000: 4.225013969185793\n",
      "three-layer Loss after iteration 54000: 4.223167678041769\n",
      "three-layer Loss after iteration 55000: 4.221340288553064\n",
      "three-layer Loss after iteration 56000: 4.2195316067277515\n",
      "three-layer Loss after iteration 57000: 4.217741425415032\n",
      "three-layer Loss after iteration 58000: 4.21596952854977\n",
      "three-layer Loss after iteration 59000: 4.214215698740588\n",
      "three-layer Loss after iteration 60000: 4.212479723663637\n",
      "three-layer Loss after iteration 61000: 4.210761398949283\n",
      "three-layer Loss after iteration 62000: 4.209060527613094\n",
      "three-layer Loss after iteration 63000: 4.207376917529042\n",
      "three-layer Loss after iteration 64000: 4.205710378709817\n",
      "three-layer Loss after iteration 65000: 4.204060721590103\n",
      "three-layer Loss after iteration 66000: 4.202427756653522\n",
      "three-layer Loss after iteration 67000: 4.200811295067624\n",
      "three-layer Loss after iteration 68000: 4.199211149701702\n",
      "three-layer Loss after iteration 69000: 4.19762713597218\n",
      "three-layer Loss after iteration 70000: 4.196059072229099\n",
      "three-layer Loss after iteration 71000: 4.194506779681519\n",
      "three-layer Loss after iteration 72000: 4.192970082043851\n",
      "three-layer Loss after iteration 73000: 4.1914488051301975\n",
      "three-layer Loss after iteration 74000: 4.189942776564434\n",
      "three-layer Loss after iteration 75000: 4.188451825670776\n",
      "three-layer Loss after iteration 76000: 4.186975783517557\n",
      "three-layer Loss after iteration 77000: 4.185514483038386\n",
      "three-layer Loss after iteration 78000: 4.184067759150532\n",
      "three-layer Loss after iteration 79000: 4.182635448819573\n",
      "three-layer Loss after iteration 80000: 4.18121739105645\n",
      "three-layer Loss after iteration 81000: 4.179813426862491\n",
      "three-layer Loss after iteration 82000: 4.178423399152409\n",
      "three-layer Loss after iteration 83000: 4.177047152682769\n",
      "three-layer Loss after iteration 84000: 4.175684534003087\n",
      "three-layer Loss after iteration 85000: 4.1743353914323444\n",
      "three-layer Loss after iteration 86000: 4.172999575054168\n",
      "three-layer Loss after iteration 87000: 4.171676936719156\n",
      "three-layer Loss after iteration 88000: 4.17036733004451\n",
      "three-layer Loss after iteration 89000: 4.169070610404701\n",
      "three-layer Loss after iteration 90000: 4.167786634912718\n",
      "three-layer Loss after iteration 91000: 4.166515262394278\n",
      "three-layer Loss after iteration 92000: 4.165256353359641\n",
      "three-layer Loss after iteration 93000: 4.164009769976404\n",
      "three-layer Loss after iteration 94000: 4.162775376045878\n",
      "three-layer Loss after iteration 95000: 4.1615530369838565\n",
      "three-layer Loss after iteration 96000: 4.160342619803936\n",
      "three-layer Loss after iteration 97000: 4.159143993102204\n",
      "three-layer Loss after iteration 98000: 4.157957027042092\n",
      "three-layer Loss after iteration 99000: 4.156781593337777\n",
      "three-layer Loss after iteration 100000: 4.155617565236395\n",
      "three-layer Loss after iteration 101000: 4.154464817499208\n",
      "three-layer Loss after iteration 102000: 4.1533232263821045\n",
      "three-layer Loss after iteration 103000: 4.152192669616499\n",
      "three-layer Loss after iteration 104000: 4.151073026390806\n",
      "three-layer Loss after iteration 105000: 4.14996417733246\n",
      "three-layer Loss after iteration 106000: 4.148866004490958\n",
      "three-layer Loss after iteration 107000: 4.147778391321206\n",
      "three-layer Loss after iteration 108000: 4.146701222667122\n",
      "three-layer Loss after iteration 109000: 4.145634384745302\n",
      "three-layer Loss after iteration 110000: 4.144577765128851\n",
      "three-layer Loss after iteration 111000: 4.143531252730486\n",
      "three-layer Loss after iteration 112000: 4.1424947377862935\n",
      "three-layer Loss after iteration 113000: 4.141468111839452\n",
      "three-layer Loss after iteration 114000: 4.1404512677238055\n",
      "three-layer Loss after iteration 115000: 4.139444099547965\n",
      "three-layer Loss after iteration 116000: 4.138446502679664\n",
      "three-layer Loss after iteration 117000: 4.137458373730389\n",
      "three-layer Loss after iteration 118000: 4.13647961054022\n",
      "three-layer Loss after iteration 119000: 4.1355101121627\n",
      "three-layer Loss after iteration 120000: 4.134549778850048\n",
      "three-layer Loss after iteration 121000: 4.1335985120383425\n",
      "three-layer Loss after iteration 122000: 4.132656214332969\n",
      "three-layer Loss after iteration 123000: 4.1317227894940896\n",
      "three-layer Loss after iteration 124000: 4.130798142422207\n",
      "three-layer Loss after iteration 125000: 4.129882179144189\n",
      "three-layer Loss after iteration 126000: 4.1289748067990715\n",
      "three-layer Loss after iteration 127000: 4.128075933624352\n",
      "three-layer Loss after iteration 128000: 4.12718546894233\n",
      "three-layer Loss after iteration 129000: 4.126303323146611\n",
      "three-layer Loss after iteration 130000: 4.125429407688757\n",
      "three-layer Loss after iteration 131000: 4.124563635065236\n",
      "three-layer Loss after iteration 132000: 4.123705918804227\n",
      "three-layer Loss after iteration 133000: 4.122856173453011\n",
      "three-layer Loss after iteration 134000: 4.122014314565076\n",
      "three-layer Loss after iteration 135000: 4.121180258687633\n",
      "three-layer Loss after iteration 136000: 4.120353923349148\n",
      "three-layer Loss after iteration 137000: 4.1195352270470975\n",
      "three-layer Loss after iteration 138000: 4.118724089235859\n",
      "three-layer Loss after iteration 139000: 4.117920430314772\n",
      "three-layer Loss after iteration 140000: 4.117124171616323\n",
      "three-layer Loss after iteration 141000: 4.116335235394438\n",
      "three-layer Loss after iteration 142000: 4.115553544813051\n",
      "three-layer Loss after iteration 143000: 4.114779023934634\n",
      "three-layer Loss after iteration 144000: 4.114011597709\n",
      "three-layer Loss after iteration 145000: 4.113251191962222\n",
      "three-layer Loss after iteration 146000: 4.112497733385542\n",
      "three-layer Loss after iteration 147000: 4.111751149524727\n",
      "three-layer Loss after iteration 148000: 4.111011368769245\n",
      "three-layer Loss after iteration 149000: 4.110278320341826\n",
      "three-layer Loss after iteration 150000: 4.109551934287885\n",
      "three-layer Loss after iteration 151000: 4.108832141465346\n",
      "three-layer Loss after iteration 152000: 4.108118873534461\n",
      "three-layer Loss after iteration 153000: 4.107412062947713\n",
      "three-layer Loss after iteration 154000: 4.10671164293994\n",
      "three-layer Loss after iteration 155000: 4.106017547518619\n",
      "three-layer Loss after iteration 156000: 4.105329711454097\n",
      "three-layer Loss after iteration 157000: 4.104648070270134\n",
      "three-layer Loss after iteration 158000: 4.103972560234446\n",
      "three-layer Loss after iteration 159000: 4.103303118349501\n",
      "three-layer Loss after iteration 160000: 4.102639682343199\n",
      "three-layer Loss after iteration 161000: 4.10198219065995\n",
      "three-layer Loss after iteration 162000: 4.1013305824516415\n",
      "three-layer Loss after iteration 163000: 4.1006847975688325\n",
      "three-layer Loss after iteration 164000: 4.100044776552051\n",
      "three-layer Loss after iteration 165000: 4.099410460623177\n",
      "three-layer Loss after iteration 166000: 4.0987817916770615\n",
      "three-layer Loss after iteration 167000: 4.09815871227293\n",
      "three-layer Loss after iteration 168000: 4.097541165626312\n",
      "three-layer Loss after iteration 169000: 4.096929095600788\n",
      "three-layer Loss after iteration 170000: 4.096322446699918\n",
      "three-layer Loss after iteration 171000: 4.095721164059283\n",
      "three-layer Loss after iteration 172000: 4.095125193438716\n",
      "three-layer Loss after iteration 173000: 4.094534481214434\n",
      "three-layer Loss after iteration 174000: 4.093948974371442\n",
      "three-layer Loss after iteration 175000: 4.0933686204959745\n",
      "three-layer Loss after iteration 176000: 4.0927933677680715\n",
      "three-layer Loss after iteration 177000: 4.092223164954177\n",
      "three-layer Loss after iteration 178000: 4.091657961399871\n",
      "three-layer Loss after iteration 179000: 4.0910977070227235\n",
      "three-layer Loss after iteration 180000: 4.090542352305269\n",
      "three-layer Loss after iteration 181000: 4.089991848287972\n",
      "three-layer Loss after iteration 182000: 4.089446146562293\n",
      "three-layer Loss after iteration 183000: 4.088905199263971\n",
      "three-layer Loss after iteration 184000: 4.088368959066255\n",
      "three-layer Loss after iteration 185000: 4.087837379173227\n",
      "three-layer Loss after iteration 186000: 4.087310413313446\n",
      "three-layer Loss after iteration 187000: 4.086788015733288\n",
      "three-layer Loss after iteration 188000: 4.086270141190716\n",
      "three-layer Loss after iteration 189000: 4.085756744948909\n",
      "three-layer Loss after iteration 190000: 4.085247782770069\n",
      "three-layer Loss after iteration 191000: 4.084743210909361\n",
      "three-layer Loss after iteration 192000: 4.084242986108803\n",
      "three-layer Loss after iteration 193000: 4.083747065591319\n",
      "three-layer Loss after iteration 194000: 4.083255407054858\n",
      "three-layer Loss after iteration 195000: 4.0827679686665395\n",
      "three-layer Loss after iteration 196000: 4.082284709056959\n",
      "three-layer Loss after iteration 197000: 4.081805587314447\n",
      "three-layer Loss after iteration 198000: 4.081330562979672\n",
      "three-layer Loss after iteration 199000: 4.080859596039902\n",
      "three-layer Loss after iteration 200000: 4.080392646923739\n",
      "three-layer Loss after iteration 201000: 4.0799296764956\n",
      "three-layer Loss after iteration 202000: 4.0794706460505274\n",
      "three-layer Loss after iteration 203000: 4.079015517308897\n",
      "three-layer Loss after iteration 204000: 4.078564252411256\n",
      "three-layer Loss after iteration 205000: 4.078116813913258\n",
      "three-layer Loss after iteration 206000: 4.077673164780622\n",
      "three-layer Loss after iteration 207000: 4.077233268384087\n",
      "three-layer Loss after iteration 208000: 4.0767970884946445\n",
      "three-layer Loss after iteration 209000: 4.0763645892785645\n",
      "three-layer Loss after iteration 210000: 4.075935735292692\n",
      "three-layer Loss after iteration 211000: 4.075510491479785\n",
      "three-layer Loss after iteration 212000: 4.075088823163713\n",
      "three-layer Loss after iteration 213000: 4.074670696045064\n",
      "three-layer Loss after iteration 214000: 4.074256076196355\n",
      "three-layer Loss after iteration 215000: 4.073844930057823\n",
      "three-layer Loss after iteration 216000: 4.073437224432857\n",
      "three-layer Loss after iteration 217000: 4.073032926483688\n",
      "9.925228422422878e-05 4.073437224432857 4.073032926483688\n",
      "three-layer Loss after iteration 0: 1487.8819849501936\n",
      "three-layer Loss after iteration 1000: 22.674044065846378\n",
      "three-layer Loss after iteration 2000: 21.19454975397985\n",
      "three-layer Loss after iteration 3000: 19.083842822405412\n",
      "three-layer Loss after iteration 4000: 16.33091067457729\n",
      "three-layer Loss after iteration 5000: 13.482486373964539\n",
      "three-layer Loss after iteration 6000: 10.093322658830253\n",
      "three-layer Loss after iteration 7000: 6.782826742077761\n",
      "three-layer Loss after iteration 8000: 5.01927168025624\n",
      "three-layer Loss after iteration 9000: 5.08394570782408\n",
      "three-layer Loss after iteration 10000: 5.085371559919409\n",
      "three-layer Loss after iteration 11000: 5.7933142951820376\n",
      "three-layer Loss after iteration 12000: 3.908589788672575\n",
      "three-layer Loss after iteration 13000: 3.1210829965294624\n",
      "three-layer Loss after iteration 14000: 5.7946649030844615\n",
      "three-layer Loss after iteration 15000: 2.9780118874478414\n",
      "three-layer Loss after iteration 16000: 3.649957844332124\n",
      "three-layer Loss after iteration 17000: 3.7027999900098147\n",
      "three-layer Loss after iteration 18000: 3.2481930482569736\n",
      "three-layer Loss after iteration 19000: 3.32889188207631\n",
      "three-layer Loss after iteration 20000: 2.881629433539509\n",
      "three-layer Loss after iteration 21000: 2.9640039616765086\n",
      "three-layer Loss after iteration 22000: 3.0243701983244087\n",
      "three-layer Loss after iteration 23000: 2.8410006446110754\n",
      "three-layer Loss after iteration 24000: 2.7922584598933944\n",
      "three-layer Loss after iteration 25000: 2.921027641380782\n",
      "three-layer Loss after iteration 26000: 2.551264489760728\n",
      "three-layer Loss after iteration 27000: 2.548663866025484\n",
      "three-layer Loss after iteration 28000: 2.5484331201211363\n",
      "9.053602847504265e-05 2.548663866025484 2.5484331201211363\n",
      "three-layer Loss after iteration 0: 1768.5128940369052\n",
      "three-layer Loss after iteration 1000: 12.695026424770296\n",
      "three-layer Loss after iteration 2000: 10.522601128505853\n",
      "three-layer Loss after iteration 3000: 11.124044879430743\n",
      "three-layer Loss after iteration 4000: 10.676908286604686\n",
      "three-layer Loss after iteration 5000: 9.85309260802107\n",
      "three-layer Loss after iteration 6000: 9.69964200037487\n",
      "three-layer Loss after iteration 7000: 9.586275762126457\n",
      "three-layer Loss after iteration 8000: 8.645070372605177\n",
      "three-layer Loss after iteration 9000: 8.511250516235677\n",
      "three-layer Loss after iteration 10000: 8.515345137795823\n",
      "three-layer Loss after iteration 11000: 8.233352847682513\n",
      "three-layer Loss after iteration 12000: 6.69343877077055\n",
      "three-layer Loss after iteration 13000: 5.785275758399415\n",
      "three-layer Loss after iteration 14000: 7.133431315438039\n",
      "three-layer Loss after iteration 15000: 5.963152553012274\n",
      "three-layer Loss after iteration 16000: 6.753429100715687\n",
      "three-layer Loss after iteration 17000: 6.002490998389609\n",
      "three-layer Loss after iteration 18000: 9.75010481479446\n",
      "three-layer Loss after iteration 19000: 4.763003332449685\n",
      "three-layer Loss after iteration 20000: 4.850845359551267\n",
      "three-layer Loss after iteration 21000: 5.971894876033176\n",
      "three-layer Loss after iteration 22000: 4.3488133532107325\n",
      "three-layer Loss after iteration 23000: 4.505732701995405\n",
      "three-layer Loss after iteration 24000: 4.264609953182411\n",
      "three-layer Loss after iteration 25000: 4.234171143623856\n",
      "three-layer Loss after iteration 26000: 4.561453710139497\n",
      "three-layer Loss after iteration 27000: 4.52444502119782\n",
      "three-layer Loss after iteration 28000: 4.428815152865526\n",
      "three-layer Loss after iteration 29000: 4.378393573902088\n",
      "three-layer Loss after iteration 30000: 4.340481335862387\n",
      "three-layer Loss after iteration 31000: 4.311066935214353\n",
      "three-layer Loss after iteration 32000: 4.28793569483837\n",
      "three-layer Loss after iteration 33000: 4.269597361966504\n",
      "three-layer Loss after iteration 34000: 4.254945138249398\n",
      "three-layer Loss after iteration 35000: 4.243133927657971\n",
      "three-layer Loss after iteration 36000: 4.233517335452893\n",
      "three-layer Loss after iteration 37000: 4.225602740847706\n",
      "three-layer Loss after iteration 38000: 4.155670888470111\n",
      "three-layer Loss after iteration 39000: 4.15473397152223\n",
      "three-layer Loss after iteration 40000: 4.153940319355718\n",
      "three-layer Loss after iteration 41000: 4.1532315548290475\n",
      "three-layer Loss after iteration 42000: 4.152591268128752\n",
      "three-layer Loss after iteration 43000: 4.152008443915831\n",
      "three-layer Loss after iteration 44000: 4.160812373106776\n",
      "three-layer Loss after iteration 45000: 4.159003841050291\n",
      "three-layer Loss after iteration 46000: 4.152660283008383\n",
      "three-layer Loss after iteration 47000: 4.150967895966773\n",
      "three-layer Loss after iteration 48000: 4.15087425216183\n",
      "2.2559510767173828e-05 4.150967895966773 4.15087425216183\n",
      "three-layer Loss after iteration 0: 1661.8114000100695\n",
      "three-layer Loss after iteration 1000: 26.75234606673477\n",
      "three-layer Loss after iteration 2000: 16.299469555722897\n",
      "three-layer Loss after iteration 3000: 10.471426226971582\n",
      "three-layer Loss after iteration 4000: 8.648143839629617\n",
      "three-layer Loss after iteration 5000: 8.459506527132461\n",
      "three-layer Loss after iteration 6000: 7.996511748167019\n",
      "three-layer Loss after iteration 7000: 7.909402908961645\n",
      "three-layer Loss after iteration 8000: 7.801491466321306\n",
      "three-layer Loss after iteration 9000: 7.556497579209412\n",
      "three-layer Loss after iteration 10000: 7.4568327643155135\n",
      "three-layer Loss after iteration 11000: 7.38206903679987\n",
      "three-layer Loss after iteration 12000: 7.324427119519496\n",
      "three-layer Loss after iteration 13000: 7.266752931766141\n",
      "three-layer Loss after iteration 14000: 7.213356766684619\n",
      "three-layer Loss after iteration 15000: 7.165921227554381\n",
      "three-layer Loss after iteration 16000: 7.059104328255053\n",
      "three-layer Loss after iteration 17000: 6.755752624266651\n",
      "three-layer Loss after iteration 18000: 5.767726214019546\n",
      "three-layer Loss after iteration 19000: 5.512513558845512\n",
      "three-layer Loss after iteration 20000: 5.363825349825183\n",
      "three-layer Loss after iteration 21000: 5.278966201242564\n",
      "three-layer Loss after iteration 22000: 5.203093167861284\n",
      "three-layer Loss after iteration 23000: 5.211522546758596\n",
      "three-layer Loss after iteration 24000: 5.132001182653924\n",
      "three-layer Loss after iteration 25000: 5.112932100785585\n",
      "three-layer Loss after iteration 26000: 5.359603655167032\n",
      "three-layer Loss after iteration 27000: 5.059577122230289\n",
      "three-layer Loss after iteration 28000: 5.0968882706472005\n",
      "three-layer Loss after iteration 29000: 5.066730712977244\n",
      "three-layer Loss after iteration 30000: 5.049920751430487\n",
      "three-layer Loss after iteration 31000: 5.037952460368914\n",
      "three-layer Loss after iteration 32000: 5.028661506938844\n",
      "three-layer Loss after iteration 33000: 5.020880363137207\n",
      "three-layer Loss after iteration 34000: 5.014443415430233\n",
      "three-layer Loss after iteration 35000: 5.009110254324644\n",
      "three-layer Loss after iteration 36000: 5.004406142369981\n",
      "three-layer Loss after iteration 37000: 5.0025880733840165\n",
      "three-layer Loss after iteration 38000: 5.000003790472771\n",
      "three-layer Loss after iteration 39000: 4.997635721927905\n",
      "three-layer Loss after iteration 40000: 4.996308366102474\n",
      "three-layer Loss after iteration 41000: 4.995275648675388\n",
      "three-layer Loss after iteration 42000: 4.994439022039199\n",
      "three-layer Loss after iteration 43000: 4.993502116487408\n",
      "three-layer Loss after iteration 44000: 4.992329620378377\n",
      "three-layer Loss after iteration 45000: 4.989680539800184\n",
      "three-layer Loss after iteration 46000: 4.98270622554563\n",
      "three-layer Loss after iteration 47000: 4.971192436224557\n",
      "three-layer Loss after iteration 48000: 4.970937536476383\n",
      "5.127537335251413e-05 4.971192436224557 4.970937536476383\n",
      "three-layer Loss after iteration 0: 1197.1881199271843\n",
      "three-layer Loss after iteration 1000: 9.188940547080337\n",
      "three-layer Loss after iteration 2000: 8.399201313010687\n",
      "three-layer Loss after iteration 3000: 7.853546796397196\n",
      "three-layer Loss after iteration 4000: 7.153916559049632\n",
      "three-layer Loss after iteration 5000: 6.55806996718959\n",
      "three-layer Loss after iteration 6000: 6.459475162460967\n",
      "three-layer Loss after iteration 7000: 6.520830471330337\n",
      "three-layer Loss after iteration 8000: 5.394446325487364\n",
      "three-layer Loss after iteration 9000: 5.140126650330175\n",
      "three-layer Loss after iteration 10000: 4.856084270674009\n",
      "three-layer Loss after iteration 11000: 4.668719385531638\n",
      "three-layer Loss after iteration 12000: 4.5284497575932035\n",
      "three-layer Loss after iteration 13000: 4.548825632595628\n",
      "three-layer Loss after iteration 14000: 4.515491491956859\n",
      "three-layer Loss after iteration 15000: 4.495041872917437\n",
      "three-layer Loss after iteration 16000: 4.49019918135614\n",
      "three-layer Loss after iteration 17000: 4.4848433979489455\n",
      "three-layer Loss after iteration 18000: 4.480084794132202\n",
      "three-layer Loss after iteration 19000: 4.4761443922493935\n",
      "three-layer Loss after iteration 20000: 4.472855400417488\n",
      "three-layer Loss after iteration 21000: 4.469249139893902\n",
      "three-layer Loss after iteration 22000: 4.466056313670784\n",
      "three-layer Loss after iteration 23000: 4.463520977731907\n",
      "three-layer Loss after iteration 24000: 4.447379987392846\n",
      "three-layer Loss after iteration 25000: 4.444061748822843\n",
      "three-layer Loss after iteration 26000: 4.444949497297334\n",
      "three-layer Loss after iteration 27000: 4.445395821335077\n",
      "three-layer Loss after iteration 28000: 4.436665274452007\n",
      "three-layer Loss after iteration 29000: 4.430425593828381\n",
      "three-layer Loss after iteration 30000: 4.4513804490819355\n",
      "three-layer Loss after iteration 31000: 4.417574408277528\n",
      "three-layer Loss after iteration 32000: 4.290478193024387\n",
      "three-layer Loss after iteration 33000: 4.158169575801836\n",
      "three-layer Loss after iteration 34000: 4.130824247004485\n",
      "three-layer Loss after iteration 35000: 4.0951767674511155\n",
      "three-layer Loss after iteration 36000: 4.052710964331025\n",
      "three-layer Loss after iteration 37000: 4.029943602705653\n",
      "three-layer Loss after iteration 38000: 4.010689788978266\n",
      "three-layer Loss after iteration 39000: 4.000935771259372\n",
      "three-layer Loss after iteration 40000: 3.992105423581666\n",
      "three-layer Loss after iteration 41000: 3.981818747629642\n",
      "three-layer Loss after iteration 42000: 3.976545979422344\n",
      "three-layer Loss after iteration 43000: 3.974758078402488\n",
      "three-layer Loss after iteration 44000: 3.9714637395818384\n",
      "three-layer Loss after iteration 45000: 3.968790286074371\n",
      "three-layer Loss after iteration 46000: 3.966479552314427\n",
      "three-layer Loss after iteration 47000: 3.9643103330344354\n",
      "three-layer Loss after iteration 48000: 3.963897990623149\n",
      "three-layer Loss after iteration 49000: 3.9625561141755012\n",
      "three-layer Loss after iteration 50000: 3.9606240938226396\n",
      "three-layer Loss after iteration 51000: 3.9601309322598266\n",
      "three-layer Loss after iteration 52000: 3.9584024890883827\n",
      "three-layer Loss after iteration 53000: 3.9576530948632183\n",
      "three-layer Loss after iteration 54000: 3.9567234535484\n",
      "three-layer Loss after iteration 55000: 3.9557616980814476\n",
      "three-layer Loss after iteration 56000: 3.9549110810637265\n",
      "three-layer Loss after iteration 57000: 3.9540892061497765\n",
      "three-layer Loss after iteration 58000: 3.953356791710388\n",
      "three-layer Loss after iteration 59000: 3.953024052541145\n",
      "8.416623815503556e-05 3.953356791710388 3.953024052541145\n",
      "three-layer Loss after iteration 0: 1667.9102316131987\n",
      "three-layer Loss after iteration 1000: 25.610366520710475\n",
      "three-layer Loss after iteration 2000: 26.312059840557783\n",
      "three-layer Loss after iteration 3000: 26.22273602906148\n",
      "three-layer Loss after iteration 4000: 22.337901956831164\n",
      "three-layer Loss after iteration 5000: 17.359396616679696\n",
      "three-layer Loss after iteration 6000: 20.168843151633766\n",
      "three-layer Loss after iteration 7000: 19.62796367650668\n",
      "three-layer Loss after iteration 8000: 17.247155027362354\n",
      "three-layer Loss after iteration 9000: 19.565926623101063\n",
      "three-layer Loss after iteration 10000: 18.570355198264668\n",
      "three-layer Loss after iteration 11000: 17.591401229981987\n",
      "three-layer Loss after iteration 12000: 16.827411102722163\n",
      "three-layer Loss after iteration 13000: 16.19787823031084\n",
      "three-layer Loss after iteration 14000: 14.558512643722441\n",
      "three-layer Loss after iteration 15000: 13.958966376544522\n",
      "three-layer Loss after iteration 16000: 31.132631359444623\n",
      "three-layer Loss after iteration 17000: 17.13019137731261\n",
      "three-layer Loss after iteration 18000: 15.468196278814126\n",
      "three-layer Loss after iteration 19000: 15.802790473335836\n",
      "three-layer Loss after iteration 20000: 15.86393575392839\n",
      "three-layer Loss after iteration 21000: 14.406458966484998\n",
      "three-layer Loss after iteration 22000: 14.18256937700701\n",
      "three-layer Loss after iteration 23000: 14.40416547860518\n",
      "three-layer Loss after iteration 24000: 14.352624634400534\n",
      "three-layer Loss after iteration 25000: 13.290266937195156\n",
      "three-layer Loss after iteration 26000: 13.132268764826017\n",
      "three-layer Loss after iteration 27000: 13.453410491253967\n",
      "three-layer Loss after iteration 28000: 13.45034697240956\n",
      "three-layer Loss after iteration 29000: 13.66445156051935\n",
      "three-layer Loss after iteration 30000: 13.635679778048903\n",
      "three-layer Loss after iteration 31000: 13.35067380682725\n",
      "three-layer Loss after iteration 32000: 13.141741589284631\n",
      "three-layer Loss after iteration 33000: 13.003676706000977\n",
      "three-layer Loss after iteration 34000: 12.895789045723953\n",
      "three-layer Loss after iteration 35000: 12.812007513729238\n",
      "three-layer Loss after iteration 36000: 12.771310620011178\n",
      "three-layer Loss after iteration 37000: 12.704109779411738\n",
      "three-layer Loss after iteration 38000: 12.666432685256948\n",
      "three-layer Loss after iteration 39000: 12.648896464943066\n",
      "three-layer Loss after iteration 40000: 12.59603390208623\n",
      "three-layer Loss after iteration 41000: 12.558349135260698\n",
      "three-layer Loss after iteration 42000: 12.51548664890258\n",
      "three-layer Loss after iteration 43000: 12.472233552246992\n",
      "three-layer Loss after iteration 44000: 12.471272220210485\n",
      "7.707777700600807e-05 12.472233552246992 12.471272220210485\n",
      "three-layer Loss after iteration 0: 1433.8592952194153\n",
      "three-layer Loss after iteration 1000: 12.664425603465892\n",
      "three-layer Loss after iteration 2000: 11.562585122029539\n",
      "three-layer Loss after iteration 3000: 9.552726385336587\n",
      "three-layer Loss after iteration 4000: 9.135298005999026\n",
      "three-layer Loss after iteration 5000: 10.37511659127154\n",
      "three-layer Loss after iteration 6000: 8.629616669146538\n",
      "three-layer Loss after iteration 7000: 6.327353545384136\n",
      "three-layer Loss after iteration 8000: 5.551436817131806\n",
      "three-layer Loss after iteration 9000: 5.1044203751512915\n",
      "three-layer Loss after iteration 10000: 4.633901113261861\n",
      "three-layer Loss after iteration 11000: 4.334118579316313\n",
      "three-layer Loss after iteration 12000: 4.732579262255753\n",
      "three-layer Loss after iteration 13000: 3.831259851268122\n",
      "three-layer Loss after iteration 14000: 4.407571889634999\n",
      "three-layer Loss after iteration 15000: 3.8052345200106608\n",
      "three-layer Loss after iteration 16000: 3.679957957660405\n",
      "three-layer Loss after iteration 17000: 3.666845610283937\n",
      "three-layer Loss after iteration 18000: 3.5830968378318144\n",
      "three-layer Loss after iteration 19000: 3.5550307687921157\n",
      "three-layer Loss after iteration 20000: 3.540355506531089\n",
      "three-layer Loss after iteration 21000: 3.5276339911318013\n",
      "three-layer Loss after iteration 22000: 3.546824145940157\n",
      "three-layer Loss after iteration 23000: 3.53098346547173\n",
      "three-layer Loss after iteration 24000: 3.5184320009773864\n",
      "three-layer Loss after iteration 25000: 3.513899621912336\n",
      "three-layer Loss after iteration 26000: 3.5114052501506365\n",
      "three-layer Loss after iteration 27000: 3.509809847499098\n",
      "three-layer Loss after iteration 28000: 3.508743028537658\n",
      "three-layer Loss after iteration 29000: 3.5089355002278566\n",
      "5.485488353898189e-05 3.508743028537658 3.5089355002278566\n",
      "three-layer Loss after iteration 0: 1697.0212430212937\n",
      "three-layer Loss after iteration 1000: 19.14498953857325\n",
      "three-layer Loss after iteration 2000: 16.199089336107903\n",
      "three-layer Loss after iteration 3000: 12.9586393880114\n",
      "three-layer Loss after iteration 4000: 10.603799223918386\n",
      "three-layer Loss after iteration 5000: 11.29608446017856\n",
      "three-layer Loss after iteration 6000: 13.132094785018085\n",
      "three-layer Loss after iteration 7000: 10.247339620579568\n",
      "three-layer Loss after iteration 8000: 9.756515857819366\n",
      "three-layer Loss after iteration 9000: 8.961614992691898\n",
      "three-layer Loss after iteration 10000: 8.358834500285555\n",
      "three-layer Loss after iteration 11000: 7.8659925801578074\n",
      "three-layer Loss after iteration 12000: 7.466646719288414\n",
      "three-layer Loss after iteration 13000: 7.279207151225299\n",
      "three-layer Loss after iteration 14000: 7.282314406180902\n",
      "three-layer Loss after iteration 15000: 5.9666033898064335\n",
      "three-layer Loss after iteration 16000: 4.949256587659247\n",
      "three-layer Loss after iteration 17000: 4.980700985862506\n",
      "three-layer Loss after iteration 18000: 4.5886386665548615\n",
      "three-layer Loss after iteration 19000: 4.528744624808011\n",
      "three-layer Loss after iteration 20000: 4.476480214016997\n",
      "three-layer Loss after iteration 21000: 4.3680511288862025\n",
      "three-layer Loss after iteration 22000: 4.302919975247508\n",
      "three-layer Loss after iteration 23000: 4.26446459762196\n",
      "three-layer Loss after iteration 24000: 4.237574198563802\n",
      "three-layer Loss after iteration 25000: 4.213457081132972\n",
      "three-layer Loss after iteration 26000: 4.197155313282748\n",
      "three-layer Loss after iteration 27000: 4.186231987257581\n",
      "three-layer Loss after iteration 28000: 4.184889804370424\n",
      "three-layer Loss after iteration 29000: 4.180641543106311\n",
      "three-layer Loss after iteration 30000: 4.177404400210506\n",
      "three-layer Loss after iteration 31000: 4.168305182397047\n",
      "three-layer Loss after iteration 32000: 4.164383317673461\n",
      "three-layer Loss after iteration 33000: 4.1628461123780065\n",
      "three-layer Loss after iteration 34000: 4.16162099506446\n",
      "three-layer Loss after iteration 35000: 4.160606111256957\n",
      "three-layer Loss after iteration 36000: 4.159744018112247\n",
      "three-layer Loss after iteration 37000: 4.158993825848868\n",
      "three-layer Loss after iteration 38000: 4.1583261249072345\n",
      "three-layer Loss after iteration 39000: 4.157773574767477\n",
      "three-layer Loss after iteration 40000: 4.157327817165959\n",
      "three-layer Loss after iteration 41000: 4.156918602486972\n",
      "9.843214126565416e-05 4.157327817165959 4.156918602486972\n",
      "three-layer Loss after iteration 0: 1605.1164577461805\n",
      "three-layer Loss after iteration 1000: 11.261288754179343\n",
      "three-layer Loss after iteration 2000: 7.512195356518706\n",
      "three-layer Loss after iteration 3000: 6.516140486330702\n",
      "three-layer Loss after iteration 4000: 5.699790188272503\n",
      "three-layer Loss after iteration 5000: 5.618243558774915\n",
      "three-layer Loss after iteration 6000: 5.363872700327533\n",
      "three-layer Loss after iteration 7000: 5.711070243009459\n",
      "three-layer Loss after iteration 8000: 5.257790769131207\n",
      "three-layer Loss after iteration 9000: 5.04361106002367\n",
      "three-layer Loss after iteration 10000: 5.729012194476215\n",
      "three-layer Loss after iteration 11000: 4.506751425293064\n",
      "three-layer Loss after iteration 12000: 4.257420895735145\n",
      "three-layer Loss after iteration 13000: 4.229206964904714\n",
      "three-layer Loss after iteration 14000: 4.211410429998537\n",
      "three-layer Loss after iteration 15000: 4.184289155255088\n",
      "three-layer Loss after iteration 16000: 4.166377422050567\n",
      "three-layer Loss after iteration 17000: 4.151568742704528\n",
      "three-layer Loss after iteration 18000: 4.143499464312998\n",
      "three-layer Loss after iteration 19000: 4.12757901449019\n",
      "three-layer Loss after iteration 20000: 4.12014988678927\n",
      "three-layer Loss after iteration 21000: 4.1146659016799445\n",
      "three-layer Loss after iteration 22000: 4.110107777152654\n",
      "three-layer Loss after iteration 23000: 4.106099978913422\n",
      "three-layer Loss after iteration 24000: 4.102410280757492\n",
      "three-layer Loss after iteration 25000: 4.098918336459045\n",
      "three-layer Loss after iteration 26000: 4.095602005008319\n",
      "three-layer Loss after iteration 27000: 4.092435640833081\n",
      "three-layer Loss after iteration 28000: 4.08935490097885\n",
      "three-layer Loss after iteration 29000: 4.086340392770806\n",
      "three-layer Loss after iteration 30000: 4.083501630668005\n",
      "three-layer Loss after iteration 31000: 4.080627111240528\n",
      "three-layer Loss after iteration 32000: 4.077744780100478\n",
      "three-layer Loss after iteration 33000: 4.075141359349623\n",
      "three-layer Loss after iteration 34000: 4.072359877897537\n",
      "three-layer Loss after iteration 35000: 4.06964007698289\n",
      "three-layer Loss after iteration 36000: 4.067190509519804\n",
      "three-layer Loss after iteration 37000: 4.064435571210223\n",
      "three-layer Loss after iteration 38000: 4.062244621097656\n",
      "three-layer Loss after iteration 39000: 4.059531682644091\n",
      "three-layer Loss after iteration 40000: 4.057263552254257\n",
      "three-layer Loss after iteration 41000: 4.05476733738319\n",
      "three-layer Loss after iteration 42000: 4.052575677606251\n",
      "three-layer Loss after iteration 43000: 4.049840679451207\n",
      "three-layer Loss after iteration 44000: 4.047853865765454\n",
      "three-layer Loss after iteration 45000: 4.0455068960881615\n",
      "three-layer Loss after iteration 46000: 4.043530366495062\n",
      "three-layer Loss after iteration 47000: 4.0413014817213755\n",
      "three-layer Loss after iteration 48000: 4.038793038871397\n",
      "three-layer Loss after iteration 49000: 4.036978026242538\n",
      "three-layer Loss after iteration 50000: 4.034915949573719\n",
      "three-layer Loss after iteration 51000: 4.033078306155014\n",
      "three-layer Loss after iteration 52000: 4.031115730172533\n",
      "three-layer Loss after iteration 53000: 4.027354507086864\n",
      "three-layer Loss after iteration 54000: 3.998087259349871\n",
      "three-layer Loss after iteration 55000: 3.9939848444069264\n",
      "three-layer Loss after iteration 56000: 3.9902881151114937\n",
      "three-layer Loss after iteration 57000: 3.9867165137401006\n",
      "three-layer Loss after iteration 58000: 3.9831909505862026\n",
      "three-layer Loss after iteration 59000: 3.9797381813586274\n",
      "three-layer Loss after iteration 60000: 3.9762788085434706\n",
      "three-layer Loss after iteration 61000: 3.9729366098000445\n",
      "three-layer Loss after iteration 62000: 3.9696233696513503\n",
      "three-layer Loss after iteration 63000: 3.966400483457615\n",
      "three-layer Loss after iteration 64000: 3.9634319923974766\n",
      "three-layer Loss after iteration 65000: 3.9606954212128573\n",
      "three-layer Loss after iteration 66000: 3.958014160952675\n",
      "three-layer Loss after iteration 67000: 3.9539805135395136\n",
      "three-layer Loss after iteration 68000: 3.9513531956552166\n",
      "three-layer Loss after iteration 69000: 3.950981926582412\n",
      "9.395998140905672e-05 3.9513531956552166 3.950981926582412\n",
      "three-layer Loss after iteration 0: 1301.478215004037\n",
      "three-layer Loss after iteration 1000: 14.18041224408803\n",
      "three-layer Loss after iteration 2000: 18.740502907883958\n",
      "three-layer Loss after iteration 3000: 8.402180910155709\n",
      "three-layer Loss after iteration 4000: 6.45853399590752\n",
      "three-layer Loss after iteration 5000: 6.15466835504251\n",
      "three-layer Loss after iteration 6000: 5.108691776294118\n",
      "three-layer Loss after iteration 7000: 4.849060390560527\n",
      "three-layer Loss after iteration 8000: 3.725452093768573\n",
      "three-layer Loss after iteration 9000: 2.8541016229445733\n",
      "three-layer Loss after iteration 10000: 2.3802509776687315\n",
      "three-layer Loss after iteration 11000: 2.781429495852513\n",
      "three-layer Loss after iteration 12000: 2.6352529326191747\n",
      "three-layer Loss after iteration 13000: 2.9602004783273417\n",
      "three-layer Loss after iteration 14000: 2.2423967180598683\n",
      "three-layer Loss after iteration 15000: 2.3498758636518504\n",
      "three-layer Loss after iteration 16000: 1.985706281165355\n",
      "three-layer Loss after iteration 17000: 2.1723247383514637\n",
      "three-layer Loss after iteration 18000: 1.8966293848441709\n",
      "three-layer Loss after iteration 19000: 1.8940781848155008\n",
      "three-layer Loss after iteration 20000: 1.8079422435608246\n",
      "three-layer Loss after iteration 21000: 1.6912908522004484\n",
      "three-layer Loss after iteration 22000: 1.6526144222974004\n",
      "three-layer Loss after iteration 23000: 1.6123667127633852\n",
      "three-layer Loss after iteration 24000: 1.598152075414467\n",
      "three-layer Loss after iteration 25000: 1.589017184572401\n",
      "three-layer Loss after iteration 26000: 1.5822336545948996\n",
      "three-layer Loss after iteration 27000: 1.5779985169444257\n",
      "three-layer Loss after iteration 28000: 1.5384235131166633\n",
      "three-layer Loss after iteration 29000: 1.6056140998435549\n",
      "three-layer Loss after iteration 30000: 1.5762506341219078\n",
      "three-layer Loss after iteration 31000: 1.5626778195806004\n",
      "three-layer Loss after iteration 32000: 1.5560070288700865\n",
      "three-layer Loss after iteration 33000: 1.5333792219726152\n",
      "three-layer Loss after iteration 34000: 1.511497178626591\n",
      "three-layer Loss after iteration 35000: 1.6538733796383291\n",
      "three-layer Loss after iteration 36000: 1.5006203064136283\n",
      "three-layer Loss after iteration 37000: 1.4757123238990337\n",
      "three-layer Loss after iteration 38000: 1.3901034654739803\n",
      "three-layer Loss after iteration 39000: 1.6883009923256365\n",
      "three-layer Loss after iteration 40000: 1.4383007370697167\n",
      "three-layer Loss after iteration 41000: 1.1878585041763614\n",
      "three-layer Loss after iteration 42000: 1.1820989985237964\n",
      "three-layer Loss after iteration 43000: 1.1788696767931948\n",
      "three-layer Loss after iteration 44000: 1.1799072686023664\n",
      "three-layer Loss after iteration 45000: 1.2785800063857424\n",
      "three-layer Loss after iteration 46000: 1.2098080734938261\n",
      "three-layer Loss after iteration 47000: 1.205093852639344\n",
      "three-layer Loss after iteration 48000: 1.1997151765462275\n",
      "three-layer Loss after iteration 49000: 1.1956867614293631\n",
      "three-layer Loss after iteration 50000: 1.192499157157923\n",
      "three-layer Loss after iteration 51000: 1.1993859239295943\n",
      "three-layer Loss after iteration 52000: 1.1820076049281725\n",
      "three-layer Loss after iteration 53000: 1.1751729355007539\n",
      "three-layer Loss after iteration 54000: 1.1721798534847427\n",
      "three-layer Loss after iteration 55000: 1.1701698981757949\n",
      "three-layer Loss after iteration 56000: 1.1696290371525235\n",
      "three-layer Loss after iteration 57000: 1.169479263262428\n",
      "three-layer Loss after iteration 58000: 1.1694023918167655\n",
      "6.57313456315946e-05 1.169479263262428 1.1694023918167655\n",
      "{'8': {'losses': [3.97093550866198, 3.6619827016446536, 3.085765480887662, 3.9873337400913584, 4.479365450180403, 3.0319564252377442, 3.959494433512119, 3.4613459166536718, 2.787631159277667, 1.8598413793523605, 2.2039962032249174, 2.8100643652317117, 2.978429878400804, 3.7232280456909344, 3.3602932439289983, 2.950214555469808, 1.9080539186148608, 3.6805388471412335, 3.3464298792705462, 2.6574032589954517], 'iterations': [53001, 94001, 196001, 56001, 39001, 41001, 58001, 56001, 188001, 440001, 135001, 105001, 72001, 76001, 112001, 92001, 93001, 144001, 50001, 43001]}} {'4+4': {'losses': [7.5260866386460075, 7.417241967082176, 3.1625328078748556, 0.5596305310910482, 5.498785209476185, 8.908815656921707, 8.026483177248004, 9.738102428890357, 4.396210529374679, 10.097862291784363, 4.073032926483688, 2.5484331201211363, 4.15087425216183, 4.970937536476383, 3.953024052541145, 12.471272220210485, 3.5089355002278566, 4.156918602486972, 3.950981926582412, 1.1694023918167655], 'iterations': [47001, 121001, 90001, 290001, 98001, 108001, 172001, 128001, 437001, 74001, 217001, 28001, 48001, 48001, 59001, 44001, 29001, 41001, 69001, 58001]}}\n",
      "two-layer Loss after iteration 0: 1651.350547073658\n",
      "two-layer Loss after iteration 1000: 22.368888524759363\n",
      "two-layer Loss after iteration 2000: 8.846941430825124\n",
      "two-layer Loss after iteration 3000: 6.085104941523815\n",
      "two-layer Loss after iteration 4000: 4.196315966841668\n",
      "two-layer Loss after iteration 5000: 3.151766980784154\n",
      "two-layer Loss after iteration 6000: 2.2049603226917784\n",
      "two-layer Loss after iteration 7000: 1.8135833582529082\n",
      "two-layer Loss after iteration 8000: 1.5576023390761107\n",
      "two-layer Loss after iteration 9000: 1.236900149675466\n",
      "two-layer Loss after iteration 10000: 1.0606820144637095\n",
      "two-layer Loss after iteration 11000: 0.9427557165169368\n",
      "two-layer Loss after iteration 12000: 0.8553064469424208\n",
      "two-layer Loss after iteration 13000: 0.7851766209884565\n",
      "two-layer Loss after iteration 14000: 0.7215185308174576\n",
      "two-layer Loss after iteration 15000: 0.6717599142644342\n",
      "two-layer Loss after iteration 16000: 0.6356204580363038\n",
      "two-layer Loss after iteration 17000: 0.6072333261755631\n",
      "two-layer Loss after iteration 18000: 0.5845709813297736\n",
      "two-layer Loss after iteration 19000: 0.558994525372753\n",
      "two-layer Loss after iteration 20000: 0.5405160058208005\n",
      "two-layer Loss after iteration 21000: 0.5253458158861288\n",
      "two-layer Loss after iteration 22000: 0.5129667456516714\n",
      "two-layer Loss after iteration 23000: 0.5009750229151034\n",
      "two-layer Loss after iteration 24000: 0.4911943196139847\n",
      "two-layer Loss after iteration 25000: 0.4836839385258261\n",
      "two-layer Loss after iteration 26000: 0.4776829675509531\n",
      "two-layer Loss after iteration 27000: 0.47273482711707665\n",
      "two-layer Loss after iteration 28000: 0.4675451452827718\n",
      "two-layer Loss after iteration 29000: 0.46336714304871424\n",
      "two-layer Loss after iteration 30000: 0.4599581852437535\n",
      "two-layer Loss after iteration 31000: 0.4571541134095688\n",
      "two-layer Loss after iteration 32000: 0.45484197281756816\n",
      "two-layer Loss after iteration 33000: 0.45288167234435506\n",
      "two-layer Loss after iteration 34000: 0.45062122159804613\n",
      "two-layer Loss after iteration 35000: 0.44877449801534586\n",
      "two-layer Loss after iteration 36000: 0.4470122742322763\n",
      "two-layer Loss after iteration 37000: 0.445568098867053\n",
      "two-layer Loss after iteration 38000: 0.4443574703473981\n",
      "two-layer Loss after iteration 39000: 0.44293585056153423\n",
      "two-layer Loss after iteration 40000: 0.44178174527213965\n",
      "two-layer Loss after iteration 41000: 0.44097183980476606\n",
      "two-layer Loss after iteration 42000: 0.44035244141394964\n",
      "two-layer Loss after iteration 43000: 0.4398120482650779\n",
      "two-layer Loss after iteration 44000: 0.439339936601711\n",
      "two-layer Loss after iteration 45000: 0.4387334749747061\n",
      "two-layer Loss after iteration 46000: 0.43806179595200245\n",
      "two-layer Loss after iteration 47000: 0.4374895048166082\n",
      "two-layer Loss after iteration 48000: 0.4369930877892255\n",
      "two-layer Loss after iteration 49000: 0.43655600672894657\n",
      "two-layer Loss after iteration 50000: 0.43617075281627266\n",
      "two-layer Loss after iteration 51000: 0.4358282609192149\n",
      "two-layer Loss after iteration 52000: 0.43552276591939143\n",
      "two-layer Loss after iteration 53000: 0.43525004829750047\n",
      "two-layer Loss after iteration 54000: 0.43500563440031825\n",
      "two-layer Loss after iteration 55000: 0.43478417415159515\n",
      "two-layer Loss after iteration 56000: 0.43458322644407127\n",
      "two-layer Loss after iteration 57000: 0.4344026025316227\n",
      "two-layer Loss after iteration 58000: 0.43423746366609367\n",
      "two-layer Loss after iteration 59000: 0.4340866082233826\n",
      "two-layer Loss after iteration 60000: 0.4339496413444755\n",
      "two-layer Loss after iteration 61000: 0.4338248860400628\n",
      "two-layer Loss after iteration 62000: 0.4337101161316506\n",
      "two-layer Loss after iteration 63000: 0.43360517942990456\n",
      "two-layer Loss after iteration 64000: 0.4335095859094283\n",
      "two-layer Loss after iteration 65000: 0.4334207077440972\n",
      "two-layer Loss after iteration 66000: 0.4333389279062515\n",
      "two-layer Loss after iteration 67000: 0.4309062412277399\n",
      "two-layer Loss after iteration 68000: 0.4300633858086663\n",
      "two-layer Loss after iteration 69000: 0.4293946546365439\n",
      "two-layer Loss after iteration 70000: 0.4287461484128167\n",
      "two-layer Loss after iteration 71000: 0.42667730160147344\n",
      "two-layer Loss after iteration 72000: 0.42556729726354453\n",
      "two-layer Loss after iteration 73000: 0.4249753224486034\n",
      "two-layer Loss after iteration 74000: 0.42440099840301515\n",
      "two-layer Loss after iteration 75000: 0.4239957436627613\n",
      "two-layer Loss after iteration 76000: 0.42360456373206923\n",
      "two-layer Loss after iteration 77000: 0.42332289665581013\n",
      "two-layer Loss after iteration 78000: 0.4228521273053187\n",
      "two-layer Loss after iteration 79000: 0.4225079034117919\n",
      "two-layer Loss after iteration 80000: 0.4222694492666797\n",
      "two-layer Loss after iteration 81000: 0.422048344468086\n",
      "two-layer Loss after iteration 82000: 0.42185763495170503\n",
      "two-layer Loss after iteration 83000: 0.4216886360441119\n",
      "two-layer Loss after iteration 84000: 0.4215362042466488\n",
      "two-layer Loss after iteration 85000: 0.4213974312022002\n",
      "two-layer Loss after iteration 86000: 0.4212700978635905\n",
      "two-layer Loss after iteration 87000: 0.4211526747763974\n",
      "two-layer Loss after iteration 88000: 0.4210435969627162\n",
      "two-layer Loss after iteration 89000: 0.42094183489296155\n",
      "two-layer Loss after iteration 90000: 0.42084909769466056\n",
      "two-layer Loss after iteration 91000: 0.42076548389924523\n",
      "two-layer Loss after iteration 92000: 0.42053771107207133\n",
      "two-layer Loss after iteration 93000: 0.4202668464634337\n",
      "two-layer Loss after iteration 94000: 0.4200432010024924\n",
      "two-layer Loss after iteration 95000: 0.41985311848705753\n",
      "two-layer Loss after iteration 96000: 0.4196974758244048\n",
      "two-layer Loss after iteration 97000: 0.41958463289703485\n",
      "two-layer Loss after iteration 98000: 0.4194837567070049\n",
      "two-layer Loss after iteration 99000: 0.4193933343588348\n",
      "two-layer Loss after iteration 100000: 0.4193111810857942\n",
      "two-layer Loss after iteration 101000: 0.41923624690068817\n",
      "two-layer Loss after iteration 102000: 0.419168324263354\n",
      "two-layer Loss after iteration 103000: 0.41910592952730596\n",
      "two-layer Loss after iteration 104000: 0.41904857118894084\n",
      "two-layer Loss after iteration 105000: 0.41899588457536285\n",
      "two-layer Loss after iteration 106000: 0.4187073242741662\n",
      "two-layer Loss after iteration 107000: 0.4183419545826942\n",
      "two-layer Loss after iteration 108000: 0.4182163936800413\n",
      "two-layer Loss after iteration 109000: 0.41809753006010597\n",
      "two-layer Loss after iteration 110000: 0.41800108412039105\n",
      "two-layer Loss after iteration 111000: 0.4179216507710575\n",
      "two-layer Loss after iteration 112000: 0.4178519969310917\n",
      "two-layer Loss after iteration 113000: 0.41779083232586717\n",
      "two-layer Loss after iteration 114000: 0.41773963034607664\n",
      "two-layer Loss after iteration 115000: 0.41769622479002905\n",
      "two-layer Loss after iteration 116000: 0.4176574679126932\n",
      "9.278723396489012e-05 0.41769622479002905 0.4176574679126932\n",
      "two-layer Loss after iteration 0: 1370.941091048169\n",
      "two-layer Loss after iteration 1000: 23.000378012450824\n",
      "two-layer Loss after iteration 2000: 8.877430060103244\n",
      "two-layer Loss after iteration 3000: 6.117121423784659\n",
      "two-layer Loss after iteration 4000: 4.578091323579656\n",
      "two-layer Loss after iteration 5000: 3.1622148535719496\n",
      "two-layer Loss after iteration 6000: 2.4038906647128635\n",
      "two-layer Loss after iteration 7000: 2.140799338815795\n",
      "two-layer Loss after iteration 8000: 1.9952440243647227\n",
      "two-layer Loss after iteration 9000: 1.8457055302359813\n",
      "two-layer Loss after iteration 10000: 1.755653979163325\n",
      "two-layer Loss after iteration 11000: 1.6700741217156736\n",
      "two-layer Loss after iteration 12000: 1.6031724931508784\n",
      "two-layer Loss after iteration 13000: 1.5515798522409865\n",
      "two-layer Loss after iteration 14000: 1.4999191815288595\n",
      "two-layer Loss after iteration 15000: 1.447427210198803\n",
      "two-layer Loss after iteration 16000: 1.409620507888629\n",
      "two-layer Loss after iteration 17000: 1.3805989268685868\n",
      "two-layer Loss after iteration 18000: 1.36053292703187\n",
      "two-layer Loss after iteration 19000: 1.3422815981524387\n",
      "two-layer Loss after iteration 20000: 1.3140151660182162\n",
      "two-layer Loss after iteration 21000: 1.2749450126472788\n",
      "two-layer Loss after iteration 22000: 1.2456712988857765\n",
      "two-layer Loss after iteration 23000: 1.228172317833245\n",
      "two-layer Loss after iteration 24000: 1.2128045458738153\n",
      "two-layer Loss after iteration 25000: 1.1917110957778458\n",
      "two-layer Loss after iteration 26000: 1.1777292161430515\n",
      "two-layer Loss after iteration 27000: 1.166084302015591\n",
      "two-layer Loss after iteration 28000: 1.151981751878382\n",
      "two-layer Loss after iteration 29000: 1.1390900130722552\n",
      "two-layer Loss after iteration 30000: 1.1213191273199514\n",
      "two-layer Loss after iteration 31000: 1.1090109570343027\n",
      "two-layer Loss after iteration 32000: 1.0940173519982213\n",
      "two-layer Loss after iteration 33000: 1.0827888273102348\n",
      "two-layer Loss after iteration 34000: 1.0725920570084928\n",
      "two-layer Loss after iteration 35000: 1.0611863811304487\n",
      "two-layer Loss after iteration 36000: 1.050261525713368\n",
      "two-layer Loss after iteration 37000: 1.0361952769051226\n",
      "two-layer Loss after iteration 38000: 1.0253092897406144\n",
      "two-layer Loss after iteration 39000: 1.0160677409189667\n",
      "two-layer Loss after iteration 40000: 1.0080843813962153\n",
      "two-layer Loss after iteration 41000: 0.9932544929716564\n",
      "two-layer Loss after iteration 42000: 0.9850882491867922\n",
      "two-layer Loss after iteration 43000: 0.9772257117910054\n",
      "two-layer Loss after iteration 44000: 0.9633359367513261\n",
      "two-layer Loss after iteration 45000: 0.9558140115351115\n",
      "two-layer Loss after iteration 46000: 0.9418025625647325\n",
      "two-layer Loss after iteration 47000: 0.929852327572992\n",
      "two-layer Loss after iteration 48000: 0.9217570433229785\n",
      "two-layer Loss after iteration 49000: 0.910356311825584\n",
      "two-layer Loss after iteration 50000: 0.9027305506506248\n",
      "two-layer Loss after iteration 51000: 0.8950720319112985\n",
      "two-layer Loss after iteration 52000: 0.8893919776373432\n",
      "two-layer Loss after iteration 53000: 0.8835513003574496\n",
      "two-layer Loss after iteration 54000: 0.878000552165552\n",
      "two-layer Loss after iteration 55000: 0.8653877031823723\n",
      "two-layer Loss after iteration 56000: 0.8601752309485318\n",
      "two-layer Loss after iteration 57000: 0.8539017730176859\n",
      "two-layer Loss after iteration 58000: 0.8501438601313589\n",
      "two-layer Loss after iteration 59000: 0.8472188063383879\n",
      "two-layer Loss after iteration 60000: 0.8441705088620718\n",
      "two-layer Loss after iteration 61000: 0.8387992185096046\n",
      "two-layer Loss after iteration 62000: 0.8356653600178101\n",
      "two-layer Loss after iteration 63000: 0.8330095858986146\n",
      "two-layer Loss after iteration 64000: 0.8301358974002051\n",
      "two-layer Loss after iteration 65000: 0.8174795005324328\n",
      "two-layer Loss after iteration 66000: 0.806088921904452\n",
      "two-layer Loss after iteration 67000: 0.8001325678089953\n",
      "two-layer Loss after iteration 68000: 0.7961511610877942\n",
      "two-layer Loss after iteration 69000: 0.7930264931693005\n",
      "two-layer Loss after iteration 70000: 0.790466110601435\n",
      "two-layer Loss after iteration 71000: 0.7883372105474107\n",
      "two-layer Loss after iteration 72000: 0.786504525682671\n",
      "two-layer Loss after iteration 73000: 0.7841951419017474\n",
      "two-layer Loss after iteration 74000: 0.7821903246000587\n",
      "two-layer Loss after iteration 75000: 0.77988017214327\n",
      "two-layer Loss after iteration 76000: 0.7774427038113598\n",
      "two-layer Loss after iteration 77000: 0.7756816069091583\n",
      "two-layer Loss after iteration 78000: 0.7738077869772305\n",
      "two-layer Loss after iteration 79000: 0.772508237223721\n",
      "two-layer Loss after iteration 80000: 0.7714135283537593\n",
      "two-layer Loss after iteration 81000: 0.770470715519729\n",
      "two-layer Loss after iteration 82000: 0.7686116955559742\n",
      "two-layer Loss after iteration 83000: 0.76716287407401\n",
      "two-layer Loss after iteration 84000: 0.7660668665586955\n",
      "two-layer Loss after iteration 85000: 0.76519005295923\n",
      "two-layer Loss after iteration 86000: 0.7644599799957814\n",
      "two-layer Loss after iteration 87000: 0.7638384518756959\n",
      "two-layer Loss after iteration 88000: 0.7632971342809467\n",
      "two-layer Loss after iteration 89000: 0.7610856328662425\n",
      "two-layer Loss after iteration 90000: 0.7593264442296097\n",
      "two-layer Loss after iteration 91000: 0.7548763320180149\n",
      "two-layer Loss after iteration 92000: 0.7532283979660911\n",
      "two-layer Loss after iteration 93000: 0.7519681204341399\n",
      "two-layer Loss after iteration 94000: 0.7499114782239248\n",
      "two-layer Loss after iteration 95000: 0.7485873784983802\n",
      "two-layer Loss after iteration 96000: 0.7474045052746886\n",
      "two-layer Loss after iteration 97000: 0.7455970406680869\n",
      "two-layer Loss after iteration 98000: 0.7442574711203881\n",
      "two-layer Loss after iteration 99000: 0.7432016820964455\n",
      "two-layer Loss after iteration 100000: 0.742402921788044\n",
      "two-layer Loss after iteration 101000: 0.7417327778301391\n",
      "two-layer Loss after iteration 102000: 0.741157512905765\n",
      "two-layer Loss after iteration 103000: 0.739560574800872\n",
      "two-layer Loss after iteration 104000: 0.7388194916343893\n",
      "two-layer Loss after iteration 105000: 0.7382497319208644\n",
      "two-layer Loss after iteration 106000: 0.7377891997570906\n",
      "two-layer Loss after iteration 107000: 0.737420960399604\n",
      "two-layer Loss after iteration 108000: 0.7370964447174426\n",
      "two-layer Loss after iteration 109000: 0.7368131265835821\n",
      "two-layer Loss after iteration 110000: 0.7365622114087652\n",
      "two-layer Loss after iteration 111000: 0.7363281759012132\n",
      "two-layer Loss after iteration 112000: 0.7361201820153653\n",
      "two-layer Loss after iteration 113000: 0.7359251077345802\n",
      "two-layer Loss after iteration 114000: 0.735750192985297\n",
      "two-layer Loss after iteration 115000: 0.7355813709903741\n",
      "two-layer Loss after iteration 116000: 0.7354229967700993\n",
      "two-layer Loss after iteration 117000: 0.735274120236385\n",
      "two-layer Loss after iteration 118000: 0.7351328056428089\n",
      "two-layer Loss after iteration 119000: 0.735002218679751\n",
      "two-layer Loss after iteration 120000: 0.7348760494522457\n",
      "two-layer Loss after iteration 121000: 0.7347607836454089\n",
      "two-layer Loss after iteration 122000: 0.734642935777628\n",
      "two-layer Loss after iteration 123000: 0.7345387753728624\n",
      "two-layer Loss after iteration 124000: 0.7344363654350823\n",
      "two-layer Loss after iteration 125000: 0.7343339352982222\n",
      "two-layer Loss after iteration 126000: 0.7342411355180957\n",
      "two-layer Loss after iteration 127000: 0.7341498487460117\n",
      "two-layer Loss after iteration 128000: 0.7340712610249875\n",
      "two-layer Loss after iteration 129000: 0.7339815107963369\n",
      "two-layer Loss after iteration 130000: 0.7339031503962348\n",
      "two-layer Loss after iteration 131000: 0.7338361473924283\n",
      "9.129679273115275e-05 0.7339031503962348 0.7338361473924283\n",
      "two-layer Loss after iteration 0: 1359.6688464535505\n",
      "two-layer Loss after iteration 1000: 23.20029765947446\n",
      "two-layer Loss after iteration 2000: 9.43952349904383\n",
      "two-layer Loss after iteration 3000: 7.262946810776759\n",
      "two-layer Loss after iteration 4000: 5.81356093410295\n",
      "two-layer Loss after iteration 5000: 4.24723625958913\n",
      "two-layer Loss after iteration 6000: 3.163296710214448\n",
      "two-layer Loss after iteration 7000: 2.5932888587747263\n",
      "two-layer Loss after iteration 8000: 2.332844229871615\n",
      "two-layer Loss after iteration 9000: 2.092298595661559\n",
      "two-layer Loss after iteration 10000: 1.2995513868489539\n",
      "two-layer Loss after iteration 11000: 1.0947801902360244\n",
      "two-layer Loss after iteration 12000: 0.9860276763848383\n",
      "two-layer Loss after iteration 13000: 0.9218903143297954\n",
      "two-layer Loss after iteration 14000: 0.8761837089215795\n",
      "two-layer Loss after iteration 15000: 0.8446251567603476\n",
      "two-layer Loss after iteration 16000: 0.821592901614765\n",
      "two-layer Loss after iteration 17000: 0.8026168446455396\n",
      "two-layer Loss after iteration 18000: 0.7883758597890377\n",
      "two-layer Loss after iteration 19000: 0.776371675295466\n",
      "two-layer Loss after iteration 20000: 0.7670338962256793\n",
      "two-layer Loss after iteration 21000: 0.7597123709739286\n",
      "two-layer Loss after iteration 22000: 0.7499392755002734\n",
      "two-layer Loss after iteration 23000: 0.742442022272791\n",
      "two-layer Loss after iteration 24000: 0.73729702142728\n",
      "two-layer Loss after iteration 25000: 0.7331296746099961\n",
      "two-layer Loss after iteration 26000: 0.7296522157482878\n",
      "two-layer Loss after iteration 27000: 0.7266651179999531\n",
      "two-layer Loss after iteration 28000: 0.7241098161975035\n",
      "two-layer Loss after iteration 29000: 0.7219290368619689\n",
      "two-layer Loss after iteration 30000: 0.7200233288407306\n",
      "two-layer Loss after iteration 31000: 0.7183748774096838\n",
      "two-layer Loss after iteration 32000: 0.7169408786239948\n",
      "two-layer Loss after iteration 33000: 0.7156616754910908\n",
      "two-layer Loss after iteration 34000: 0.7144061301947937\n",
      "two-layer Loss after iteration 35000: 0.7132644031645055\n",
      "two-layer Loss after iteration 36000: 0.7122210285542182\n",
      "two-layer Loss after iteration 37000: 0.7112485182820931\n",
      "two-layer Loss after iteration 38000: 0.7103481107561533\n",
      "two-layer Loss after iteration 39000: 0.7095090825771457\n",
      "two-layer Loss after iteration 40000: 0.7087240254398222\n",
      "two-layer Loss after iteration 41000: 0.7079524608987905\n",
      "two-layer Loss after iteration 42000: 0.7072169190188955\n",
      "two-layer Loss after iteration 43000: 0.7065332563821939\n",
      "two-layer Loss after iteration 44000: 0.7058999542242487\n",
      "two-layer Loss after iteration 45000: 0.7052956998050498\n",
      "two-layer Loss after iteration 46000: 0.7047169257606732\n",
      "two-layer Loss after iteration 47000: 0.7041730355526377\n",
      "two-layer Loss after iteration 48000: 0.7036648038034096\n",
      "two-layer Loss after iteration 49000: 0.7031767758508282\n",
      "two-layer Loss after iteration 50000: 0.7027072430471533\n",
      "two-layer Loss after iteration 51000: 0.7022551100862205\n",
      "two-layer Loss after iteration 52000: 0.7018189083439483\n",
      "two-layer Loss after iteration 53000: 0.7013978175451483\n",
      "two-layer Loss after iteration 54000: 0.7009908843429187\n",
      "two-layer Loss after iteration 55000: 0.7005972298337924\n",
      "two-layer Loss after iteration 56000: 0.7002161483135748\n",
      "two-layer Loss after iteration 57000: 0.6998465422231963\n",
      "two-layer Loss after iteration 58000: 0.6994882803488008\n",
      "two-layer Loss after iteration 59000: 0.6991402507310307\n",
      "two-layer Loss after iteration 60000: 0.6988020565430493\n",
      "two-layer Loss after iteration 61000: 0.6984685277614322\n",
      "two-layer Loss after iteration 62000: 0.6981405051153724\n",
      "two-layer Loss after iteration 63000: 0.6978233504575587\n",
      "two-layer Loss after iteration 64000: 0.6975151251616598\n",
      "two-layer Loss after iteration 65000: 0.6972151342298116\n",
      "two-layer Loss after iteration 66000: 0.6969228001067642\n",
      "two-layer Loss after iteration 67000: 0.6966378425684133\n",
      "two-layer Loss after iteration 68000: 0.6963597491756887\n",
      "two-layer Loss after iteration 69000: 0.6960887380650727\n",
      "two-layer Loss after iteration 70000: 0.695824063886894\n",
      "two-layer Loss after iteration 71000: 0.69556526430625\n",
      "two-layer Loss after iteration 72000: 0.6953128260592434\n",
      "two-layer Loss after iteration 73000: 0.6950662310591512\n",
      "two-layer Loss after iteration 74000: 0.6948251022753121\n",
      "two-layer Loss after iteration 75000: 0.6945895353827716\n",
      "two-layer Loss after iteration 76000: 0.6943593352930219\n",
      "two-layer Loss after iteration 77000: 0.6941341932726802\n",
      "two-layer Loss after iteration 78000: 0.693914198285295\n",
      "two-layer Loss after iteration 79000: 0.6936991460556675\n",
      "two-layer Loss after iteration 80000: 0.6934890102894302\n",
      "two-layer Loss after iteration 81000: 0.6932832705254153\n",
      "two-layer Loss after iteration 82000: 0.6930827717815552\n",
      "two-layer Loss after iteration 83000: 0.6928865531726696\n",
      "two-layer Loss after iteration 84000: 0.6926949098072871\n",
      "two-layer Loss after iteration 85000: 0.692507755085242\n",
      "two-layer Loss after iteration 86000: 0.6923247635865576\n",
      "two-layer Loss after iteration 87000: 0.6921462224571643\n",
      "two-layer Loss after iteration 88000: 0.691972178978456\n",
      "two-layer Loss after iteration 89000: 0.6918022942789432\n",
      "two-layer Loss after iteration 90000: 0.6916363796922826\n",
      "two-layer Loss after iteration 91000: 0.6914746855656794\n",
      "two-layer Loss after iteration 92000: 0.6913169918338438\n",
      "two-layer Loss after iteration 93000: 0.6911633682317797\n",
      "two-layer Loss after iteration 94000: 0.6910137611396241\n",
      "two-layer Loss after iteration 95000: 0.6908681108869889\n",
      "two-layer Loss after iteration 96000: 0.6907264260861737\n",
      "two-layer Loss after iteration 97000: 0.6905889153802935\n",
      "two-layer Loss after iteration 98000: 0.6904547064190201\n",
      "two-layer Loss after iteration 99000: 0.6887830946088119\n",
      "two-layer Loss after iteration 100000: 0.6881536159834105\n",
      "two-layer Loss after iteration 101000: 0.6876773159556473\n",
      "two-layer Loss after iteration 102000: 0.6872348912912061\n",
      "two-layer Loss after iteration 103000: 0.6868025160235129\n",
      "two-layer Loss after iteration 104000: 0.686423087157184\n",
      "two-layer Loss after iteration 105000: 0.6860898384965571\n",
      "two-layer Loss after iteration 106000: 0.6857827335710428\n",
      "two-layer Loss after iteration 107000: 0.6854976928618063\n",
      "two-layer Loss after iteration 108000: 0.6852321965522219\n",
      "two-layer Loss after iteration 109000: 0.6849841889193279\n",
      "two-layer Loss after iteration 110000: 0.6847513067322908\n",
      "two-layer Loss after iteration 111000: 0.6845442307754727\n",
      "two-layer Loss after iteration 112000: 0.684224838885637\n",
      "two-layer Loss after iteration 113000: 0.6839820889781364\n",
      "two-layer Loss after iteration 114000: 0.6836644109003449\n",
      "two-layer Loss after iteration 115000: 0.6833511853249506\n",
      "two-layer Loss after iteration 116000: 0.6830660268432626\n",
      "two-layer Loss after iteration 117000: 0.6827963856678952\n",
      "two-layer Loss after iteration 118000: 0.682538171553611\n",
      "two-layer Loss after iteration 119000: 0.6822883209364623\n",
      "two-layer Loss after iteration 120000: 0.6820330452627538\n",
      "two-layer Loss after iteration 121000: 0.6815947617221058\n",
      "two-layer Loss after iteration 122000: 0.6812261674783487\n",
      "two-layer Loss after iteration 123000: 0.6809869865471528\n",
      "two-layer Loss after iteration 124000: 0.6807558211861482\n",
      "two-layer Loss after iteration 125000: 0.6805319092772669\n",
      "two-layer Loss after iteration 126000: 0.6803145413526653\n",
      "two-layer Loss after iteration 127000: 0.6800565818934653\n",
      "two-layer Loss after iteration 128000: 0.6796714056190631\n",
      "two-layer Loss after iteration 129000: 0.6793382401446951\n",
      "two-layer Loss after iteration 130000: 0.6790326689786207\n",
      "two-layer Loss after iteration 131000: 0.6787959624047448\n",
      "two-layer Loss after iteration 132000: 0.6786285776842218\n",
      "two-layer Loss after iteration 133000: 0.678472389029864\n",
      "two-layer Loss after iteration 134000: 0.6783260043236321\n",
      "two-layer Loss after iteration 135000: 0.6781886046693406\n",
      "two-layer Loss after iteration 136000: 0.6780610942928427\n",
      "two-layer Loss after iteration 137000: 0.677943770204733\n",
      "two-layer Loss after iteration 138000: 0.6778334198536918\n",
      "two-layer Loss after iteration 139000: 0.6777292296423576\n",
      "two-layer Loss after iteration 140000: 0.677630153661924\n",
      "two-layer Loss after iteration 141000: 0.6775368771330803\n",
      "two-layer Loss after iteration 142000: 0.6774483771151836\n",
      "two-layer Loss after iteration 143000: 0.6773639516678511\n",
      "two-layer Loss after iteration 144000: 0.6772819374104774\n",
      "two-layer Loss after iteration 145000: 0.6772006343192376\n",
      "two-layer Loss after iteration 146000: 0.6771238639523635\n",
      "two-layer Loss after iteration 147000: 0.6770501701659766\n",
      "two-layer Loss after iteration 148000: 0.676981238597464\n",
      "two-layer Loss after iteration 149000: 0.6769175891871339\n",
      "9.401945977405204e-05 0.676981238597464 0.6769175891871339\n",
      "two-layer Loss after iteration 0: 1416.5807883393677\n",
      "two-layer Loss after iteration 1000: 23.974457708226588\n",
      "two-layer Loss after iteration 2000: 8.99939052033017\n",
      "two-layer Loss after iteration 3000: 6.56064152390295\n",
      "two-layer Loss after iteration 4000: 4.980250065008586\n",
      "two-layer Loss after iteration 5000: 3.392724830564633\n",
      "two-layer Loss after iteration 6000: 2.675271302076437\n",
      "two-layer Loss after iteration 7000: 2.104360670412584\n",
      "two-layer Loss after iteration 8000: 1.4975595277274945\n",
      "two-layer Loss after iteration 9000: 1.213761792601844\n",
      "two-layer Loss after iteration 10000: 1.0666029540931887\n",
      "two-layer Loss after iteration 11000: 0.9823160704336785\n",
      "two-layer Loss after iteration 12000: 0.9396864722751679\n",
      "two-layer Loss after iteration 13000: 0.8980414898053661\n",
      "two-layer Loss after iteration 14000: 0.8502128896905419\n",
      "two-layer Loss after iteration 15000: 0.8368166652470413\n",
      "two-layer Loss after iteration 16000: 0.8271641254361386\n",
      "two-layer Loss after iteration 17000: 0.8194291124976992\n",
      "two-layer Loss after iteration 18000: 0.812903721173252\n",
      "two-layer Loss after iteration 19000: 0.8074997477196012\n",
      "two-layer Loss after iteration 20000: 0.8020311529884\n",
      "two-layer Loss after iteration 21000: 0.7968395768879446\n",
      "two-layer Loss after iteration 22000: 0.7921917651098428\n",
      "two-layer Loss after iteration 23000: 0.7878283450954019\n",
      "two-layer Loss after iteration 24000: 0.7838348667854065\n",
      "two-layer Loss after iteration 25000: 0.7793053724006594\n",
      "two-layer Loss after iteration 26000: 0.7749174886429769\n",
      "two-layer Loss after iteration 27000: 0.7710412065170938\n",
      "two-layer Loss after iteration 28000: 0.7674817206150166\n",
      "two-layer Loss after iteration 29000: 0.76296548479173\n",
      "two-layer Loss after iteration 30000: 0.7584288885360189\n",
      "two-layer Loss after iteration 31000: 0.753375699018804\n",
      "two-layer Loss after iteration 32000: 0.7487952589000114\n",
      "two-layer Loss after iteration 33000: 0.7446993238307931\n",
      "two-layer Loss after iteration 34000: 0.7409558123916511\n",
      "two-layer Loss after iteration 35000: 0.7374666506265698\n",
      "two-layer Loss after iteration 36000: 0.7341482046261505\n",
      "two-layer Loss after iteration 37000: 0.7309826727285186\n",
      "two-layer Loss after iteration 38000: 0.7279363193975562\n",
      "two-layer Loss after iteration 39000: 0.7249874914260398\n",
      "two-layer Loss after iteration 40000: 0.7220272704845483\n",
      "two-layer Loss after iteration 41000: 0.7192174815222624\n",
      "two-layer Loss after iteration 42000: 0.7164592996467736\n",
      "two-layer Loss after iteration 43000: 0.7137375069711315\n",
      "two-layer Loss after iteration 44000: 0.7106347600082282\n",
      "two-layer Loss after iteration 45000: 0.7066473896958408\n",
      "two-layer Loss after iteration 46000: 0.7030621323654631\n",
      "two-layer Loss after iteration 47000: 0.699695060488817\n",
      "two-layer Loss after iteration 48000: 0.6964978110885643\n",
      "two-layer Loss after iteration 49000: 0.6919748488654703\n",
      "two-layer Loss after iteration 50000: 0.6880516583694841\n",
      "two-layer Loss after iteration 51000: 0.680974071733788\n",
      "two-layer Loss after iteration 52000: 0.6766993956230977\n",
      "two-layer Loss after iteration 53000: 0.6728027381513453\n",
      "two-layer Loss after iteration 54000: 0.6692367458651302\n",
      "two-layer Loss after iteration 55000: 0.6657251127210689\n",
      "two-layer Loss after iteration 56000: 0.6621198404556226\n",
      "two-layer Loss after iteration 57000: 0.658791826580138\n",
      "two-layer Loss after iteration 58000: 0.6556134328665995\n",
      "two-layer Loss after iteration 59000: 0.6525170919120337\n",
      "two-layer Loss after iteration 60000: 0.649397215160324\n",
      "two-layer Loss after iteration 61000: 0.6463590215583446\n",
      "two-layer Loss after iteration 62000: 0.643434662497758\n",
      "two-layer Loss after iteration 63000: 0.6406094846854843\n",
      "two-layer Loss after iteration 64000: 0.6378728899809352\n",
      "two-layer Loss after iteration 65000: 0.6352193224708983\n",
      "two-layer Loss after iteration 66000: 0.632662297265896\n",
      "two-layer Loss after iteration 67000: 0.6302169120734551\n",
      "two-layer Loss after iteration 68000: 0.6278345889793607\n",
      "two-layer Loss after iteration 69000: 0.625510792481833\n",
      "two-layer Loss after iteration 70000: 0.6232500543622898\n",
      "two-layer Loss after iteration 71000: 0.6210465237800661\n",
      "two-layer Loss after iteration 72000: 0.6188986787828485\n",
      "two-layer Loss after iteration 73000: 0.6167533292835718\n",
      "two-layer Loss after iteration 74000: 0.6142184302021633\n",
      "two-layer Loss after iteration 75000: 0.6112016444561429\n",
      "two-layer Loss after iteration 76000: 0.6067523698538907\n",
      "two-layer Loss after iteration 77000: 0.6034726692337147\n",
      "two-layer Loss after iteration 78000: 0.6002577548486958\n",
      "two-layer Loss after iteration 79000: 0.5973675484467402\n",
      "two-layer Loss after iteration 80000: 0.5946656979333182\n",
      "two-layer Loss after iteration 81000: 0.5904034456614226\n",
      "two-layer Loss after iteration 82000: 0.5855575679234921\n",
      "two-layer Loss after iteration 83000: 0.5821303728794284\n",
      "two-layer Loss after iteration 84000: 0.5790851550020156\n",
      "two-layer Loss after iteration 85000: 0.5762123927767887\n",
      "two-layer Loss after iteration 86000: 0.5721834201460207\n",
      "two-layer Loss after iteration 87000: 0.5244597466324649\n",
      "two-layer Loss after iteration 88000: 0.5163617902714744\n",
      "two-layer Loss after iteration 89000: 0.5113268510653642\n",
      "two-layer Loss after iteration 90000: 0.5066783011161541\n",
      "two-layer Loss after iteration 91000: 0.502256838780857\n",
      "two-layer Loss after iteration 92000: 0.49388112181111976\n",
      "two-layer Loss after iteration 93000: 0.48904330827773196\n",
      "two-layer Loss after iteration 94000: 0.4845561851738112\n",
      "two-layer Loss after iteration 95000: 0.4794923142917478\n",
      "two-layer Loss after iteration 96000: 0.4749247051608127\n",
      "two-layer Loss after iteration 97000: 0.4705071478769184\n",
      "two-layer Loss after iteration 98000: 0.4657492334213066\n",
      "two-layer Loss after iteration 99000: 0.4612261621916845\n",
      "two-layer Loss after iteration 100000: 0.4568794984514745\n",
      "two-layer Loss after iteration 101000: 0.4526560115519022\n",
      "two-layer Loss after iteration 102000: 0.4485439479702996\n",
      "two-layer Loss after iteration 103000: 0.44448677045783486\n",
      "two-layer Loss after iteration 104000: 0.44024633684512937\n",
      "two-layer Loss after iteration 105000: 0.4361478642253659\n",
      "two-layer Loss after iteration 106000: 0.4321712490095695\n",
      "two-layer Loss after iteration 107000: 0.42830838220950757\n",
      "two-layer Loss after iteration 108000: 0.4245526551539218\n",
      "two-layer Loss after iteration 109000: 0.42089898333555725\n",
      "two-layer Loss after iteration 110000: 0.4173424166257915\n",
      "two-layer Loss after iteration 111000: 0.4138781540945066\n",
      "two-layer Loss after iteration 112000: 0.410485548884727\n",
      "two-layer Loss after iteration 113000: 0.40715763230098684\n",
      "two-layer Loss after iteration 114000: 0.40392054540083766\n",
      "two-layer Loss after iteration 115000: 0.4007670992027391\n",
      "two-layer Loss after iteration 116000: 0.3976935873363808\n",
      "two-layer Loss after iteration 117000: 0.3946964892498641\n",
      "two-layer Loss after iteration 118000: 0.39177298001490346\n",
      "two-layer Loss after iteration 119000: 0.38892076696533945\n",
      "two-layer Loss after iteration 120000: 0.3861355191991045\n",
      "two-layer Loss after iteration 121000: 0.3834111608522405\n",
      "two-layer Loss after iteration 122000: 0.3807533962282537\n",
      "two-layer Loss after iteration 123000: 0.37820411843157503\n",
      "two-layer Loss after iteration 124000: 0.3757195922369306\n",
      "two-layer Loss after iteration 125000: 0.37329351182236925\n",
      "two-layer Loss after iteration 126000: 0.3709241152655641\n",
      "two-layer Loss after iteration 127000: 0.36860990769272123\n",
      "two-layer Loss after iteration 128000: 0.36634912976681855\n",
      "two-layer Loss after iteration 129000: 0.3641405406581435\n",
      "two-layer Loss after iteration 130000: 0.3619827809891004\n",
      "two-layer Loss after iteration 131000: 0.35987491124780907\n",
      "two-layer Loss after iteration 132000: 0.3578155674635309\n",
      "two-layer Loss after iteration 133000: 0.3558038131732533\n",
      "two-layer Loss after iteration 134000: 0.3538386593117226\n",
      "two-layer Loss after iteration 135000: 0.3519193829673683\n",
      "two-layer Loss after iteration 136000: 0.3500450531392077\n",
      "two-layer Loss after iteration 137000: 0.34821469087349916\n",
      "two-layer Loss after iteration 138000: 0.3464277511575857\n",
      "two-layer Loss after iteration 139000: 0.3446834360677038\n",
      "two-layer Loss after iteration 140000: 0.3429811632487748\n",
      "two-layer Loss after iteration 141000: 0.3413203653913895\n",
      "two-layer Loss after iteration 142000: 0.3396836377039704\n",
      "two-layer Loss after iteration 143000: 0.3379171783245434\n",
      "two-layer Loss after iteration 144000: 0.3362575257340063\n",
      "two-layer Loss after iteration 145000: 0.3346510495273785\n",
      "two-layer Loss after iteration 146000: 0.33309348430085584\n",
      "two-layer Loss after iteration 147000: 0.33158547745453787\n",
      "two-layer Loss after iteration 148000: 0.33013362561779536\n",
      "two-layer Loss after iteration 149000: 0.32873204122583033\n",
      "two-layer Loss after iteration 150000: 0.3273778130845807\n",
      "two-layer Loss after iteration 151000: 0.32606693583780805\n",
      "two-layer Loss after iteration 152000: 0.32468766765582385\n",
      "two-layer Loss after iteration 153000: 0.32338265638901154\n",
      "two-layer Loss after iteration 154000: 0.3199438207915014\n",
      "two-layer Loss after iteration 155000: 0.31768867900268555\n",
      "two-layer Loss after iteration 156000: 0.31614816306249854\n",
      "two-layer Loss after iteration 157000: 0.3148753225554037\n",
      "two-layer Loss after iteration 158000: 0.3137083974868047\n",
      "two-layer Loss after iteration 159000: 0.3125349850331786\n",
      "two-layer Loss after iteration 160000: 0.311432895254814\n",
      "two-layer Loss after iteration 161000: 0.31038893609503476\n",
      "two-layer Loss after iteration 162000: 0.3093939916643707\n",
      "two-layer Loss after iteration 163000: 0.30844044283814925\n",
      "two-layer Loss after iteration 164000: 0.30752666206604684\n",
      "two-layer Loss after iteration 165000: 0.3066501573928767\n",
      "two-layer Loss after iteration 166000: 0.30580500675439604\n",
      "two-layer Loss after iteration 167000: 0.30499376152027213\n",
      "two-layer Loss after iteration 168000: 0.30421085140410126\n",
      "two-layer Loss after iteration 169000: 0.3034582788222517\n",
      "two-layer Loss after iteration 170000: 0.30272845951687166\n",
      "two-layer Loss after iteration 171000: 0.30202723602795534\n",
      "two-layer Loss after iteration 172000: 0.30134985022844984\n",
      "two-layer Loss after iteration 173000: 0.30069743903213314\n",
      "two-layer Loss after iteration 174000: 0.30006316861445537\n",
      "two-layer Loss after iteration 175000: 0.2994555484619981\n",
      "two-layer Loss after iteration 176000: 0.2988651842666751\n",
      "two-layer Loss after iteration 177000: 0.2982957738173072\n",
      "two-layer Loss after iteration 178000: 0.2977495798040129\n",
      "two-layer Loss after iteration 179000: 0.2972186088998602\n",
      "two-layer Loss after iteration 180000: 0.29670840045910885\n",
      "two-layer Loss after iteration 181000: 0.29621215924065986\n",
      "two-layer Loss after iteration 182000: 0.2957367073327132\n",
      "two-layer Loss after iteration 183000: 0.29527533108096055\n",
      "two-layer Loss after iteration 184000: 0.2948318758414553\n",
      "two-layer Loss after iteration 185000: 0.2944017724157354\n",
      "two-layer Loss after iteration 186000: 0.2939883380424698\n",
      "two-layer Loss after iteration 187000: 0.2935873716100556\n",
      "two-layer Loss after iteration 188000: 0.2932015442478818\n",
      "two-layer Loss after iteration 189000: 0.2928275646069912\n",
      "two-layer Loss after iteration 190000: 0.29247078922192826\n",
      "two-layer Loss after iteration 191000: 0.29212364653926115\n",
      "two-layer Loss after iteration 192000: 0.2917887908968156\n",
      "two-layer Loss after iteration 193000: 0.29146531242548745\n",
      "two-layer Loss after iteration 194000: 0.29115163871987004\n",
      "two-layer Loss after iteration 195000: 0.29085221442825904\n",
      "two-layer Loss after iteration 196000: 0.29056200859167564\n",
      "two-layer Loss after iteration 197000: 0.2902802801932809\n",
      "two-layer Loss after iteration 198000: 0.29001037193124424\n",
      "two-layer Loss after iteration 199000: 0.2897500725355663\n",
      "two-layer Loss after iteration 200000: 0.2894988723366598\n",
      "two-layer Loss after iteration 201000: 0.2892564533405695\n",
      "two-layer Loss after iteration 202000: 0.2890209308861986\n",
      "two-layer Loss after iteration 203000: 0.2887952973829164\n",
      "two-layer Loss after iteration 204000: 0.2885749165670486\n",
      "two-layer Loss after iteration 205000: 0.2883641538707666\n",
      "two-layer Loss after iteration 206000: 0.28816154084500756\n",
      "two-layer Loss after iteration 207000: 0.2879643174205675\n",
      "two-layer Loss after iteration 208000: 0.2877741925001206\n",
      "two-layer Loss after iteration 209000: 0.28759081402206194\n",
      "two-layer Loss after iteration 210000: 0.2874121339782721\n",
      "two-layer Loss after iteration 211000: 0.2872427280113233\n",
      "two-layer Loss after iteration 212000: 0.28707927409675205\n",
      "two-layer Loss after iteration 213000: 0.28691848325473923\n",
      "two-layer Loss after iteration 214000: 0.28676542734598076\n",
      "two-layer Loss after iteration 215000: 0.28661653030145834\n",
      "two-layer Loss after iteration 216000: 0.28647240448152733\n",
      "two-layer Loss after iteration 217000: 0.2863327830426184\n",
      "two-layer Loss after iteration 218000: 0.28619867684245165\n",
      "two-layer Loss after iteration 219000: 0.2860678755185963\n",
      "two-layer Loss after iteration 220000: 0.28594098319913014\n",
      "two-layer Loss after iteration 221000: 0.28581954540776744\n",
      "two-layer Loss after iteration 222000: 0.28570240910316824\n",
      "two-layer Loss after iteration 223000: 0.28558836936239756\n",
      "two-layer Loss after iteration 224000: 0.28547918812987466\n",
      "two-layer Loss after iteration 225000: 0.28537174598860665\n",
      "two-layer Loss after iteration 226000: 0.28526883745449805\n",
      "two-layer Loss after iteration 227000: 0.2851669153654952\n",
      "two-layer Loss after iteration 228000: 0.28507089554698645\n",
      "two-layer Loss after iteration 229000: 0.2849769065321326\n",
      "two-layer Loss after iteration 230000: 0.2848837544285757\n",
      "two-layer Loss after iteration 231000: 0.2847970596602903\n",
      "two-layer Loss after iteration 232000: 0.2847093927728371\n",
      "two-layer Loss after iteration 233000: 0.28462820154327373\n",
      "two-layer Loss after iteration 234000: 0.2845457237965379\n",
      "two-layer Loss after iteration 235000: 0.2844690409157375\n",
      "two-layer Loss after iteration 236000: 0.2843912064123831\n",
      "two-layer Loss after iteration 237000: 0.2843186997246341\n",
      "two-layer Loss after iteration 238000: 0.2842462317058258\n",
      "two-layer Loss after iteration 239000: 0.28417680715094484\n",
      "two-layer Loss after iteration 240000: 0.2841103712903598\n",
      "two-layer Loss after iteration 241000: 0.28404287863958505\n",
      "two-layer Loss after iteration 242000: 0.2839803320800697\n",
      "two-layer Loss after iteration 243000: 0.28391771628589335\n",
      "two-layer Loss after iteration 244000: 0.283851766807505\n",
      "two-layer Loss after iteration 245000: 0.28377157625297733\n",
      "two-layer Loss after iteration 246000: 0.2836929526478672\n",
      "two-layer Loss after iteration 247000: 0.2836151271615029\n",
      "two-layer Loss after iteration 248000: 0.28354658182482195\n",
      "two-layer Loss after iteration 249000: 0.28348125931228074\n",
      "two-layer Loss after iteration 250000: 0.2834186369184604\n",
      "two-layer Loss after iteration 251000: 0.2833543440432868\n",
      "two-layer Loss after iteration 252000: 0.2832875451510155\n",
      "two-layer Loss after iteration 253000: 0.2832232256428077\n",
      "two-layer Loss after iteration 254000: 0.2831586250170256\n",
      "two-layer Loss after iteration 255000: 0.2830995818930313\n",
      "two-layer Loss after iteration 256000: 0.28304169252768646\n",
      "two-layer Loss after iteration 257000: 0.28297622640388653\n",
      "two-layer Loss after iteration 258000: 0.2829094987044667\n",
      "two-layer Loss after iteration 259000: 0.2828463702695792\n",
      "two-layer Loss after iteration 260000: 0.28278321536021633\n",
      "two-layer Loss after iteration 261000: 0.2827202412715762\n",
      "two-layer Loss after iteration 262000: 0.28266116702016536\n",
      "two-layer Loss after iteration 263000: 0.28260108299322473\n",
      "two-layer Loss after iteration 264000: 0.2825407267026042\n",
      "two-layer Loss after iteration 265000: 0.2824835695935354\n",
      "two-layer Loss after iteration 266000: 0.2824257444274379\n",
      "two-layer Loss after iteration 267000: 0.2823688468894319\n",
      "two-layer Loss after iteration 268000: 0.28231271493541626\n",
      "two-layer Loss after iteration 269000: 0.2822566378890504\n",
      "two-layer Loss after iteration 270000: 0.28220312755257126\n",
      "two-layer Loss after iteration 271000: 0.2821505279977558\n",
      "two-layer Loss after iteration 272000: 0.27505978483851334\n",
      "two-layer Loss after iteration 273000: 0.2734628432606275\n",
      "two-layer Loss after iteration 274000: 0.2725481448840636\n",
      "two-layer Loss after iteration 275000: 0.2718743685702364\n",
      "two-layer Loss after iteration 276000: 0.27135933232426096\n",
      "two-layer Loss after iteration 277000: 0.2709543959107732\n",
      "two-layer Loss after iteration 278000: 0.2704956729404363\n",
      "two-layer Loss after iteration 279000: 0.2696218410731396\n",
      "two-layer Loss after iteration 280000: 0.2688437458557749\n",
      "two-layer Loss after iteration 281000: 0.26821656567290253\n",
      "two-layer Loss after iteration 282000: 0.26770311738541663\n",
      "two-layer Loss after iteration 283000: 0.2672777442617919\n",
      "two-layer Loss after iteration 284000: 0.26669131344367664\n",
      "two-layer Loss after iteration 285000: 0.26615108856129666\n",
      "two-layer Loss after iteration 286000: 0.26572710614548245\n",
      "two-layer Loss after iteration 287000: 0.26537118980339813\n",
      "two-layer Loss after iteration 288000: 0.2650675863571574\n",
      "two-layer Loss after iteration 289000: 0.26480465008231824\n",
      "two-layer Loss after iteration 290000: 0.26457449010134887\n",
      "two-layer Loss after iteration 291000: 0.2643701932948804\n",
      "two-layer Loss after iteration 292000: 0.26418695804473924\n",
      "two-layer Loss after iteration 293000: 0.2640210941573649\n",
      "two-layer Loss after iteration 294000: 0.26372306777946203\n",
      "two-layer Loss after iteration 295000: 0.26344849447396623\n",
      "two-layer Loss after iteration 296000: 0.26319923384010496\n",
      "two-layer Loss after iteration 297000: 0.2629877678864427\n",
      "two-layer Loss after iteration 298000: 0.2628004025796534\n",
      "two-layer Loss after iteration 299000: 0.2626329156501014\n",
      "two-layer Loss after iteration 300000: 0.26248178982225007\n",
      "two-layer Loss after iteration 301000: 0.26234282003068987\n",
      "two-layer Loss after iteration 302000: 0.26221528298800223\n",
      "two-layer Loss after iteration 303000: 0.2620248714047861\n",
      "two-layer Loss after iteration 304000: 0.26181675175455715\n",
      "two-layer Loss after iteration 305000: 0.261631544504596\n",
      "two-layer Loss after iteration 306000: 0.26146421443013457\n",
      "two-layer Loss after iteration 307000: 0.26131408754017005\n",
      "two-layer Loss after iteration 308000: 0.26117797485425365\n",
      "two-layer Loss after iteration 309000: 0.2610408055280637\n",
      "two-layer Loss after iteration 310000: 0.2609118160175356\n",
      "two-layer Loss after iteration 311000: 0.2607874976899654\n",
      "two-layer Loss after iteration 312000: 0.2605882439761977\n",
      "two-layer Loss after iteration 313000: 0.2604176555875843\n",
      "two-layer Loss after iteration 314000: 0.26026752001761766\n",
      "two-layer Loss after iteration 315000: 0.26013198534217374\n",
      "two-layer Loss after iteration 316000: 0.2600082562312214\n",
      "two-layer Loss after iteration 317000: 0.2598976568677897\n",
      "two-layer Loss after iteration 318000: 0.25972322934198083\n",
      "two-layer Loss after iteration 319000: 0.2594820879522787\n",
      "two-layer Loss after iteration 320000: 0.2592058024028167\n",
      "two-layer Loss after iteration 321000: 0.2589915849731685\n",
      "two-layer Loss after iteration 322000: 0.25876658121460683\n",
      "two-layer Loss after iteration 323000: 0.2582420969469845\n",
      "two-layer Loss after iteration 324000: 0.257509106877421\n",
      "two-layer Loss after iteration 325000: 0.25696359688096215\n",
      "two-layer Loss after iteration 326000: 0.25647938336571885\n",
      "two-layer Loss after iteration 327000: 0.25598092308575227\n",
      "two-layer Loss after iteration 328000: 0.2555352291134929\n",
      "two-layer Loss after iteration 329000: 0.2551263301861497\n",
      "two-layer Loss after iteration 330000: 0.2547600097103309\n",
      "two-layer Loss after iteration 331000: 0.2542041161290481\n",
      "two-layer Loss after iteration 332000: 0.25316629479667624\n",
      "two-layer Loss after iteration 333000: 0.2496599543379329\n",
      "two-layer Loss after iteration 334000: 0.24397386574290394\n",
      "two-layer Loss after iteration 335000: 0.2388132231804076\n",
      "two-layer Loss after iteration 336000: 0.23470670164157742\n",
      "two-layer Loss after iteration 337000: 0.23143597195769408\n",
      "two-layer Loss after iteration 338000: 0.2279306125080479\n",
      "two-layer Loss after iteration 339000: 0.22593486258121057\n",
      "two-layer Loss after iteration 340000: 0.2224280345099876\n",
      "two-layer Loss after iteration 341000: 0.220594597785564\n",
      "two-layer Loss after iteration 342000: 0.21910055999473665\n",
      "two-layer Loss after iteration 343000: 0.2177922800085949\n",
      "two-layer Loss after iteration 344000: 0.21664236269809367\n",
      "two-layer Loss after iteration 345000: 0.21548368535180987\n",
      "two-layer Loss after iteration 346000: 0.2144120949123705\n",
      "two-layer Loss after iteration 347000: 0.21344329514495017\n",
      "two-layer Loss after iteration 348000: 0.21249438674465548\n",
      "two-layer Loss after iteration 349000: 0.21146515880271094\n",
      "two-layer Loss after iteration 350000: 0.20824802042322685\n",
      "two-layer Loss after iteration 351000: 0.2056684870106661\n",
      "two-layer Loss after iteration 352000: 0.20417362679357876\n",
      "two-layer Loss after iteration 353000: 0.20291210371879068\n",
      "two-layer Loss after iteration 354000: 0.20199543702939632\n",
      "two-layer Loss after iteration 355000: 0.2012572569417324\n",
      "two-layer Loss after iteration 356000: 0.20057352938485307\n",
      "two-layer Loss after iteration 357000: 0.19982996009604195\n",
      "two-layer Loss after iteration 358000: 0.19918363593601937\n",
      "two-layer Loss after iteration 359000: 0.19860824812113978\n",
      "two-layer Loss after iteration 360000: 0.19805457847719918\n",
      "two-layer Loss after iteration 361000: 0.1975258917746011\n",
      "two-layer Loss after iteration 362000: 0.19702406931361977\n",
      "two-layer Loss after iteration 363000: 0.19653419258247318\n",
      "two-layer Loss after iteration 364000: 0.1960550092298423\n",
      "two-layer Loss after iteration 365000: 0.19559598484639876\n",
      "two-layer Loss after iteration 366000: 0.19514659191596795\n",
      "two-layer Loss after iteration 367000: 0.1947164375379237\n",
      "two-layer Loss after iteration 368000: 0.19429058509578986\n",
      "two-layer Loss after iteration 369000: 0.19386935537829741\n",
      "two-layer Loss after iteration 370000: 0.19346225315987817\n",
      "two-layer Loss after iteration 371000: 0.19306809280740253\n",
      "two-layer Loss after iteration 372000: 0.19267396569853135\n",
      "two-layer Loss after iteration 373000: 0.19229231303514224\n",
      "two-layer Loss after iteration 374000: 0.19191647871296902\n",
      "two-layer Loss after iteration 375000: 0.1915493815670485\n",
      "two-layer Loss after iteration 376000: 0.19119293313757166\n",
      "two-layer Loss after iteration 377000: 0.19083747840761267\n",
      "two-layer Loss after iteration 378000: 0.19049320753016954\n",
      "two-layer Loss after iteration 379000: 0.19015609213062026\n",
      "two-layer Loss after iteration 380000: 0.18982615135635678\n",
      "two-layer Loss after iteration 381000: 0.18950160689755363\n",
      "two-layer Loss after iteration 382000: 0.1891864593351466\n",
      "two-layer Loss after iteration 383000: 0.18887209608313363\n",
      "two-layer Loss after iteration 384000: 0.1863050617717543\n",
      "two-layer Loss after iteration 385000: 0.18546185133636214\n",
      "two-layer Loss after iteration 386000: 0.18481217943365497\n",
      "two-layer Loss after iteration 387000: 0.18426341397394558\n",
      "two-layer Loss after iteration 388000: 0.18377944385601114\n",
      "two-layer Loss after iteration 389000: 0.18333001630475665\n",
      "two-layer Loss after iteration 390000: 0.18290066481818948\n",
      "two-layer Loss after iteration 391000: 0.18248420137774976\n",
      "two-layer Loss after iteration 392000: 0.1820764865488194\n",
      "two-layer Loss after iteration 393000: 0.1816761709691367\n",
      "two-layer Loss after iteration 394000: 0.18128200332820307\n",
      "two-layer Loss after iteration 395000: 0.18089373388283495\n",
      "two-layer Loss after iteration 396000: 0.18051047337616033\n",
      "two-layer Loss after iteration 397000: 0.18013234103863018\n",
      "two-layer Loss after iteration 398000: 0.17819490909420224\n",
      "two-layer Loss after iteration 399000: 0.17768245516410383\n",
      "two-layer Loss after iteration 400000: 0.1772194704776188\n",
      "two-layer Loss after iteration 401000: 0.17678160740271406\n",
      "two-layer Loss after iteration 402000: 0.1763571003394409\n",
      "two-layer Loss after iteration 403000: 0.17594300722999337\n",
      "two-layer Loss after iteration 404000: 0.17553670981115102\n",
      "two-layer Loss after iteration 405000: 0.1751370316948318\n",
      "two-layer Loss after iteration 406000: 0.17474501165603637\n",
      "two-layer Loss after iteration 407000: 0.17435978613237246\n",
      "two-layer Loss after iteration 408000: 0.17398061086248734\n",
      "two-layer Loss after iteration 409000: 0.17360787932161564\n",
      "two-layer Loss after iteration 410000: 0.17324120563706205\n",
      "two-layer Loss after iteration 411000: 0.17288024551838768\n",
      "two-layer Loss after iteration 412000: 0.1725255970777252\n",
      "two-layer Loss after iteration 413000: 0.1721766182417555\n",
      "two-layer Loss after iteration 414000: 0.17183335159089588\n",
      "two-layer Loss after iteration 415000: 0.17149632342150425\n",
      "two-layer Loss after iteration 416000: 0.17116527617823857\n",
      "two-layer Loss after iteration 417000: 0.1703288569924624\n",
      "two-layer Loss after iteration 418000: 0.16988387453999684\n",
      "two-layer Loss after iteration 419000: 0.16946810672469137\n",
      "two-layer Loss after iteration 420000: 0.16907004545508683\n",
      "two-layer Loss after iteration 421000: 0.16868513050394454\n",
      "two-layer Loss after iteration 422000: 0.16831127303117274\n",
      "two-layer Loss after iteration 423000: 0.16794688537760813\n",
      "two-layer Loss after iteration 424000: 0.16759077677022266\n",
      "two-layer Loss after iteration 425000: 0.16724274199352987\n",
      "two-layer Loss after iteration 426000: 0.16690257435604058\n",
      "two-layer Loss after iteration 427000: 0.16630255885681225\n",
      "two-layer Loss after iteration 428000: 0.16503213944720846\n",
      "two-layer Loss after iteration 429000: 0.1645939047047855\n",
      "two-layer Loss after iteration 430000: 0.1641856918465538\n",
      "two-layer Loss after iteration 431000: 0.16379540299372897\n",
      "two-layer Loss after iteration 432000: 0.16327721411005816\n",
      "two-layer Loss after iteration 433000: 0.16286930749912762\n",
      "two-layer Loss after iteration 434000: 0.162476712536644\n",
      "two-layer Loss after iteration 435000: 0.16209586948929436\n",
      "two-layer Loss after iteration 436000: 0.16172544917699594\n",
      "two-layer Loss after iteration 437000: 0.16136323882075335\n",
      "two-layer Loss after iteration 438000: 0.16101016128790377\n",
      "two-layer Loss after iteration 439000: 0.16066563776513515\n",
      "two-layer Loss after iteration 440000: 0.16032818301436136\n",
      "two-layer Loss after iteration 441000: 0.15999668698233452\n",
      "two-layer Loss after iteration 442000: 0.15967423763242336\n",
      "two-layer Loss after iteration 443000: 0.15936020443266272\n",
      "two-layer Loss after iteration 444000: 0.15905189009344434\n",
      "two-layer Loss after iteration 445000: 0.15875214450861802\n",
      "two-layer Loss after iteration 446000: 0.1584576278126013\n",
      "two-layer Loss after iteration 447000: 0.15817027760925212\n",
      "two-layer Loss after iteration 448000: 0.1578905333187289\n",
      "two-layer Loss after iteration 449000: 0.1576180094101551\n",
      "two-layer Loss after iteration 450000: 0.15735152451897808\n",
      "two-layer Loss after iteration 451000: 0.1570916491683689\n",
      "two-layer Loss after iteration 452000: 0.15683841457041356\n",
      "two-layer Loss after iteration 453000: 0.15658955365893723\n",
      "two-layer Loss after iteration 454000: 0.15634918058378147\n",
      "two-layer Loss after iteration 455000: 0.15611418317090145\n",
      "two-layer Loss after iteration 456000: 0.15588493072267146\n",
      "two-layer Loss after iteration 457000: 0.15566185954120343\n",
      "two-layer Loss after iteration 458000: 0.15544318322223222\n",
      "two-layer Loss after iteration 459000: 0.15523000513933538\n",
      "two-layer Loss after iteration 460000: 0.15502213550113514\n",
      "two-layer Loss after iteration 461000: 0.1548199217540222\n",
      "two-layer Loss after iteration 462000: 0.15462224484357787\n",
      "two-layer Loss after iteration 463000: 0.15442679790802338\n",
      "two-layer Loss after iteration 464000: 0.15423636323634685\n",
      "two-layer Loss after iteration 465000: 0.15404816560504517\n",
      "two-layer Loss after iteration 466000: 0.1538645927048945\n",
      "two-layer Loss after iteration 467000: 0.15368376388328614\n",
      "two-layer Loss after iteration 468000: 0.15350455912389738\n",
      "two-layer Loss after iteration 469000: 0.15333074542520558\n",
      "two-layer Loss after iteration 470000: 0.1531584395766579\n",
      "two-layer Loss after iteration 471000: 0.15299108655813767\n",
      "two-layer Loss after iteration 472000: 0.15282543599206524\n",
      "two-layer Loss after iteration 473000: 0.1526612880272552\n",
      "two-layer Loss after iteration 474000: 0.1525030701997061\n",
      "two-layer Loss after iteration 475000: 0.15234869361525932\n",
      "two-layer Loss after iteration 476000: 0.1521988294196246\n",
      "two-layer Loss after iteration 477000: 0.15121483087302193\n",
      "two-layer Loss after iteration 478000: 0.1507810165361862\n",
      "two-layer Loss after iteration 479000: 0.15054554654715066\n",
      "two-layer Loss after iteration 480000: 0.15033086341459517\n",
      "two-layer Loss after iteration 481000: 0.15013112036514317\n",
      "two-layer Loss after iteration 482000: 0.1498569462323883\n",
      "two-layer Loss after iteration 483000: 0.14952246683557696\n",
      "two-layer Loss after iteration 484000: 0.14931571983713135\n",
      "two-layer Loss after iteration 485000: 0.14911820686606309\n",
      "two-layer Loss after iteration 486000: 0.14886699885429627\n",
      "two-layer Loss after iteration 487000: 0.14859651911092325\n",
      "two-layer Loss after iteration 488000: 0.14838309607839584\n",
      "two-layer Loss after iteration 489000: 0.14817575223551027\n",
      "two-layer Loss after iteration 490000: 0.14797235158457012\n",
      "two-layer Loss after iteration 491000: 0.14777241591840595\n",
      "two-layer Loss after iteration 492000: 0.14757577301016883\n",
      "two-layer Loss after iteration 493000: 0.14738221250350522\n",
      "two-layer Loss after iteration 494000: 0.1471917055467068\n",
      "two-layer Loss after iteration 495000: 0.1470040255285748\n",
      "two-layer Loss after iteration 496000: 0.1468192038730332\n",
      "two-layer Loss after iteration 497000: 0.14663714023988958\n",
      "two-layer Loss after iteration 498000: 0.14645783105715768\n",
      "two-layer Loss after iteration 499000: 0.14628123119951406\n",
      "two-layer Loss after iteration 500000: 0.1461072972830523\n",
      "two-layer Loss after iteration 501000: 0.14593594627098816\n",
      "two-layer Loss after iteration 502000: 0.14576725297606347\n",
      "two-layer Loss after iteration 503000: 0.14560124154459808\n",
      "two-layer Loss after iteration 504000: 0.14543740814968537\n",
      "two-layer Loss after iteration 505000: 0.14527614055895988\n",
      "two-layer Loss after iteration 506000: 0.14511744066248178\n",
      "two-layer Loss after iteration 507000: 0.1441928954938225\n",
      "two-layer Loss after iteration 508000: 0.1439442253974443\n",
      "two-layer Loss after iteration 509000: 0.14371205616065122\n",
      "two-layer Loss after iteration 510000: 0.14349154682315762\n",
      "two-layer Loss after iteration 511000: 0.14327905186612408\n",
      "two-layer Loss after iteration 512000: 0.14307277429165696\n",
      "two-layer Loss after iteration 513000: 0.14287267793054728\n",
      "two-layer Loss after iteration 514000: 0.1426781286747766\n",
      "two-layer Loss after iteration 515000: 0.1424887770399439\n",
      "two-layer Loss after iteration 516000: 0.1423032117409031\n",
      "two-layer Loss after iteration 517000: 0.14212161995163577\n",
      "two-layer Loss after iteration 518000: 0.14194460486778182\n",
      "two-layer Loss after iteration 519000: 0.14177127569070305\n",
      "two-layer Loss after iteration 520000: 0.14160203319504014\n",
      "two-layer Loss after iteration 521000: 0.14143615719453678\n",
      "two-layer Loss after iteration 522000: 0.1412744316615683\n",
      "two-layer Loss after iteration 523000: 0.1401613899799858\n",
      "two-layer Loss after iteration 524000: 0.13988133669727598\n",
      "two-layer Loss after iteration 525000: 0.13961836204078396\n",
      "two-layer Loss after iteration 526000: 0.13936733080944133\n",
      "two-layer Loss after iteration 527000: 0.13913137620855534\n",
      "two-layer Loss after iteration 528000: 0.13890796945451142\n",
      "two-layer Loss after iteration 529000: 0.13869770805016132\n",
      "two-layer Loss after iteration 530000: 0.13849597408990852\n",
      "two-layer Loss after iteration 531000: 0.13830502268178016\n",
      "two-layer Loss after iteration 532000: 0.13744957052722698\n",
      "two-layer Loss after iteration 533000: 0.13717604506231856\n",
      "two-layer Loss after iteration 534000: 0.13694352851111433\n",
      "two-layer Loss after iteration 535000: 0.13656764107827324\n",
      "two-layer Loss after iteration 536000: 0.13632296272423475\n",
      "two-layer Loss after iteration 537000: 0.13609405122731774\n",
      "two-layer Loss after iteration 538000: 0.13587928543573297\n",
      "two-layer Loss after iteration 539000: 0.13567147327564194\n",
      "two-layer Loss after iteration 540000: 0.1354705893395653\n",
      "two-layer Loss after iteration 541000: 0.1352763788869814\n",
      "two-layer Loss after iteration 542000: 0.13508771992227273\n",
      "two-layer Loss after iteration 543000: 0.13490268420414744\n",
      "two-layer Loss after iteration 544000: 0.13472397484554982\n",
      "two-layer Loss after iteration 545000: 0.1345472284007242\n",
      "two-layer Loss after iteration 546000: 0.13437629169786527\n",
      "two-layer Loss after iteration 547000: 0.134208223502118\n",
      "two-layer Loss after iteration 548000: 0.1340450325937709\n",
      "two-layer Loss after iteration 549000: 0.13388522360229788\n",
      "two-layer Loss after iteration 550000: 0.13372872245785147\n",
      "two-layer Loss after iteration 551000: 0.13336999397641128\n",
      "two-layer Loss after iteration 552000: 0.1331865885131849\n",
      "two-layer Loss after iteration 553000: 0.13302203054647588\n",
      "two-layer Loss after iteration 554000: 0.13286322020658364\n",
      "two-layer Loss after iteration 555000: 0.13270992773235452\n",
      "two-layer Loss after iteration 556000: 0.132560254337845\n",
      "two-layer Loss after iteration 557000: 0.1324161441141418\n",
      "two-layer Loss after iteration 558000: 0.1322746535374494\n",
      "two-layer Loss after iteration 559000: 0.13213644650685524\n",
      "two-layer Loss after iteration 560000: 0.13200147684150293\n",
      "two-layer Loss after iteration 561000: 0.13186898855614848\n",
      "two-layer Loss after iteration 562000: 0.13174050086252012\n",
      "two-layer Loss after iteration 563000: 0.1316152646285304\n",
      "two-layer Loss after iteration 564000: 0.13149300649359538\n",
      "two-layer Loss after iteration 565000: 0.13137202661042546\n",
      "two-layer Loss after iteration 566000: 0.13125540476992373\n",
      "two-layer Loss after iteration 567000: 0.13113956016798886\n",
      "two-layer Loss after iteration 568000: 0.13102747523338634\n",
      "two-layer Loss after iteration 569000: 0.13091781894349194\n",
      "two-layer Loss after iteration 570000: 0.13081029063755317\n",
      "two-layer Loss after iteration 571000: 0.13070552767006613\n",
      "two-layer Loss after iteration 572000: 0.13060266917156954\n",
      "two-layer Loss after iteration 573000: 0.12993002225849035\n",
      "two-layer Loss after iteration 574000: 0.12972692278134085\n",
      "two-layer Loss after iteration 575000: 0.12955022903410549\n",
      "two-layer Loss after iteration 576000: 0.1293788490157672\n",
      "two-layer Loss after iteration 577000: 0.12921445529034575\n",
      "two-layer Loss after iteration 578000: 0.12905455551577763\n",
      "two-layer Loss after iteration 579000: 0.12889734406284528\n",
      "two-layer Loss after iteration 580000: 0.12874638743468284\n",
      "two-layer Loss after iteration 581000: 0.1285991523545901\n",
      "two-layer Loss after iteration 582000: 0.12845484145938085\n",
      "two-layer Loss after iteration 583000: 0.12831478200402774\n",
      "two-layer Loss after iteration 584000: 0.12817928956377586\n",
      "two-layer Loss after iteration 585000: 0.12804464441932542\n",
      "two-layer Loss after iteration 586000: 0.1279153164313499\n",
      "two-layer Loss after iteration 587000: 0.12778837928255354\n",
      "two-layer Loss after iteration 588000: 0.12766567241685925\n",
      "two-layer Loss after iteration 589000: 0.12754516583215886\n",
      "two-layer Loss after iteration 590000: 0.12742699598546586\n",
      "two-layer Loss after iteration 591000: 0.12731190475881138\n",
      "two-layer Loss after iteration 592000: 0.1272005506584546\n",
      "two-layer Loss after iteration 593000: 0.12708915053065856\n",
      "two-layer Loss after iteration 594000: 0.1269821287571497\n",
      "two-layer Loss after iteration 595000: 0.1268777088810581\n",
      "two-layer Loss after iteration 596000: 0.1267762587805168\n",
      "two-layer Loss after iteration 597000: 0.126676688701272\n",
      "two-layer Loss after iteration 598000: 0.12657899331697559\n",
      "two-layer Loss after iteration 599000: 0.12648272732467944\n",
      "two-layer Loss after iteration 600000: 0.12639050119228964\n",
      "two-layer Loss after iteration 601000: 0.12629833368602303\n",
      "two-layer Loss after iteration 602000: 0.12620888529636415\n",
      "two-layer Loss after iteration 603000: 0.12612161052516854\n",
      "two-layer Loss after iteration 604000: 0.126036375017278\n",
      "two-layer Loss after iteration 605000: 0.12595329598053723\n",
      "two-layer Loss after iteration 606000: 0.12587152593711343\n",
      "two-layer Loss after iteration 607000: 0.12579237909034952\n",
      "two-layer Loss after iteration 608000: 0.1257143020062665\n",
      "two-layer Loss after iteration 609000: 0.1256395068567599\n",
      "two-layer Loss after iteration 610000: 0.12556278609325297\n",
      "two-layer Loss after iteration 611000: 0.12549111781404854\n",
      "two-layer Loss after iteration 612000: 0.12541970947786701\n",
      "two-layer Loss after iteration 613000: 0.1253483558867778\n",
      "two-layer Loss after iteration 614000: 0.12527924017311956\n",
      "two-layer Loss after iteration 615000: 0.12521106761484546\n",
      "two-layer Loss after iteration 616000: 0.12514498558191733\n",
      "two-layer Loss after iteration 617000: 0.1250816385207172\n",
      "two-layer Loss after iteration 618000: 0.12501844117459268\n",
      "two-layer Loss after iteration 619000: 0.12495421011530997\n",
      "two-layer Loss after iteration 620000: 0.12489355480976405\n",
      "two-layer Loss after iteration 621000: 0.12483317346998134\n",
      "two-layer Loss after iteration 622000: 0.12477392038439812\n",
      "two-layer Loss after iteration 623000: 0.12471660494016867\n",
      "two-layer Loss after iteration 624000: 0.12465984732902535\n",
      "two-layer Loss after iteration 625000: 0.1246046663933554\n",
      "two-layer Loss after iteration 626000: 0.12454978274358629\n",
      "two-layer Loss after iteration 627000: 0.12449429515580586\n",
      "two-layer Loss after iteration 628000: 0.12444370762076024\n",
      "two-layer Loss after iteration 629000: 0.12439025671585358\n",
      "two-layer Loss after iteration 630000: 0.12433898489171363\n",
      "two-layer Loss after iteration 631000: 0.12429026438449917\n",
      "two-layer Loss after iteration 632000: 0.12423969721459353\n",
      "two-layer Loss after iteration 633000: 0.12419003906218577\n",
      "two-layer Loss after iteration 634000: 0.12414289851880685\n",
      "two-layer Loss after iteration 635000: 0.12409563372509494\n",
      "two-layer Loss after iteration 636000: 0.1240479497336833\n",
      "two-layer Loss after iteration 637000: 0.12400383137698198\n",
      "two-layer Loss after iteration 638000: 0.12395652719356519\n",
      "two-layer Loss after iteration 639000: 0.12391340538264609\n",
      "two-layer Loss after iteration 640000: 0.12386834463685199\n",
      "two-layer Loss after iteration 641000: 0.12382669188394757\n",
      "two-layer Loss after iteration 642000: 0.12378386962463175\n",
      "two-layer Loss after iteration 643000: 0.1237408591957931\n",
      "two-layer Loss after iteration 644000: 0.12369979123320732\n",
      "two-layer Loss after iteration 645000: 0.12365850264436049\n",
      "two-layer Loss after iteration 646000: 0.12361668180752315\n",
      "two-layer Loss after iteration 647000: 0.12357611226058988\n",
      "two-layer Loss after iteration 648000: 0.12353633488656417\n",
      "two-layer Loss after iteration 649000: 0.12349895737867535\n",
      "two-layer Loss after iteration 650000: 0.12345905317996467\n",
      "two-layer Loss after iteration 651000: 0.12342065968921472\n",
      "two-layer Loss after iteration 652000: 0.12338464960352978\n",
      "two-layer Loss after iteration 653000: 0.12334577737841032\n",
      "two-layer Loss after iteration 654000: 0.12330854059426996\n",
      "two-layer Loss after iteration 655000: 0.12327402446242434\n",
      "two-layer Loss after iteration 656000: 0.1232374060039783\n",
      "two-layer Loss after iteration 657000: 0.12320115264284087\n",
      "two-layer Loss after iteration 658000: 0.12316648985876832\n",
      "two-layer Loss after iteration 659000: 0.1231292686238844\n",
      "two-layer Loss after iteration 660000: 0.12309516004945863\n",
      "two-layer Loss after iteration 661000: 0.12306019295485512\n",
      "two-layer Loss after iteration 662000: 0.12302596653200658\n",
      "two-layer Loss after iteration 663000: 0.12299437759710842\n",
      "two-layer Loss after iteration 664000: 0.12296004736820411\n",
      "two-layer Loss after iteration 665000: 0.12292593042515516\n",
      "two-layer Loss after iteration 666000: 0.1228942272426062\n",
      "two-layer Loss after iteration 667000: 0.12286078319646059\n",
      "two-layer Loss after iteration 668000: 0.12282700812036534\n",
      "two-layer Loss after iteration 669000: 0.12279384552768195\n",
      "two-layer Loss after iteration 670000: 0.12276256226222393\n",
      "two-layer Loss after iteration 671000: 0.12273194538281831\n",
      "two-layer Loss after iteration 672000: 0.12270158025873609\n",
      "two-layer Loss after iteration 673000: 0.12266942077253733\n",
      "two-layer Loss after iteration 674000: 0.1226370955320767\n",
      "two-layer Loss after iteration 675000: 0.12260741406904212\n",
      "two-layer Loss after iteration 676000: 0.12257638918305996\n",
      "two-layer Loss after iteration 677000: 0.12254711772928734\n",
      "two-layer Loss after iteration 678000: 0.12251567273315087\n",
      "two-layer Loss after iteration 679000: 0.1224856264263339\n",
      "two-layer Loss after iteration 680000: 0.12245361377454521\n",
      "two-layer Loss after iteration 681000: 0.12242462147899448\n",
      "two-layer Loss after iteration 682000: 0.12239444402775307\n",
      "two-layer Loss after iteration 683000: 0.12236634939435898\n",
      "two-layer Loss after iteration 684000: 0.12233819937619336\n",
      "two-layer Loss after iteration 685000: 0.12230724901355003\n",
      "two-layer Loss after iteration 686000: 0.12227962662932491\n",
      "two-layer Loss after iteration 687000: 0.12225112789072816\n",
      "two-layer Loss after iteration 688000: 0.12222190837518536\n",
      "two-layer Loss after iteration 689000: 0.1221929544483564\n",
      "two-layer Loss after iteration 690000: 0.12216465151984551\n",
      "two-layer Loss after iteration 691000: 0.12213697112348448\n",
      "two-layer Loss after iteration 692000: 0.12210920414284904\n",
      "two-layer Loss after iteration 693000: 0.12207968416496998\n",
      "two-layer Loss after iteration 694000: 0.12205110701296507\n",
      "two-layer Loss after iteration 695000: 0.12202445988746374\n",
      "two-layer Loss after iteration 696000: 0.1219973314951581\n",
      "two-layer Loss after iteration 697000: 0.12196811092807523\n",
      "two-layer Loss after iteration 698000: 0.12194238973243025\n",
      "two-layer Loss after iteration 699000: 0.1219154799594334\n",
      "two-layer Loss after iteration 700000: 0.12188664385779235\n",
      "two-layer Loss after iteration 701000: 0.12186060660300249\n",
      "two-layer Loss after iteration 702000: 0.12183516860006442\n",
      "two-layer Loss after iteration 703000: 0.12180705942705114\n",
      "two-layer Loss after iteration 704000: 0.12177955294593551\n",
      "two-layer Loss after iteration 705000: 0.12175206476831427\n",
      "two-layer Loss after iteration 706000: 0.12172540886008588\n",
      "two-layer Loss after iteration 707000: 0.12169893259100889\n",
      "two-layer Loss after iteration 708000: 0.12167326966252041\n",
      "two-layer Loss after iteration 709000: 0.12164803617794198\n",
      "two-layer Loss after iteration 710000: 0.12162186803359286\n",
      "two-layer Loss after iteration 711000: 0.12159435008603768\n",
      "two-layer Loss after iteration 712000: 0.12157080389990717\n",
      "two-layer Loss after iteration 713000: 0.12154204426894179\n",
      "two-layer Loss after iteration 714000: 0.12151681684638183\n",
      "two-layer Loss after iteration 715000: 0.12149165430479074\n",
      "two-layer Loss after iteration 716000: 0.12146563019167171\n",
      "two-layer Loss after iteration 717000: 0.12143997837019223\n",
      "two-layer Loss after iteration 718000: 0.12141315381473912\n",
      "two-layer Loss after iteration 719000: 0.12139000146335471\n",
      "two-layer Loss after iteration 720000: 0.12136234578814517\n",
      "two-layer Loss after iteration 721000: 0.1213386237855713\n",
      "two-layer Loss after iteration 722000: 0.12131441390692749\n",
      "two-layer Loss after iteration 723000: 0.12128837797383761\n",
      "two-layer Loss after iteration 724000: 0.12126137935872668\n",
      "two-layer Loss after iteration 725000: 0.12123759948821099\n",
      "two-layer Loss after iteration 726000: 0.12121163091051122\n",
      "two-layer Loss after iteration 727000: 0.1211876314848507\n",
      "two-layer Loss after iteration 728000: 0.12116320502726051\n",
      "two-layer Loss after iteration 729000: 0.1211376139035404\n",
      "two-layer Loss after iteration 730000: 0.12111254439421139\n",
      "two-layer Loss after iteration 731000: 0.121087589311545\n",
      "two-layer Loss after iteration 732000: 0.12106451523304493\n",
      "two-layer Loss after iteration 733000: 0.12103951870535148\n",
      "two-layer Loss after iteration 734000: 0.12101372609002403\n",
      "two-layer Loss after iteration 735000: 0.1209906311795098\n",
      "two-layer Loss after iteration 736000: 0.1209645544615579\n",
      "two-layer Loss after iteration 737000: 0.12093956937100415\n",
      "two-layer Loss after iteration 738000: 0.12091597969077165\n",
      "two-layer Loss after iteration 739000: 0.12089424458953778\n",
      "two-layer Loss after iteration 740000: 0.12086729822193673\n",
      "two-layer Loss after iteration 741000: 0.12084351783141126\n",
      "two-layer Loss after iteration 742000: 0.12082076292938852\n",
      "two-layer Loss after iteration 743000: 0.1207967051060164\n",
      "two-layer Loss after iteration 744000: 0.12077258568123629\n",
      "two-layer Loss after iteration 745000: 0.12074902667946401\n",
      "two-layer Loss after iteration 746000: 0.12072562101292822\n",
      "two-layer Loss after iteration 747000: 0.12069955643705953\n",
      "two-layer Loss after iteration 748000: 0.12067657649150751\n",
      "two-layer Loss after iteration 749000: 0.12065290330962287\n",
      "two-layer Loss after iteration 750000: 0.12062835274632609\n",
      "two-layer Loss after iteration 751000: 0.12060563252844672\n",
      "two-layer Loss after iteration 752000: 0.12058242263656103\n",
      "two-layer Loss after iteration 753000: 0.12055697544607503\n",
      "two-layer Loss after iteration 754000: 0.12053538806531623\n",
      "two-layer Loss after iteration 755000: 0.12051077095704632\n",
      "two-layer Loss after iteration 756000: 0.12048718079114673\n",
      "two-layer Loss after iteration 757000: 0.12046396893220564\n",
      "two-layer Loss after iteration 758000: 0.120441587114521\n",
      "two-layer Loss after iteration 759000: 0.12041936427455495\n",
      "two-layer Loss after iteration 760000: 0.12039469702925489\n",
      "two-layer Loss after iteration 761000: 0.1203715220022136\n",
      "two-layer Loss after iteration 762000: 0.12034724152429681\n",
      "two-layer Loss after iteration 763000: 0.1203246695143296\n",
      "two-layer Loss after iteration 764000: 0.12030290056518132\n",
      "two-layer Loss after iteration 765000: 0.12028078918158176\n",
      "two-layer Loss after iteration 766000: 0.12025742536813522\n",
      "two-layer Loss after iteration 767000: 0.12023293637362858\n",
      "two-layer Loss after iteration 768000: 0.1202121998802003\n",
      "two-layer Loss after iteration 769000: 0.12018871608736542\n",
      "two-layer Loss after iteration 770000: 0.12016418299205521\n",
      "two-layer Loss after iteration 771000: 0.1201425728378628\n",
      "two-layer Loss after iteration 772000: 0.12011951382388884\n",
      "two-layer Loss after iteration 773000: 0.1200969566853484\n",
      "two-layer Loss after iteration 774000: 0.12007299320307109\n",
      "two-layer Loss after iteration 775000: 0.12005088083165423\n",
      "two-layer Loss after iteration 776000: 0.12002916807774842\n",
      "two-layer Loss after iteration 777000: 0.12000564667811416\n",
      "two-layer Loss after iteration 778000: 0.11998357864530669\n",
      "two-layer Loss after iteration 779000: 0.11996245554727743\n",
      "two-layer Loss after iteration 780000: 0.11993838919047962\n",
      "two-layer Loss after iteration 781000: 0.11991614184700612\n",
      "two-layer Loss after iteration 782000: 0.11989595380286397\n",
      "two-layer Loss after iteration 783000: 0.11987247557878873\n",
      "two-layer Loss after iteration 784000: 0.11985141435823575\n",
      "two-layer Loss after iteration 785000: 0.11982914635502126\n",
      "two-layer Loss after iteration 786000: 0.11980777976378967\n",
      "two-layer Loss after iteration 787000: 0.11978493305863115\n",
      "two-layer Loss after iteration 788000: 0.11976141537476091\n",
      "two-layer Loss after iteration 789000: 0.11973875116999876\n",
      "two-layer Loss after iteration 790000: 0.11971725471330226\n",
      "two-layer Loss after iteration 791000: 0.11969699953955645\n",
      "two-layer Loss after iteration 792000: 0.11967500454171023\n",
      "two-layer Loss after iteration 793000: 0.11965144296396457\n",
      "two-layer Loss after iteration 794000: 0.1196321951851449\n",
      "two-layer Loss after iteration 795000: 0.11961048586628832\n",
      "two-layer Loss after iteration 796000: 0.11958655048088973\n",
      "two-layer Loss after iteration 797000: 0.1195677043828808\n",
      "two-layer Loss after iteration 798000: 0.11954489259620131\n",
      "two-layer Loss after iteration 799000: 0.11952193437700154\n",
      "two-layer Loss after iteration 800000: 0.11949919656604499\n",
      "two-layer Loss after iteration 801000: 0.11947869214504807\n",
      "two-layer Loss after iteration 802000: 0.11945842054259088\n",
      "two-layer Loss after iteration 803000: 0.11943796141263972\n",
      "two-layer Loss after iteration 804000: 0.11941451471678047\n",
      "two-layer Loss after iteration 805000: 0.11939475409191319\n",
      "two-layer Loss after iteration 806000: 0.11937397063277842\n",
      "two-layer Loss after iteration 807000: 0.11934993133767635\n",
      "two-layer Loss after iteration 808000: 0.11932964932237183\n",
      "two-layer Loss after iteration 809000: 0.11930730199693632\n",
      "two-layer Loss after iteration 810000: 0.1192879452016664\n",
      "two-layer Loss after iteration 811000: 0.11926668451179295\n",
      "two-layer Loss after iteration 812000: 0.11924625107520806\n",
      "two-layer Loss after iteration 813000: 0.11922298528778112\n",
      "two-layer Loss after iteration 814000: 0.1192015460137789\n",
      "two-layer Loss after iteration 815000: 0.11918199774056955\n",
      "two-layer Loss after iteration 816000: 0.11916170269182381\n",
      "two-layer Loss after iteration 817000: 0.11914118145997071\n",
      "two-layer Loss after iteration 818000: 0.11911736243874774\n",
      "two-layer Loss after iteration 819000: 0.1190967959933512\n",
      "two-layer Loss after iteration 820000: 0.11907739271699017\n",
      "two-layer Loss after iteration 821000: 0.11905473763436628\n",
      "two-layer Loss after iteration 822000: 0.1190344112981324\n",
      "two-layer Loss after iteration 823000: 0.11901629343770168\n",
      "two-layer Loss after iteration 824000: 0.11899452822396475\n",
      "two-layer Loss after iteration 825000: 0.11897316105005744\n",
      "two-layer Loss after iteration 826000: 0.11895278393382386\n",
      "two-layer Loss after iteration 827000: 0.11893257207981653\n",
      "two-layer Loss after iteration 828000: 0.11891198210819\n",
      "two-layer Loss after iteration 829000: 0.1188924329702464\n",
      "two-layer Loss after iteration 830000: 0.11886977727372729\n",
      "two-layer Loss after iteration 831000: 0.11885035457392147\n",
      "two-layer Loss after iteration 832000: 0.11883046532657676\n",
      "two-layer Loss after iteration 833000: 0.11880754412421612\n",
      "two-layer Loss after iteration 834000: 0.11878878064188866\n",
      "two-layer Loss after iteration 835000: 0.11876673010028412\n",
      "two-layer Loss after iteration 836000: 0.11874843347448069\n",
      "two-layer Loss after iteration 837000: 0.11872674957939551\n",
      "two-layer Loss after iteration 838000: 0.1187076425962724\n",
      "two-layer Loss after iteration 839000: 0.11868819885682806\n",
      "two-layer Loss after iteration 840000: 0.11866646023947383\n",
      "two-layer Loss after iteration 841000: 0.1186460910896385\n",
      "two-layer Loss after iteration 842000: 0.11862841535771691\n",
      "two-layer Loss after iteration 843000: 0.11860575888580528\n",
      "two-layer Loss after iteration 844000: 0.11858828792705714\n",
      "two-layer Loss after iteration 845000: 0.11856843878870549\n",
      "two-layer Loss after iteration 846000: 0.11854636281087416\n",
      "two-layer Loss after iteration 847000: 0.11852610293605745\n",
      "two-layer Loss after iteration 848000: 0.11850858973650125\n",
      "two-layer Loss after iteration 849000: 0.11848797379442794\n",
      "two-layer Loss after iteration 850000: 0.11846705576928103\n",
      "two-layer Loss after iteration 851000: 0.11844782180982776\n",
      "two-layer Loss after iteration 852000: 0.11842823602463634\n",
      "two-layer Loss after iteration 853000: 0.11840769781226808\n",
      "two-layer Loss after iteration 854000: 0.11838971421777363\n",
      "two-layer Loss after iteration 855000: 0.11836724438204813\n",
      "two-layer Loss after iteration 856000: 0.11834983656715316\n",
      "two-layer Loss after iteration 857000: 0.11833020503766378\n",
      "two-layer Loss after iteration 858000: 0.11830892853940544\n",
      "two-layer Loss after iteration 859000: 0.11829043240389164\n",
      "two-layer Loss after iteration 860000: 0.1182693276423621\n",
      "two-layer Loss after iteration 861000: 0.11825195098603974\n",
      "two-layer Loss after iteration 862000: 0.11823229044803198\n",
      "two-layer Loss after iteration 863000: 0.1182121537437558\n",
      "two-layer Loss after iteration 864000: 0.11819400076495924\n",
      "two-layer Loss after iteration 865000: 0.11817435441715222\n",
      "two-layer Loss after iteration 866000: 0.11815361982269414\n",
      "two-layer Loss after iteration 867000: 0.11813427001061974\n",
      "two-layer Loss after iteration 868000: 0.1181168047259844\n",
      "two-layer Loss after iteration 869000: 0.11809559051244317\n",
      "two-layer Loss after iteration 870000: 0.11807640764528775\n",
      "two-layer Loss after iteration 871000: 0.11806015311575696\n",
      "two-layer Loss after iteration 872000: 0.11804059783543075\n",
      "two-layer Loss after iteration 873000: 0.11801981448004735\n",
      "two-layer Loss after iteration 874000: 0.11800173370556127\n",
      "two-layer Loss after iteration 875000: 0.11798262622112526\n",
      "two-layer Loss after iteration 876000: 0.11796270608932044\n",
      "two-layer Loss after iteration 877000: 0.1179454201083788\n",
      "two-layer Loss after iteration 878000: 0.11792483385499516\n",
      "two-layer Loss after iteration 879000: 0.1179068591652316\n",
      "two-layer Loss after iteration 880000: 0.11788635816594428\n",
      "two-layer Loss after iteration 881000: 0.11786957539632804\n",
      "two-layer Loss after iteration 882000: 0.1178492401578485\n",
      "two-layer Loss after iteration 883000: 0.11783180890487031\n",
      "two-layer Loss after iteration 884000: 0.11781108709744037\n",
      "two-layer Loss after iteration 885000: 0.11779251461881343\n",
      "two-layer Loss after iteration 886000: 0.11777333948877276\n",
      "two-layer Loss after iteration 887000: 0.11775733507209986\n",
      "two-layer Loss after iteration 888000: 0.11773866691004796\n",
      "two-layer Loss after iteration 889000: 0.1177172736854343\n",
      "two-layer Loss after iteration 890000: 0.11769899947997002\n",
      "two-layer Loss after iteration 891000: 0.11768047567506953\n",
      "two-layer Loss after iteration 892000: 0.11766407560651199\n",
      "two-layer Loss after iteration 893000: 0.11764602713838108\n",
      "two-layer Loss after iteration 894000: 0.11762502066691699\n",
      "two-layer Loss after iteration 895000: 0.11760914800756593\n",
      "two-layer Loss after iteration 896000: 0.11759070268648167\n",
      "two-layer Loss after iteration 897000: 0.11756960733294229\n",
      "two-layer Loss after iteration 898000: 0.11755172804340286\n",
      "two-layer Loss after iteration 899000: 0.11753334262586766\n",
      "two-layer Loss after iteration 900000: 0.11751637804679718\n",
      "two-layer Loss after iteration 901000: 0.11749799747487043\n",
      "two-layer Loss after iteration 902000: 0.11747872786587357\n",
      "two-layer Loss after iteration 903000: 0.11746027871484468\n",
      "two-layer Loss after iteration 904000: 0.11744202762593828\n",
      "two-layer Loss after iteration 905000: 0.11742636139304366\n",
      "two-layer Loss after iteration 906000: 0.11740797778629274\n",
      "two-layer Loss after iteration 907000: 0.11738786577952548\n",
      "two-layer Loss after iteration 908000: 0.1173717261274524\n",
      "two-layer Loss after iteration 909000: 0.1173534832146508\n",
      "two-layer Loss after iteration 910000: 0.11733502929073937\n",
      "two-layer Loss after iteration 911000: 0.11731775457481831\n",
      "two-layer Loss after iteration 912000: 0.1173005462722021\n",
      "two-layer Loss after iteration 913000: 0.11728038712751157\n",
      "two-layer Loss after iteration 914000: 0.1172618540566925\n",
      "two-layer Loss after iteration 915000: 0.11724628500618223\n",
      "two-layer Loss after iteration 916000: 0.11722622569130471\n",
      "two-layer Loss after iteration 917000: 0.11720907919592692\n",
      "two-layer Loss after iteration 918000: 0.11719351061051324\n",
      "two-layer Loss after iteration 919000: 0.11717424410057599\n",
      "two-layer Loss after iteration 920000: 0.11715794990051667\n",
      "two-layer Loss after iteration 921000: 0.11713799534318535\n",
      "two-layer Loss after iteration 922000: 0.11712012368097195\n",
      "two-layer Loss after iteration 923000: 0.11710428552404796\n",
      "two-layer Loss after iteration 924000: 0.11708554697925343\n",
      "two-layer Loss after iteration 925000: 0.11706830416600579\n",
      "two-layer Loss after iteration 926000: 0.11705151702512986\n",
      "two-layer Loss after iteration 927000: 0.11703419485869945\n",
      "two-layer Loss after iteration 928000: 0.11701596608169601\n",
      "two-layer Loss after iteration 929000: 0.11699838240114754\n",
      "two-layer Loss after iteration 930000: 0.11698021042874103\n",
      "two-layer Loss after iteration 931000: 0.1169633560409522\n",
      "two-layer Loss after iteration 932000: 0.11694521015255999\n",
      "two-layer Loss after iteration 933000: 0.11692784682379656\n",
      "two-layer Loss after iteration 934000: 0.11691189282082834\n",
      "two-layer Loss after iteration 935000: 0.11689380526027504\n",
      "two-layer Loss after iteration 936000: 0.11687716870147133\n",
      "two-layer Loss after iteration 937000: 0.11686016132000111\n",
      "two-layer Loss after iteration 938000: 0.11684457216623523\n",
      "two-layer Loss after iteration 939000: 0.11682475557489552\n",
      "two-layer Loss after iteration 940000: 0.116810136908298\n",
      "two-layer Loss after iteration 941000: 0.11679365095956049\n",
      "two-layer Loss after iteration 942000: 0.11677340321918955\n",
      "two-layer Loss after iteration 943000: 0.11675782189812714\n",
      "two-layer Loss after iteration 944000: 0.1167401353648483\n",
      "two-layer Loss after iteration 945000: 0.11672370835429167\n",
      "two-layer Loss after iteration 946000: 0.11670649197686388\n",
      "two-layer Loss after iteration 947000: 0.11669140194689429\n",
      "two-layer Loss after iteration 948000: 0.11667198908141072\n",
      "two-layer Loss after iteration 949000: 0.1166553369886537\n",
      "two-layer Loss after iteration 950000: 0.11664057235884825\n",
      "two-layer Loss after iteration 951000: 0.1166218300357605\n",
      "two-layer Loss after iteration 952000: 0.11660557871415755\n",
      "two-layer Loss after iteration 953000: 0.1165888850342358\n",
      "two-layer Loss after iteration 954000: 0.11657387607885059\n",
      "two-layer Loss after iteration 955000: 0.11655535146598045\n",
      "two-layer Loss after iteration 956000: 0.116539602725421\n",
      "two-layer Loss after iteration 957000: 0.11652280252561473\n",
      "two-layer Loss after iteration 958000: 0.11650628120918537\n",
      "two-layer Loss after iteration 959000: 0.11648966337485285\n",
      "two-layer Loss after iteration 960000: 0.11647361662831315\n",
      "two-layer Loss after iteration 961000: 0.1164546966153371\n",
      "two-layer Loss after iteration 962000: 0.11643840891228302\n",
      "two-layer Loss after iteration 963000: 0.11642185644391645\n",
      "two-layer Loss after iteration 964000: 0.11640609643004504\n",
      "two-layer Loss after iteration 965000: 0.11638924598658801\n",
      "two-layer Loss after iteration 966000: 0.11637256986730404\n",
      "two-layer Loss after iteration 967000: 0.11635837795704355\n",
      "two-layer Loss after iteration 968000: 0.11634047115375339\n",
      "two-layer Loss after iteration 969000: 0.11632402671512863\n",
      "two-layer Loss after iteration 970000: 0.11630698580251514\n",
      "two-layer Loss after iteration 971000: 0.11629151533609224\n",
      "two-layer Loss after iteration 972000: 0.11627460110684451\n",
      "two-layer Loss after iteration 973000: 0.11625959907478958\n",
      "two-layer Loss after iteration 974000: 0.1162430027036013\n",
      "two-layer Loss after iteration 975000: 0.11622693375640203\n",
      "two-layer Loss after iteration 976000: 0.11621004373518905\n",
      "two-layer Loss after iteration 977000: 0.11619447260045374\n",
      "two-layer Loss after iteration 978000: 0.11617909437776604\n",
      "two-layer Loss after iteration 979000: 0.11616445417061975\n",
      "two-layer Loss after iteration 980000: 0.11614546996701249\n",
      "two-layer Loss after iteration 981000: 0.11613221494785674\n",
      "two-layer Loss after iteration 982000: 0.11611597647784026\n",
      "two-layer Loss after iteration 983000: 0.11609865211484628\n",
      "two-layer Loss after iteration 984000: 0.11608420657279864\n",
      "two-layer Loss after iteration 985000: 0.11606848359320747\n",
      "two-layer Loss after iteration 986000: 0.116049882431884\n",
      "two-layer Loss after iteration 987000: 0.11603498799194423\n",
      "two-layer Loss after iteration 988000: 0.1160177542465035\n",
      "two-layer Loss after iteration 989000: 0.11600478311535499\n",
      "two-layer Loss after iteration 990000: 0.11598754469482436\n",
      "two-layer Loss after iteration 991000: 0.11597333437726712\n",
      "two-layer Loss after iteration 992000: 0.11595538756939251\n",
      "two-layer Loss after iteration 993000: 0.11593967925641809\n",
      "two-layer Loss after iteration 994000: 0.11592428874573092\n",
      "two-layer Loss after iteration 995000: 0.11590797090463396\n",
      "two-layer Loss after iteration 996000: 0.11589222812037335\n",
      "two-layer Loss after iteration 997000: 0.11587814533828936\n",
      "two-layer Loss after iteration 998000: 0.11586120758218353\n",
      "two-layer Loss after iteration 999000: 0.1158460148396272\n",
      "two-layer Loss after iteration 0: 1669.1585058340395\n",
      "two-layer Loss after iteration 1000: 23.588167727709386\n",
      "two-layer Loss after iteration 2000: 8.939227018398935\n",
      "two-layer Loss after iteration 3000: 6.099038687492887\n",
      "two-layer Loss after iteration 4000: 4.608697290201669\n",
      "two-layer Loss after iteration 5000: 3.7859554333220244\n",
      "two-layer Loss after iteration 6000: 3.26857347588804\n",
      "two-layer Loss after iteration 7000: 2.745560704351675\n",
      "two-layer Loss after iteration 8000: 1.7440140416308152\n",
      "two-layer Loss after iteration 9000: 1.3946918618913804\n",
      "two-layer Loss after iteration 10000: 1.0806706512832225\n",
      "two-layer Loss after iteration 11000: 0.8751201869174972\n",
      "two-layer Loss after iteration 12000: 0.754781164140003\n",
      "two-layer Loss after iteration 13000: 0.6719831546905541\n",
      "two-layer Loss after iteration 14000: 0.6286910763843443\n",
      "two-layer Loss after iteration 15000: 0.5971397073253373\n",
      "two-layer Loss after iteration 16000: 0.5736266397560962\n",
      "two-layer Loss after iteration 17000: 0.5564281312964521\n",
      "two-layer Loss after iteration 18000: 0.542408825327466\n",
      "two-layer Loss after iteration 19000: 0.531633427267199\n",
      "two-layer Loss after iteration 20000: 0.5229909107695948\n",
      "two-layer Loss after iteration 21000: 0.5153777658256182\n",
      "two-layer Loss after iteration 22000: 0.5066348927717128\n",
      "two-layer Loss after iteration 23000: 0.4993035723180505\n",
      "two-layer Loss after iteration 24000: 0.494191478124361\n",
      "two-layer Loss after iteration 25000: 0.48981912877838363\n",
      "two-layer Loss after iteration 26000: 0.4860735401283652\n",
      "two-layer Loss after iteration 27000: 0.4827846823747116\n",
      "two-layer Loss after iteration 28000: 0.47963452243266147\n",
      "two-layer Loss after iteration 29000: 0.4768927465114102\n",
      "two-layer Loss after iteration 30000: 0.47341460299726584\n",
      "two-layer Loss after iteration 31000: 0.4709153956471151\n",
      "two-layer Loss after iteration 32000: 0.46875074280267687\n",
      "two-layer Loss after iteration 33000: 0.46684688248607265\n",
      "two-layer Loss after iteration 34000: 0.4651395720943804\n",
      "two-layer Loss after iteration 35000: 0.46342692638768035\n",
      "two-layer Loss after iteration 36000: 0.46193921553695505\n",
      "two-layer Loss after iteration 37000: 0.4606036166845138\n",
      "two-layer Loss after iteration 38000: 0.4593920477591625\n",
      "two-layer Loss after iteration 39000: 0.45827096768711206\n",
      "two-layer Loss after iteration 40000: 0.4572288082225806\n",
      "two-layer Loss after iteration 41000: 0.45629482739276656\n",
      "two-layer Loss after iteration 42000: 0.45543718657016447\n",
      "two-layer Loss after iteration 43000: 0.4546472154841383\n",
      "two-layer Loss after iteration 44000: 0.4535508595655649\n",
      "two-layer Loss after iteration 45000: 0.4523336310736821\n",
      "two-layer Loss after iteration 46000: 0.45152766192668004\n",
      "two-layer Loss after iteration 47000: 0.45080387757728263\n",
      "two-layer Loss after iteration 48000: 0.4501343371570668\n",
      "two-layer Loss after iteration 49000: 0.44829773789918564\n",
      "two-layer Loss after iteration 50000: 0.4468494656963287\n",
      "two-layer Loss after iteration 51000: 0.4460271998656465\n",
      "two-layer Loss after iteration 52000: 0.44538267249591107\n",
      "two-layer Loss after iteration 53000: 0.4448129230424617\n",
      "two-layer Loss after iteration 54000: 0.44430190811271536\n",
      "two-layer Loss after iteration 55000: 0.4438394034164169\n",
      "two-layer Loss after iteration 56000: 0.4434175367505744\n",
      "two-layer Loss after iteration 57000: 0.4430288305862013\n",
      "two-layer Loss after iteration 58000: 0.44266867433033275\n",
      "two-layer Loss after iteration 59000: 0.4423316306768993\n",
      "two-layer Loss after iteration 60000: 0.44201015745272154\n",
      "two-layer Loss after iteration 61000: 0.44121116650760656\n",
      "two-layer Loss after iteration 62000: 0.4407221508382956\n",
      "two-layer Loss after iteration 63000: 0.44034183333868415\n",
      "two-layer Loss after iteration 64000: 0.440009761657032\n",
      "two-layer Loss after iteration 65000: 0.43971609981833076\n",
      "two-layer Loss after iteration 66000: 0.43945297440114567\n",
      "two-layer Loss after iteration 67000: 0.4392147591128674\n",
      "two-layer Loss after iteration 68000: 0.4389975623594301\n",
      "two-layer Loss after iteration 69000: 0.4388015952986379\n",
      "two-layer Loss after iteration 70000: 0.4386197818610429\n",
      "two-layer Loss after iteration 71000: 0.43845523840685346\n",
      "two-layer Loss after iteration 72000: 0.43830067274755\n",
      "two-layer Loss after iteration 73000: 0.4381579915882062\n",
      "two-layer Loss after iteration 74000: 0.4380241476747529\n",
      "two-layer Loss after iteration 75000: 0.43789630837440163\n",
      "two-layer Loss after iteration 76000: 0.43777510138413156\n",
      "two-layer Loss after iteration 77000: 0.43765711318904865\n",
      "two-layer Loss after iteration 78000: 0.43754725655538185\n",
      "two-layer Loss after iteration 79000: 0.437443457902422\n",
      "two-layer Loss after iteration 80000: 0.4373425174564668\n",
      "two-layer Loss after iteration 81000: 0.4372333695053095\n",
      "two-layer Loss after iteration 82000: 0.43712971091255814\n",
      "two-layer Loss after iteration 83000: 0.4370326529887028\n",
      "two-layer Loss after iteration 84000: 0.43693004589633433\n",
      "two-layer Loss after iteration 85000: 0.43683503894374937\n",
      "two-layer Loss after iteration 86000: 0.4367442761768085\n",
      "two-layer Loss after iteration 87000: 0.436660113047086\n",
      "two-layer Loss after iteration 88000: 0.43659373911422006\n",
      "two-layer Loss after iteration 89000: 0.4365326417475476\n",
      "two-layer Loss after iteration 90000: 0.4364744033369007\n",
      "two-layer Loss after iteration 91000: 0.4364185282842658\n",
      "two-layer Loss after iteration 92000: 0.4363642499700297\n",
      "two-layer Loss after iteration 93000: 0.43631237399920414\n",
      "two-layer Loss after iteration 94000: 0.4362627028301841\n",
      "two-layer Loss after iteration 95000: 0.4362158667675998\n",
      "two-layer Loss after iteration 96000: 0.43616877028743034\n",
      "two-layer Loss after iteration 97000: 0.43612461878443126\n",
      "two-layer Loss after iteration 98000: 0.43608212295947546\n",
      "9.743963795083662e-05 0.43612461878443126 0.43608212295947546\n",
      "two-layer Loss after iteration 0: 1509.7559313684071\n",
      "two-layer Loss after iteration 1000: 24.985418787044157\n",
      "two-layer Loss after iteration 2000: 9.16375738119016\n",
      "two-layer Loss after iteration 3000: 6.772745819439892\n",
      "two-layer Loss after iteration 4000: 4.886607142831416\n",
      "two-layer Loss after iteration 5000: 3.4034567672894163\n",
      "two-layer Loss after iteration 6000: 2.433445676322002\n",
      "two-layer Loss after iteration 7000: 1.6094541920019036\n",
      "two-layer Loss after iteration 8000: 1.2731361140503694\n",
      "two-layer Loss after iteration 9000: 1.0947918186773904\n",
      "two-layer Loss after iteration 10000: 0.99218972929636\n",
      "two-layer Loss after iteration 11000: 0.9181239277902696\n",
      "two-layer Loss after iteration 12000: 0.8139322088286031\n",
      "two-layer Loss after iteration 13000: 0.7653132920421206\n",
      "two-layer Loss after iteration 14000: 0.7306709583192337\n",
      "two-layer Loss after iteration 15000: 0.7007302426682751\n",
      "two-layer Loss after iteration 16000: 0.6683351305193481\n",
      "two-layer Loss after iteration 17000: 0.6339842983577141\n",
      "two-layer Loss after iteration 18000: 0.6115031346593974\n",
      "two-layer Loss after iteration 19000: 0.5983471036046487\n",
      "two-layer Loss after iteration 20000: 0.5870024123886453\n",
      "two-layer Loss after iteration 21000: 0.5759862188629424\n",
      "two-layer Loss after iteration 22000: 0.5690891333311677\n",
      "two-layer Loss after iteration 23000: 0.562080676084142\n",
      "two-layer Loss after iteration 24000: 0.5561438499777687\n",
      "two-layer Loss after iteration 25000: 0.5511673819583992\n",
      "two-layer Loss after iteration 26000: 0.5467076227481906\n",
      "two-layer Loss after iteration 27000: 0.542521889984438\n",
      "two-layer Loss after iteration 28000: 0.5386096548765289\n",
      "two-layer Loss after iteration 29000: 0.5350339555641526\n",
      "two-layer Loss after iteration 30000: 0.5316748633261422\n",
      "two-layer Loss after iteration 31000: 0.5274824669051714\n",
      "two-layer Loss after iteration 32000: 0.5239719661453371\n",
      "two-layer Loss after iteration 33000: 0.5206593134427493\n",
      "two-layer Loss after iteration 34000: 0.517511092291227\n",
      "two-layer Loss after iteration 35000: 0.5144510982302418\n",
      "two-layer Loss after iteration 36000: 0.5110765369064284\n",
      "two-layer Loss after iteration 37000: 0.5081054666629611\n",
      "two-layer Loss after iteration 38000: 0.5052316661480869\n",
      "two-layer Loss after iteration 39000: 0.502148897788824\n",
      "two-layer Loss after iteration 40000: 0.49853968776423946\n",
      "two-layer Loss after iteration 41000: 0.4955248402823913\n",
      "two-layer Loss after iteration 42000: 0.49257606573745405\n",
      "two-layer Loss after iteration 43000: 0.4892955715099172\n",
      "two-layer Loss after iteration 44000: 0.484584135515151\n",
      "two-layer Loss after iteration 45000: 0.48091057091680117\n",
      "two-layer Loss after iteration 46000: 0.4778016789505077\n",
      "two-layer Loss after iteration 47000: 0.4748058953247101\n",
      "two-layer Loss after iteration 48000: 0.4714516869880221\n",
      "two-layer Loss after iteration 49000: 0.46716488478156004\n",
      "two-layer Loss after iteration 50000: 0.4626471463549512\n",
      "two-layer Loss after iteration 51000: 0.45964167072356216\n",
      "two-layer Loss after iteration 52000: 0.4568193880288599\n",
      "two-layer Loss after iteration 53000: 0.4539841401307462\n",
      "two-layer Loss after iteration 54000: 0.4512783724640149\n",
      "two-layer Loss after iteration 55000: 0.44845654401799045\n",
      "two-layer Loss after iteration 56000: 0.44562112675456633\n",
      "two-layer Loss after iteration 57000: 0.4425059076779608\n",
      "two-layer Loss after iteration 58000: 0.4371834418556828\n",
      "two-layer Loss after iteration 59000: 0.43172740306152835\n",
      "two-layer Loss after iteration 60000: 0.4272677120261848\n",
      "two-layer Loss after iteration 61000: 0.423293906714935\n",
      "two-layer Loss after iteration 62000: 0.41972075902228373\n",
      "two-layer Loss after iteration 63000: 0.4164556414520315\n",
      "two-layer Loss after iteration 64000: 0.4133661409215421\n",
      "two-layer Loss after iteration 65000: 0.41049248937761856\n",
      "two-layer Loss after iteration 66000: 0.4078021655099735\n",
      "two-layer Loss after iteration 67000: 0.4052274372886168\n",
      "two-layer Loss after iteration 68000: 0.4027713990770531\n",
      "two-layer Loss after iteration 69000: 0.40043943524729636\n",
      "two-layer Loss after iteration 70000: 0.39797788708839943\n",
      "two-layer Loss after iteration 71000: 0.3958540542134866\n",
      "two-layer Loss after iteration 72000: 0.39384054514760103\n",
      "two-layer Loss after iteration 73000: 0.3919230953841091\n",
      "two-layer Loss after iteration 74000: 0.39009379331066335\n",
      "two-layer Loss after iteration 75000: 0.3883387212418263\n",
      "two-layer Loss after iteration 76000: 0.38665087743040477\n",
      "two-layer Loss after iteration 77000: 0.3850318485287305\n",
      "two-layer Loss after iteration 78000: 0.38348221746965006\n",
      "two-layer Loss after iteration 79000: 0.3818348794051226\n",
      "two-layer Loss after iteration 80000: 0.3803318045634556\n",
      "two-layer Loss after iteration 81000: 0.37888550283925204\n",
      "two-layer Loss after iteration 82000: 0.37749277001513887\n",
      "two-layer Loss after iteration 83000: 0.3761496702337858\n",
      "two-layer Loss after iteration 84000: 0.3748552483231161\n",
      "two-layer Loss after iteration 85000: 0.3736054056537078\n",
      "two-layer Loss after iteration 86000: 0.3724011207034507\n",
      "two-layer Loss after iteration 87000: 0.37123670818286075\n",
      "two-layer Loss after iteration 88000: 0.3701140984630235\n",
      "two-layer Loss after iteration 89000: 0.36903036968839525\n",
      "two-layer Loss after iteration 90000: 0.3676659148148464\n",
      "two-layer Loss after iteration 91000: 0.3666257098443645\n",
      "two-layer Loss after iteration 92000: 0.36562821675687696\n",
      "two-layer Loss after iteration 93000: 0.36466319210969184\n",
      "two-layer Loss after iteration 94000: 0.3637336107083339\n",
      "two-layer Loss after iteration 95000: 0.36283748252151465\n",
      "two-layer Loss after iteration 96000: 0.3619745152246511\n",
      "two-layer Loss after iteration 97000: 0.36115149607702884\n",
      "two-layer Loss after iteration 98000: 0.360357482925248\n",
      "two-layer Loss after iteration 99000: 0.35959214992447674\n",
      "two-layer Loss after iteration 100000: 0.358853671437704\n",
      "two-layer Loss after iteration 101000: 0.3581423796491409\n",
      "two-layer Loss after iteration 102000: 0.3574530807280688\n",
      "two-layer Loss after iteration 103000: 0.3567909541564742\n",
      "two-layer Loss after iteration 104000: 0.35615104540074666\n",
      "two-layer Loss after iteration 105000: 0.35553418924495883\n",
      "two-layer Loss after iteration 106000: 0.3549398736797639\n",
      "two-layer Loss after iteration 107000: 0.354366742900682\n",
      "two-layer Loss after iteration 108000: 0.35381404912018105\n",
      "two-layer Loss after iteration 109000: 0.35328226403008456\n",
      "two-layer Loss after iteration 110000: 0.35276826669141853\n",
      "two-layer Loss after iteration 111000: 0.3522739538675156\n",
      "two-layer Loss after iteration 112000: 0.3517969335490795\n",
      "two-layer Loss after iteration 113000: 0.35133744479098644\n",
      "two-layer Loss after iteration 114000: 0.3508952844010816\n",
      "two-layer Loss after iteration 115000: 0.35046896072377975\n",
      "two-layer Loss after iteration 116000: 0.35005785733849204\n",
      "two-layer Loss after iteration 117000: 0.3496626250177794\n",
      "two-layer Loss after iteration 118000: 0.3492818646516054\n",
      "two-layer Loss after iteration 119000: 0.3489148695095888\n",
      "two-layer Loss after iteration 120000: 0.34856195463868556\n",
      "two-layer Loss after iteration 121000: 0.3482221656145541\n",
      "two-layer Loss after iteration 122000: 0.34789432357182026\n",
      "two-layer Loss after iteration 123000: 0.34757988159413755\n",
      "two-layer Loss after iteration 124000: 0.34727644135195285\n",
      "two-layer Loss after iteration 125000: 0.3469817277246973\n",
      "two-layer Loss after iteration 126000: 0.34669847918534513\n",
      "two-layer Loss after iteration 127000: 0.34642611919110927\n",
      "two-layer Loss after iteration 128000: 0.3461631854872986\n",
      "two-layer Loss after iteration 129000: 0.3459112486037375\n",
      "two-layer Loss after iteration 130000: 0.34566882753118505\n",
      "two-layer Loss after iteration 131000: 0.3454360135700416\n",
      "two-layer Loss after iteration 132000: 0.34521194984193304\n",
      "two-layer Loss after iteration 133000: 0.34499594707249076\n",
      "two-layer Loss after iteration 134000: 0.3447895789996225\n",
      "two-layer Loss after iteration 135000: 0.34459033146890206\n",
      "two-layer Loss after iteration 136000: 0.34439913763661884\n",
      "two-layer Loss after iteration 137000: 0.3442151674365022\n",
      "two-layer Loss after iteration 138000: 0.3440384857343104\n",
      "two-layer Loss after iteration 139000: 0.3438686653097806\n",
      "two-layer Loss after iteration 140000: 0.34370511871556636\n",
      "two-layer Loss after iteration 141000: 0.34354814242946635\n",
      "two-layer Loss after iteration 142000: 0.3433983804047005\n",
      "two-layer Loss after iteration 143000: 0.3432537525825274\n",
      "two-layer Loss after iteration 144000: 0.34311324640618446\n",
      "two-layer Loss after iteration 145000: 0.3429795400719201\n",
      "two-layer Loss after iteration 146000: 0.34285081398493844\n",
      "two-layer Loss after iteration 147000: 0.34272691189285254\n",
      "two-layer Loss after iteration 148000: 0.3426081744395699\n",
      "two-layer Loss after iteration 149000: 0.3424944498812067\n",
      "two-layer Loss after iteration 150000: 0.3423848219465431\n",
      "two-layer Loss after iteration 151000: 0.34227923096096496\n",
      "two-layer Loss after iteration 152000: 0.3421778950372556\n",
      "two-layer Loss after iteration 153000: 0.3420803358406292\n",
      "two-layer Loss after iteration 154000: 0.341987498133571\n",
      "two-layer Loss after iteration 155000: 0.3418972766400796\n",
      "two-layer Loss after iteration 156000: 0.34181111279023835\n",
      "two-layer Loss after iteration 157000: 0.3417281502271591\n",
      "two-layer Loss after iteration 158000: 0.3416484821962793\n",
      "two-layer Loss after iteration 159000: 0.34157170528770897\n",
      "two-layer Loss after iteration 160000: 0.34149820546030635\n",
      "two-layer Loss after iteration 161000: 0.3414276846859611\n",
      "two-layer Loss after iteration 162000: 0.3413590345146714\n",
      "two-layer Loss after iteration 163000: 0.34129453367991763\n",
      "two-layer Loss after iteration 164000: 0.34123049405373546\n",
      "two-layer Loss after iteration 165000: 0.34117011331631236\n",
      "two-layer Loss after iteration 166000: 0.34111238423512275\n",
      "two-layer Loss after iteration 167000: 0.3410562223024322\n",
      "two-layer Loss after iteration 168000: 0.3410027814526459\n",
      "two-layer Loss after iteration 169000: 0.34095073182121055\n",
      "two-layer Loss after iteration 170000: 0.340900963912967\n",
      "two-layer Loss after iteration 171000: 0.34085292581684457\n",
      "two-layer Loss after iteration 172000: 0.340805698518828\n",
      "two-layer Loss after iteration 173000: 0.3407563100531129\n",
      "two-layer Loss after iteration 174000: 0.3407082166282283\n",
      "two-layer Loss after iteration 175000: 0.3406630248927779\n",
      "two-layer Loss after iteration 176000: 0.3406190746494114\n",
      "two-layer Loss after iteration 177000: 0.34057660266743267\n",
      "two-layer Loss after iteration 178000: 0.34053570171009484\n",
      "two-layer Loss after iteration 179000: 0.3404966115389166\n",
      "two-layer Loss after iteration 180000: 0.340459690237384\n",
      "two-layer Loss after iteration 181000: 0.3404219635311567\n",
      "two-layer Loss after iteration 182000: 0.3403881651818145\n",
      "9.928369189715271e-05 0.3404219635311567 0.3403881651818145\n",
      "two-layer Loss after iteration 0: 1463.9225225664345\n",
      "two-layer Loss after iteration 1000: 23.33666136099222\n",
      "two-layer Loss after iteration 2000: 9.091597911300287\n",
      "two-layer Loss after iteration 3000: 6.489930206201607\n",
      "two-layer Loss after iteration 4000: 5.027545779281831\n",
      "two-layer Loss after iteration 5000: 4.18113511105714\n",
      "two-layer Loss after iteration 6000: 3.194576389566811\n",
      "two-layer Loss after iteration 7000: 2.8063415257088966\n",
      "two-layer Loss after iteration 8000: 2.5682567045558593\n",
      "two-layer Loss after iteration 9000: 2.3639328023621027\n",
      "two-layer Loss after iteration 10000: 2.1377955858294704\n",
      "two-layer Loss after iteration 11000: 1.931836404754614\n",
      "two-layer Loss after iteration 12000: 1.7678077800461234\n",
      "two-layer Loss after iteration 13000: 1.623970183928724\n",
      "two-layer Loss after iteration 14000: 1.3072624349596282\n",
      "two-layer Loss after iteration 15000: 1.126433817323084\n",
      "two-layer Loss after iteration 16000: 1.042661522919625\n",
      "two-layer Loss after iteration 17000: 0.9796021321832964\n",
      "two-layer Loss after iteration 18000: 0.926713388578658\n",
      "two-layer Loss after iteration 19000: 0.8812095882428274\n",
      "two-layer Loss after iteration 20000: 0.8414523154649838\n",
      "two-layer Loss after iteration 21000: 0.8058331884425819\n",
      "two-layer Loss after iteration 22000: 0.7738308381244755\n",
      "two-layer Loss after iteration 23000: 0.7450597831897398\n",
      "two-layer Loss after iteration 24000: 0.7188932750003753\n",
      "two-layer Loss after iteration 25000: 0.6954884839924578\n",
      "two-layer Loss after iteration 26000: 0.6747162763382974\n",
      "two-layer Loss after iteration 27000: 0.6563917875445193\n",
      "two-layer Loss after iteration 28000: 0.6357758378844228\n",
      "two-layer Loss after iteration 29000: 0.6181680745181732\n",
      "two-layer Loss after iteration 30000: 0.6029625573935951\n",
      "two-layer Loss after iteration 31000: 0.5897282543429208\n",
      "two-layer Loss after iteration 32000: 0.5768500343574163\n",
      "two-layer Loss after iteration 33000: 0.5645761547111661\n",
      "two-layer Loss after iteration 34000: 0.5513674123280311\n",
      "two-layer Loss after iteration 35000: 0.5416517338209281\n",
      "two-layer Loss after iteration 36000: 0.5336522064650749\n",
      "two-layer Loss after iteration 37000: 0.5268324712018787\n",
      "two-layer Loss after iteration 38000: 0.5187012520984315\n",
      "two-layer Loss after iteration 39000: 0.5125830457039577\n",
      "two-layer Loss after iteration 40000: 0.507490187462197\n",
      "two-layer Loss after iteration 41000: 0.4998389770253435\n",
      "two-layer Loss after iteration 42000: 0.49056850411745245\n",
      "two-layer Loss after iteration 43000: 0.4842093444816347\n",
      "two-layer Loss after iteration 44000: 0.4787418885061424\n",
      "two-layer Loss after iteration 45000: 0.47295378740458227\n",
      "two-layer Loss after iteration 46000: 0.4688260904310745\n",
      "two-layer Loss after iteration 47000: 0.4653466622641164\n",
      "two-layer Loss after iteration 48000: 0.46241066250292245\n",
      "two-layer Loss after iteration 49000: 0.46005916092596244\n",
      "two-layer Loss after iteration 50000: 0.458185722160702\n",
      "two-layer Loss after iteration 51000: 0.4566680406378376\n",
      "two-layer Loss after iteration 52000: 0.4553533505663006\n",
      "two-layer Loss after iteration 53000: 0.45428650449730906\n",
      "two-layer Loss after iteration 54000: 0.4528120418168692\n",
      "two-layer Loss after iteration 55000: 0.4516955928437015\n",
      "two-layer Loss after iteration 56000: 0.4501454872963737\n",
      "two-layer Loss after iteration 57000: 0.44907655827134135\n",
      "two-layer Loss after iteration 58000: 0.44809091799022177\n",
      "two-layer Loss after iteration 59000: 0.4472845253100647\n",
      "two-layer Loss after iteration 60000: 0.4466218736092949\n",
      "two-layer Loss after iteration 61000: 0.44607111734802185\n",
      "two-layer Loss after iteration 62000: 0.44558835078651277\n",
      "two-layer Loss after iteration 63000: 0.4441155094412017\n",
      "two-layer Loss after iteration 64000: 0.44335885793262025\n",
      "two-layer Loss after iteration 65000: 0.44281208369917535\n",
      "two-layer Loss after iteration 66000: 0.4423838180367136\n",
      "two-layer Loss after iteration 67000: 0.44203331092421233\n",
      "two-layer Loss after iteration 68000: 0.44159638266788664\n",
      "two-layer Loss after iteration 69000: 0.44121229241809007\n",
      "two-layer Loss after iteration 70000: 0.44088972655050257\n",
      "two-layer Loss after iteration 71000: 0.44042166446636444\n",
      "two-layer Loss after iteration 72000: 0.4400170296094464\n",
      "two-layer Loss after iteration 73000: 0.4393055673300543\n",
      "two-layer Loss after iteration 74000: 0.43871661730406375\n",
      "two-layer Loss after iteration 75000: 0.4382459656254349\n",
      "two-layer Loss after iteration 76000: 0.43739816411400007\n",
      "two-layer Loss after iteration 77000: 0.4357744945148868\n",
      "two-layer Loss after iteration 78000: 0.43016127126394504\n",
      "two-layer Loss after iteration 79000: 0.42805040143682227\n",
      "two-layer Loss after iteration 80000: 0.4263101557867032\n",
      "two-layer Loss after iteration 81000: 0.42476324278096766\n",
      "two-layer Loss after iteration 82000: 0.4233689588250785\n",
      "two-layer Loss after iteration 83000: 0.4218032855813322\n",
      "two-layer Loss after iteration 84000: 0.4203335012370608\n",
      "two-layer Loss after iteration 85000: 0.4191051280515178\n",
      "two-layer Loss after iteration 86000: 0.4180399154909732\n",
      "two-layer Loss after iteration 87000: 0.4170992926556522\n",
      "two-layer Loss after iteration 88000: 0.4162484455038359\n",
      "two-layer Loss after iteration 89000: 0.41546901442751594\n",
      "two-layer Loss after iteration 90000: 0.41464254445297277\n",
      "two-layer Loss after iteration 91000: 0.4135886511184536\n",
      "two-layer Loss after iteration 92000: 0.412603471565206\n",
      "two-layer Loss after iteration 93000: 0.4119367524286117\n",
      "two-layer Loss after iteration 94000: 0.41134468074568453\n",
      "two-layer Loss after iteration 95000: 0.41083578897272044\n",
      "two-layer Loss after iteration 96000: 0.4104121312201203\n",
      "two-layer Loss after iteration 97000: 0.4100242072581797\n",
      "two-layer Loss after iteration 98000: 0.40968952453089663\n",
      "two-layer Loss after iteration 99000: 0.4093997079827865\n",
      "two-layer Loss after iteration 100000: 0.40914694212541136\n",
      "two-layer Loss after iteration 101000: 0.40892165144228226\n",
      "two-layer Loss after iteration 102000: 0.40872097161706394\n",
      "two-layer Loss after iteration 103000: 0.40825567692197823\n",
      "two-layer Loss after iteration 104000: 0.407876733004257\n",
      "two-layer Loss after iteration 105000: 0.40758051291228053\n",
      "two-layer Loss after iteration 106000: 0.4073300295752397\n",
      "two-layer Loss after iteration 107000: 0.40711484020233324\n",
      "two-layer Loss after iteration 108000: 0.4069280940815064\n",
      "two-layer Loss after iteration 109000: 0.40676351067648003\n",
      "two-layer Loss after iteration 110000: 0.4066155668065407\n",
      "two-layer Loss after iteration 111000: 0.4064842152127416\n",
      "two-layer Loss after iteration 112000: 0.4063674263429491\n",
      "two-layer Loss after iteration 113000: 0.40626199138437924\n",
      "two-layer Loss after iteration 114000: 0.4061675540922252\n",
      "two-layer Loss after iteration 115000: 0.4060811334483156\n",
      "two-layer Loss after iteration 116000: 0.4060028923850505\n",
      "two-layer Loss after iteration 117000: 0.40593318908582293\n",
      "two-layer Loss after iteration 118000: 0.40586964088142485\n",
      "two-layer Loss after iteration 119000: 0.4058125804934573\n",
      "two-layer Loss after iteration 120000: 0.4057617342566336\n",
      "two-layer Loss after iteration 121000: 0.40571423090363284\n",
      "two-layer Loss after iteration 122000: 0.4056698975435389\n",
      "two-layer Loss after iteration 123000: 0.40563138255112435\n",
      "9.494170666285457e-05 0.4056698975435389 0.40563138255112435\n",
      "two-layer Loss after iteration 0: 1468.438775440349\n",
      "two-layer Loss after iteration 1000: 23.255827251798078\n",
      "two-layer Loss after iteration 2000: 9.359585891728385\n",
      "two-layer Loss after iteration 3000: 6.805695717261157\n",
      "two-layer Loss after iteration 4000: 5.67433658868858\n",
      "two-layer Loss after iteration 5000: 4.783925168414261\n",
      "two-layer Loss after iteration 6000: 3.6642102990621503\n",
      "two-layer Loss after iteration 7000: 2.506847280195505\n",
      "two-layer Loss after iteration 8000: 2.056202017622451\n",
      "two-layer Loss after iteration 9000: 1.7763673711243624\n",
      "two-layer Loss after iteration 10000: 1.5306055324919898\n",
      "two-layer Loss after iteration 11000: 1.370058879653785\n",
      "two-layer Loss after iteration 12000: 1.2355605903588984\n",
      "two-layer Loss after iteration 13000: 1.138157801742185\n",
      "two-layer Loss after iteration 14000: 1.0727349225507015\n",
      "two-layer Loss after iteration 15000: 1.024142228901457\n",
      "two-layer Loss after iteration 16000: 0.9882122848344547\n",
      "two-layer Loss after iteration 17000: 0.9600867809290379\n",
      "two-layer Loss after iteration 18000: 0.9327782215506751\n",
      "two-layer Loss after iteration 19000: 0.9002754387206298\n",
      "two-layer Loss after iteration 20000: 0.8775352741017137\n",
      "two-layer Loss after iteration 21000: 0.8590697245076234\n",
      "two-layer Loss after iteration 22000: 0.8447775031642529\n",
      "two-layer Loss after iteration 23000: 0.833785520137443\n",
      "two-layer Loss after iteration 24000: 0.8224582788674343\n",
      "two-layer Loss after iteration 25000: 0.8121352093905515\n",
      "two-layer Loss after iteration 26000: 0.7515231402840385\n",
      "two-layer Loss after iteration 27000: 0.7106520639397412\n",
      "two-layer Loss after iteration 28000: 0.6950352993167912\n",
      "two-layer Loss after iteration 29000: 0.6754333480986207\n",
      "two-layer Loss after iteration 30000: 0.6622453753056734\n",
      "two-layer Loss after iteration 31000: 0.6522221849228462\n",
      "two-layer Loss after iteration 32000: 0.6430505276837829\n",
      "two-layer Loss after iteration 33000: 0.6351168164923279\n",
      "two-layer Loss after iteration 34000: 0.6277686069126085\n",
      "two-layer Loss after iteration 35000: 0.6211749490530465\n",
      "two-layer Loss after iteration 36000: 0.6159326807057088\n",
      "two-layer Loss after iteration 37000: 0.6093618680728489\n",
      "two-layer Loss after iteration 38000: 0.6049724702804082\n",
      "two-layer Loss after iteration 39000: 0.6015399190518139\n",
      "two-layer Loss after iteration 40000: 0.5986891288557226\n",
      "two-layer Loss after iteration 41000: 0.5954280003326651\n",
      "two-layer Loss after iteration 42000: 0.5887875898336667\n",
      "two-layer Loss after iteration 43000: 0.5856522542962915\n",
      "two-layer Loss after iteration 44000: 0.5827506966433642\n",
      "two-layer Loss after iteration 45000: 0.580289502394208\n",
      "two-layer Loss after iteration 46000: 0.5781803843262827\n",
      "two-layer Loss after iteration 47000: 0.5763003035893868\n",
      "two-layer Loss after iteration 48000: 0.5745924891903176\n",
      "two-layer Loss after iteration 49000: 0.5724959959629589\n",
      "two-layer Loss after iteration 50000: 0.570938626791238\n",
      "two-layer Loss after iteration 51000: 0.5695528561994829\n",
      "two-layer Loss after iteration 52000: 0.5682799947555273\n",
      "two-layer Loss after iteration 53000: 0.567094631070527\n",
      "two-layer Loss after iteration 54000: 0.565978240637778\n",
      "two-layer Loss after iteration 55000: 0.5649198815348495\n",
      "two-layer Loss after iteration 56000: 0.563911807881469\n",
      "two-layer Loss after iteration 57000: 0.5628475715985556\n",
      "two-layer Loss after iteration 58000: 0.5619256738271282\n",
      "two-layer Loss after iteration 59000: 0.5610472981055672\n",
      "two-layer Loss after iteration 60000: 0.5602115455177611\n",
      "two-layer Loss after iteration 61000: 0.5594117700137694\n",
      "two-layer Loss after iteration 62000: 0.5586469610959861\n",
      "two-layer Loss after iteration 63000: 0.5579141453559675\n",
      "two-layer Loss after iteration 64000: 0.557212972365756\n",
      "two-layer Loss after iteration 65000: 0.5565421644776224\n",
      "two-layer Loss after iteration 66000: 0.5558986146329215\n",
      "two-layer Loss after iteration 67000: 0.5552822653371102\n",
      "two-layer Loss after iteration 68000: 0.5546922970356516\n",
      "two-layer Loss after iteration 69000: 0.5541266722680676\n",
      "two-layer Loss after iteration 70000: 0.5535852532930466\n",
      "two-layer Loss after iteration 71000: 0.5530658673171229\n",
      "two-layer Loss after iteration 72000: 0.5525691752957232\n",
      "two-layer Loss after iteration 73000: 0.5520936760734647\n",
      "two-layer Loss after iteration 74000: 0.5516381563813215\n",
      "two-layer Loss after iteration 75000: 0.5512022710679897\n",
      "two-layer Loss after iteration 76000: 0.55078462396346\n",
      "two-layer Loss after iteration 77000: 0.5503856681423096\n",
      "two-layer Loss after iteration 78000: 0.5500030727333538\n",
      "two-layer Loss after iteration 79000: 0.5496374176501736\n",
      "two-layer Loss after iteration 80000: 0.5492871418602446\n",
      "two-layer Loss after iteration 81000: 0.5489520564984459\n",
      "two-layer Loss after iteration 82000: 0.5486317378688749\n",
      "two-layer Loss after iteration 83000: 0.5483238474529866\n",
      "two-layer Loss after iteration 84000: 0.548030330989055\n",
      "two-layer Loss after iteration 85000: 0.5477499600831501\n",
      "two-layer Loss after iteration 86000: 0.5474814746032228\n",
      "two-layer Loss after iteration 87000: 0.5472252656951344\n",
      "two-layer Loss after iteration 88000: 0.5469811745115528\n",
      "two-layer Loss after iteration 89000: 0.5467474015640347\n",
      "two-layer Loss after iteration 90000: 0.546524862966452\n",
      "two-layer Loss after iteration 91000: 0.5463113968477886\n",
      "two-layer Loss after iteration 92000: 0.5461076184273078\n",
      "two-layer Loss after iteration 93000: 0.5459120173550261\n",
      "two-layer Loss after iteration 94000: 0.5457268689082745\n",
      "two-layer Loss after iteration 95000: 0.545547488890762\n",
      "two-layer Loss after iteration 96000: 0.5453765164543813\n",
      "two-layer Loss after iteration 97000: 0.5452127273529407\n",
      "two-layer Loss after iteration 98000: 0.5450559643664298\n",
      "two-layer Loss after iteration 99000: 0.5449065657769301\n",
      "two-layer Loss after iteration 100000: 0.544761649472361\n",
      "two-layer Loss after iteration 101000: 0.5446244322722732\n",
      "two-layer Loss after iteration 102000: 0.5444929906623455\n",
      "two-layer Loss after iteration 103000: 0.5443660808354972\n",
      "two-layer Loss after iteration 104000: 0.5442457450231658\n",
      "two-layer Loss after iteration 105000: 0.5439996204479112\n",
      "two-layer Loss after iteration 106000: 0.5437169779891737\n",
      "two-layer Loss after iteration 107000: 0.543482474100746\n",
      "two-layer Loss after iteration 108000: 0.543279545212805\n",
      "two-layer Loss after iteration 109000: 0.5430987983730446\n",
      "two-layer Loss after iteration 110000: 0.5429376357061664\n",
      "two-layer Loss after iteration 111000: 0.5427894004396776\n",
      "two-layer Loss after iteration 112000: 0.5426539749085075\n",
      "two-layer Loss after iteration 113000: 0.5425286207893962\n",
      "two-layer Loss after iteration 114000: 0.5424114258142392\n",
      "two-layer Loss after iteration 115000: 0.5423027920648305\n",
      "two-layer Loss after iteration 116000: 0.5421988735510047\n",
      "two-layer Loss after iteration 117000: 0.5421016160662787\n",
      "two-layer Loss after iteration 118000: 0.5420100808404772\n",
      "two-layer Loss after iteration 119000: 0.5419244175826269\n",
      "two-layer Loss after iteration 120000: 0.5418411840665399\n",
      "two-layer Loss after iteration 121000: 0.5417631664319066\n",
      "two-layer Loss after iteration 122000: 0.5416885464842217\n",
      "two-layer Loss after iteration 123000: 0.5416182202234909\n",
      "two-layer Loss after iteration 124000: 0.5415519847486101\n",
      "two-layer Loss after iteration 125000: 0.5414894814166041\n",
      "two-layer Loss after iteration 126000: 0.5414283290404489\n",
      "two-layer Loss after iteration 127000: 0.5413717550572725\n",
      "two-layer Loss after iteration 128000: 0.5413179326835923\n",
      "9.941851080601835e-05 0.5413717550572725 0.5413179326835923\n",
      "two-layer Loss after iteration 0: 1593.5792014356396\n",
      "two-layer Loss after iteration 1000: 22.985406719573188\n",
      "two-layer Loss after iteration 2000: 9.043536760271225\n",
      "two-layer Loss after iteration 3000: 6.418681877779625\n",
      "two-layer Loss after iteration 4000: 4.813893834479568\n",
      "two-layer Loss after iteration 5000: 3.6224312092070776\n",
      "two-layer Loss after iteration 6000: 2.930128538917134\n",
      "two-layer Loss after iteration 7000: 2.329169726526567\n",
      "two-layer Loss after iteration 8000: 1.5605606892206154\n",
      "two-layer Loss after iteration 9000: 1.1656201308708538\n",
      "two-layer Loss after iteration 10000: 0.9328788546713811\n",
      "two-layer Loss after iteration 11000: 0.7907482435478563\n",
      "two-layer Loss after iteration 12000: 0.7061183441854377\n",
      "two-layer Loss after iteration 13000: 0.6496930355422965\n",
      "two-layer Loss after iteration 14000: 0.6025457230663425\n",
      "two-layer Loss after iteration 15000: 0.5653974331797773\n",
      "two-layer Loss after iteration 16000: 0.532509252183799\n",
      "two-layer Loss after iteration 17000: 0.5065792302311675\n",
      "two-layer Loss after iteration 18000: 0.4831848098738428\n",
      "two-layer Loss after iteration 19000: 0.4663356161085556\n",
      "two-layer Loss after iteration 20000: 0.45240343102079983\n",
      "two-layer Loss after iteration 21000: 0.4413169312098948\n",
      "two-layer Loss after iteration 22000: 0.4318341891214378\n",
      "two-layer Loss after iteration 23000: 0.423155027083329\n",
      "two-layer Loss after iteration 24000: 0.4150764175473389\n",
      "two-layer Loss after iteration 25000: 0.4073761972603202\n",
      "two-layer Loss after iteration 26000: 0.40056255276727737\n",
      "two-layer Loss after iteration 27000: 0.3943938586014416\n",
      "two-layer Loss after iteration 28000: 0.3872558501236007\n",
      "two-layer Loss after iteration 29000: 0.38139093179644645\n",
      "two-layer Loss after iteration 30000: 0.3760382386212699\n",
      "two-layer Loss after iteration 31000: 0.3708406710232319\n",
      "two-layer Loss after iteration 32000: 0.36401973689367684\n",
      "two-layer Loss after iteration 33000: 0.35944396669229495\n",
      "two-layer Loss after iteration 34000: 0.3551531871726839\n",
      "two-layer Loss after iteration 35000: 0.3510118702782956\n",
      "two-layer Loss after iteration 36000: 0.3462866362279113\n",
      "two-layer Loss after iteration 37000: 0.3421826805230947\n",
      "two-layer Loss after iteration 38000: 0.337866769344847\n",
      "two-layer Loss after iteration 39000: 0.33335060898047886\n",
      "two-layer Loss after iteration 40000: 0.3292913963556756\n",
      "two-layer Loss after iteration 41000: 0.32547275163570943\n",
      "two-layer Loss after iteration 42000: 0.32184191202318946\n",
      "two-layer Loss after iteration 43000: 0.3162028607369401\n",
      "two-layer Loss after iteration 44000: 0.3124368372114053\n",
      "two-layer Loss after iteration 45000: 0.3088199053878275\n",
      "two-layer Loss after iteration 46000: 0.30533294287993573\n",
      "two-layer Loss after iteration 47000: 0.3019669946575016\n",
      "two-layer Loss after iteration 48000: 0.2987146195592249\n",
      "two-layer Loss after iteration 49000: 0.2955693011238987\n",
      "two-layer Loss after iteration 50000: 0.2925271100622378\n",
      "two-layer Loss after iteration 51000: 0.2895889817277335\n",
      "two-layer Loss after iteration 52000: 0.28675052426030945\n",
      "two-layer Loss after iteration 53000: 0.28400874846834323\n",
      "two-layer Loss after iteration 54000: 0.28135978927284044\n",
      "two-layer Loss after iteration 55000: 0.27879600995157305\n",
      "two-layer Loss after iteration 56000: 0.27631840791021306\n",
      "two-layer Loss after iteration 57000: 0.27392178501521935\n",
      "two-layer Loss after iteration 58000: 0.27160434213013956\n",
      "two-layer Loss after iteration 59000: 0.2693636291907611\n",
      "two-layer Loss after iteration 60000: 0.2669131248210995\n",
      "two-layer Loss after iteration 61000: 0.263991876261017\n",
      "two-layer Loss after iteration 62000: 0.26166251342943114\n",
      "two-layer Loss after iteration 63000: 0.25949912314926704\n",
      "two-layer Loss after iteration 64000: 0.2574543446250526\n",
      "two-layer Loss after iteration 65000: 0.2555026390193106\n",
      "two-layer Loss after iteration 66000: 0.2536287500743263\n",
      "two-layer Loss after iteration 67000: 0.2512157382058149\n",
      "two-layer Loss after iteration 68000: 0.2491762362075107\n",
      "two-layer Loss after iteration 69000: 0.24732399744137454\n",
      "two-layer Loss after iteration 70000: 0.2455866883289256\n",
      "two-layer Loss after iteration 71000: 0.24393640589290214\n",
      "two-layer Loss after iteration 72000: 0.2423554731880883\n",
      "two-layer Loss after iteration 73000: 0.24084172183651045\n",
      "two-layer Loss after iteration 74000: 0.2393875294677893\n",
      "two-layer Loss after iteration 75000: 0.23744134430617791\n",
      "two-layer Loss after iteration 76000: 0.23590794701418633\n",
      "two-layer Loss after iteration 77000: 0.2344955319543828\n",
      "two-layer Loss after iteration 78000: 0.23316359714167365\n",
      "two-layer Loss after iteration 79000: 0.23189748233177065\n",
      "two-layer Loss after iteration 80000: 0.23068473833142472\n",
      "two-layer Loss after iteration 81000: 0.22952641024008263\n",
      "two-layer Loss after iteration 82000: 0.22841402699789348\n",
      "two-layer Loss after iteration 83000: 0.22647597731357413\n",
      "two-layer Loss after iteration 84000: 0.22496106780080866\n",
      "two-layer Loss after iteration 85000: 0.22373662293043356\n",
      "two-layer Loss after iteration 86000: 0.2226412928511164\n",
      "two-layer Loss after iteration 87000: 0.22163348671684932\n",
      "two-layer Loss after iteration 88000: 0.22068045724843596\n",
      "two-layer Loss after iteration 89000: 0.21966347805701048\n",
      "two-layer Loss after iteration 90000: 0.21874963031077072\n",
      "two-layer Loss after iteration 91000: 0.2178947664158285\n",
      "two-layer Loss after iteration 92000: 0.2170745694440076\n",
      "two-layer Loss after iteration 93000: 0.2162931704964806\n",
      "two-layer Loss after iteration 94000: 0.21554323637625059\n",
      "two-layer Loss after iteration 95000: 0.21482519978940362\n",
      "two-layer Loss after iteration 96000: 0.2141340240998951\n",
      "two-layer Loss after iteration 97000: 0.21246569337072543\n",
      "two-layer Loss after iteration 98000: 0.21168897876920287\n",
      "two-layer Loss after iteration 99000: 0.2110150800328856\n",
      "two-layer Loss after iteration 100000: 0.21037556983135913\n",
      "two-layer Loss after iteration 101000: 0.20933825260090128\n",
      "two-layer Loss after iteration 102000: 0.20864538099288382\n",
      "two-layer Loss after iteration 103000: 0.20801675663454033\n",
      "two-layer Loss after iteration 104000: 0.20742620728150998\n",
      "two-layer Loss after iteration 105000: 0.20686842430858765\n",
      "two-layer Loss after iteration 106000: 0.206335335436044\n",
      "two-layer Loss after iteration 107000: 0.20582774424365158\n",
      "two-layer Loss after iteration 108000: 0.2053425930691572\n",
      "two-layer Loss after iteration 109000: 0.2048783971691389\n",
      "two-layer Loss after iteration 110000: 0.20443300081087765\n",
      "two-layer Loss after iteration 111000: 0.2040034143038155\n",
      "two-layer Loss after iteration 112000: 0.2035950704331394\n",
      "two-layer Loss after iteration 113000: 0.20320117133390717\n",
      "two-layer Loss after iteration 114000: 0.20282694005023094\n",
      "two-layer Loss after iteration 115000: 0.20246514701943089\n",
      "two-layer Loss after iteration 116000: 0.20211712139415738\n",
      "two-layer Loss after iteration 117000: 0.2017851462945693\n",
      "two-layer Loss after iteration 118000: 0.2014665050819402\n",
      "two-layer Loss after iteration 119000: 0.2011575816838453\n",
      "two-layer Loss after iteration 120000: 0.20086097642135672\n",
      "two-layer Loss after iteration 121000: 0.20057876851489306\n",
      "two-layer Loss after iteration 122000: 0.20030606967151016\n",
      "two-layer Loss after iteration 123000: 0.20004159264666377\n",
      "two-layer Loss after iteration 124000: 0.19979302493767306\n",
      "two-layer Loss after iteration 125000: 0.1995520815695124\n",
      "two-layer Loss after iteration 126000: 0.1993179087819107\n",
      "two-layer Loss after iteration 127000: 0.19909644207010063\n",
      "two-layer Loss after iteration 128000: 0.19888464034791084\n",
      "two-layer Loss after iteration 129000: 0.19867877791720026\n",
      "two-layer Loss after iteration 130000: 0.19848401107124808\n",
      "two-layer Loss after iteration 131000: 0.19829401621263995\n",
      "two-layer Loss after iteration 132000: 0.19811711856253145\n",
      "two-layer Loss after iteration 133000: 0.19794123713614345\n",
      "two-layer Loss after iteration 134000: 0.19777463568342796\n",
      "two-layer Loss after iteration 135000: 0.19761452440294183\n",
      "two-layer Loss after iteration 136000: 0.19745924070347412\n",
      "two-layer Loss after iteration 137000: 0.1973102616650434\n",
      "two-layer Loss after iteration 138000: 0.19717083240708688\n",
      "two-layer Loss after iteration 139000: 0.19703077314259632\n",
      "two-layer Loss after iteration 140000: 0.19690304113578555\n",
      "two-layer Loss after iteration 141000: 0.19677434235588911\n",
      "two-layer Loss after iteration 142000: 0.19665470211026437\n",
      "two-layer Loss after iteration 143000: 0.19651408070183662\n",
      "two-layer Loss after iteration 144000: 0.19639335454195286\n",
      "two-layer Loss after iteration 145000: 0.19627512895427243\n",
      "two-layer Loss after iteration 146000: 0.19615997964179446\n",
      "two-layer Loss after iteration 147000: 0.1960531348084813\n",
      "two-layer Loss after iteration 148000: 0.19594906485082617\n",
      "two-layer Loss after iteration 149000: 0.19584895074039074\n",
      "two-layer Loss after iteration 150000: 0.1957513988220591\n",
      "two-layer Loss after iteration 151000: 0.19565702696092127\n",
      "two-layer Loss after iteration 152000: 0.19556604458010926\n",
      "two-layer Loss after iteration 153000: 0.19548037958117065\n",
      "two-layer Loss after iteration 154000: 0.19539382236841563\n",
      "two-layer Loss after iteration 155000: 0.19531386516271962\n",
      "two-layer Loss after iteration 156000: 0.1952343196400641\n",
      "two-layer Loss after iteration 157000: 0.19516005244152873\n",
      "two-layer Loss after iteration 158000: 0.19508638985983515\n",
      "two-layer Loss after iteration 159000: 0.19501496027121729\n",
      "two-layer Loss after iteration 160000: 0.19494528613161885\n",
      "two-layer Loss after iteration 161000: 0.19488150629235407\n",
      "two-layer Loss after iteration 162000: 0.19481674682450606\n",
      "two-layer Loss after iteration 163000: 0.19475700885360506\n",
      "two-layer Loss after iteration 164000: 0.1946975859583827\n",
      "two-layer Loss after iteration 165000: 0.19463939966947688\n",
      "two-layer Loss after iteration 166000: 0.19458263964617692\n",
      "two-layer Loss after iteration 167000: 0.19453134038735237\n",
      "two-layer Loss after iteration 168000: 0.19447876675179604\n",
      "two-layer Loss after iteration 169000: 0.19442786070576681\n",
      "two-layer Loss after iteration 170000: 0.1943784058996648\n",
      "two-layer Loss after iteration 171000: 0.19433029824235176\n",
      "two-layer Loss after iteration 172000: 0.1942899837952493\n",
      "two-layer Loss after iteration 173000: 0.19424233015016532\n",
      "two-layer Loss after iteration 174000: 0.19419951475053338\n",
      "two-layer Loss after iteration 175000: 0.19416028745366634\n",
      "two-layer Loss after iteration 176000: 0.19412100429566848\n",
      "two-layer Loss after iteration 177000: 0.19407987479135516\n",
      "two-layer Loss after iteration 178000: 0.1940455887249676\n",
      "two-layer Loss after iteration 179000: 0.19400906174024038\n",
      "two-layer Loss after iteration 180000: 0.19397282666905785\n",
      "two-layer Loss after iteration 181000: 0.19393806052012127\n",
      "two-layer Loss after iteration 182000: 0.19390674889568216\n",
      "two-layer Loss after iteration 183000: 0.1938735428082685\n",
      "two-layer Loss after iteration 184000: 0.19384683198276112\n",
      "two-layer Loss after iteration 185000: 0.19381540088661145\n",
      "two-layer Loss after iteration 186000: 0.1937853194947043\n",
      "two-layer Loss after iteration 187000: 0.19375742886989328\n",
      "two-layer Loss after iteration 188000: 0.19373162768163235\n",
      "two-layer Loss after iteration 189000: 0.19370431342775027\n",
      "two-layer Loss after iteration 190000: 0.19367909966023583\n",
      "two-layer Loss after iteration 191000: 0.19365114533585825\n",
      "two-layer Loss after iteration 192000: 0.19363134275281077\n",
      "two-layer Loss after iteration 193000: 0.1936039146162337\n",
      "two-layer Loss after iteration 194000: 0.19358295239826942\n",
      "two-layer Loss after iteration 195000: 0.1935613717706074\n",
      "two-layer Loss after iteration 196000: 0.19353948158107287\n",
      "two-layer Loss after iteration 197000: 0.1935192650060441\n",
      "two-layer Loss after iteration 198000: 0.19349943068159567\n",
      "two-layer Loss after iteration 199000: 0.1934797999335302\n",
      "two-layer Loss after iteration 200000: 0.19346223261650922\n",
      "9.079664661129554e-05 0.1934797999335302 0.19346223261650922\n",
      "two-layer Loss after iteration 0: 1313.7510454043763\n",
      "two-layer Loss after iteration 1000: 25.93057392794905\n",
      "two-layer Loss after iteration 2000: 10.279886815531642\n",
      "two-layer Loss after iteration 3000: 7.366855977988253\n",
      "two-layer Loss after iteration 4000: 5.855736760145048\n",
      "two-layer Loss after iteration 5000: 5.092123306139369\n",
      "two-layer Loss after iteration 6000: 3.968429412711264\n",
      "two-layer Loss after iteration 7000: 3.2378121360409824\n",
      "two-layer Loss after iteration 8000: 2.899783926345383\n",
      "two-layer Loss after iteration 9000: 2.6603309412499603\n",
      "two-layer Loss after iteration 10000: 2.5060729671621216\n",
      "two-layer Loss after iteration 11000: 2.1079875546446702\n",
      "two-layer Loss after iteration 12000: 1.8265549220111472\n",
      "two-layer Loss after iteration 13000: 1.6911988585484696\n",
      "two-layer Loss after iteration 14000: 1.6015269203941809\n",
      "two-layer Loss after iteration 15000: 1.5356215029405473\n",
      "two-layer Loss after iteration 16000: 1.487287129949743\n",
      "two-layer Loss after iteration 17000: 1.4391550646916822\n",
      "two-layer Loss after iteration 18000: 1.3988186243067475\n",
      "two-layer Loss after iteration 19000: 1.360776982034953\n",
      "two-layer Loss after iteration 20000: 1.3215157969269729\n",
      "two-layer Loss after iteration 21000: 1.2975388590844996\n",
      "two-layer Loss after iteration 22000: 1.276451627377167\n",
      "two-layer Loss after iteration 23000: 1.2627030295580508\n",
      "two-layer Loss after iteration 24000: 1.2458785503273646\n",
      "two-layer Loss after iteration 25000: 1.220103792280931\n",
      "two-layer Loss after iteration 26000: 1.210565844827723\n",
      "two-layer Loss after iteration 27000: 1.2060681816815524\n",
      "two-layer Loss after iteration 28000: 1.2027078526705601\n",
      "two-layer Loss after iteration 29000: 1.1999816321390542\n",
      "two-layer Loss after iteration 30000: 1.197699487867525\n",
      "two-layer Loss after iteration 31000: 1.195732762045131\n",
      "two-layer Loss after iteration 32000: 1.1940083603093161\n",
      "two-layer Loss after iteration 33000: 1.192303381200781\n",
      "two-layer Loss after iteration 34000: 1.1907193440261905\n",
      "two-layer Loss after iteration 35000: 1.1892920196631682\n",
      "two-layer Loss after iteration 36000: 1.187985680827375\n",
      "two-layer Loss after iteration 37000: 1.1867112765879533\n",
      "two-layer Loss after iteration 38000: 1.1854748365700485\n",
      "two-layer Loss after iteration 39000: 1.1844340100044577\n",
      "two-layer Loss after iteration 40000: 1.181110423625092\n",
      "two-layer Loss after iteration 41000: 1.1789963623427744\n",
      "two-layer Loss after iteration 42000: 1.1779463178360534\n",
      "two-layer Loss after iteration 43000: 1.177046218917045\n",
      "two-layer Loss after iteration 44000: 1.176259872406726\n",
      "two-layer Loss after iteration 45000: 1.1755354781526615\n",
      "two-layer Loss after iteration 46000: 1.1748786773340711\n",
      "two-layer Loss after iteration 47000: 1.1742817172185338\n",
      "two-layer Loss after iteration 48000: 1.1737183762607701\n",
      "two-layer Loss after iteration 49000: 1.1731883438913862\n",
      "two-layer Loss after iteration 50000: 1.1727074664984047\n",
      "two-layer Loss after iteration 51000: 1.1722449004126732\n",
      "two-layer Loss after iteration 52000: 1.1718090339254716\n",
      "two-layer Loss after iteration 53000: 1.1713992844522676\n",
      "two-layer Loss after iteration 54000: 1.1710238167975457\n",
      "two-layer Loss after iteration 55000: 1.1706501411444434\n",
      "two-layer Loss after iteration 56000: 1.1703118471264118\n",
      "two-layer Loss after iteration 57000: 1.16997974683191\n",
      "two-layer Loss after iteration 58000: 1.1696626416268125\n",
      "two-layer Loss after iteration 59000: 1.1693693037987918\n",
      "two-layer Loss after iteration 60000: 1.1690854573837124\n",
      "two-layer Loss after iteration 61000: 1.168815059566387\n",
      "two-layer Loss after iteration 62000: 1.168559873170263\n",
      "two-layer Loss after iteration 63000: 1.1683228596868795\n",
      "two-layer Loss after iteration 64000: 1.1680822302532745\n",
      "two-layer Loss after iteration 65000: 1.167858630834704\n",
      "two-layer Loss after iteration 66000: 1.1676467263250228\n",
      "two-layer Loss after iteration 67000: 1.167440359923649\n",
      "two-layer Loss after iteration 68000: 1.1672437325988934\n",
      "two-layer Loss after iteration 69000: 1.1670539199748484\n",
      "two-layer Loss after iteration 70000: 1.1668738574291886\n",
      "two-layer Loss after iteration 71000: 1.1666980518884227\n",
      "two-layer Loss after iteration 72000: 1.1664422496735436\n",
      "two-layer Loss after iteration 73000: 1.1662105255833486\n",
      "two-layer Loss after iteration 74000: 1.1660043758236862\n",
      "two-layer Loss after iteration 75000: 1.1657989196076377\n",
      "two-layer Loss after iteration 76000: 1.1656217012740229\n",
      "two-layer Loss after iteration 77000: 1.1654386158041325\n",
      "two-layer Loss after iteration 78000: 1.1652762000102874\n",
      "two-layer Loss after iteration 79000: 1.1651171055003038\n",
      "two-layer Loss after iteration 80000: 1.1649571201232312\n",
      "two-layer Loss after iteration 81000: 1.1648065647205623\n",
      "two-layer Loss after iteration 82000: 1.1646716264243613\n",
      "two-layer Loss after iteration 83000: 1.1645347591106547\n",
      "two-layer Loss after iteration 84000: 1.1643979533732132\n",
      "two-layer Loss after iteration 85000: 1.1642586908309767\n",
      "two-layer Loss after iteration 86000: 1.164111885364123\n",
      "two-layer Loss after iteration 87000: 1.1639686589322265\n",
      "two-layer Loss after iteration 88000: 1.1638452887182518\n",
      "two-layer Loss after iteration 89000: 1.1637101970658694\n",
      "two-layer Loss after iteration 90000: 1.1635905411240242\n",
      "two-layer Loss after iteration 91000: 1.1634748104566022\n",
      "9.945995892179712e-05 1.1635905411240242 1.1634748104566022\n",
      "two-layer Loss after iteration 0: 1232.0195804937732\n",
      "two-layer Loss after iteration 1000: 24.402402900948772\n",
      "two-layer Loss after iteration 2000: 9.312979728279938\n",
      "two-layer Loss after iteration 3000: 6.278990706371587\n",
      "two-layer Loss after iteration 4000: 4.916108414217898\n",
      "two-layer Loss after iteration 5000: 3.332911590043944\n",
      "two-layer Loss after iteration 6000: 2.299412906210722\n",
      "two-layer Loss after iteration 7000: 1.7049336006330704\n",
      "two-layer Loss after iteration 8000: 1.3974740746099517\n",
      "two-layer Loss after iteration 9000: 1.2219555221353642\n",
      "two-layer Loss after iteration 10000: 1.1524227252405914\n",
      "two-layer Loss after iteration 11000: 1.1074522150399078\n",
      "two-layer Loss after iteration 12000: 1.0813598303098142\n",
      "two-layer Loss after iteration 13000: 1.0579909619801051\n",
      "two-layer Loss after iteration 14000: 1.0314698996613532\n",
      "two-layer Loss after iteration 15000: 1.0131315382885564\n",
      "two-layer Loss after iteration 16000: 1.0013680840970094\n",
      "two-layer Loss after iteration 17000: 0.9897801403219133\n",
      "two-layer Loss after iteration 18000: 0.9798037255884131\n",
      "two-layer Loss after iteration 19000: 0.9713555001489629\n",
      "two-layer Loss after iteration 20000: 0.9628608411885118\n",
      "two-layer Loss after iteration 21000: 0.9559465746976185\n",
      "two-layer Loss after iteration 22000: 0.9502624150812127\n",
      "two-layer Loss after iteration 23000: 0.9442971393055167\n",
      "two-layer Loss after iteration 24000: 0.9355517217599512\n",
      "two-layer Loss after iteration 25000: 0.9305214645956809\n",
      "two-layer Loss after iteration 26000: 0.9264371891613212\n",
      "two-layer Loss after iteration 27000: 0.9216610001419673\n",
      "two-layer Loss after iteration 28000: 0.9173841124426986\n",
      "two-layer Loss after iteration 29000: 0.9124848439913898\n",
      "two-layer Loss after iteration 30000: 0.9076022161806229\n",
      "two-layer Loss after iteration 31000: 0.9032595683174097\n",
      "two-layer Loss after iteration 32000: 0.8994318858750985\n",
      "two-layer Loss after iteration 33000: 0.895675641548494\n",
      "two-layer Loss after iteration 34000: 0.8910099159487802\n",
      "two-layer Loss after iteration 35000: 0.8863188234509779\n",
      "two-layer Loss after iteration 36000: 0.8836229766087541\n",
      "two-layer Loss after iteration 37000: 0.8802930512399892\n",
      "two-layer Loss after iteration 38000: 0.8767831380862607\n",
      "two-layer Loss after iteration 39000: 0.8704024906637856\n",
      "two-layer Loss after iteration 40000: 0.8645994494752348\n",
      "two-layer Loss after iteration 41000: 0.8595146347150906\n",
      "two-layer Loss after iteration 42000: 0.8549874101195163\n",
      "two-layer Loss after iteration 43000: 0.8508694441310489\n",
      "two-layer Loss after iteration 44000: 0.8473562475179541\n",
      "two-layer Loss after iteration 45000: 0.8440921703838106\n",
      "two-layer Loss after iteration 46000: 0.8411561975218178\n",
      "two-layer Loss after iteration 47000: 0.838342441663491\n",
      "two-layer Loss after iteration 48000: 0.8356784280015896\n",
      "two-layer Loss after iteration 49000: 0.8331388560795959\n",
      "two-layer Loss after iteration 50000: 0.83066372519362\n",
      "two-layer Loss after iteration 51000: 0.8282489336582441\n",
      "two-layer Loss after iteration 52000: 0.8258800149848652\n",
      "two-layer Loss after iteration 53000: 0.8231952613316997\n",
      "two-layer Loss after iteration 54000: 0.8200623867419604\n",
      "two-layer Loss after iteration 55000: 0.8171047072521731\n",
      "two-layer Loss after iteration 56000: 0.814297211879342\n",
      "two-layer Loss after iteration 57000: 0.8116127272673318\n",
      "two-layer Loss after iteration 58000: 0.8090363982193963\n",
      "two-layer Loss after iteration 59000: 0.8065562572107497\n",
      "two-layer Loss after iteration 60000: 0.8041590537048949\n",
      "two-layer Loss after iteration 61000: 0.8015856549387698\n",
      "two-layer Loss after iteration 62000: 0.7992429757531692\n",
      "two-layer Loss after iteration 63000: 0.7970687936416084\n",
      "two-layer Loss after iteration 64000: 0.7949655400220222\n",
      "two-layer Loss after iteration 65000: 0.7929241118183038\n",
      "two-layer Loss after iteration 66000: 0.7909509650054437\n",
      "two-layer Loss after iteration 67000: 0.7889956624570833\n",
      "two-layer Loss after iteration 68000: 0.7871043850900552\n",
      "two-layer Loss after iteration 69000: 0.7852664891477366\n",
      "two-layer Loss after iteration 70000: 0.7834822363432598\n",
      "two-layer Loss after iteration 71000: 0.7807367956586325\n",
      "two-layer Loss after iteration 72000: 0.7782868373880245\n",
      "two-layer Loss after iteration 73000: 0.7277529463630454\n",
      "two-layer Loss after iteration 74000: 0.7188732011013758\n",
      "two-layer Loss after iteration 75000: 0.7136753606742026\n",
      "two-layer Loss after iteration 76000: 0.7094096770971828\n",
      "two-layer Loss after iteration 77000: 0.7052706207343608\n",
      "two-layer Loss after iteration 78000: 0.7012888824157729\n",
      "two-layer Loss after iteration 79000: 0.6973970235879052\n",
      "two-layer Loss after iteration 80000: 0.6935852668818312\n",
      "two-layer Loss after iteration 81000: 0.6878108998995955\n",
      "two-layer Loss after iteration 82000: 0.6831474913136898\n",
      "two-layer Loss after iteration 83000: 0.678678679868721\n",
      "two-layer Loss after iteration 84000: 0.6741883679576895\n",
      "two-layer Loss after iteration 85000: 0.6699029232652928\n",
      "two-layer Loss after iteration 86000: 0.6657725896564054\n",
      "two-layer Loss after iteration 87000: 0.6591334750614781\n",
      "two-layer Loss after iteration 88000: 0.6537957587380909\n",
      "two-layer Loss after iteration 89000: 0.6489148829006883\n",
      "two-layer Loss after iteration 90000: 0.6436305092345821\n",
      "two-layer Loss after iteration 91000: 0.6383749138656485\n",
      "two-layer Loss after iteration 92000: 0.632670696862089\n",
      "two-layer Loss after iteration 93000: 0.6276109003296992\n",
      "two-layer Loss after iteration 94000: 0.6228167217126899\n",
      "two-layer Loss after iteration 95000: 0.6184002595355156\n",
      "two-layer Loss after iteration 96000: 0.6142078021330086\n",
      "two-layer Loss after iteration 97000: 0.6101922300560817\n",
      "two-layer Loss after iteration 98000: 0.6061693237969565\n",
      "two-layer Loss after iteration 99000: 0.6019836895815485\n",
      "two-layer Loss after iteration 100000: 0.5980243497257015\n",
      "two-layer Loss after iteration 101000: 0.5942014036090756\n",
      "two-layer Loss after iteration 102000: 0.5905355067802704\n",
      "two-layer Loss after iteration 103000: 0.5870667714432326\n",
      "two-layer Loss after iteration 104000: 0.5816073640667868\n",
      "two-layer Loss after iteration 105000: 0.5780793162034854\n",
      "two-layer Loss after iteration 106000: 0.573874178407475\n",
      "two-layer Loss after iteration 107000: 0.5704647981725686\n",
      "two-layer Loss after iteration 108000: 0.5672609367513733\n",
      "two-layer Loss after iteration 109000: 0.5642651074281247\n",
      "two-layer Loss after iteration 110000: 0.5614396335821624\n",
      "two-layer Loss after iteration 111000: 0.5581924844137958\n",
      "two-layer Loss after iteration 112000: 0.5554423472797507\n",
      "two-layer Loss after iteration 113000: 0.55283008149366\n",
      "two-layer Loss after iteration 114000: 0.5503314896219828\n",
      "two-layer Loss after iteration 115000: 0.5479301520842385\n",
      "two-layer Loss after iteration 116000: 0.5456146147804021\n",
      "two-layer Loss after iteration 117000: 0.5424704672927083\n",
      "two-layer Loss after iteration 118000: 0.5399105842158222\n",
      "two-layer Loss after iteration 119000: 0.5374532528404978\n",
      "two-layer Loss after iteration 120000: 0.5350788405893951\n",
      "two-layer Loss after iteration 121000: 0.5327808332156879\n",
      "two-layer Loss after iteration 122000: 0.5305502030424724\n",
      "two-layer Loss after iteration 123000: 0.528386307074538\n",
      "two-layer Loss after iteration 124000: 0.5262826864946564\n",
      "two-layer Loss after iteration 125000: 0.5242401875663854\n",
      "two-layer Loss after iteration 126000: 0.5222530771064828\n",
      "two-layer Loss after iteration 127000: 0.5203213134503633\n",
      "two-layer Loss after iteration 128000: 0.5184427746725466\n",
      "two-layer Loss after iteration 129000: 0.5166173109033086\n",
      "two-layer Loss after iteration 130000: 0.5148414536461633\n",
      "two-layer Loss after iteration 131000: 0.513024659523303\n",
      "two-layer Loss after iteration 132000: 0.5112413816977539\n",
      "two-layer Loss after iteration 133000: 0.5094616141021339\n",
      "two-layer Loss after iteration 134000: 0.507325629048015\n",
      "two-layer Loss after iteration 135000: 0.5052902203372622\n",
      "two-layer Loss after iteration 136000: 0.5034899884096043\n",
      "two-layer Loss after iteration 137000: 0.5017775017815842\n",
      "two-layer Loss after iteration 138000: 0.5001427357639467\n",
      "two-layer Loss after iteration 139000: 0.498578327939708\n",
      "two-layer Loss after iteration 140000: 0.49707877490481645\n",
      "two-layer Loss after iteration 141000: 0.4956193013839755\n",
      "two-layer Loss after iteration 142000: 0.4942005257104439\n",
      "two-layer Loss after iteration 143000: 0.4928390467393761\n",
      "two-layer Loss after iteration 144000: 0.4912812523309402\n",
      "two-layer Loss after iteration 145000: 0.4897017761588022\n",
      "two-layer Loss after iteration 146000: 0.4882345917176823\n",
      "two-layer Loss after iteration 147000: 0.4868653204753359\n",
      "two-layer Loss after iteration 148000: 0.4855831562921432\n",
      "two-layer Loss after iteration 149000: 0.4843783259989392\n",
      "two-layer Loss after iteration 150000: 0.4832271994363861\n",
      "two-layer Loss after iteration 151000: 0.48180869073267885\n",
      "two-layer Loss after iteration 152000: 0.48048736912403683\n",
      "two-layer Loss after iteration 153000: 0.4791466042209789\n",
      "two-layer Loss after iteration 154000: 0.4779160479955499\n",
      "two-layer Loss after iteration 155000: 0.47679314612431767\n",
      "two-layer Loss after iteration 156000: 0.4756744517326418\n",
      "two-layer Loss after iteration 157000: 0.47455097210140235\n",
      "two-layer Loss after iteration 158000: 0.47341408611965297\n",
      "two-layer Loss after iteration 159000: 0.4723770070438046\n",
      "two-layer Loss after iteration 160000: 0.4714048287546311\n",
      "two-layer Loss after iteration 161000: 0.4704893407824387\n",
      "two-layer Loss after iteration 162000: 0.46962515652490033\n",
      "two-layer Loss after iteration 163000: 0.46880578313776483\n",
      "two-layer Loss after iteration 164000: 0.46802804227638856\n",
      "two-layer Loss after iteration 165000: 0.4672851223153924\n",
      "two-layer Loss after iteration 166000: 0.4665839944969773\n",
      "two-layer Loss after iteration 167000: 0.46585488413251513\n",
      "two-layer Loss after iteration 168000: 0.46523059801121397\n",
      "two-layer Loss after iteration 169000: 0.46397143592319834\n",
      "two-layer Loss after iteration 170000: 0.46269550628955824\n",
      "two-layer Loss after iteration 171000: 0.46154935518172907\n",
      "two-layer Loss after iteration 172000: 0.46045043379493944\n",
      "two-layer Loss after iteration 173000: 0.45627212886592283\n",
      "two-layer Loss after iteration 174000: 0.45446083849332547\n",
      "two-layer Loss after iteration 175000: 0.4521494302250063\n",
      "two-layer Loss after iteration 176000: 0.44739225762522966\n",
      "two-layer Loss after iteration 177000: 0.44319608465949717\n",
      "two-layer Loss after iteration 178000: 0.43986259542435113\n",
      "two-layer Loss after iteration 179000: 0.4350938074858182\n",
      "two-layer Loss after iteration 180000: 0.4320193322552465\n",
      "two-layer Loss after iteration 181000: 0.4305474967500037\n",
      "two-layer Loss after iteration 182000: 0.4295134741233032\n",
      "two-layer Loss after iteration 183000: 0.42870650082400125\n",
      "two-layer Loss after iteration 184000: 0.4280257856196459\n",
      "two-layer Loss after iteration 185000: 0.4274223641612111\n",
      "two-layer Loss after iteration 186000: 0.42686665061010387\n",
      "two-layer Loss after iteration 187000: 0.4263473485362474\n",
      "two-layer Loss after iteration 188000: 0.4258468420008618\n",
      "two-layer Loss after iteration 189000: 0.42537718072042646\n",
      "two-layer Loss after iteration 190000: 0.42493253281761106\n",
      "two-layer Loss after iteration 191000: 0.4244952710835067\n",
      "two-layer Loss after iteration 192000: 0.42407270723950685\n",
      "two-layer Loss after iteration 193000: 0.4236637342002114\n",
      "two-layer Loss after iteration 194000: 0.4232774852888771\n",
      "two-layer Loss after iteration 195000: 0.4228944615097497\n",
      "two-layer Loss after iteration 196000: 0.42252309707677466\n",
      "two-layer Loss after iteration 197000: 0.42215168664992914\n",
      "two-layer Loss after iteration 198000: 0.42180210083191116\n",
      "two-layer Loss after iteration 199000: 0.421465385731407\n",
      "two-layer Loss after iteration 200000: 0.4211248645256457\n",
      "two-layer Loss after iteration 201000: 0.4207993850120531\n",
      "two-layer Loss after iteration 202000: 0.4204869096370209\n",
      "two-layer Loss after iteration 203000: 0.4201745948648539\n",
      "two-layer Loss after iteration 204000: 0.4198656355658316\n",
      "two-layer Loss after iteration 205000: 0.4195756556292641\n",
      "two-layer Loss after iteration 206000: 0.41927668741310425\n",
      "two-layer Loss after iteration 207000: 0.4189937934132155\n",
      "two-layer Loss after iteration 208000: 0.41871241543544696\n",
      "two-layer Loss after iteration 209000: 0.41844532943373225\n",
      "two-layer Loss after iteration 210000: 0.41818090923878687\n",
      "two-layer Loss after iteration 211000: 0.41792071340475295\n",
      "two-layer Loss after iteration 212000: 0.4176645410228162\n",
      "two-layer Loss after iteration 213000: 0.41741494701696397\n",
      "two-layer Loss after iteration 214000: 0.41717472117665466\n",
      "two-layer Loss after iteration 215000: 0.41693016547165623\n",
      "two-layer Loss after iteration 216000: 0.4167009935468671\n",
      "two-layer Loss after iteration 217000: 0.4164786179506037\n",
      "two-layer Loss after iteration 218000: 0.4162598318467633\n",
      "two-layer Loss after iteration 219000: 0.41604504304599393\n",
      "two-layer Loss after iteration 220000: 0.4158359597444464\n",
      "two-layer Loss after iteration 221000: 0.4156291178785011\n",
      "two-layer Loss after iteration 222000: 0.4154323278821358\n",
      "two-layer Loss after iteration 223000: 0.41522528541774567\n",
      "two-layer Loss after iteration 224000: 0.4150372963372302\n",
      "two-layer Loss after iteration 225000: 0.4148435948130293\n",
      "two-layer Loss after iteration 226000: 0.4146619578644887\n",
      "two-layer Loss after iteration 227000: 0.4144739708090688\n",
      "two-layer Loss after iteration 228000: 0.41429356866913886\n",
      "two-layer Loss after iteration 229000: 0.4141191113298116\n",
      "two-layer Loss after iteration 230000: 0.41395301337188145\n",
      "two-layer Loss after iteration 231000: 0.4137860637058677\n",
      "two-layer Loss after iteration 232000: 0.4136198620287342\n",
      "two-layer Loss after iteration 233000: 0.4134481863423217\n",
      "two-layer Loss after iteration 234000: 0.4132902672904672\n",
      "two-layer Loss after iteration 235000: 0.4131379940516423\n",
      "two-layer Loss after iteration 236000: 0.41298283118857015\n",
      "two-layer Loss after iteration 237000: 0.41283144613059297\n",
      "two-layer Loss after iteration 238000: 0.41267905861616666\n",
      "two-layer Loss after iteration 239000: 0.4125299193565308\n",
      "two-layer Loss after iteration 240000: 0.41238652076026394\n",
      "two-layer Loss after iteration 241000: 0.41223905795286164\n",
      "two-layer Loss after iteration 242000: 0.41210074274570957\n",
      "two-layer Loss after iteration 243000: 0.4119553531633595\n",
      "two-layer Loss after iteration 244000: 0.41182213769802123\n",
      "two-layer Loss after iteration 245000: 0.4116852676426487\n",
      "two-layer Loss after iteration 246000: 0.4115530310904335\n",
      "two-layer Loss after iteration 247000: 0.41142171342472994\n",
      "two-layer Loss after iteration 248000: 0.4112818785569665\n",
      "two-layer Loss after iteration 249000: 0.41116357808326676\n",
      "two-layer Loss after iteration 250000: 0.41103238122331504\n",
      "two-layer Loss after iteration 251000: 0.41091085812322004\n",
      "two-layer Loss after iteration 252000: 0.4107884169623528\n",
      "two-layer Loss after iteration 253000: 0.41066597502868274\n",
      "two-layer Loss after iteration 254000: 0.4105413961366719\n",
      "two-layer Loss after iteration 255000: 0.4104274125035978\n",
      "two-layer Loss after iteration 256000: 0.41030998199659696\n",
      "two-layer Loss after iteration 257000: 0.4101996303477746\n",
      "two-layer Loss after iteration 258000: 0.41007994406611487\n",
      "two-layer Loss after iteration 259000: 0.4099748659873724\n",
      "two-layer Loss after iteration 260000: 0.4098582535503538\n",
      "two-layer Loss after iteration 261000: 0.4097485558424778\n",
      "two-layer Loss after iteration 262000: 0.40964280721916846\n",
      "two-layer Loss after iteration 263000: 0.409538373465548\n",
      "two-layer Loss after iteration 264000: 0.4094378098128431\n",
      "two-layer Loss after iteration 265000: 0.40933247712421184\n",
      "two-layer Loss after iteration 266000: 0.4092278005984924\n",
      "two-layer Loss after iteration 267000: 0.4091341223808181\n",
      "two-layer Loss after iteration 268000: 0.40902927022353325\n",
      "two-layer Loss after iteration 269000: 0.40893057691817475\n",
      "two-layer Loss after iteration 270000: 0.4088346464799588\n",
      "two-layer Loss after iteration 271000: 0.40874679881616194\n",
      "two-layer Loss after iteration 272000: 0.40864895404561924\n",
      "two-layer Loss after iteration 273000: 0.4085601373486625\n",
      "two-layer Loss after iteration 274000: 0.4084643100688767\n",
      "two-layer Loss after iteration 275000: 0.4083728527442391\n",
      "two-layer Loss after iteration 276000: 0.4082800146252201\n",
      "two-layer Loss after iteration 277000: 0.40819092488263364\n",
      "two-layer Loss after iteration 278000: 0.4057181779381546\n",
      "two-layer Loss after iteration 279000: 0.40527889125542527\n",
      "two-layer Loss after iteration 280000: 0.4050084487692669\n",
      "two-layer Loss after iteration 281000: 0.4048255547667993\n",
      "two-layer Loss after iteration 282000: 0.40466907347486364\n",
      "two-layer Loss after iteration 283000: 0.40453693645543204\n",
      "two-layer Loss after iteration 284000: 0.40441675102496333\n",
      "two-layer Loss after iteration 285000: 0.4043040459370844\n",
      "two-layer Loss after iteration 286000: 0.40419904825928027\n",
      "two-layer Loss after iteration 287000: 0.4029853957810857\n",
      "two-layer Loss after iteration 288000: 0.40152193448716855\n",
      "two-layer Loss after iteration 289000: 0.4011128484382136\n",
      "two-layer Loss after iteration 290000: 0.4008435620020008\n",
      "two-layer Loss after iteration 291000: 0.40065084696805875\n",
      "two-layer Loss after iteration 292000: 0.4004909423494291\n",
      "two-layer Loss after iteration 293000: 0.40034647258250017\n",
      "two-layer Loss after iteration 294000: 0.40022376949255434\n",
      "two-layer Loss after iteration 295000: 0.4001004998652811\n",
      "two-layer Loss after iteration 296000: 0.3999835302401938\n",
      "two-layer Loss after iteration 297000: 0.39986646796230485\n",
      "two-layer Loss after iteration 298000: 0.3997525027749026\n",
      "two-layer Loss after iteration 299000: 0.3996474139246912\n",
      "two-layer Loss after iteration 300000: 0.39953078940690845\n",
      "two-layer Loss after iteration 301000: 0.39942238476754977\n",
      "two-layer Loss after iteration 302000: 0.3993182937939919\n",
      "two-layer Loss after iteration 303000: 0.3992131216123495\n",
      "two-layer Loss after iteration 304000: 0.39910791281812924\n",
      "two-layer Loss after iteration 305000: 0.3990053669167922\n",
      "two-layer Loss after iteration 306000: 0.3989018807893648\n",
      "two-layer Loss after iteration 307000: 0.39880712773222393\n",
      "two-layer Loss after iteration 308000: 0.39870753342483767\n",
      "two-layer Loss after iteration 309000: 0.3986105887839386\n",
      "two-layer Loss after iteration 310000: 0.3985147294393491\n",
      "two-layer Loss after iteration 311000: 0.39841854401403753\n",
      "two-layer Loss after iteration 312000: 0.39832509448853654\n",
      "two-layer Loss after iteration 313000: 0.39823229320964926\n",
      "two-layer Loss after iteration 314000: 0.39813988795311295\n",
      "two-layer Loss after iteration 315000: 0.3980486231745143\n",
      "two-layer Loss after iteration 316000: 0.3979590526380369\n",
      "two-layer Loss after iteration 317000: 0.39786978481163976\n",
      "two-layer Loss after iteration 318000: 0.39778238583531256\n",
      "two-layer Loss after iteration 319000: 0.39769825771953976\n",
      "two-layer Loss after iteration 320000: 0.3976133733470671\n",
      "two-layer Loss after iteration 321000: 0.39752659268556223\n",
      "two-layer Loss after iteration 322000: 0.39744358559022697\n",
      "two-layer Loss after iteration 323000: 0.39736172121658175\n",
      "two-layer Loss after iteration 324000: 0.3972794498535476\n",
      "two-layer Loss after iteration 325000: 0.3971990783122269\n",
      "two-layer Loss after iteration 326000: 0.3971184804837283\n",
      "two-layer Loss after iteration 327000: 0.39704143605770875\n",
      "two-layer Loss after iteration 328000: 0.39696219486089235\n",
      "two-layer Loss after iteration 329000: 0.39688665189936717\n",
      "two-layer Loss after iteration 330000: 0.39680816876805375\n",
      "two-layer Loss after iteration 331000: 0.3967354829669832\n",
      "two-layer Loss after iteration 332000: 0.3966609643919493\n",
      "two-layer Loss after iteration 333000: 0.3965871527174109\n",
      "two-layer Loss after iteration 334000: 0.3965144326511688\n",
      "two-layer Loss after iteration 335000: 0.3964432829307029\n",
      "two-layer Loss after iteration 336000: 0.3963697952495905\n",
      "two-layer Loss after iteration 337000: 0.3963065157065183\n",
      "two-layer Loss after iteration 338000: 0.39622967358173633\n",
      "two-layer Loss after iteration 339000: 0.3961643232638339\n",
      "two-layer Loss after iteration 340000: 0.39609674322382427\n",
      "two-layer Loss after iteration 341000: 0.39603676756089085\n",
      "two-layer Loss after iteration 342000: 0.39596383863976303\n",
      "two-layer Loss after iteration 343000: 0.3958957761608905\n",
      "two-layer Loss after iteration 344000: 0.39583297292912845\n",
      "two-layer Loss after iteration 345000: 0.3957687603786676\n",
      "two-layer Loss after iteration 346000: 0.3957026523012016\n",
      "two-layer Loss after iteration 347000: 0.3956399565852674\n",
      "two-layer Loss after iteration 348000: 0.3955800466807667\n",
      "two-layer Loss after iteration 349000: 0.395525785914817\n",
      "two-layer Loss after iteration 350000: 0.3954608633073422\n",
      "two-layer Loss after iteration 351000: 0.39540090413329454\n",
      "two-layer Loss after iteration 352000: 0.3953455229053482\n",
      "two-layer Loss after iteration 353000: 0.3952833816982641\n",
      "two-layer Loss after iteration 354000: 0.3952256929738936\n",
      "two-layer Loss after iteration 355000: 0.39517422646841244\n",
      "two-layer Loss after iteration 356000: 0.39510987247483703\n",
      "two-layer Loss after iteration 357000: 0.395054568514323\n",
      "two-layer Loss after iteration 358000: 0.3950004231036566\n",
      "two-layer Loss after iteration 359000: 0.39494558033841487\n",
      "two-layer Loss after iteration 360000: 0.3948929987882314\n",
      "two-layer Loss after iteration 361000: 0.3948393719723369\n",
      "two-layer Loss after iteration 362000: 0.39478776044928887\n",
      "two-layer Loss after iteration 363000: 0.39473577329293247\n",
      "two-layer Loss after iteration 364000: 0.3946858655273423\n",
      "two-layer Loss after iteration 365000: 0.3946369759605179\n",
      "two-layer Loss after iteration 366000: 0.39458332610263797\n",
      "two-layer Loss after iteration 367000: 0.39454095396743094\n",
      "two-layer Loss after iteration 368000: 0.3944847227728674\n",
      "two-layer Loss after iteration 369000: 0.3944349867212888\n",
      "two-layer Loss after iteration 370000: 0.39438726182607364\n",
      "two-layer Loss after iteration 371000: 0.39434111070770755\n",
      "two-layer Loss after iteration 372000: 0.39429364512264536\n",
      "two-layer Loss after iteration 373000: 0.3942489047044133\n",
      "two-layer Loss after iteration 374000: 0.3942020241545082\n",
      "two-layer Loss after iteration 375000: 0.39415807166772127\n",
      "two-layer Loss after iteration 376000: 0.3941144168777501\n",
      "two-layer Loss after iteration 377000: 0.3940678937803346\n",
      "two-layer Loss after iteration 378000: 0.3940244742073757\n",
      "two-layer Loss after iteration 379000: 0.3939814282188117\n",
      "two-layer Loss after iteration 380000: 0.3939388408556129\n",
      "two-layer Loss after iteration 381000: 0.3938962840481304\n",
      "two-layer Loss after iteration 382000: 0.39385821108032576\n",
      "9.665734191075075e-05 0.3938962840481304 0.39385821108032576\n",
      "two-layer Loss after iteration 0: 1305.0526300741753\n",
      "two-layer Loss after iteration 1000: 24.571108212539354\n",
      "two-layer Loss after iteration 2000: 9.380997550134188\n",
      "two-layer Loss after iteration 3000: 6.450912770424051\n",
      "two-layer Loss after iteration 4000: 4.610930209819483\n",
      "two-layer Loss after iteration 5000: 3.600367922931768\n",
      "two-layer Loss after iteration 6000: 2.84850294069212\n",
      "two-layer Loss after iteration 7000: 2.255684281966982\n",
      "two-layer Loss after iteration 8000: 1.9121346950737905\n",
      "two-layer Loss after iteration 9000: 1.6399007613567789\n",
      "two-layer Loss after iteration 10000: 1.3756489233930345\n",
      "two-layer Loss after iteration 11000: 1.1575964985952574\n",
      "two-layer Loss after iteration 12000: 0.9712821541775006\n",
      "two-layer Loss after iteration 13000: 0.7457152546951847\n",
      "two-layer Loss after iteration 14000: 0.6035989406826571\n",
      "two-layer Loss after iteration 15000: 0.5336558483285769\n",
      "two-layer Loss after iteration 16000: 0.48355184642511706\n",
      "two-layer Loss after iteration 17000: 0.4452702162517916\n",
      "two-layer Loss after iteration 18000: 0.41499951480169683\n",
      "two-layer Loss after iteration 19000: 0.38879068891871554\n",
      "two-layer Loss after iteration 20000: 0.36957259021072836\n",
      "two-layer Loss after iteration 21000: 0.3491224813370411\n",
      "two-layer Loss after iteration 22000: 0.3322307448461842\n",
      "two-layer Loss after iteration 23000: 0.3147035167633249\n",
      "two-layer Loss after iteration 24000: 0.3002916166074712\n",
      "two-layer Loss after iteration 25000: 0.28766767748305716\n",
      "two-layer Loss after iteration 26000: 0.2781638849905637\n",
      "two-layer Loss after iteration 27000: 0.2704920886940061\n",
      "two-layer Loss after iteration 28000: 0.2646378202659461\n",
      "two-layer Loss after iteration 29000: 0.260795362435437\n",
      "two-layer Loss after iteration 30000: 0.2575571573707756\n",
      "two-layer Loss after iteration 31000: 0.2549655187604908\n",
      "two-layer Loss after iteration 32000: 0.25283300939205977\n",
      "two-layer Loss after iteration 33000: 0.25104100418759046\n",
      "two-layer Loss after iteration 34000: 0.24950856036873884\n",
      "two-layer Loss after iteration 35000: 0.24588552394301225\n",
      "two-layer Loss after iteration 36000: 0.24327025937908256\n",
      "two-layer Loss after iteration 37000: 0.23797064285877814\n",
      "two-layer Loss after iteration 38000: 0.23472402023495234\n",
      "two-layer Loss after iteration 39000: 0.2309628712528081\n",
      "two-layer Loss after iteration 40000: 0.22584659898623216\n",
      "two-layer Loss after iteration 41000: 0.22283244652806922\n",
      "two-layer Loss after iteration 42000: 0.2218243150202926\n",
      "two-layer Loss after iteration 43000: 0.22117303749928813\n",
      "two-layer Loss after iteration 44000: 0.2206550502069418\n",
      "two-layer Loss after iteration 45000: 0.2202128597450459\n",
      "two-layer Loss after iteration 46000: 0.21982363278032804\n",
      "two-layer Loss after iteration 47000: 0.21947561799582185\n",
      "two-layer Loss after iteration 48000: 0.21916189836125216\n",
      "two-layer Loss after iteration 49000: 0.21887772471308597\n",
      "two-layer Loss after iteration 50000: 0.21861942844911098\n",
      "two-layer Loss after iteration 51000: 0.21838421995345975\n",
      "two-layer Loss after iteration 52000: 0.21816944411742073\n",
      "two-layer Loss after iteration 53000: 0.2179141542311965\n",
      "two-layer Loss after iteration 54000: 0.2176646475901058\n",
      "two-layer Loss after iteration 55000: 0.21749076846908932\n",
      "two-layer Loss after iteration 56000: 0.2173379420680294\n",
      "two-layer Loss after iteration 57000: 0.21719853752926863\n",
      "two-layer Loss after iteration 58000: 0.21707101163231263\n",
      "two-layer Loss after iteration 59000: 0.2169542909728269\n",
      "two-layer Loss after iteration 60000: 0.21684719342647074\n",
      "two-layer Loss after iteration 61000: 0.21674880954994463\n",
      "two-layer Loss after iteration 62000: 0.21665839575988366\n",
      "two-layer Loss after iteration 63000: 0.21657525412274725\n",
      "two-layer Loss after iteration 64000: 0.216498687629787\n",
      "two-layer Loss after iteration 65000: 0.21642799214345815\n",
      "two-layer Loss after iteration 66000: 0.21636303248091412\n",
      "two-layer Loss after iteration 67000: 0.21630284568338054\n",
      "two-layer Loss after iteration 68000: 0.21624738984293515\n",
      "two-layer Loss after iteration 69000: 0.21619600266870603\n",
      "two-layer Loss after iteration 70000: 0.2161483386470747\n",
      "two-layer Loss after iteration 71000: 0.21610444364003953\n",
      "two-layer Loss after iteration 72000: 0.2160635665168853\n",
      "two-layer Loss after iteration 73000: 0.21602560407316887\n",
      "two-layer Loss after iteration 74000: 0.21599044897048475\n",
      "two-layer Loss after iteration 75000: 0.21595764479646706\n",
      "two-layer Loss after iteration 76000: 0.21592721714505594\n",
      "two-layer Loss after iteration 77000: 0.2158988033792887\n",
      "two-layer Loss after iteration 78000: 0.21587244207647788\n",
      "two-layer Loss after iteration 79000: 0.21584772848772227\n",
      "two-layer Loss after iteration 80000: 0.21582477985859205\n",
      "two-layer Loss after iteration 81000: 0.2158031451972834\n",
      "two-layer Loss after iteration 82000: 0.21578312616403023\n",
      "9.276525249382692e-05 0.2158031451972834 0.21578312616403023\n",
      "two-layer Loss after iteration 0: 1749.6006810777433\n",
      "two-layer Loss after iteration 1000: 23.895175507997116\n",
      "two-layer Loss after iteration 2000: 9.665197524393177\n",
      "two-layer Loss after iteration 3000: 6.7022934988118\n",
      "two-layer Loss after iteration 4000: 5.620676475066836\n",
      "two-layer Loss after iteration 5000: 4.841889406633448\n",
      "two-layer Loss after iteration 6000: 4.101282114644781\n",
      "two-layer Loss after iteration 7000: 3.83079336015135\n",
      "two-layer Loss after iteration 8000: 3.139578679831296\n",
      "two-layer Loss after iteration 9000: 2.630743373542188\n",
      "two-layer Loss after iteration 10000: 2.5042947504893602\n",
      "two-layer Loss after iteration 11000: 2.432769075373161\n",
      "two-layer Loss after iteration 12000: 2.3574371256884104\n",
      "two-layer Loss after iteration 13000: 2.253843322074648\n",
      "two-layer Loss after iteration 14000: 2.153713876343628\n",
      "two-layer Loss after iteration 15000: 2.086486333180069\n",
      "two-layer Loss after iteration 16000: 2.0379147003560574\n",
      "two-layer Loss after iteration 17000: 2.0007929575282195\n",
      "two-layer Loss after iteration 18000: 1.9697277891592142\n",
      "two-layer Loss after iteration 19000: 1.9442370215745823\n",
      "two-layer Loss after iteration 20000: 1.9211886313993116\n",
      "two-layer Loss after iteration 21000: 1.9031731744699485\n",
      "two-layer Loss after iteration 22000: 1.8833279948442139\n",
      "two-layer Loss after iteration 23000: 1.8650286800828508\n",
      "two-layer Loss after iteration 24000: 1.842172355739685\n",
      "two-layer Loss after iteration 25000: 1.813435331600007\n",
      "two-layer Loss after iteration 26000: 1.7905923902510212\n",
      "two-layer Loss after iteration 27000: 1.7658877155124941\n",
      "two-layer Loss after iteration 28000: 1.7356147219073896\n",
      "two-layer Loss after iteration 29000: 1.7061285929859669\n",
      "two-layer Loss after iteration 30000: 1.6526123010309015\n",
      "two-layer Loss after iteration 31000: 1.6013271383566487\n",
      "two-layer Loss after iteration 32000: 1.5551831537709728\n",
      "two-layer Loss after iteration 33000: 1.511279456206138\n",
      "two-layer Loss after iteration 34000: 1.4797096324022962\n",
      "two-layer Loss after iteration 35000: 1.45020168353314\n",
      "two-layer Loss after iteration 36000: 1.4093158542193256\n",
      "two-layer Loss after iteration 37000: 1.3776477700809948\n",
      "two-layer Loss after iteration 38000: 1.3504428929923054\n",
      "two-layer Loss after iteration 39000: 1.3265502864995717\n",
      "two-layer Loss after iteration 40000: 1.3065893604658758\n",
      "two-layer Loss after iteration 41000: 1.286196644238161\n",
      "two-layer Loss after iteration 42000: 1.2732108178444757\n",
      "two-layer Loss after iteration 43000: 1.2617763451310189\n",
      "two-layer Loss after iteration 44000: 1.2497390303695637\n",
      "two-layer Loss after iteration 45000: 1.238824724117553\n",
      "two-layer Loss after iteration 46000: 1.2083260700506868\n",
      "two-layer Loss after iteration 47000: 1.1965605237850767\n",
      "two-layer Loss after iteration 48000: 1.1882776712593803\n",
      "two-layer Loss after iteration 49000: 1.182896670495565\n",
      "two-layer Loss after iteration 50000: 1.178893002543089\n",
      "two-layer Loss after iteration 51000: 1.1690950454484688\n",
      "two-layer Loss after iteration 52000: 1.1630731785676154\n",
      "two-layer Loss after iteration 53000: 1.1581168365788777\n",
      "two-layer Loss after iteration 54000: 1.1538480037373966\n",
      "two-layer Loss after iteration 55000: 1.150158217067605\n",
      "two-layer Loss after iteration 56000: 1.1468868066456679\n",
      "two-layer Loss after iteration 57000: 1.1439887899401715\n",
      "two-layer Loss after iteration 58000: 1.1414487227531385\n",
      "two-layer Loss after iteration 59000: 1.1379698068707946\n",
      "two-layer Loss after iteration 60000: 1.1352310245621808\n",
      "two-layer Loss after iteration 61000: 1.1327735178468032\n",
      "two-layer Loss after iteration 62000: 1.1305084018277154\n",
      "two-layer Loss after iteration 63000: 1.1284958924455488\n",
      "two-layer Loss after iteration 64000: 1.1266210739322804\n",
      "two-layer Loss after iteration 65000: 1.1248814404259255\n",
      "two-layer Loss after iteration 66000: 1.1232619630464378\n",
      "two-layer Loss after iteration 67000: 1.1217967446732577\n",
      "two-layer Loss after iteration 68000: 1.1204326227533117\n",
      "two-layer Loss after iteration 69000: 1.117234391970597\n",
      "two-layer Loss after iteration 70000: 1.113790200930511\n",
      "two-layer Loss after iteration 71000: 1.1120931814493034\n",
      "two-layer Loss after iteration 72000: 1.110662046577609\n",
      "two-layer Loss after iteration 73000: 1.1093955970233509\n",
      "two-layer Loss after iteration 74000: 1.1082333178748194\n",
      "two-layer Loss after iteration 75000: 1.107149881550037\n",
      "two-layer Loss after iteration 76000: 1.1061541423752683\n",
      "two-layer Loss after iteration 77000: 1.1052039974011496\n",
      "two-layer Loss after iteration 78000: 1.1037131727447373\n",
      "two-layer Loss after iteration 79000: 1.1025398654757963\n",
      "two-layer Loss after iteration 80000: 1.1015032780688996\n",
      "two-layer Loss after iteration 81000: 1.1005373050523082\n",
      "two-layer Loss after iteration 82000: 1.0996302717219377\n",
      "two-layer Loss after iteration 83000: 1.0987713527085938\n",
      "two-layer Loss after iteration 84000: 1.0979553029890115\n",
      "two-layer Loss after iteration 85000: 1.097179400179049\n",
      "two-layer Loss after iteration 86000: 1.0964365151560327\n",
      "two-layer Loss after iteration 87000: 1.095725126942191\n",
      "two-layer Loss after iteration 88000: 1.095046730449816\n",
      "two-layer Loss after iteration 89000: 1.0943992090119739\n",
      "two-layer Loss after iteration 90000: 1.0937773242762516\n",
      "two-layer Loss after iteration 91000: 1.0931779419984244\n",
      "two-layer Loss after iteration 92000: 1.0926097439538063\n",
      "two-layer Loss after iteration 93000: 1.0920605612580465\n",
      "two-layer Loss after iteration 94000: 1.0915306755395981\n",
      "two-layer Loss after iteration 95000: 1.091026181389822\n",
      "two-layer Loss after iteration 96000: 1.0905390327667963\n",
      "two-layer Loss after iteration 97000: 1.0900751822277681\n",
      "two-layer Loss after iteration 98000: 1.0896236602712799\n",
      "two-layer Loss after iteration 99000: 1.0891945788531092\n",
      "two-layer Loss after iteration 100000: 1.0887807989198988\n",
      "two-layer Loss after iteration 101000: 1.0883853058457744\n",
      "two-layer Loss after iteration 102000: 1.088003812300344\n",
      "two-layer Loss after iteration 103000: 1.0876386948597383\n",
      "two-layer Loss after iteration 104000: 1.0872899194563406\n",
      "two-layer Loss after iteration 105000: 1.086948342103023\n",
      "two-layer Loss after iteration 106000: 1.086624798099099\n",
      "two-layer Loss after iteration 107000: 1.0863175688703548\n",
      "two-layer Loss after iteration 108000: 1.0860135420999506\n",
      "two-layer Loss after iteration 109000: 1.085724262977386\n",
      "two-layer Loss after iteration 110000: 1.0854478295663104\n",
      "two-layer Loss after iteration 111000: 1.0851859014245937\n",
      "two-layer Loss after iteration 112000: 1.084933712506527\n",
      "two-layer Loss after iteration 113000: 1.0846915871009482\n",
      "two-layer Loss after iteration 114000: 1.0844588059828126\n",
      "two-layer Loss after iteration 115000: 1.0842316901676956\n",
      "two-layer Loss after iteration 116000: 1.0840159820456228\n",
      "two-layer Loss after iteration 117000: 1.0838119497311316\n",
      "two-layer Loss after iteration 118000: 1.0836090637939157\n",
      "two-layer Loss after iteration 119000: 1.0834143183976617\n",
      "two-layer Loss after iteration 120000: 1.083232864658806\n",
      "two-layer Loss after iteration 121000: 1.0830508025636532\n",
      "two-layer Loss after iteration 122000: 1.0818663057387745\n",
      "two-layer Loss after iteration 123000: 1.0815933573756362\n",
      "two-layer Loss after iteration 124000: 1.0813453491207838\n",
      "two-layer Loss after iteration 125000: 1.0811171345970585\n",
      "two-layer Loss after iteration 126000: 1.0809054054769942\n",
      "two-layer Loss after iteration 127000: 1.0807050925393287\n",
      "two-layer Loss after iteration 128000: 1.0805189187913407\n",
      "two-layer Loss after iteration 129000: 1.0803319366676754\n",
      "two-layer Loss after iteration 130000: 1.0801585374939873\n",
      "two-layer Loss after iteration 131000: 1.0797226182552968\n",
      "two-layer Loss after iteration 132000: 1.079494759896891\n",
      "two-layer Loss after iteration 133000: 1.0792929894664607\n",
      "two-layer Loss after iteration 134000: 1.0789046502802357\n",
      "two-layer Loss after iteration 135000: 1.0786428233009189\n",
      "two-layer Loss after iteration 136000: 1.0784040771695382\n",
      "two-layer Loss after iteration 137000: 1.0781981542880879\n",
      "two-layer Loss after iteration 138000: 1.078011187797199\n",
      "two-layer Loss after iteration 139000: 1.0769527698094221\n",
      "two-layer Loss after iteration 140000: 1.0765456671392621\n",
      "two-layer Loss after iteration 141000: 1.0762074458246174\n",
      "two-layer Loss after iteration 142000: 1.075914913720623\n",
      "two-layer Loss after iteration 143000: 1.0756619592700953\n",
      "two-layer Loss after iteration 144000: 1.075432579353936\n",
      "two-layer Loss after iteration 145000: 1.0752301203293153\n",
      "two-layer Loss after iteration 146000: 1.0750299967435677\n",
      "two-layer Loss after iteration 147000: 1.0742546281074374\n",
      "two-layer Loss after iteration 148000: 1.0739842663696504\n",
      "two-layer Loss after iteration 149000: 1.0735001955078953\n",
      "two-layer Loss after iteration 150000: 1.0732048850413562\n",
      "two-layer Loss after iteration 151000: 1.0729177839512805\n",
      "two-layer Loss after iteration 152000: 1.0726789434290673\n",
      "two-layer Loss after iteration 153000: 1.072440365045923\n",
      "two-layer Loss after iteration 154000: 1.0722135434461606\n",
      "two-layer Loss after iteration 155000: 1.0720105822458301\n",
      "two-layer Loss after iteration 156000: 1.07184252974261\n",
      "two-layer Loss after iteration 157000: 1.0716746938341293\n",
      "two-layer Loss after iteration 158000: 1.071506960357029\n",
      "two-layer Loss after iteration 159000: 1.0713586683275382\n",
      "two-layer Loss after iteration 160000: 1.071235278206676\n",
      "two-layer Loss after iteration 161000: 1.071085856960819\n",
      "two-layer Loss after iteration 162000: 1.0709722803444817\n",
      "two-layer Loss after iteration 163000: 1.0708594314006248\n",
      "two-layer Loss after iteration 164000: 1.0707461561830864\n",
      "two-layer Loss after iteration 165000: 1.070638934361539\n",
      "two-layer Loss after iteration 166000: 1.0705483561241849\n",
      "8.460203944307263e-05 1.070638934361539 1.0705483561241849\n",
      "two-layer Loss after iteration 0: 1403.9734511644547\n",
      "two-layer Loss after iteration 1000: 23.886887790927673\n",
      "two-layer Loss after iteration 2000: 9.522448776516896\n",
      "two-layer Loss after iteration 3000: 6.976068077754397\n",
      "two-layer Loss after iteration 4000: 5.348955088551761\n",
      "two-layer Loss after iteration 5000: 3.718416184147685\n",
      "two-layer Loss after iteration 6000: 2.7877904661970914\n",
      "two-layer Loss after iteration 7000: 2.3969436448101864\n",
      "two-layer Loss after iteration 8000: 2.10261233159397\n",
      "two-layer Loss after iteration 9000: 1.7776690447013153\n",
      "two-layer Loss after iteration 10000: 1.5752046603640781\n",
      "two-layer Loss after iteration 11000: 1.4484243579138754\n",
      "two-layer Loss after iteration 12000: 1.3420108863973566\n",
      "two-layer Loss after iteration 13000: 1.2550888415239316\n",
      "two-layer Loss after iteration 14000: 1.187220659646761\n",
      "two-layer Loss after iteration 15000: 1.144549501681044\n",
      "two-layer Loss after iteration 16000: 1.1114438968126232\n",
      "two-layer Loss after iteration 17000: 1.0894756718882053\n",
      "two-layer Loss after iteration 18000: 1.0733694175691655\n",
      "two-layer Loss after iteration 19000: 1.0598026460654406\n",
      "two-layer Loss after iteration 20000: 1.0493210976785337\n",
      "two-layer Loss after iteration 21000: 1.0293930334773351\n",
      "two-layer Loss after iteration 22000: 1.0085002766238331\n",
      "two-layer Loss after iteration 23000: 0.9938698709532746\n",
      "two-layer Loss after iteration 24000: 0.974682891940113\n",
      "two-layer Loss after iteration 25000: 0.9581476398758826\n",
      "two-layer Loss after iteration 26000: 0.9403829839069942\n",
      "two-layer Loss after iteration 27000: 0.9309877641514115\n",
      "two-layer Loss after iteration 28000: 0.923389503180982\n",
      "two-layer Loss after iteration 29000: 0.9172150904116727\n",
      "two-layer Loss after iteration 30000: 0.9084951181844184\n",
      "two-layer Loss after iteration 31000: 0.8973816030123296\n",
      "two-layer Loss after iteration 32000: 0.8886855769635565\n",
      "two-layer Loss after iteration 33000: 0.8795725888590886\n",
      "two-layer Loss after iteration 34000: 0.873395433071891\n",
      "two-layer Loss after iteration 35000: 0.8676983206241484\n",
      "two-layer Loss after iteration 36000: 0.8626841178016488\n",
      "two-layer Loss after iteration 37000: 0.8582232515920836\n",
      "two-layer Loss after iteration 38000: 0.854130108093122\n",
      "two-layer Loss after iteration 39000: 0.850372805251855\n",
      "two-layer Loss after iteration 40000: 0.8469063804488312\n",
      "two-layer Loss after iteration 41000: 0.8437038060632981\n",
      "two-layer Loss after iteration 42000: 0.840276674808926\n",
      "two-layer Loss after iteration 43000: 0.8370296477090604\n",
      "two-layer Loss after iteration 44000: 0.8293072223554125\n",
      "two-layer Loss after iteration 45000: 0.8248924484106763\n",
      "two-layer Loss after iteration 46000: 0.8214692140402465\n",
      "two-layer Loss after iteration 47000: 0.8172709974320744\n",
      "two-layer Loss after iteration 48000: 0.8145301317341053\n",
      "two-layer Loss after iteration 49000: 0.8123058491306494\n",
      "two-layer Loss after iteration 50000: 0.8076936898646451\n",
      "two-layer Loss after iteration 51000: 0.8039169111043563\n",
      "two-layer Loss after iteration 52000: 0.8008245469444715\n",
      "two-layer Loss after iteration 53000: 0.7982252041654949\n",
      "two-layer Loss after iteration 54000: 0.7960002601041877\n",
      "two-layer Loss after iteration 55000: 0.7939569950108805\n",
      "two-layer Loss after iteration 56000: 0.7916312107396795\n",
      "two-layer Loss after iteration 57000: 0.7891068632637452\n",
      "two-layer Loss after iteration 58000: 0.787094300460732\n",
      "two-layer Loss after iteration 59000: 0.7852566000083018\n",
      "two-layer Loss after iteration 60000: 0.7835585673658124\n",
      "two-layer Loss after iteration 61000: 0.7819851837768566\n",
      "two-layer Loss after iteration 62000: 0.7745701216389456\n",
      "two-layer Loss after iteration 63000: 0.7394753006325288\n",
      "two-layer Loss after iteration 64000: 0.7157075427272998\n",
      "two-layer Loss after iteration 65000: 0.6789665872557007\n",
      "two-layer Loss after iteration 66000: 0.6617093880922824\n",
      "two-layer Loss after iteration 67000: 0.6245543495782461\n",
      "two-layer Loss after iteration 68000: 0.5822229916744044\n",
      "two-layer Loss after iteration 69000: 0.5634806114521361\n",
      "two-layer Loss after iteration 70000: 0.5483089942539507\n",
      "two-layer Loss after iteration 71000: 0.5430294241053936\n",
      "two-layer Loss after iteration 72000: 0.539569842548537\n",
      "two-layer Loss after iteration 73000: 0.5370554607933448\n",
      "two-layer Loss after iteration 74000: 0.5350735858606374\n",
      "two-layer Loss after iteration 75000: 0.5334194476162795\n",
      "two-layer Loss after iteration 76000: 0.5319201222428244\n",
      "two-layer Loss after iteration 77000: 0.5281153679040342\n",
      "two-layer Loss after iteration 78000: 0.5204895350013538\n",
      "two-layer Loss after iteration 79000: 0.5087629981520569\n",
      "two-layer Loss after iteration 80000: 0.5016701689112235\n",
      "two-layer Loss after iteration 81000: 0.4909327664267\n",
      "two-layer Loss after iteration 82000: 0.48009279073891487\n",
      "two-layer Loss after iteration 83000: 0.47235924404029217\n",
      "two-layer Loss after iteration 84000: 0.4686252322020166\n",
      "two-layer Loss after iteration 85000: 0.4653961083758517\n",
      "two-layer Loss after iteration 86000: 0.46193545172945255\n",
      "two-layer Loss after iteration 87000: 0.4558551915485972\n",
      "two-layer Loss after iteration 88000: 0.4506373750898457\n",
      "two-layer Loss after iteration 89000: 0.44492321830955084\n",
      "two-layer Loss after iteration 90000: 0.43978500960921424\n",
      "two-layer Loss after iteration 91000: 0.4361429379968466\n",
      "two-layer Loss after iteration 92000: 0.4329990704178995\n",
      "two-layer Loss after iteration 93000: 0.4301419722314996\n",
      "two-layer Loss after iteration 94000: 0.42746993967215613\n",
      "two-layer Loss after iteration 95000: 0.4249604991658065\n",
      "two-layer Loss after iteration 96000: 0.42258092939637665\n",
      "two-layer Loss after iteration 97000: 0.4196101387306237\n",
      "two-layer Loss after iteration 98000: 0.4168874689612709\n",
      "two-layer Loss after iteration 99000: 0.414766105605184\n",
      "two-layer Loss after iteration 100000: 0.4128291147004537\n",
      "two-layer Loss after iteration 101000: 0.4110497321659732\n",
      "two-layer Loss after iteration 102000: 0.40939454510192086\n",
      "two-layer Loss after iteration 103000: 0.4078631044698639\n",
      "two-layer Loss after iteration 104000: 0.4046091382104766\n",
      "two-layer Loss after iteration 105000: 0.40216103293291117\n",
      "two-layer Loss after iteration 106000: 0.40025264813376893\n",
      "two-layer Loss after iteration 107000: 0.39847102148669794\n",
      "two-layer Loss after iteration 108000: 0.39687207718423756\n",
      "two-layer Loss after iteration 109000: 0.3954689053780737\n",
      "two-layer Loss after iteration 110000: 0.39418477914366873\n",
      "two-layer Loss after iteration 111000: 0.3929909175412672\n",
      "two-layer Loss after iteration 112000: 0.39133211725188954\n",
      "two-layer Loss after iteration 113000: 0.3892711993564076\n",
      "two-layer Loss after iteration 114000: 0.38771757342088176\n",
      "two-layer Loss after iteration 115000: 0.3862404671112194\n",
      "two-layer Loss after iteration 116000: 0.384856143525125\n",
      "two-layer Loss after iteration 117000: 0.3835502603715543\n",
      "two-layer Loss after iteration 118000: 0.38205317742494077\n",
      "two-layer Loss after iteration 119000: 0.3804968289815739\n",
      "two-layer Loss after iteration 120000: 0.3791479356096491\n",
      "two-layer Loss after iteration 121000: 0.3777609360380888\n",
      "two-layer Loss after iteration 122000: 0.3763848137975293\n",
      "two-layer Loss after iteration 123000: 0.37507767979245626\n",
      "two-layer Loss after iteration 124000: 0.37331444690212273\n",
      "two-layer Loss after iteration 125000: 0.37085503622391847\n",
      "two-layer Loss after iteration 126000: 0.36960238922296357\n",
      "two-layer Loss after iteration 127000: 0.368553756829564\n",
      "two-layer Loss after iteration 128000: 0.36763751949373374\n",
      "two-layer Loss after iteration 129000: 0.3668266244267602\n",
      "two-layer Loss after iteration 130000: 0.3661064065660162\n",
      "two-layer Loss after iteration 131000: 0.36544383983358164\n",
      "two-layer Loss after iteration 132000: 0.3648326304371824\n",
      "two-layer Loss after iteration 133000: 0.36413366211084175\n",
      "two-layer Loss after iteration 134000: 0.36345154480724867\n",
      "two-layer Loss after iteration 135000: 0.362767683789957\n",
      "two-layer Loss after iteration 136000: 0.3621440138830444\n",
      "two-layer Loss after iteration 137000: 0.35983172739147373\n",
      "two-layer Loss after iteration 138000: 0.3582636310361023\n",
      "two-layer Loss after iteration 139000: 0.35720737697776395\n",
      "two-layer Loss after iteration 140000: 0.3545555539644026\n",
      "two-layer Loss after iteration 141000: 0.3533892087551141\n",
      "two-layer Loss after iteration 142000: 0.3524576821155031\n",
      "two-layer Loss after iteration 143000: 0.3504991301811007\n",
      "two-layer Loss after iteration 144000: 0.3494130186905645\n",
      "two-layer Loss after iteration 145000: 0.348576954695925\n",
      "two-layer Loss after iteration 146000: 0.3479149965215091\n",
      "two-layer Loss after iteration 147000: 0.34737448768789214\n",
      "two-layer Loss after iteration 148000: 0.34691595514676027\n",
      "two-layer Loss after iteration 149000: 0.34652294468808426\n",
      "two-layer Loss after iteration 150000: 0.3461794316865978\n",
      "two-layer Loss after iteration 151000: 0.3453996933701345\n",
      "two-layer Loss after iteration 152000: 0.34502386232409993\n",
      "two-layer Loss after iteration 153000: 0.34473192900407096\n",
      "two-layer Loss after iteration 154000: 0.34446264777983376\n",
      "two-layer Loss after iteration 155000: 0.34422463812264364\n",
      "two-layer Loss after iteration 156000: 0.34401276352176846\n",
      "two-layer Loss after iteration 157000: 0.34382319959324126\n",
      "two-layer Loss after iteration 158000: 0.34365275350802754\n",
      "two-layer Loss after iteration 159000: 0.34349902476587574\n",
      "two-layer Loss after iteration 160000: 0.34335968132374384\n",
      "two-layer Loss after iteration 161000: 0.3432321979066687\n",
      "two-layer Loss after iteration 162000: 0.34311532598672123\n",
      "two-layer Loss after iteration 163000: 0.34300797727458404\n",
      "two-layer Loss after iteration 164000: 0.3429092171895824\n",
      "two-layer Loss after iteration 165000: 0.34281804126050947\n",
      "two-layer Loss after iteration 166000: 0.34273382997570345\n",
      "two-layer Loss after iteration 167000: 0.34265588074187336\n",
      "two-layer Loss after iteration 168000: 0.3425835578414871\n",
      "two-layer Loss after iteration 169000: 0.3425156164467251\n",
      "two-layer Loss after iteration 170000: 0.34244440812761695\n",
      "two-layer Loss after iteration 171000: 0.34237811074399\n",
      "two-layer Loss after iteration 172000: 0.3423162422186705\n",
      "two-layer Loss after iteration 173000: 0.3422584180562937\n",
      "two-layer Loss after iteration 174000: 0.34220427627752237\n",
      "two-layer Loss after iteration 175000: 0.3421535697789317\n",
      "two-layer Loss after iteration 176000: 0.34210624279385865\n",
      "two-layer Loss after iteration 177000: 0.3420617792065034\n",
      "two-layer Loss after iteration 178000: 0.342019911430977\n",
      "two-layer Loss after iteration 179000: 0.3419438175554999\n",
      "two-layer Loss after iteration 180000: 0.3418593451057105\n",
      "two-layer Loss after iteration 181000: 0.34178916505561713\n",
      "two-layer Loss after iteration 182000: 0.3417266143439217\n",
      "two-layer Loss after iteration 183000: 0.34167506305134127\n",
      "two-layer Loss after iteration 184000: 0.3416317596394711\n",
      "two-layer Loss after iteration 185000: 0.34159106943033496\n",
      "two-layer Loss after iteration 186000: 0.3415423359171858\n",
      "two-layer Loss after iteration 187000: 0.34149049974145884\n",
      "two-layer Loss after iteration 188000: 0.3414435966944236\n",
      "two-layer Loss after iteration 189000: 0.34140060407981065\n",
      "two-layer Loss after iteration 190000: 0.34136076091087725\n",
      "two-layer Loss after iteration 191000: 0.33933389519703905\n",
      "two-layer Loss after iteration 192000: 0.33893855118810706\n",
      "two-layer Loss after iteration 193000: 0.33872095097723093\n",
      "two-layer Loss after iteration 194000: 0.33858544398414536\n",
      "two-layer Loss after iteration 195000: 0.3384912974145307\n",
      "two-layer Loss after iteration 196000: 0.33818936896829865\n",
      "two-layer Loss after iteration 197000: 0.3380774983372789\n",
      "two-layer Loss after iteration 198000: 0.33799431714191924\n",
      "two-layer Loss after iteration 199000: 0.3379076749869715\n",
      "two-layer Loss after iteration 200000: 0.33781987617228515\n",
      "two-layer Loss after iteration 201000: 0.33774159615237903\n",
      "two-layer Loss after iteration 202000: 0.33767011467669816\n",
      "two-layer Loss after iteration 203000: 0.3376038054970695\n",
      "two-layer Loss after iteration 204000: 0.33754167051012735\n",
      "two-layer Loss after iteration 205000: 0.3374843241055546\n",
      "two-layer Loss after iteration 206000: 0.3374413347003184\n",
      "two-layer Loss after iteration 207000: 0.3374149705549629\n",
      "7.812956696286289e-05 0.3374413347003184 0.3374149705549629\n",
      "two-layer Loss after iteration 0: 1382.638535085395\n",
      "two-layer Loss after iteration 1000: 24.86872363486098\n",
      "two-layer Loss after iteration 2000: 9.117206209328804\n",
      "two-layer Loss after iteration 3000: 5.786566415219622\n",
      "two-layer Loss after iteration 4000: 4.586477774676282\n",
      "two-layer Loss after iteration 5000: 3.7899484779305648\n",
      "two-layer Loss after iteration 6000: 3.069544927022678\n",
      "two-layer Loss after iteration 7000: 2.5100709233263157\n",
      "two-layer Loss after iteration 8000: 2.215600313096832\n",
      "two-layer Loss after iteration 9000: 1.996385528306513\n",
      "two-layer Loss after iteration 10000: 1.838126474301443\n",
      "two-layer Loss after iteration 11000: 1.7249513233945744\n",
      "two-layer Loss after iteration 12000: 1.624309118498653\n",
      "two-layer Loss after iteration 13000: 1.5462794013986285\n",
      "two-layer Loss after iteration 14000: 1.4942884647293402\n",
      "two-layer Loss after iteration 15000: 1.454115002438031\n",
      "two-layer Loss after iteration 16000: 1.4260284754779173\n",
      "two-layer Loss after iteration 17000: 1.4070448720633582\n",
      "two-layer Loss after iteration 18000: 1.380176600957484\n",
      "two-layer Loss after iteration 19000: 1.363879377679652\n",
      "two-layer Loss after iteration 20000: 1.350513108863397\n",
      "two-layer Loss after iteration 21000: 1.337784424858695\n",
      "two-layer Loss after iteration 22000: 1.3279545660548033\n",
      "two-layer Loss after iteration 23000: 1.3149459028633017\n",
      "two-layer Loss after iteration 24000: 1.3072482410844797\n",
      "two-layer Loss after iteration 25000: 1.30085454005422\n",
      "two-layer Loss after iteration 26000: 1.295045396277289\n",
      "two-layer Loss after iteration 27000: 1.2897214673989528\n",
      "two-layer Loss after iteration 28000: 1.284984982578879\n",
      "two-layer Loss after iteration 29000: 1.2805734153783672\n",
      "two-layer Loss after iteration 30000: 1.2766177058284238\n",
      "two-layer Loss after iteration 31000: 1.273015894910239\n",
      "two-layer Loss after iteration 32000: 1.2625982131714204\n",
      "two-layer Loss after iteration 33000: 1.2576571534235\n",
      "two-layer Loss after iteration 34000: 1.2539227382670062\n",
      "two-layer Loss after iteration 35000: 1.2507250239619156\n",
      "two-layer Loss after iteration 36000: 1.2478867900127115\n",
      "two-layer Loss after iteration 37000: 1.2453016036676878\n",
      "two-layer Loss after iteration 38000: 1.2429272526518917\n",
      "two-layer Loss after iteration 39000: 1.2406456418123843\n",
      "two-layer Loss after iteration 40000: 1.2372390412383714\n",
      "two-layer Loss after iteration 41000: 1.2349519867455654\n",
      "two-layer Loss after iteration 42000: 1.222207271775172\n",
      "two-layer Loss after iteration 43000: 1.2099300042532317\n",
      "two-layer Loss after iteration 44000: 1.2060682466764143\n",
      "two-layer Loss after iteration 45000: 1.203405476299329\n",
      "two-layer Loss after iteration 46000: 1.2004791843765579\n",
      "two-layer Loss after iteration 47000: 1.1979553082161263\n",
      "two-layer Loss after iteration 48000: 1.1957874918160898\n",
      "two-layer Loss after iteration 49000: 1.194019201657734\n",
      "two-layer Loss after iteration 50000: 1.1896826309034305\n",
      "two-layer Loss after iteration 51000: 1.1878897893598384\n",
      "two-layer Loss after iteration 52000: 1.186356295380883\n",
      "two-layer Loss after iteration 53000: 1.185022555565212\n",
      "two-layer Loss after iteration 54000: 1.183838706382367\n",
      "two-layer Loss after iteration 55000: 1.1824764703866415\n",
      "two-layer Loss after iteration 56000: 1.1795274653585395\n",
      "two-layer Loss after iteration 57000: 1.1778017499641866\n",
      "two-layer Loss after iteration 58000: 1.1756736537565176\n",
      "two-layer Loss after iteration 59000: 1.1738197770161065\n",
      "two-layer Loss after iteration 60000: 1.1701732779493803\n",
      "two-layer Loss after iteration 61000: 1.1678652785166732\n",
      "two-layer Loss after iteration 62000: 1.1660324589927262\n",
      "two-layer Loss after iteration 63000: 1.1645100220870943\n",
      "two-layer Loss after iteration 64000: 1.1633169471297542\n",
      "two-layer Loss after iteration 65000: 1.1623819352237559\n",
      "two-layer Loss after iteration 66000: 1.1611646865325167\n",
      "two-layer Loss after iteration 67000: 1.1603048422889106\n",
      "two-layer Loss after iteration 68000: 1.1595825342230857\n",
      "two-layer Loss after iteration 69000: 1.1589466369409376\n",
      "two-layer Loss after iteration 70000: 1.158384235882834\n",
      "two-layer Loss after iteration 71000: 1.1578758896723274\n",
      "two-layer Loss after iteration 72000: 1.1574091261072028\n",
      "two-layer Loss after iteration 73000: 1.1570578086994003\n",
      "two-layer Loss after iteration 74000: 1.1567369797768778\n",
      "two-layer Loss after iteration 75000: 1.1564677479741237\n",
      "two-layer Loss after iteration 76000: 1.1562138374462805\n",
      "two-layer Loss after iteration 77000: 1.155980889344834\n",
      "two-layer Loss after iteration 78000: 1.1557664903722877\n",
      "two-layer Loss after iteration 79000: 1.1555591892429857\n",
      "two-layer Loss after iteration 80000: 1.1553673604259056\n",
      "two-layer Loss after iteration 81000: 1.155184000295804\n",
      "two-layer Loss after iteration 82000: 1.1550137040892972\n",
      "two-layer Loss after iteration 83000: 1.15484716948927\n",
      "two-layer Loss after iteration 84000: 1.1546926224131109\n",
      "two-layer Loss after iteration 85000: 1.1545393709610028\n",
      "two-layer Loss after iteration 86000: 1.154398909665837\n",
      "two-layer Loss after iteration 87000: 1.1542637923262111\n",
      "two-layer Loss after iteration 88000: 1.1541308935743544\n",
      "two-layer Loss after iteration 89000: 1.1540054710654177\n",
      "two-layer Loss after iteration 90000: 1.153881804822211\n",
      "two-layer Loss after iteration 91000: 1.1537648418982585\n",
      "two-layer Loss after iteration 92000: 1.153652120203471\n",
      "9.769902036702609e-05 1.1537648418982585 1.153652120203471\n",
      "two-layer Loss after iteration 0: 1445.1480579632046\n",
      "two-layer Loss after iteration 1000: 24.612656375807557\n",
      "two-layer Loss after iteration 2000: 9.426054523114084\n",
      "two-layer Loss after iteration 3000: 6.329772844299718\n",
      "two-layer Loss after iteration 4000: 5.095715624356698\n",
      "two-layer Loss after iteration 5000: 4.241520346327349\n",
      "two-layer Loss after iteration 6000: 3.5169167486117563\n",
      "two-layer Loss after iteration 7000: 2.8303393932255263\n",
      "two-layer Loss after iteration 8000: 2.2282495950904417\n",
      "two-layer Loss after iteration 9000: 1.8482710607951134\n",
      "two-layer Loss after iteration 10000: 1.3055656674017062\n",
      "two-layer Loss after iteration 11000: 0.9617128485386703\n",
      "two-layer Loss after iteration 12000: 0.8049522624756332\n",
      "two-layer Loss after iteration 13000: 0.6249797049792184\n",
      "two-layer Loss after iteration 14000: 0.5630361464686247\n",
      "two-layer Loss after iteration 15000: 0.525607589414554\n",
      "two-layer Loss after iteration 16000: 0.49838431686364126\n",
      "two-layer Loss after iteration 17000: 0.47868957303934\n",
      "two-layer Loss after iteration 18000: 0.46349808706485457\n",
      "two-layer Loss after iteration 19000: 0.44953332588190836\n",
      "two-layer Loss after iteration 20000: 0.43823226589533154\n",
      "two-layer Loss after iteration 21000: 0.42895254887194945\n",
      "two-layer Loss after iteration 22000: 0.420915582990491\n",
      "two-layer Loss after iteration 23000: 0.41299559973772587\n",
      "two-layer Loss after iteration 24000: 0.40717833004733495\n",
      "two-layer Loss after iteration 25000: 0.4019173318568359\n",
      "two-layer Loss after iteration 26000: 0.39687819226376764\n",
      "two-layer Loss after iteration 27000: 0.3925729878724729\n",
      "two-layer Loss after iteration 28000: 0.3888404436210875\n",
      "two-layer Loss after iteration 29000: 0.38085061578432944\n",
      "two-layer Loss after iteration 30000: 0.37601904074398584\n",
      "two-layer Loss after iteration 31000: 0.3653347395539775\n",
      "two-layer Loss after iteration 32000: 0.3611447054083182\n",
      "two-layer Loss after iteration 33000: 0.3581478437889839\n",
      "two-layer Loss after iteration 34000: 0.3556838853916405\n",
      "two-layer Loss after iteration 35000: 0.35357953996367675\n",
      "two-layer Loss after iteration 36000: 0.3518247414878561\n",
      "two-layer Loss after iteration 37000: 0.3501391958588288\n",
      "two-layer Loss after iteration 38000: 0.34860079009876294\n",
      "two-layer Loss after iteration 39000: 0.34723492997676747\n",
      "two-layer Loss after iteration 40000: 0.3460168455498819\n",
      "two-layer Loss after iteration 41000: 0.34475371265043414\n",
      "two-layer Loss after iteration 42000: 0.343669496032001\n",
      "two-layer Loss after iteration 43000: 0.342693406139166\n",
      "two-layer Loss after iteration 44000: 0.33967674417823596\n",
      "two-layer Loss after iteration 45000: 0.33825252685381657\n",
      "two-layer Loss after iteration 46000: 0.337175149475369\n",
      "two-layer Loss after iteration 47000: 0.33624383594969925\n",
      "two-layer Loss after iteration 48000: 0.3354315850996316\n",
      "two-layer Loss after iteration 49000: 0.3347261895351007\n",
      "two-layer Loss after iteration 50000: 0.33410269614747384\n",
      "two-layer Loss after iteration 51000: 0.3335461661763401\n",
      "two-layer Loss after iteration 52000: 0.33305873240160216\n",
      "two-layer Loss after iteration 53000: 0.3326150677758228\n",
      "two-layer Loss after iteration 54000: 0.33222160622283164\n",
      "two-layer Loss after iteration 55000: 0.3318620442370775\n",
      "two-layer Loss after iteration 56000: 0.3313965359429673\n",
      "two-layer Loss after iteration 57000: 0.33095916803558423\n",
      "two-layer Loss after iteration 58000: 0.3305793666184968\n",
      "two-layer Loss after iteration 59000: 0.33024622196083103\n",
      "two-layer Loss after iteration 60000: 0.32994928508945554\n",
      "two-layer Loss after iteration 61000: 0.32968354638782377\n",
      "two-layer Loss after iteration 62000: 0.3294459664270482\n",
      "two-layer Loss after iteration 63000: 0.32922987182056557\n",
      "two-layer Loss after iteration 64000: 0.32903194079180414\n",
      "two-layer Loss after iteration 65000: 0.32885030015292593\n",
      "two-layer Loss after iteration 66000: 0.32868253706684647\n",
      "two-layer Loss after iteration 67000: 0.32852717098873596\n",
      "two-layer Loss after iteration 68000: 0.3283802699636975\n",
      "two-layer Loss after iteration 69000: 0.32824374828482633\n",
      "two-layer Loss after iteration 70000: 0.3281167018522545\n",
      "two-layer Loss after iteration 71000: 0.32799711568149176\n",
      "two-layer Loss after iteration 72000: 0.3278813773011407\n",
      "two-layer Loss after iteration 73000: 0.3277731503149256\n",
      "two-layer Loss after iteration 74000: 0.32767233078661967\n",
      "two-layer Loss after iteration 75000: 0.32757512498153707\n",
      "two-layer Loss after iteration 76000: 0.3274839190536832\n",
      "two-layer Loss after iteration 77000: 0.32739183263257077\n",
      "two-layer Loss after iteration 78000: 0.3272557618683509\n",
      "two-layer Loss after iteration 79000: 0.3271104891524519\n",
      "two-layer Loss after iteration 80000: 0.32698759505478403\n",
      "two-layer Loss after iteration 81000: 0.3268733135097592\n",
      "two-layer Loss after iteration 82000: 0.3267747923310473\n",
      "two-layer Loss after iteration 83000: 0.32668314712532437\n",
      "two-layer Loss after iteration 84000: 0.3266038570612062\n",
      "two-layer Loss after iteration 85000: 0.32652611219958133\n",
      "two-layer Loss after iteration 86000: 0.32645581387501826\n",
      "two-layer Loss after iteration 87000: 0.32638804236706387\n",
      "two-layer Loss after iteration 88000: 0.3263273418783043\n",
      "two-layer Loss after iteration 89000: 0.32626723325287543\n",
      "two-layer Loss after iteration 90000: 0.3262122469613629\n",
      "two-layer Loss after iteration 91000: 0.32615958049909866\n",
      "two-layer Loss after iteration 92000: 0.3261094194684439\n",
      "two-layer Loss after iteration 93000: 0.3260584333677647\n",
      "two-layer Loss after iteration 94000: 0.3260125826185449\n",
      "two-layer Loss after iteration 95000: 0.32596558013367866\n",
      "two-layer Loss after iteration 96000: 0.32592299552316706\n",
      "two-layer Loss after iteration 97000: 0.32588264075527396\n",
      "two-layer Loss after iteration 98000: 0.3258410226474175\n",
      "two-layer Loss after iteration 99000: 0.32580422506283363\n",
      "two-layer Loss after iteration 100000: 0.3257681619558291\n",
      "two-layer Loss after iteration 101000: 0.32573152734378374\n",
      "two-layer Loss after iteration 102000: 0.32569626039241517\n",
      "two-layer Loss after iteration 103000: 0.32566314381461586\n",
      "two-layer Loss after iteration 104000: 0.32562973797738504\n",
      "two-layer Loss after iteration 105000: 0.32559802073406613\n",
      "9.740278488050744e-05 0.32562973797738504 0.32559802073406613\n",
      "two-layer Loss after iteration 0: 1656.4951915187678\n",
      "two-layer Loss after iteration 1000: 22.703759491679246\n",
      "two-layer Loss after iteration 2000: 8.782399340590429\n",
      "two-layer Loss after iteration 3000: 6.014566606267832\n",
      "two-layer Loss after iteration 4000: 4.62304493099699\n",
      "two-layer Loss after iteration 5000: 3.9473118993815377\n",
      "two-layer Loss after iteration 6000: 3.5014975692257577\n",
      "two-layer Loss after iteration 7000: 3.143523876958647\n",
      "two-layer Loss after iteration 8000: 2.71999568257086\n",
      "two-layer Loss after iteration 9000: 1.8751896897998162\n",
      "two-layer Loss after iteration 10000: 1.6474786502324898\n",
      "two-layer Loss after iteration 11000: 1.4909476552307281\n",
      "two-layer Loss after iteration 12000: 1.377994087739126\n",
      "two-layer Loss after iteration 13000: 1.2761599196610178\n",
      "two-layer Loss after iteration 14000: 1.1937273610979713\n",
      "two-layer Loss after iteration 15000: 1.1277394521648583\n",
      "two-layer Loss after iteration 16000: 1.0746662321220544\n",
      "two-layer Loss after iteration 17000: 1.0287928517985616\n",
      "two-layer Loss after iteration 18000: 0.9894183189608091\n",
      "two-layer Loss after iteration 19000: 0.958929642111545\n",
      "two-layer Loss after iteration 20000: 0.9342385269913178\n",
      "two-layer Loss after iteration 21000: 0.912807955426702\n",
      "two-layer Loss after iteration 22000: 0.8907309617703967\n",
      "two-layer Loss after iteration 23000: 0.8689451077972928\n",
      "two-layer Loss after iteration 24000: 0.8469853043951979\n",
      "two-layer Loss after iteration 25000: 0.8276821968160203\n",
      "two-layer Loss after iteration 26000: 0.8100579229105777\n",
      "two-layer Loss after iteration 27000: 0.7931226065169229\n",
      "two-layer Loss after iteration 28000: 0.7786192854853814\n",
      "two-layer Loss after iteration 29000: 0.7640503017048822\n",
      "two-layer Loss after iteration 30000: 0.752057143857312\n",
      "two-layer Loss after iteration 31000: 0.7414262073198677\n",
      "two-layer Loss after iteration 32000: 0.7261010741012383\n",
      "two-layer Loss after iteration 33000: 0.7156131139228484\n",
      "two-layer Loss after iteration 34000: 0.7064098852003314\n",
      "two-layer Loss after iteration 35000: 0.6974793971582371\n",
      "two-layer Loss after iteration 36000: 0.6898135901724365\n",
      "two-layer Loss after iteration 37000: 0.683047118650832\n",
      "two-layer Loss after iteration 38000: 0.6766782381411518\n",
      "two-layer Loss after iteration 39000: 0.6709254831975621\n",
      "two-layer Loss after iteration 40000: 0.6646067057962057\n",
      "two-layer Loss after iteration 41000: 0.6530883600326974\n",
      "two-layer Loss after iteration 42000: 0.628045911312049\n",
      "two-layer Loss after iteration 43000: 0.6108931767801924\n",
      "two-layer Loss after iteration 44000: 0.5986011110093397\n",
      "two-layer Loss after iteration 45000: 0.5865223230332479\n",
      "two-layer Loss after iteration 46000: 0.5748895142937156\n",
      "two-layer Loss after iteration 47000: 0.5644802082466462\n",
      "two-layer Loss after iteration 48000: 0.5529656040083686\n",
      "two-layer Loss after iteration 49000: 0.5437171970548362\n",
      "two-layer Loss after iteration 50000: 0.5363876115913981\n",
      "two-layer Loss after iteration 51000: 0.5285212597974358\n",
      "two-layer Loss after iteration 52000: 0.5225005269415075\n",
      "two-layer Loss after iteration 53000: 0.5175713076865576\n",
      "two-layer Loss after iteration 54000: 0.513225772773981\n",
      "two-layer Loss after iteration 55000: 0.5081296400062658\n",
      "two-layer Loss after iteration 56000: 0.5042792726709143\n",
      "two-layer Loss after iteration 57000: 0.5009193087387983\n",
      "two-layer Loss after iteration 58000: 0.49760212059194686\n",
      "two-layer Loss after iteration 59000: 0.49400168901738095\n",
      "two-layer Loss after iteration 60000: 0.49049589341815597\n",
      "two-layer Loss after iteration 61000: 0.4878422898950587\n",
      "two-layer Loss after iteration 62000: 0.48567644331310944\n",
      "two-layer Loss after iteration 63000: 0.48295307788640257\n",
      "two-layer Loss after iteration 64000: 0.48110906294877537\n",
      "two-layer Loss after iteration 65000: 0.4794787700146061\n",
      "two-layer Loss after iteration 66000: 0.4773561171483135\n",
      "two-layer Loss after iteration 67000: 0.4756994766857163\n",
      "two-layer Loss after iteration 68000: 0.4744421165295269\n",
      "two-layer Loss after iteration 69000: 0.4733305836593149\n",
      "two-layer Loss after iteration 70000: 0.4723447300386627\n",
      "two-layer Loss after iteration 71000: 0.4714710519941017\n",
      "two-layer Loss after iteration 72000: 0.47068056790035473\n",
      "two-layer Loss after iteration 73000: 0.4699616328640929\n",
      "two-layer Loss after iteration 74000: 0.4692974549792795\n",
      "two-layer Loss after iteration 75000: 0.4686358244314599\n",
      "two-layer Loss after iteration 76000: 0.4677454946029732\n",
      "two-layer Loss after iteration 77000: 0.4670211239042991\n",
      "two-layer Loss after iteration 78000: 0.46636745777327754\n",
      "two-layer Loss after iteration 79000: 0.4656360776221884\n",
      "two-layer Loss after iteration 80000: 0.46495268832012376\n",
      "two-layer Loss after iteration 81000: 0.46433037255677456\n",
      "two-layer Loss after iteration 82000: 0.46376565869290404\n",
      "two-layer Loss after iteration 83000: 0.4629727316448878\n",
      "two-layer Loss after iteration 84000: 0.4624565005666368\n",
      "two-layer Loss after iteration 85000: 0.4602409988412087\n",
      "two-layer Loss after iteration 86000: 0.4591733799091774\n",
      "two-layer Loss after iteration 87000: 0.45806115439129724\n",
      "two-layer Loss after iteration 88000: 0.45684068548155493\n",
      "two-layer Loss after iteration 89000: 0.4558482007892514\n",
      "two-layer Loss after iteration 90000: 0.4519624754273401\n",
      "two-layer Loss after iteration 91000: 0.4495658371121361\n",
      "two-layer Loss after iteration 92000: 0.4480675917815947\n",
      "two-layer Loss after iteration 93000: 0.44688959636092135\n",
      "two-layer Loss after iteration 94000: 0.4458744316122688\n",
      "two-layer Loss after iteration 95000: 0.4447639446255658\n",
      "two-layer Loss after iteration 96000: 0.4438181616427351\n",
      "two-layer Loss after iteration 97000: 0.44297901508951865\n",
      "two-layer Loss after iteration 98000: 0.44219107896980325\n",
      "two-layer Loss after iteration 99000: 0.44145151676493344\n",
      "two-layer Loss after iteration 100000: 0.4407664385561093\n",
      "two-layer Loss after iteration 101000: 0.44014511836408965\n",
      "two-layer Loss after iteration 102000: 0.43960276412195326\n",
      "two-layer Loss after iteration 103000: 0.43912145424070304\n",
      "two-layer Loss after iteration 104000: 0.43868463753449244\n",
      "two-layer Loss after iteration 105000: 0.43828270506082995\n",
      "two-layer Loss after iteration 106000: 0.4379219416808967\n",
      "two-layer Loss after iteration 107000: 0.43759395374265725\n",
      "two-layer Loss after iteration 108000: 0.4372959257476268\n",
      "two-layer Loss after iteration 109000: 0.43701987874812015\n",
      "two-layer Loss after iteration 110000: 0.43425032760364135\n",
      "two-layer Loss after iteration 111000: 0.43380841981069923\n",
      "two-layer Loss after iteration 112000: 0.4334845567676545\n",
      "two-layer Loss after iteration 113000: 0.43319726647531437\n",
      "two-layer Loss after iteration 114000: 0.43293826227374255\n",
      "two-layer Loss after iteration 115000: 0.4327005092454558\n",
      "two-layer Loss after iteration 116000: 0.43248002798985125\n",
      "two-layer Loss after iteration 117000: 0.4322712564569204\n",
      "two-layer Loss after iteration 118000: 0.43206887746597067\n",
      "two-layer Loss after iteration 119000: 0.4318795278705008\n",
      "two-layer Loss after iteration 120000: 0.4316972373878084\n",
      "two-layer Loss after iteration 121000: 0.43151732340571636\n",
      "two-layer Loss after iteration 122000: 0.43134486421365664\n",
      "two-layer Loss after iteration 123000: 0.4311791710687427\n",
      "two-layer Loss after iteration 124000: 0.43101793791686166\n",
      "two-layer Loss after iteration 125000: 0.43085929816783836\n",
      "two-layer Loss after iteration 126000: 0.4307070735322591\n",
      "two-layer Loss after iteration 127000: 0.4305543916316556\n",
      "two-layer Loss after iteration 128000: 0.43040917891936054\n",
      "two-layer Loss after iteration 129000: 0.4302651957017031\n",
      "two-layer Loss after iteration 130000: 0.4301254017133397\n",
      "two-layer Loss after iteration 131000: 0.4299868901092016\n",
      "two-layer Loss after iteration 132000: 0.4298529206583891\n",
      "two-layer Loss after iteration 133000: 0.4297208264682049\n",
      "two-layer Loss after iteration 134000: 0.4295901998207913\n",
      "two-layer Loss after iteration 135000: 0.4294641358369191\n",
      "two-layer Loss after iteration 136000: 0.42933996795389895\n",
      "two-layer Loss after iteration 137000: 0.4292160712729152\n",
      "two-layer Loss after iteration 138000: 0.42909502134375416\n",
      "two-layer Loss after iteration 139000: 0.4289795751369405\n",
      "two-layer Loss after iteration 140000: 0.42886290563765456\n",
      "two-layer Loss after iteration 141000: 0.42874714816467363\n",
      "two-layer Loss after iteration 142000: 0.42863576102976647\n",
      "two-layer Loss after iteration 143000: 0.428525615476383\n",
      "two-layer Loss after iteration 144000: 0.4284159670643543\n",
      "two-layer Loss after iteration 145000: 0.4283097612175744\n",
      "two-layer Loss after iteration 146000: 0.4282028466367864\n",
      "two-layer Loss after iteration 147000: 0.42810056317927153\n",
      "two-layer Loss after iteration 148000: 0.4279971257046044\n",
      "two-layer Loss after iteration 149000: 0.42789684274562306\n",
      "two-layer Loss after iteration 150000: 0.42779754459120073\n",
      "two-layer Loss after iteration 151000: 0.4276963610933779\n",
      "two-layer Loss after iteration 152000: 0.4276027990054326\n",
      "two-layer Loss after iteration 153000: 0.4275074400902117\n",
      "two-layer Loss after iteration 154000: 0.4274132503351087\n",
      "two-layer Loss after iteration 155000: 0.4273213042195611\n",
      "two-layer Loss after iteration 156000: 0.4272329989165166\n",
      "two-layer Loss after iteration 157000: 0.42713804910525904\n",
      "two-layer Loss after iteration 158000: 0.427051728874142\n",
      "two-layer Loss after iteration 159000: 0.4269642970447584\n",
      "two-layer Loss after iteration 160000: 0.42687554572349906\n",
      "two-layer Loss after iteration 161000: 0.4267950936139706\n",
      "two-layer Loss after iteration 162000: 0.42670799453444513\n",
      "two-layer Loss after iteration 163000: 0.4266243259421297\n",
      "two-layer Loss after iteration 164000: 0.42654260521819426\n",
      "two-layer Loss after iteration 165000: 0.42646155280142584\n",
      "two-layer Loss after iteration 166000: 0.42638249572344106\n",
      "two-layer Loss after iteration 167000: 0.4263050901274074\n",
      "two-layer Loss after iteration 168000: 0.4262273395805998\n",
      "two-layer Loss after iteration 169000: 0.42614980831052446\n",
      "two-layer Loss after iteration 170000: 0.4260716364818792\n",
      "two-layer Loss after iteration 171000: 0.42599933803620077\n",
      "two-layer Loss after iteration 172000: 0.4259256582957009\n",
      "two-layer Loss after iteration 173000: 0.4258501088957161\n",
      "two-layer Loss after iteration 174000: 0.425778565772893\n",
      "two-layer Loss after iteration 175000: 0.42570673137984866\n",
      "two-layer Loss after iteration 176000: 0.4256394024818346\n",
      "two-layer Loss after iteration 177000: 0.42556826301360073\n",
      "two-layer Loss after iteration 178000: 0.4255010298010365\n",
      "two-layer Loss after iteration 179000: 0.4254322417979377\n",
      "two-layer Loss after iteration 180000: 0.4253711374413326\n",
      "two-layer Loss after iteration 181000: 0.42530220966789567\n",
      "two-layer Loss after iteration 182000: 0.4252343428933807\n",
      "two-layer Loss after iteration 183000: 0.425169987186066\n",
      "two-layer Loss after iteration 184000: 0.4251067845675987\n",
      "two-layer Loss after iteration 185000: 0.42504672879263367\n",
      "two-layer Loss after iteration 186000: 0.424980189654044\n",
      "two-layer Loss after iteration 187000: 0.42491968132850244\n",
      "two-layer Loss after iteration 188000: 0.42486175295205514\n",
      "two-layer Loss after iteration 189000: 0.4248025787244094\n",
      "two-layer Loss after iteration 190000: 0.4247395322636482\n",
      "two-layer Loss after iteration 191000: 0.4246817501762852\n",
      "two-layer Loss after iteration 192000: 0.42462348224539875\n",
      "two-layer Loss after iteration 193000: 0.42457032793647814\n",
      "two-layer Loss after iteration 194000: 0.42451294376207765\n",
      "two-layer Loss after iteration 195000: 0.42445843203297157\n",
      "two-layer Loss after iteration 196000: 0.4243991347942688\n",
      "two-layer Loss after iteration 197000: 0.4243453852530681\n",
      "two-layer Loss after iteration 198000: 0.42429068964504774\n",
      "two-layer Loss after iteration 199000: 0.4242384618514667\n",
      "two-layer Loss after iteration 200000: 0.4241847781668199\n",
      "two-layer Loss after iteration 201000: 0.4241321148988594\n",
      "two-layer Loss after iteration 202000: 0.4240813890138546\n",
      "two-layer Loss after iteration 203000: 0.424029938834006\n",
      "two-layer Loss after iteration 204000: 0.4239818095449516\n",
      "two-layer Loss after iteration 205000: 0.4239311275106722\n",
      "two-layer Loss after iteration 206000: 0.4238823747994665\n",
      "two-layer Loss after iteration 207000: 0.4238348660288191\n",
      "two-layer Loss after iteration 208000: 0.42378391565692525\n",
      "two-layer Loss after iteration 209000: 0.42373948895107144\n",
      "two-layer Loss after iteration 210000: 0.42369197125794167\n",
      "two-layer Loss after iteration 211000: 0.4236462219520018\n",
      "two-layer Loss after iteration 212000: 0.42359736236607004\n",
      "two-layer Loss after iteration 213000: 0.42355166419794177\n",
      "two-layer Loss after iteration 214000: 0.42350943613790265\n",
      "9.96999034795034e-05 0.42355166419794177 0.42350943613790265\n",
      "two-layer Loss after iteration 0: 1607.1872927920024\n",
      "two-layer Loss after iteration 1000: 23.340512664511884\n",
      "two-layer Loss after iteration 2000: 9.324223229144858\n",
      "two-layer Loss after iteration 3000: 7.0202998202022835\n",
      "two-layer Loss after iteration 4000: 5.574139944527314\n",
      "two-layer Loss after iteration 5000: 4.297483675941303\n",
      "two-layer Loss after iteration 6000: 3.2692235044600846\n",
      "two-layer Loss after iteration 7000: 2.669034236428873\n",
      "two-layer Loss after iteration 8000: 2.2829937588146123\n",
      "two-layer Loss after iteration 9000: 1.9525390622543692\n",
      "two-layer Loss after iteration 10000: 1.636107765895764\n",
      "two-layer Loss after iteration 11000: 1.3133202434684679\n",
      "two-layer Loss after iteration 12000: 1.1007214179122407\n",
      "two-layer Loss after iteration 13000: 0.9688568937028906\n",
      "two-layer Loss after iteration 14000: 0.8825790269417982\n",
      "two-layer Loss after iteration 15000: 0.7596238254607097\n",
      "two-layer Loss after iteration 16000: 0.7010382598601395\n",
      "two-layer Loss after iteration 17000: 0.6571622575160733\n",
      "two-layer Loss after iteration 18000: 0.6286978649868472\n",
      "two-layer Loss after iteration 19000: 0.608583122588999\n",
      "two-layer Loss after iteration 20000: 0.5802180299989914\n",
      "two-layer Loss after iteration 21000: 0.5646072472812627\n",
      "two-layer Loss after iteration 22000: 0.549131064140308\n",
      "two-layer Loss after iteration 23000: 0.5371449525877154\n",
      "two-layer Loss after iteration 24000: 0.5248611825740404\n",
      "two-layer Loss after iteration 25000: 0.5069040899878722\n",
      "two-layer Loss after iteration 26000: 0.4964245425157928\n",
      "two-layer Loss after iteration 27000: 0.48466014623453924\n",
      "two-layer Loss after iteration 28000: 0.4739303676415309\n",
      "two-layer Loss after iteration 29000: 0.46620489470732374\n",
      "two-layer Loss after iteration 30000: 0.45641542508791744\n",
      "two-layer Loss after iteration 31000: 0.4498274677690272\n",
      "two-layer Loss after iteration 32000: 0.4443000434603045\n",
      "two-layer Loss after iteration 33000: 0.4373562567775875\n",
      "two-layer Loss after iteration 34000: 0.4324356970304885\n",
      "two-layer Loss after iteration 35000: 0.4279943293986547\n",
      "two-layer Loss after iteration 36000: 0.4246084914092445\n",
      "two-layer Loss after iteration 37000: 0.42191110849333996\n",
      "two-layer Loss after iteration 38000: 0.4176002289872007\n",
      "two-layer Loss after iteration 39000: 0.41542383965671864\n",
      "two-layer Loss after iteration 40000: 0.413629214064721\n",
      "two-layer Loss after iteration 41000: 0.411790570395679\n",
      "two-layer Loss after iteration 42000: 0.4099301131359923\n",
      "two-layer Loss after iteration 43000: 0.4083540695927237\n",
      "two-layer Loss after iteration 44000: 0.4069931999977764\n",
      "two-layer Loss after iteration 45000: 0.40581361755622025\n",
      "two-layer Loss after iteration 46000: 0.4036386125404742\n",
      "two-layer Loss after iteration 47000: 0.4020828587630015\n",
      "two-layer Loss after iteration 48000: 0.3994153070260592\n",
      "two-layer Loss after iteration 49000: 0.3978132416226135\n",
      "two-layer Loss after iteration 50000: 0.39647628664092166\n",
      "two-layer Loss after iteration 51000: 0.39536555898743597\n",
      "two-layer Loss after iteration 52000: 0.3944252769278889\n",
      "two-layer Loss after iteration 53000: 0.39361807044163516\n",
      "two-layer Loss after iteration 54000: 0.3929163912979407\n",
      "two-layer Loss after iteration 55000: 0.3923011329544218\n",
      "two-layer Loss after iteration 56000: 0.39175748725077003\n",
      "two-layer Loss after iteration 57000: 0.3912738465565697\n",
      "two-layer Loss after iteration 58000: 0.39084319148387636\n",
      "two-layer Loss after iteration 59000: 0.38995110497270413\n",
      "two-layer Loss after iteration 60000: 0.38947057999463186\n",
      "two-layer Loss after iteration 61000: 0.3890459849533712\n",
      "two-layer Loss after iteration 62000: 0.38866286140009204\n",
      "two-layer Loss after iteration 63000: 0.38831422185902387\n",
      "two-layer Loss after iteration 64000: 0.38799489098634216\n",
      "two-layer Loss after iteration 65000: 0.38770080546427393\n",
      "two-layer Loss after iteration 66000: 0.3874287156305181\n",
      "two-layer Loss after iteration 67000: 0.38718331679192663\n",
      "two-layer Loss after iteration 68000: 0.38696347030708367\n",
      "two-layer Loss after iteration 69000: 0.38675681592041433\n",
      "two-layer Loss after iteration 70000: 0.3865615508077997\n",
      "two-layer Loss after iteration 71000: 0.3863768162174255\n",
      "two-layer Loss after iteration 72000: 0.3862018869138305\n",
      "two-layer Loss after iteration 73000: 0.3860361011787753\n",
      "two-layer Loss after iteration 74000: 0.38587888111279517\n",
      "two-layer Loss after iteration 75000: 0.3857297087520068\n",
      "two-layer Loss after iteration 76000: 0.3855880950074841\n",
      "two-layer Loss after iteration 77000: 0.38545359831533954\n",
      "two-layer Loss after iteration 78000: 0.3853258226436249\n",
      "two-layer Loss after iteration 79000: 0.3852043833606484\n",
      "two-layer Loss after iteration 80000: 0.38508894092829543\n",
      "two-layer Loss after iteration 81000: 0.3831465516434422\n",
      "two-layer Loss after iteration 82000: 0.382519376869138\n",
      "two-layer Loss after iteration 83000: 0.38206546225986865\n",
      "two-layer Loss after iteration 84000: 0.38169077712448113\n",
      "two-layer Loss after iteration 85000: 0.3813726102479056\n",
      "two-layer Loss after iteration 86000: 0.38109624867818365\n",
      "two-layer Loss after iteration 87000: 0.38054735453581745\n",
      "two-layer Loss after iteration 88000: 0.37788719314111896\n",
      "two-layer Loss after iteration 89000: 0.37689877949600165\n",
      "two-layer Loss after iteration 90000: 0.3762594530544965\n",
      "two-layer Loss after iteration 91000: 0.3757850120652088\n",
      "two-layer Loss after iteration 92000: 0.3754107797854497\n",
      "two-layer Loss after iteration 93000: 0.3751024552083038\n",
      "two-layer Loss after iteration 94000: 0.3742370138907163\n",
      "two-layer Loss after iteration 95000: 0.3706250886046069\n",
      "two-layer Loss after iteration 96000: 0.3696342061828328\n",
      "two-layer Loss after iteration 97000: 0.3689310765468621\n",
      "two-layer Loss after iteration 98000: 0.3683891401322569\n",
      "two-layer Loss after iteration 99000: 0.3679568174496382\n",
      "two-layer Loss after iteration 100000: 0.3676034859489678\n",
      "two-layer Loss after iteration 101000: 0.367309305056069\n",
      "two-layer Loss after iteration 102000: 0.3670594823548339\n",
      "two-layer Loss after iteration 103000: 0.36684492160946003\n",
      "two-layer Loss after iteration 104000: 0.3666580282917064\n",
      "two-layer Loss after iteration 105000: 0.36646784016871137\n",
      "two-layer Loss after iteration 106000: 0.3662628408823794\n",
      "two-layer Loss after iteration 107000: 0.36608844053029\n",
      "two-layer Loss after iteration 108000: 0.3648764785339223\n",
      "two-layer Loss after iteration 109000: 0.3645309947992414\n",
      "two-layer Loss after iteration 110000: 0.3642751580494288\n",
      "two-layer Loss after iteration 111000: 0.36406013277114413\n",
      "two-layer Loss after iteration 112000: 0.3638756892391133\n",
      "two-layer Loss after iteration 113000: 0.36371303193975707\n",
      "two-layer Loss after iteration 114000: 0.36356824526820175\n",
      "two-layer Loss after iteration 115000: 0.3634370758857457\n",
      "two-layer Loss after iteration 116000: 0.3627244577888392\n",
      "two-layer Loss after iteration 117000: 0.36127152600293977\n",
      "two-layer Loss after iteration 118000: 0.3603893782345415\n",
      "two-layer Loss after iteration 119000: 0.35976047046320964\n",
      "two-layer Loss after iteration 120000: 0.3593166716761752\n",
      "two-layer Loss after iteration 121000: 0.3589757564367354\n",
      "two-layer Loss after iteration 122000: 0.3587023478626676\n",
      "two-layer Loss after iteration 123000: 0.3584762047532519\n",
      "two-layer Loss after iteration 124000: 0.3582839794200972\n",
      "two-layer Loss after iteration 125000: 0.35811741359152116\n",
      "two-layer Loss after iteration 126000: 0.3579700220169254\n",
      "two-layer Loss after iteration 127000: 0.3578385074090162\n",
      "two-layer Loss after iteration 128000: 0.3577193977677635\n",
      "two-layer Loss after iteration 129000: 0.35761065077492704\n",
      "two-layer Loss after iteration 130000: 0.357511155527699\n",
      "two-layer Loss after iteration 131000: 0.3574191494952268\n",
      "two-layer Loss after iteration 132000: 0.357333937797005\n",
      "two-layer Loss after iteration 133000: 0.3572545816782252\n",
      "two-layer Loss after iteration 134000: 0.35718072404839457\n",
      "two-layer Loss after iteration 135000: 0.3571119586152294\n",
      "two-layer Loss after iteration 136000: 0.35704740361833326\n",
      "two-layer Loss after iteration 137000: 0.35698662150023014\n",
      "two-layer Loss after iteration 138000: 0.3569298404483912\n",
      "two-layer Loss after iteration 139000: 0.3568767422087681\n",
      "two-layer Loss after iteration 140000: 0.3568261412987824\n",
      "two-layer Loss after iteration 141000: 0.35677862695115375\n",
      "two-layer Loss after iteration 142000: 0.35673405423631416\n",
      "two-layer Loss after iteration 143000: 0.35669222898062886\n",
      "two-layer Loss after iteration 144000: 0.3566525256086084\n",
      "two-layer Loss after iteration 145000: 0.3566152973624095\n",
      "two-layer Loss after iteration 146000: 0.35658008150224796\n",
      "9.875027914392132e-05 0.3566152973624095 0.35658008150224796\n",
      "two-layer Loss after iteration 0: 1368.807814831985\n",
      "two-layer Loss after iteration 1000: 24.700358431818522\n",
      "two-layer Loss after iteration 2000: 9.353013096111724\n",
      "two-layer Loss after iteration 3000: 6.736525545983578\n",
      "two-layer Loss after iteration 4000: 5.493377048759859\n",
      "two-layer Loss after iteration 5000: 4.757508390947433\n",
      "two-layer Loss after iteration 6000: 3.3720537263854213\n",
      "two-layer Loss after iteration 7000: 2.7040010685227815\n",
      "two-layer Loss after iteration 8000: 2.1132555499587413\n",
      "two-layer Loss after iteration 9000: 1.6479835865132695\n",
      "two-layer Loss after iteration 10000: 1.3911268913882637\n",
      "two-layer Loss after iteration 11000: 1.2811227668286496\n",
      "two-layer Loss after iteration 12000: 1.2162789567590546\n",
      "two-layer Loss after iteration 13000: 1.1648722149509902\n",
      "two-layer Loss after iteration 14000: 1.1305340815604412\n",
      "two-layer Loss after iteration 15000: 1.1036249764063528\n",
      "two-layer Loss after iteration 16000: 1.0828617854494675\n",
      "two-layer Loss after iteration 17000: 1.067788164283655\n",
      "two-layer Loss after iteration 18000: 1.0544887599945518\n",
      "two-layer Loss after iteration 19000: 1.0440479149787287\n",
      "two-layer Loss after iteration 20000: 1.0355835162744598\n",
      "two-layer Loss after iteration 21000: 1.0240163635784605\n",
      "two-layer Loss after iteration 22000: 1.0165589229122078\n",
      "two-layer Loss after iteration 23000: 1.0105261342246994\n",
      "two-layer Loss after iteration 24000: 1.0055398905691213\n",
      "two-layer Loss after iteration 25000: 1.001336435442663\n",
      "two-layer Loss after iteration 26000: 0.9971864968572103\n",
      "two-layer Loss after iteration 27000: 0.9938600806243774\n",
      "two-layer Loss after iteration 28000: 0.9885457685449034\n",
      "two-layer Loss after iteration 29000: 0.9845289402518624\n",
      "two-layer Loss after iteration 30000: 0.9797496850710887\n",
      "two-layer Loss after iteration 31000: 0.9751833852274558\n",
      "two-layer Loss after iteration 32000: 0.9718074668518166\n",
      "two-layer Loss after iteration 33000: 0.9689630180057591\n",
      "two-layer Loss after iteration 34000: 0.9665421286382336\n",
      "two-layer Loss after iteration 35000: 0.9642871218165302\n",
      "two-layer Loss after iteration 36000: 0.9620904488743046\n",
      "two-layer Loss after iteration 37000: 0.9599587526925384\n",
      "two-layer Loss after iteration 38000: 0.9579635251261405\n",
      "two-layer Loss after iteration 39000: 0.9562122133805209\n",
      "two-layer Loss after iteration 40000: 0.9545413438546961\n",
      "two-layer Loss after iteration 41000: 0.952948949361713\n",
      "two-layer Loss after iteration 42000: 0.9514131450938266\n",
      "two-layer Loss after iteration 43000: 0.949932918704057\n",
      "two-layer Loss after iteration 44000: 0.9485305117707943\n",
      "two-layer Loss after iteration 45000: 0.9471606097478614\n",
      "two-layer Loss after iteration 46000: 0.9458653826402686\n",
      "two-layer Loss after iteration 47000: 0.944624292513702\n",
      "two-layer Loss after iteration 48000: 0.9434587433656902\n",
      "two-layer Loss after iteration 49000: 0.9418462175961066\n",
      "two-layer Loss after iteration 50000: 0.9403615959346038\n",
      "two-layer Loss after iteration 51000: 0.9389823106432813\n",
      "two-layer Loss after iteration 52000: 0.9376957535649963\n",
      "two-layer Loss after iteration 53000: 0.9364902345589632\n",
      "two-layer Loss after iteration 54000: 0.9353502488908091\n",
      "two-layer Loss after iteration 55000: 0.9330680127861785\n",
      "two-layer Loss after iteration 56000: 0.9311107972133684\n",
      "two-layer Loss after iteration 57000: 0.9294776444542457\n",
      "two-layer Loss after iteration 58000: 0.9282077191876923\n",
      "two-layer Loss after iteration 59000: 0.9270287036811959\n",
      "two-layer Loss after iteration 60000: 0.9258983464522725\n",
      "two-layer Loss after iteration 61000: 0.9249244047501459\n",
      "two-layer Loss after iteration 62000: 0.9241400742285859\n",
      "two-layer Loss after iteration 63000: 0.9211087041348829\n",
      "two-layer Loss after iteration 64000: 0.919867867058899\n",
      "two-layer Loss after iteration 65000: 0.9161156861088532\n",
      "two-layer Loss after iteration 66000: 0.9143855596801871\n",
      "two-layer Loss after iteration 67000: 0.9130979124933568\n",
      "two-layer Loss after iteration 68000: 0.9120374697069883\n",
      "two-layer Loss after iteration 69000: 0.911098650629321\n",
      "two-layer Loss after iteration 70000: 0.9095049267909814\n",
      "two-layer Loss after iteration 71000: 0.9062545210613351\n",
      "two-layer Loss after iteration 72000: 0.9042234555232471\n",
      "two-layer Loss after iteration 73000: 0.9026840849657514\n",
      "two-layer Loss after iteration 74000: 0.9013191978197597\n",
      "two-layer Loss after iteration 75000: 0.9000685059214333\n",
      "two-layer Loss after iteration 76000: 0.8989116883982635\n",
      "two-layer Loss after iteration 77000: 0.8978235271854197\n",
      "two-layer Loss after iteration 78000: 0.8968143075074904\n",
      "two-layer Loss after iteration 79000: 0.8958382794689765\n",
      "two-layer Loss after iteration 80000: 0.8949182000086451\n",
      "two-layer Loss after iteration 81000: 0.8940423710690203\n",
      "two-layer Loss after iteration 82000: 0.8931988512473332\n",
      "two-layer Loss after iteration 83000: 0.8924118843994419\n",
      "two-layer Loss after iteration 84000: 0.8916484005070687\n",
      "two-layer Loss after iteration 85000: 0.8909262040490736\n",
      "two-layer Loss after iteration 86000: 0.8902211331968298\n",
      "two-layer Loss after iteration 87000: 0.8895617122896998\n",
      "two-layer Loss after iteration 88000: 0.8889165428361365\n",
      "two-layer Loss after iteration 89000: 0.8883008525222019\n",
      "two-layer Loss after iteration 90000: 0.8877150821421835\n",
      "two-layer Loss after iteration 91000: 0.8871419074041985\n",
      "two-layer Loss after iteration 92000: 0.8865951645188312\n",
      "two-layer Loss after iteration 93000: 0.8793199106531095\n",
      "two-layer Loss after iteration 94000: 0.8776307352139523\n",
      "two-layer Loss after iteration 95000: 0.8766408033109088\n",
      "two-layer Loss after iteration 96000: 0.8759595313915003\n",
      "two-layer Loss after iteration 97000: 0.8754179303498248\n",
      "two-layer Loss after iteration 98000: 0.8749289526839596\n",
      "two-layer Loss after iteration 99000: 0.8744647184497114\n",
      "two-layer Loss after iteration 100000: 0.8740421470212472\n",
      "two-layer Loss after iteration 101000: 0.8736299036735072\n",
      "two-layer Loss after iteration 102000: 0.8732336599157935\n",
      "two-layer Loss after iteration 103000: 0.8728565932858408\n",
      "two-layer Loss after iteration 104000: 0.8724847837862285\n",
      "two-layer Loss after iteration 105000: 0.8721290574158199\n",
      "two-layer Loss after iteration 106000: 0.8717793992103613\n",
      "two-layer Loss after iteration 107000: 0.8714477581394441\n",
      "two-layer Loss after iteration 108000: 0.8682092473827971\n",
      "two-layer Loss after iteration 109000: 0.8628362880794908\n",
      "two-layer Loss after iteration 110000: 0.8594413270496084\n",
      "two-layer Loss after iteration 111000: 0.8562784065406097\n",
      "two-layer Loss after iteration 112000: 0.8537119555009203\n",
      "two-layer Loss after iteration 113000: 0.8504658998877355\n",
      "two-layer Loss after iteration 114000: 0.8481498600627276\n",
      "two-layer Loss after iteration 115000: 0.8454576228135617\n",
      "two-layer Loss after iteration 116000: 0.8441276230256259\n",
      "two-layer Loss after iteration 117000: 0.8429841354699794\n",
      "two-layer Loss after iteration 118000: 0.8420258653561111\n",
      "two-layer Loss after iteration 119000: 0.8412367380072159\n",
      "two-layer Loss after iteration 120000: 0.8405590394153152\n",
      "two-layer Loss after iteration 121000: 0.8399655348288944\n",
      "two-layer Loss after iteration 122000: 0.8394338319704983\n",
      "two-layer Loss after iteration 123000: 0.8389517904430648\n",
      "two-layer Loss after iteration 124000: 0.8385119016915663\n",
      "two-layer Loss after iteration 125000: 0.8381025786161668\n",
      "two-layer Loss after iteration 126000: 0.8377311946165381\n",
      "two-layer Loss after iteration 127000: 0.8374013862256791\n",
      "two-layer Loss after iteration 128000: 0.8371012235760319\n",
      "two-layer Loss after iteration 129000: 0.8368235746092331\n",
      "two-layer Loss after iteration 130000: 0.8365695664697248\n",
      "two-layer Loss after iteration 131000: 0.8363355813559584\n",
      "two-layer Loss after iteration 132000: 0.8361105901141617\n",
      "two-layer Loss after iteration 133000: 0.8358973226328941\n",
      "two-layer Loss after iteration 134000: 0.8356917968912583\n",
      "two-layer Loss after iteration 135000: 0.8354958497974497\n",
      "two-layer Loss after iteration 136000: 0.8353011347526651\n",
      "two-layer Loss after iteration 137000: 0.8351147164346883\n",
      "two-layer Loss after iteration 138000: 0.8349353497703347\n",
      "two-layer Loss after iteration 139000: 0.834760113459951\n",
      "two-layer Loss after iteration 140000: 0.8345898801993903\n",
      "two-layer Loss after iteration 141000: 0.8344186672962629\n",
      "two-layer Loss after iteration 142000: 0.8342577659152213\n",
      "two-layer Loss after iteration 143000: 0.834094605330307\n",
      "two-layer Loss after iteration 144000: 0.8339411143029811\n",
      "two-layer Loss after iteration 145000: 0.8337890989132224\n",
      "two-layer Loss after iteration 146000: 0.8336422422922991\n",
      "two-layer Loss after iteration 147000: 0.8334972670193808\n",
      "two-layer Loss after iteration 148000: 0.8333539933263306\n",
      "two-layer Loss after iteration 149000: 0.8332176546925307\n",
      "two-layer Loss after iteration 150000: 0.833083027412641\n",
      "two-layer Loss after iteration 151000: 0.8329509156087965\n",
      "two-layer Loss after iteration 152000: 0.8328223157261415\n",
      "two-layer Loss after iteration 153000: 0.8326970295794776\n",
      "two-layer Loss after iteration 154000: 0.8325754279577499\n",
      "two-layer Loss after iteration 155000: 0.8324540865545504\n",
      "two-layer Loss after iteration 156000: 0.8323399835565594\n",
      "two-layer Loss after iteration 157000: 0.8322282470409464\n",
      "two-layer Loss after iteration 158000: 0.8321111773632753\n",
      "two-layer Loss after iteration 159000: 0.8320038480500069\n",
      "two-layer Loss after iteration 160000: 0.8318966620168246\n",
      "two-layer Loss after iteration 161000: 0.8317880804378129\n",
      "two-layer Loss after iteration 162000: 0.8316879503105447\n",
      "two-layer Loss after iteration 163000: 0.8315850764264984\n",
      "two-layer Loss after iteration 164000: 0.8314870914864173\n",
      "two-layer Loss after iteration 165000: 0.8313921813841473\n",
      "two-layer Loss after iteration 166000: 0.8313023350545026\n",
      "two-layer Loss after iteration 167000: 0.8312077996200393\n",
      "two-layer Loss after iteration 168000: 0.8311206845840902\n",
      "two-layer Loss after iteration 169000: 0.8310315018860117\n",
      "two-layer Loss after iteration 170000: 0.8309494469053709\n",
      "9.873871261753858e-05 0.8310315018860117 0.8309494469053709\n",
      "two-layer Loss after iteration 0: 1359.2820191879193\n",
      "two-layer Loss after iteration 1000: 23.157234570574754\n",
      "two-layer Loss after iteration 2000: 9.050937736789875\n",
      "two-layer Loss after iteration 3000: 6.5250717899626975\n",
      "two-layer Loss after iteration 4000: 5.304685785577005\n",
      "two-layer Loss after iteration 5000: 4.354970479334004\n",
      "two-layer Loss after iteration 6000: 3.829790930237841\n",
      "two-layer Loss after iteration 7000: 3.08769419939008\n",
      "two-layer Loss after iteration 8000: 2.3404717915154336\n",
      "two-layer Loss after iteration 9000: 1.7928055098210305\n",
      "two-layer Loss after iteration 10000: 1.3074527501456583\n",
      "two-layer Loss after iteration 11000: 1.1030229684008281\n",
      "two-layer Loss after iteration 12000: 0.9733737529388231\n",
      "two-layer Loss after iteration 13000: 0.8615057016887128\n",
      "two-layer Loss after iteration 14000: 0.7774419019938607\n",
      "two-layer Loss after iteration 15000: 0.7084003751347933\n",
      "two-layer Loss after iteration 16000: 0.654727668712942\n",
      "two-layer Loss after iteration 17000: 0.612529590649747\n",
      "two-layer Loss after iteration 18000: 0.5805886665885089\n",
      "two-layer Loss after iteration 19000: 0.5507129605780935\n",
      "two-layer Loss after iteration 20000: 0.5296719661016414\n",
      "two-layer Loss after iteration 21000: 0.5114137461372559\n",
      "two-layer Loss after iteration 22000: 0.49614315864483083\n",
      "two-layer Loss after iteration 23000: 0.4815408714161472\n",
      "two-layer Loss after iteration 24000: 0.4682260342141704\n",
      "two-layer Loss after iteration 25000: 0.45816668236245495\n",
      "two-layer Loss after iteration 26000: 0.4504610975142323\n",
      "two-layer Loss after iteration 27000: 0.44416928811488543\n",
      "two-layer Loss after iteration 28000: 0.43882708448710483\n",
      "two-layer Loss after iteration 29000: 0.43420068179055443\n",
      "two-layer Loss after iteration 30000: 0.4288972474717198\n",
      "two-layer Loss after iteration 31000: 0.4244790916069629\n",
      "two-layer Loss after iteration 32000: 0.42054085036992267\n",
      "two-layer Loss after iteration 33000: 0.4169298028643721\n",
      "two-layer Loss after iteration 34000: 0.4137631295745197\n",
      "two-layer Loss after iteration 35000: 0.41054693635151196\n",
      "two-layer Loss after iteration 36000: 0.4076098160730422\n",
      "two-layer Loss after iteration 37000: 0.40505315990772484\n",
      "two-layer Loss after iteration 38000: 0.40278375107885284\n",
      "two-layer Loss after iteration 39000: 0.40075268818030135\n",
      "two-layer Loss after iteration 40000: 0.3989233256666954\n",
      "two-layer Loss after iteration 41000: 0.39713993933368374\n",
      "two-layer Loss after iteration 42000: 0.3954800943088206\n",
      "two-layer Loss after iteration 43000: 0.3939683352773111\n",
      "two-layer Loss after iteration 44000: 0.39256429090305256\n",
      "two-layer Loss after iteration 45000: 0.39125608058851313\n",
      "two-layer Loss after iteration 46000: 0.3900351313549268\n",
      "two-layer Loss after iteration 47000: 0.38889460631584855\n",
      "two-layer Loss after iteration 48000: 0.38734063966683857\n",
      "two-layer Loss after iteration 49000: 0.38618455216627656\n",
      "two-layer Loss after iteration 50000: 0.38489672359474525\n",
      "two-layer Loss after iteration 51000: 0.3836484834316146\n",
      "two-layer Loss after iteration 52000: 0.3825907732731381\n",
      "two-layer Loss after iteration 53000: 0.3816038973707626\n",
      "two-layer Loss after iteration 54000: 0.380678578037053\n",
      "two-layer Loss after iteration 55000: 0.3798083718486397\n",
      "two-layer Loss after iteration 56000: 0.37891675774763056\n",
      "two-layer Loss after iteration 57000: 0.37809412630337763\n",
      "two-layer Loss after iteration 58000: 0.37732763469361374\n",
      "two-layer Loss after iteration 59000: 0.37660655211181804\n",
      "two-layer Loss after iteration 60000: 0.37578029852248884\n",
      "two-layer Loss after iteration 61000: 0.3750384715268755\n",
      "two-layer Loss after iteration 62000: 0.3743686646219679\n",
      "two-layer Loss after iteration 63000: 0.37373583764000157\n",
      "two-layer Loss after iteration 64000: 0.3731363642484362\n",
      "two-layer Loss after iteration 65000: 0.3725673626792421\n",
      "two-layer Loss after iteration 66000: 0.37202728836143567\n",
      "two-layer Loss after iteration 67000: 0.37151285606803575\n",
      "two-layer Loss after iteration 68000: 0.37095493720210626\n",
      "two-layer Loss after iteration 69000: 0.3704239681266556\n",
      "two-layer Loss after iteration 70000: 0.369923396462441\n",
      "two-layer Loss after iteration 71000: 0.3694517623801152\n",
      "two-layer Loss after iteration 72000: 0.36899810852619785\n",
      "two-layer Loss after iteration 73000: 0.3685640218060039\n",
      "two-layer Loss after iteration 74000: 0.3681517911264231\n",
      "two-layer Loss after iteration 75000: 0.367759299571267\n",
      "two-layer Loss after iteration 76000: 0.36709669752993984\n",
      "two-layer Loss after iteration 77000: 0.3667052351401672\n",
      "two-layer Loss after iteration 78000: 0.3663362621074394\n",
      "two-layer Loss after iteration 79000: 0.3656098326955914\n",
      "two-layer Loss after iteration 80000: 0.364929161925476\n",
      "two-layer Loss after iteration 81000: 0.3645334990475176\n",
      "two-layer Loss after iteration 82000: 0.36417819446261934\n",
      "two-layer Loss after iteration 83000: 0.3638510712150921\n",
      "two-layer Loss after iteration 84000: 0.3635452976240571\n",
      "two-layer Loss after iteration 85000: 0.3632571118425809\n",
      "two-layer Loss after iteration 86000: 0.36298346106788926\n",
      "two-layer Loss after iteration 87000: 0.3627231143565042\n",
      "two-layer Loss after iteration 88000: 0.3624736651873953\n",
      "two-layer Loss after iteration 89000: 0.3622355769693359\n",
      "two-layer Loss after iteration 90000: 0.3620058546440167\n",
      "two-layer Loss after iteration 91000: 0.3616237252824712\n",
      "two-layer Loss after iteration 92000: 0.36113034725938004\n",
      "two-layer Loss after iteration 93000: 0.36036723247321256\n",
      "two-layer Loss after iteration 94000: 0.35980936083852294\n",
      "two-layer Loss after iteration 95000: 0.3594738332388647\n",
      "two-layer Loss after iteration 96000: 0.3587472788586002\n",
      "two-layer Loss after iteration 97000: 0.35846017965412774\n",
      "two-layer Loss after iteration 98000: 0.3582219304145648\n",
      "two-layer Loss after iteration 99000: 0.3580030765657236\n",
      "two-layer Loss after iteration 100000: 0.35779625386019154\n",
      "two-layer Loss after iteration 101000: 0.3576026620406516\n",
      "two-layer Loss after iteration 102000: 0.35741821338501223\n",
      "two-layer Loss after iteration 103000: 0.3572426788649746\n",
      "two-layer Loss after iteration 104000: 0.35707462510101823\n",
      "two-layer Loss after iteration 105000: 0.35691356421872433\n",
      "two-layer Loss after iteration 106000: 0.35675974528974913\n",
      "two-layer Loss after iteration 107000: 0.35661134269120354\n",
      "two-layer Loss after iteration 108000: 0.3564692179844024\n",
      "two-layer Loss after iteration 109000: 0.35633389619038686\n",
      "two-layer Loss after iteration 110000: 0.3562019065246284\n",
      "two-layer Loss after iteration 111000: 0.3560746523207806\n",
      "two-layer Loss after iteration 112000: 0.3559512762213121\n",
      "two-layer Loss after iteration 113000: 0.35583299589938283\n",
      "two-layer Loss after iteration 114000: 0.35571827979499815\n",
      "two-layer Loss after iteration 115000: 0.3556069292556238\n",
      "two-layer Loss after iteration 116000: 0.3554995543176311\n",
      "two-layer Loss after iteration 117000: 0.355395001600091\n",
      "two-layer Loss after iteration 118000: 0.35529455865536463\n",
      "two-layer Loss after iteration 119000: 0.35519677662160637\n",
      "two-layer Loss after iteration 120000: 0.3551019416691978\n",
      "two-layer Loss after iteration 121000: 0.35500958448478537\n",
      "two-layer Loss after iteration 122000: 0.35491891569480927\n",
      "two-layer Loss after iteration 123000: 0.35483263274590626\n",
      "two-layer Loss after iteration 124000: 0.3547468949186954\n",
      "two-layer Loss after iteration 125000: 0.35466457936873774\n",
      "two-layer Loss after iteration 126000: 0.3545838285616124\n",
      "two-layer Loss after iteration 127000: 0.35450611670606036\n",
      "two-layer Loss after iteration 128000: 0.3544294364876153\n",
      "two-layer Loss after iteration 129000: 0.3543555421852581\n",
      "two-layer Loss after iteration 130000: 0.35428236784906025\n",
      "two-layer Loss after iteration 131000: 0.35421316734730984\n",
      "two-layer Loss after iteration 132000: 0.35414418979484324\n",
      "two-layer Loss after iteration 133000: 0.354076859933346\n",
      "two-layer Loss after iteration 134000: 0.3540105672966506\n",
      "two-layer Loss after iteration 135000: 0.35394745460959837\n",
      "two-layer Loss after iteration 136000: 0.3538849266460897\n",
      "two-layer Loss after iteration 137000: 0.35382551217743013\n",
      "two-layer Loss after iteration 138000: 0.3537657033283128\n",
      "two-layer Loss after iteration 139000: 0.35370821440070593\n",
      "two-layer Loss after iteration 140000: 0.35365114551622623\n",
      "two-layer Loss after iteration 141000: 0.3535962149618169\n",
      "two-layer Loss after iteration 142000: 0.3535427129981947\n",
      "two-layer Loss after iteration 143000: 0.35348886245129735\n",
      "two-layer Loss after iteration 144000: 0.3534380571131145\n",
      "two-layer Loss after iteration 145000: 0.3533878567552971\n",
      "two-layer Loss after iteration 146000: 0.3533375141620571\n",
      "two-layer Loss after iteration 147000: 0.3532905189968037\n",
      "two-layer Loss after iteration 148000: 0.35324325538384765\n",
      "two-layer Loss after iteration 149000: 0.353196259947791\n",
      "two-layer Loss after iteration 150000: 0.35315265391129735\n",
      "two-layer Loss after iteration 151000: 0.3531070516614247\n",
      "two-layer Loss after iteration 152000: 0.35306568231893404\n",
      "two-layer Loss after iteration 153000: 0.3530220031478844\n",
      "two-layer Loss after iteration 154000: 0.35298075310204974\n",
      "two-layer Loss after iteration 155000: 0.35294124417289274\n",
      "two-layer Loss after iteration 156000: 0.35290134865127537\n",
      "two-layer Loss after iteration 157000: 0.3528611252737523\n",
      "two-layer Loss after iteration 158000: 0.3528236641157658\n",
      "two-layer Loss after iteration 159000: 0.3527869268578902\n",
      "two-layer Loss after iteration 160000: 0.35274942973094797\n",
      "two-layer Loss after iteration 161000: 0.35271444466941576\n",
      "9.917822279371295e-05 0.35274942973094797 0.35271444466941576\n",
      "three-layer Loss after iteration 0: 1517.711664635114\n",
      "three-layer Loss after iteration 1000: 9.948020811052944\n",
      "three-layer Loss after iteration 2000: 6.4883229680127705\n",
      "three-layer Loss after iteration 3000: 4.191033657308784\n",
      "three-layer Loss after iteration 4000: 2.993936711123644\n",
      "three-layer Loss after iteration 5000: 2.5512801214978706\n",
      "three-layer Loss after iteration 6000: 1.7835325522518484\n",
      "three-layer Loss after iteration 7000: 1.1992832696085234\n",
      "three-layer Loss after iteration 8000: 0.9049537941834234\n",
      "three-layer Loss after iteration 9000: 1.0770472815255006\n",
      "three-layer Loss after iteration 10000: 1.106365740586814\n",
      "three-layer Loss after iteration 11000: 1.1951428555367178\n",
      "three-layer Loss after iteration 12000: 1.1174437563377548\n",
      "three-layer Loss after iteration 13000: 0.8623701996172617\n",
      "three-layer Loss after iteration 14000: 0.7754150971176497\n",
      "three-layer Loss after iteration 15000: 0.7040892974759322\n",
      "three-layer Loss after iteration 16000: 0.6995226201003868\n",
      "three-layer Loss after iteration 17000: 0.5504394489758463\n",
      "three-layer Loss after iteration 18000: 0.6075582324494501\n",
      "three-layer Loss after iteration 19000: 0.5586719453848493\n",
      "three-layer Loss after iteration 20000: 0.5499138013145057\n",
      "three-layer Loss after iteration 21000: 0.632222341901529\n",
      "three-layer Loss after iteration 22000: 0.6036470916690296\n",
      "three-layer Loss after iteration 23000: 0.6064236086858924\n",
      "three-layer Loss after iteration 24000: 0.5852077127996804\n",
      "three-layer Loss after iteration 25000: 0.5842843372346568\n",
      "three-layer Loss after iteration 26000: 0.5577760502050954\n",
      "three-layer Loss after iteration 27000: 0.4856227250254944\n",
      "three-layer Loss after iteration 28000: 0.48175160564997127\n",
      "three-layer Loss after iteration 29000: 0.4884783360183221\n",
      "three-layer Loss after iteration 30000: 0.5696973892008518\n",
      "three-layer Loss after iteration 31000: 0.529725493736005\n",
      "three-layer Loss after iteration 32000: 0.5232043980135345\n",
      "three-layer Loss after iteration 33000: 0.5131989927465361\n",
      "three-layer Loss after iteration 34000: 0.5019669175169216\n",
      "three-layer Loss after iteration 35000: 0.507520087499116\n",
      "three-layer Loss after iteration 36000: 0.5060005202941975\n",
      "three-layer Loss after iteration 37000: 0.5734110933047578\n",
      "three-layer Loss after iteration 38000: 0.5054918589400024\n",
      "three-layer Loss after iteration 39000: 0.489084613625161\n",
      "three-layer Loss after iteration 40000: 0.49478834670772703\n",
      "three-layer Loss after iteration 41000: 0.4927437962042498\n",
      "three-layer Loss after iteration 42000: 0.4887612555597586\n",
      "three-layer Loss after iteration 43000: 0.48639519935400183\n",
      "three-layer Loss after iteration 44000: 0.48413774967449014\n",
      "three-layer Loss after iteration 45000: 0.4828637699392695\n",
      "three-layer Loss after iteration 46000: 0.5093315621435149\n",
      "three-layer Loss after iteration 47000: 0.4830318774379288\n",
      "three-layer Loss after iteration 48000: 0.48014891117271963\n",
      "three-layer Loss after iteration 49000: 0.5304616924475114\n",
      "three-layer Loss after iteration 50000: 0.49867735346160935\n",
      "three-layer Loss after iteration 51000: 0.5059317701238355\n",
      "three-layer Loss after iteration 52000: 0.5053577114678783\n",
      "three-layer Loss after iteration 53000: 0.5039067379579377\n",
      "three-layer Loss after iteration 54000: 0.5002079700663017\n",
      "three-layer Loss after iteration 55000: 0.4830032705772046\n",
      "three-layer Loss after iteration 56000: 0.4837505431946528\n",
      "three-layer Loss after iteration 57000: 0.4783347930553027\n",
      "three-layer Loss after iteration 58000: 0.4816404051550625\n",
      "three-layer Loss after iteration 59000: 0.464790760670119\n",
      "three-layer Loss after iteration 60000: 0.4467142815538445\n",
      "three-layer Loss after iteration 61000: 0.4412495562393427\n",
      "three-layer Loss after iteration 62000: 0.42855698520133806\n",
      "three-layer Loss after iteration 63000: 0.42789126567249247\n",
      "three-layer Loss after iteration 64000: 0.4234157092257691\n",
      "three-layer Loss after iteration 65000: 0.4200753682431476\n",
      "three-layer Loss after iteration 66000: 0.4162850928817956\n",
      "three-layer Loss after iteration 67000: 0.41253252878830987\n",
      "three-layer Loss after iteration 68000: 0.40904701557318857\n",
      "three-layer Loss after iteration 69000: 0.40591610333949285\n",
      "three-layer Loss after iteration 70000: 0.40310727529068385\n",
      "three-layer Loss after iteration 71000: 0.4006156809211234\n",
      "three-layer Loss after iteration 72000: 0.39836519944685655\n",
      "three-layer Loss after iteration 73000: 0.4012112290858586\n",
      "three-layer Loss after iteration 74000: 0.4570784433710372\n",
      "three-layer Loss after iteration 75000: 0.4454995060853548\n",
      "three-layer Loss after iteration 76000: 0.42360722162610615\n",
      "three-layer Loss after iteration 77000: 0.4130304804355373\n",
      "three-layer Loss after iteration 78000: 0.3997985469829309\n",
      "three-layer Loss after iteration 79000: 0.39335533469121176\n",
      "three-layer Loss after iteration 80000: 0.3890450058123795\n",
      "three-layer Loss after iteration 81000: 0.39605586467173387\n",
      "three-layer Loss after iteration 82000: 0.3842001581958366\n",
      "three-layer Loss after iteration 83000: 0.37508504007065446\n",
      "three-layer Loss after iteration 84000: 0.3696173818739628\n",
      "three-layer Loss after iteration 85000: 0.4072067215552775\n",
      "three-layer Loss after iteration 86000: 0.3852801821472201\n",
      "three-layer Loss after iteration 87000: 0.3806670774609218\n",
      "three-layer Loss after iteration 88000: 0.3767980875134531\n",
      "three-layer Loss after iteration 89000: 0.3709913405056635\n",
      "three-layer Loss after iteration 90000: 0.36216436442932465\n",
      "three-layer Loss after iteration 91000: 0.358739319579092\n",
      "three-layer Loss after iteration 92000: 0.37412512200081144\n",
      "three-layer Loss after iteration 93000: 0.3850096848676761\n",
      "three-layer Loss after iteration 94000: 0.3830916413271356\n",
      "three-layer Loss after iteration 95000: 0.37774012824143577\n",
      "three-layer Loss after iteration 96000: 0.3742809184479495\n",
      "three-layer Loss after iteration 97000: 0.37217964662325115\n",
      "three-layer Loss after iteration 98000: 0.3744129776842838\n",
      "three-layer Loss after iteration 99000: 0.37211523526350676\n",
      "three-layer Loss after iteration 100000: 0.3634085767120791\n",
      "three-layer Loss after iteration 101000: 0.3667999679717539\n",
      "three-layer Loss after iteration 102000: 1.0328808796691713\n",
      "three-layer Loss after iteration 103000: 0.34280929953657885\n",
      "three-layer Loss after iteration 104000: 0.3072706305590083\n",
      "three-layer Loss after iteration 105000: 0.8308612182984823\n",
      "three-layer Loss after iteration 106000: 0.46667557250988356\n",
      "three-layer Loss after iteration 107000: 0.29832067207522733\n",
      "three-layer Loss after iteration 108000: 0.297372196096006\n",
      "three-layer Loss after iteration 109000: 0.47994137882552346\n",
      "three-layer Loss after iteration 110000: 0.3074431661889045\n",
      "three-layer Loss after iteration 111000: 0.3185248732145933\n",
      "three-layer Loss after iteration 112000: 0.3489366144713343\n",
      "three-layer Loss after iteration 113000: 0.3341467333372028\n",
      "three-layer Loss after iteration 114000: 0.3159491670494279\n",
      "three-layer Loss after iteration 115000: 0.32656868623537366\n",
      "three-layer Loss after iteration 116000: 0.31466253797732735\n",
      "three-layer Loss after iteration 117000: 0.3124898578549993\n",
      "three-layer Loss after iteration 118000: 0.30731167212488686\n",
      "three-layer Loss after iteration 119000: 0.4364113882454864\n",
      "three-layer Loss after iteration 120000: 0.3161582242995188\n",
      "three-layer Loss after iteration 121000: 0.3089734161040365\n",
      "three-layer Loss after iteration 122000: 0.3179543556322062\n",
      "three-layer Loss after iteration 123000: 0.34746273484102014\n",
      "three-layer Loss after iteration 124000: 0.3253425058828151\n",
      "three-layer Loss after iteration 125000: 0.32600441544842057\n",
      "three-layer Loss after iteration 126000: 0.329370561378537\n",
      "three-layer Loss after iteration 127000: 0.33373133744172984\n",
      "three-layer Loss after iteration 128000: 0.3287830931988689\n",
      "three-layer Loss after iteration 129000: 0.36031620754117966\n",
      "three-layer Loss after iteration 130000: 0.3388381484088934\n",
      "three-layer Loss after iteration 131000: 0.32112893862914926\n",
      "three-layer Loss after iteration 132000: 0.3141061152853322\n",
      "three-layer Loss after iteration 133000: 0.31177501616358017\n",
      "three-layer Loss after iteration 134000: 0.31042932988961097\n",
      "three-layer Loss after iteration 135000: 0.3261039722351854\n",
      "three-layer Loss after iteration 136000: 0.31145831425382325\n",
      "three-layer Loss after iteration 137000: 0.30758545380649177\n",
      "three-layer Loss after iteration 138000: 0.30514392604865925\n",
      "three-layer Loss after iteration 139000: 0.2784002957598778\n",
      "three-layer Loss after iteration 140000: 0.29248056138590034\n",
      "three-layer Loss after iteration 141000: 0.2911560916811146\n",
      "three-layer Loss after iteration 142000: 0.2910787159612334\n",
      "three-layer Loss after iteration 143000: 0.2909662286775727\n",
      "three-layer Loss after iteration 144000: 0.290834025883028\n",
      "three-layer Loss after iteration 145000: 0.2906966833149282\n",
      "three-layer Loss after iteration 146000: 0.2900554177246646\n",
      "three-layer Loss after iteration 147000: 0.2891920706109589\n",
      "three-layer Loss after iteration 148000: 0.28876398819822546\n",
      "three-layer Loss after iteration 149000: 0.2885475446829658\n",
      "three-layer Loss after iteration 150000: 0.28818315721426657\n",
      "three-layer Loss after iteration 151000: 0.2882295065051732\n",
      "three-layer Loss after iteration 152000: 0.28786194152027295\n",
      "three-layer Loss after iteration 153000: 0.2878026802244528\n",
      "three-layer Loss after iteration 154000: 0.2875592266161834\n",
      "three-layer Loss after iteration 155000: 0.2875486025348132\n",
      "3.694571547997438e-05 0.2875592266161834 0.2875486025348132\n",
      "three-layer Loss after iteration 0: 1452.1522242977478\n",
      "three-layer Loss after iteration 1000: 12.511279919764677\n",
      "three-layer Loss after iteration 2000: 9.311914323440197\n",
      "three-layer Loss after iteration 3000: 7.582889797341825\n",
      "three-layer Loss after iteration 4000: 5.77215775021937\n",
      "three-layer Loss after iteration 5000: 4.488193964881419\n",
      "three-layer Loss after iteration 6000: 3.729795952134854\n",
      "three-layer Loss after iteration 7000: 3.448817488133697\n",
      "three-layer Loss after iteration 8000: 3.421877189758036\n",
      "three-layer Loss after iteration 9000: 3.2347638268616175\n",
      "three-layer Loss after iteration 10000: 7.278761535584244\n",
      "three-layer Loss after iteration 11000: 1.9526998660040378\n",
      "three-layer Loss after iteration 12000: 1.7771774508238902\n",
      "three-layer Loss after iteration 13000: 1.632294764568949\n",
      "three-layer Loss after iteration 14000: 1.679791732917539\n",
      "three-layer Loss after iteration 15000: 1.5039978512991607\n",
      "three-layer Loss after iteration 16000: 1.3619301088159246\n",
      "three-layer Loss after iteration 17000: 1.1341242578657693\n",
      "three-layer Loss after iteration 18000: 1.1569730918987995\n",
      "three-layer Loss after iteration 19000: 1.153753184674841\n",
      "three-layer Loss after iteration 20000: 1.1843115088289844\n",
      "three-layer Loss after iteration 21000: 0.8669710645581785\n",
      "three-layer Loss after iteration 22000: 0.8420529097512969\n",
      "three-layer Loss after iteration 23000: 1.772016923420063\n",
      "three-layer Loss after iteration 24000: 1.3982186063151862\n",
      "three-layer Loss after iteration 25000: 1.195407214547248\n",
      "three-layer Loss after iteration 26000: 0.8005764345808446\n",
      "three-layer Loss after iteration 27000: 1.1173442664482822\n",
      "three-layer Loss after iteration 28000: 0.8074130173239392\n",
      "three-layer Loss after iteration 29000: 0.736504914276955\n",
      "three-layer Loss after iteration 30000: 0.8524584540931556\n",
      "three-layer Loss after iteration 31000: 0.8402655104507115\n",
      "three-layer Loss after iteration 32000: 0.809803017709466\n",
      "three-layer Loss after iteration 33000: 0.8166665995733595\n",
      "three-layer Loss after iteration 34000: 0.7764767564046234\n",
      "three-layer Loss after iteration 35000: 0.9503821890177468\n",
      "three-layer Loss after iteration 36000: 0.7397698880959165\n",
      "three-layer Loss after iteration 37000: 0.8489749934338825\n",
      "three-layer Loss after iteration 38000: 0.8204826138291889\n",
      "three-layer Loss after iteration 39000: 0.8067000523470202\n",
      "three-layer Loss after iteration 40000: 0.8057006728382289\n",
      "three-layer Loss after iteration 41000: 0.7850304450097861\n",
      "three-layer Loss after iteration 42000: 0.8101859894363347\n",
      "three-layer Loss after iteration 43000: 0.6964164456643507\n",
      "three-layer Loss after iteration 44000: 0.7664851787805047\n",
      "three-layer Loss after iteration 45000: 0.7618387428509928\n",
      "three-layer Loss after iteration 46000: 0.7029335098222386\n",
      "three-layer Loss after iteration 47000: 0.7023238774584797\n",
      "three-layer Loss after iteration 48000: 0.7317257393772368\n",
      "three-layer Loss after iteration 49000: 0.6868606290589844\n",
      "three-layer Loss after iteration 50000: 0.7050046626567352\n",
      "three-layer Loss after iteration 51000: 0.6951910432348284\n",
      "three-layer Loss after iteration 52000: 0.6910819785154968\n",
      "three-layer Loss after iteration 53000: 0.6880697961782927\n",
      "three-layer Loss after iteration 54000: 0.6863958878047774\n",
      "three-layer Loss after iteration 55000: 0.6826310173820254\n",
      "three-layer Loss after iteration 56000: 0.6812426766748096\n",
      "three-layer Loss after iteration 57000: 0.6779122768516803\n",
      "three-layer Loss after iteration 58000: 0.6756487276547566\n",
      "three-layer Loss after iteration 59000: 0.6736093461770941\n",
      "three-layer Loss after iteration 60000: 0.6699168994393525\n",
      "three-layer Loss after iteration 61000: 0.6817240331659112\n",
      "three-layer Loss after iteration 62000: 0.674638427784172\n",
      "three-layer Loss after iteration 63000: 0.6646161396885335\n",
      "three-layer Loss after iteration 64000: 0.6831953189322217\n",
      "three-layer Loss after iteration 65000: 0.6908118265807095\n",
      "three-layer Loss after iteration 66000: 0.6778083227297819\n",
      "three-layer Loss after iteration 67000: 0.6743206883936489\n",
      "three-layer Loss after iteration 68000: 0.6791479986500017\n",
      "three-layer Loss after iteration 69000: 0.6760851855482861\n",
      "three-layer Loss after iteration 70000: 0.6740085323001708\n",
      "three-layer Loss after iteration 71000: 0.6712385379262519\n",
      "three-layer Loss after iteration 72000: 0.6792006264062802\n",
      "three-layer Loss after iteration 73000: 0.6757489333334935\n",
      "three-layer Loss after iteration 74000: 0.6878707305990768\n",
      "three-layer Loss after iteration 75000: 0.6890212154781585\n",
      "three-layer Loss after iteration 76000: 0.7246124912045085\n",
      "three-layer Loss after iteration 77000: 0.7199467703713551\n",
      "three-layer Loss after iteration 78000: 0.7119761422428795\n",
      "three-layer Loss after iteration 79000: 0.7093634149914356\n",
      "three-layer Loss after iteration 80000: 0.6827923127587185\n",
      "three-layer Loss after iteration 81000: 0.7027392572234007\n",
      "three-layer Loss after iteration 82000: 0.6882471051922883\n",
      "three-layer Loss after iteration 83000: 0.6856247035139131\n",
      "three-layer Loss after iteration 84000: 0.6827042054218102\n",
      "three-layer Loss after iteration 85000: 0.6573749512808122\n",
      "three-layer Loss after iteration 86000: 0.6680452610383365\n",
      "three-layer Loss after iteration 87000: 0.669703128888337\n",
      "three-layer Loss after iteration 88000: 0.6628969354390013\n",
      "three-layer Loss after iteration 89000: 0.6522723878131335\n",
      "three-layer Loss after iteration 90000: 0.6509990339730138\n",
      "three-layer Loss after iteration 91000: 0.6463161713410102\n",
      "three-layer Loss after iteration 92000: 0.6412432949665529\n",
      "three-layer Loss after iteration 93000: 0.6452066669689914\n",
      "three-layer Loss after iteration 94000: 0.6329361512169569\n",
      "three-layer Loss after iteration 95000: 0.6285896822476654\n",
      "three-layer Loss after iteration 96000: 0.6440617051734128\n",
      "three-layer Loss after iteration 97000: 0.6412219620124391\n",
      "three-layer Loss after iteration 98000: 0.6375418763019384\n",
      "three-layer Loss after iteration 99000: 0.6227117650362834\n",
      "three-layer Loss after iteration 100000: 0.6248970940735631\n",
      "three-layer Loss after iteration 101000: 0.6264570148304996\n",
      "three-layer Loss after iteration 102000: 0.6228545278171851\n",
      "three-layer Loss after iteration 103000: 0.6217499861192384\n",
      "three-layer Loss after iteration 104000: 0.6210093870718285\n",
      "three-layer Loss after iteration 105000: 0.6176258141633765\n",
      "three-layer Loss after iteration 106000: 0.6438220009645809\n",
      "three-layer Loss after iteration 107000: 0.6231685513924103\n",
      "three-layer Loss after iteration 108000: 0.5970545296875567\n",
      "three-layer Loss after iteration 109000: 0.6000367099836532\n",
      "three-layer Loss after iteration 110000: 0.6270251209546606\n",
      "three-layer Loss after iteration 111000: 0.6301297053564384\n",
      "three-layer Loss after iteration 112000: 0.5933188324414148\n",
      "three-layer Loss after iteration 113000: 0.6191405205613649\n",
      "three-layer Loss after iteration 114000: 0.6215397108641645\n",
      "three-layer Loss after iteration 115000: 0.6155923710935477\n",
      "three-layer Loss after iteration 116000: 0.616517685464601\n",
      "three-layer Loss after iteration 117000: 0.611065722532444\n",
      "three-layer Loss after iteration 118000: 0.6113991301979405\n",
      "three-layer Loss after iteration 119000: 0.6090750148732522\n",
      "three-layer Loss after iteration 120000: 0.6091853406618539\n",
      "three-layer Loss after iteration 121000: 0.607783515830136\n",
      "three-layer Loss after iteration 122000: 0.6064194941055151\n",
      "three-layer Loss after iteration 123000: 0.6053185505680149\n",
      "three-layer Loss after iteration 124000: 0.6072565065770643\n",
      "three-layer Loss after iteration 125000: 0.6103363702887407\n",
      "three-layer Loss after iteration 126000: 0.6081380835232499\n",
      "three-layer Loss after iteration 127000: 0.6068789363965312\n",
      "three-layer Loss after iteration 128000: 0.6061558454886178\n",
      "three-layer Loss after iteration 129000: 0.6053385254700993\n",
      "three-layer Loss after iteration 130000: 0.604432471942996\n",
      "three-layer Loss after iteration 131000: 0.6045910108145179\n",
      "three-layer Loss after iteration 132000: 0.6044324947878276\n",
      "three-layer Loss after iteration 133000: 0.6029985466316631\n",
      "three-layer Loss after iteration 134000: 0.6019607884461778\n",
      "three-layer Loss after iteration 135000: 0.598957948914856\n",
      "three-layer Loss after iteration 136000: 0.5979086185109534\n",
      "three-layer Loss after iteration 137000: 0.5975824043962189\n",
      "three-layer Loss after iteration 138000: 0.5969688756473414\n",
      "three-layer Loss after iteration 139000: 0.5963351049374521\n",
      "three-layer Loss after iteration 140000: 0.5953910146100292\n",
      "three-layer Loss after iteration 141000: 0.5951443669479833\n",
      "three-layer Loss after iteration 142000: 0.5954355189836211\n",
      "three-layer Loss after iteration 143000: 0.5949943531684163\n",
      "three-layer Loss after iteration 144000: 0.5944146497849561\n",
      "three-layer Loss after iteration 145000: 0.5941181227008573\n",
      "three-layer Loss after iteration 146000: 0.5936138570386853\n",
      "three-layer Loss after iteration 147000: 0.5933968078239501\n",
      "three-layer Loss after iteration 148000: 0.5927698637858495\n",
      "three-layer Loss after iteration 149000: 0.5925216095168501\n",
      "three-layer Loss after iteration 150000: 0.592143431490302\n",
      "three-layer Loss after iteration 151000: 0.591581781073319\n",
      "three-layer Loss after iteration 152000: 0.5912358702341897\n",
      "three-layer Loss after iteration 153000: 0.5909045382183652\n",
      "three-layer Loss after iteration 154000: 0.5903977514817225\n",
      "three-layer Loss after iteration 155000: 0.5903510324381971\n",
      "7.913147265233902e-05 0.5903977514817225 0.5903510324381971\n",
      "three-layer Loss after iteration 0: 1660.7359686628997\n",
      "three-layer Loss after iteration 1000: 13.57267999016526\n",
      "three-layer Loss after iteration 2000: 9.416428973550076\n",
      "three-layer Loss after iteration 3000: 6.45063967647226\n",
      "three-layer Loss after iteration 4000: 5.036910062675193\n",
      "three-layer Loss after iteration 5000: 4.206484247606665\n",
      "three-layer Loss after iteration 6000: 3.623280702767861\n",
      "three-layer Loss after iteration 7000: 3.3303056278598624\n",
      "three-layer Loss after iteration 8000: 2.7830178806416552\n",
      "three-layer Loss after iteration 9000: 3.1324609263871572\n",
      "three-layer Loss after iteration 10000: 2.0037702680931715\n",
      "three-layer Loss after iteration 11000: 4.369421075070575\n",
      "three-layer Loss after iteration 12000: 2.0672165388385944\n",
      "three-layer Loss after iteration 13000: 2.0040042612880864\n",
      "three-layer Loss after iteration 14000: 1.6649690071669598\n",
      "three-layer Loss after iteration 15000: 1.3879042356587692\n",
      "three-layer Loss after iteration 16000: 1.5106218451740374\n",
      "three-layer Loss after iteration 17000: 1.4227017276707763\n",
      "three-layer Loss after iteration 18000: 1.8559103354022368\n",
      "three-layer Loss after iteration 19000: 1.344412670144146\n",
      "three-layer Loss after iteration 20000: 1.785307774704231\n",
      "three-layer Loss after iteration 21000: 1.083744812057576\n",
      "three-layer Loss after iteration 22000: 1.165751154844664\n",
      "three-layer Loss after iteration 23000: 0.9204694057992003\n",
      "three-layer Loss after iteration 24000: 1.0103918231966873\n",
      "three-layer Loss after iteration 25000: 1.0590631385486715\n",
      "three-layer Loss after iteration 26000: 1.0660813763232033\n",
      "three-layer Loss after iteration 27000: 0.8739586453558513\n",
      "three-layer Loss after iteration 28000: 0.9048143851203407\n",
      "three-layer Loss after iteration 29000: 0.847263752019374\n",
      "three-layer Loss after iteration 30000: 0.8631525227242389\n",
      "three-layer Loss after iteration 31000: 0.7779059202746402\n",
      "three-layer Loss after iteration 32000: 0.9166547125395462\n",
      "three-layer Loss after iteration 33000: 0.9742487355036431\n",
      "three-layer Loss after iteration 34000: 0.8107130772826254\n",
      "three-layer Loss after iteration 35000: 0.7707833471464233\n",
      "three-layer Loss after iteration 36000: 0.7864789173205601\n",
      "three-layer Loss after iteration 37000: 0.8652262647027316\n",
      "three-layer Loss after iteration 38000: 0.7220686518986921\n",
      "three-layer Loss after iteration 39000: 0.7092292415312932\n",
      "three-layer Loss after iteration 40000: 0.6990666807498402\n",
      "three-layer Loss after iteration 41000: 0.6967217628342042\n",
      "three-layer Loss after iteration 42000: 0.7248651566853876\n",
      "three-layer Loss after iteration 43000: 0.7325755572330873\n",
      "three-layer Loss after iteration 44000: 0.7369061924721586\n",
      "three-layer Loss after iteration 45000: 0.7357256576646377\n",
      "three-layer Loss after iteration 46000: 0.7261205460367414\n",
      "three-layer Loss after iteration 47000: 0.7247259106127952\n",
      "three-layer Loss after iteration 48000: 0.7238969858847734\n",
      "three-layer Loss after iteration 49000: 0.7198010087506915\n",
      "three-layer Loss after iteration 50000: 0.7182794771012668\n",
      "three-layer Loss after iteration 51000: 0.7162478089362637\n",
      "three-layer Loss after iteration 52000: 0.7152115357990222\n",
      "three-layer Loss after iteration 53000: 0.7744835917932492\n",
      "three-layer Loss after iteration 54000: 0.7129379028796105\n",
      "three-layer Loss after iteration 55000: 0.690671407669504\n",
      "three-layer Loss after iteration 56000: 0.7189371457339716\n",
      "three-layer Loss after iteration 57000: 0.6727081390436539\n",
      "three-layer Loss after iteration 58000: 0.6438563543813407\n",
      "three-layer Loss after iteration 59000: 0.7510504685483638\n",
      "three-layer Loss after iteration 60000: 0.7506102280716992\n",
      "three-layer Loss after iteration 61000: 0.6651733435625369\n",
      "three-layer Loss after iteration 62000: 0.6853874121099729\n",
      "three-layer Loss after iteration 63000: 0.6707825314745102\n",
      "three-layer Loss after iteration 64000: 0.6197743307813408\n",
      "three-layer Loss after iteration 65000: 0.6584199107333322\n",
      "three-layer Loss after iteration 66000: 0.6939851814961568\n",
      "three-layer Loss after iteration 67000: 0.7054646325416156\n",
      "three-layer Loss after iteration 68000: 0.6922430215734877\n",
      "three-layer Loss after iteration 69000: 0.6852852743604837\n",
      "three-layer Loss after iteration 70000: 0.6800811323641929\n",
      "three-layer Loss after iteration 71000: 0.7057906571073532\n",
      "three-layer Loss after iteration 72000: 0.6594739213011839\n",
      "three-layer Loss after iteration 73000: 0.654854516731622\n",
      "three-layer Loss after iteration 74000: 0.6448427773615213\n",
      "three-layer Loss after iteration 75000: 0.6422353715803678\n",
      "three-layer Loss after iteration 76000: 0.6428712213499703\n",
      "three-layer Loss after iteration 77000: 0.6534890323739866\n",
      "three-layer Loss after iteration 78000: 0.6304529274437701\n",
      "three-layer Loss after iteration 79000: 0.632434009424552\n",
      "three-layer Loss after iteration 80000: 0.6327208534081418\n",
      "three-layer Loss after iteration 81000: 0.5990171869815155\n",
      "three-layer Loss after iteration 82000: 0.5889328893283712\n",
      "three-layer Loss after iteration 83000: 0.6021889667112893\n",
      "three-layer Loss after iteration 84000: 0.5832750009543988\n",
      "three-layer Loss after iteration 85000: 0.6629608998873864\n",
      "three-layer Loss after iteration 86000: 0.8051089752276072\n",
      "three-layer Loss after iteration 87000: 0.6445719050438141\n",
      "three-layer Loss after iteration 88000: 0.6073635936076605\n",
      "three-layer Loss after iteration 89000: 0.6518077333663983\n",
      "three-layer Loss after iteration 90000: 0.6448519381455478\n",
      "three-layer Loss after iteration 91000: 0.6411324579583009\n",
      "three-layer Loss after iteration 92000: 0.6339876875130479\n",
      "three-layer Loss after iteration 93000: 0.6436775609219422\n",
      "three-layer Loss after iteration 94000: 0.6113066997790504\n",
      "three-layer Loss after iteration 95000: 0.6191134981149488\n",
      "three-layer Loss after iteration 96000: 0.610441611312462\n",
      "three-layer Loss after iteration 97000: 0.606199819623815\n",
      "three-layer Loss after iteration 98000: 0.6036619791121093\n",
      "three-layer Loss after iteration 99000: 0.6593916483034084\n",
      "three-layer Loss after iteration 100000: 0.5772163909385247\n",
      "three-layer Loss after iteration 101000: 0.5558028184420948\n",
      "three-layer Loss after iteration 102000: 0.6175196591132384\n",
      "three-layer Loss after iteration 103000: 0.6766335454878993\n",
      "three-layer Loss after iteration 104000: 0.6804507978645875\n",
      "three-layer Loss after iteration 105000: 0.6964639612889723\n",
      "three-layer Loss after iteration 106000: 0.6620042656633897\n",
      "three-layer Loss after iteration 107000: 0.6428477508959203\n",
      "three-layer Loss after iteration 108000: 0.6362631461140253\n",
      "three-layer Loss after iteration 109000: 0.6385442566213234\n",
      "three-layer Loss after iteration 110000: 0.6352363511600055\n",
      "three-layer Loss after iteration 111000: 0.6312506553844026\n",
      "three-layer Loss after iteration 112000: 0.6279742741023758\n",
      "three-layer Loss after iteration 113000: 0.4977823733776787\n",
      "three-layer Loss after iteration 114000: 0.48798930405577884\n",
      "three-layer Loss after iteration 115000: 0.48564261409797926\n",
      "three-layer Loss after iteration 116000: 0.5253644561017395\n",
      "three-layer Loss after iteration 117000: 0.5137059080961296\n",
      "three-layer Loss after iteration 118000: 0.4779052824519035\n",
      "three-layer Loss after iteration 119000: 0.4830201314070908\n",
      "three-layer Loss after iteration 120000: 0.6227217322842614\n",
      "three-layer Loss after iteration 121000: 0.4741741838574258\n",
      "three-layer Loss after iteration 122000: 0.6650825233831208\n",
      "three-layer Loss after iteration 123000: 0.5056316805702573\n",
      "three-layer Loss after iteration 124000: 0.5268012495577145\n",
      "three-layer Loss after iteration 125000: 0.5842772899358057\n",
      "three-layer Loss after iteration 126000: 0.600422237382156\n",
      "three-layer Loss after iteration 127000: 0.4746782283560206\n",
      "three-layer Loss after iteration 128000: 0.4930023225087536\n",
      "three-layer Loss after iteration 129000: 0.5097325427145522\n",
      "three-layer Loss after iteration 130000: 0.46046295131695425\n",
      "three-layer Loss after iteration 131000: 0.574425322435368\n",
      "three-layer Loss after iteration 132000: 0.4759834382626847\n",
      "three-layer Loss after iteration 133000: 0.5803831113302511\n",
      "three-layer Loss after iteration 134000: 0.5509532965274823\n",
      "three-layer Loss after iteration 135000: 0.5073073616940188\n",
      "three-layer Loss after iteration 136000: 0.5277130384041971\n",
      "three-layer Loss after iteration 137000: 0.569533923431885\n",
      "three-layer Loss after iteration 138000: 0.521518183072935\n",
      "three-layer Loss after iteration 139000: 0.4678912848630372\n",
      "three-layer Loss after iteration 140000: 0.5054676762532557\n",
      "three-layer Loss after iteration 141000: 0.5156479318551915\n",
      "three-layer Loss after iteration 142000: 0.6299926803860393\n",
      "three-layer Loss after iteration 143000: 0.636385281837297\n",
      "three-layer Loss after iteration 144000: 0.6114660899398268\n",
      "three-layer Loss after iteration 145000: 0.5346211541670426\n",
      "three-layer Loss after iteration 146000: 0.5295408303438737\n",
      "three-layer Loss after iteration 147000: 0.5387280650010768\n",
      "three-layer Loss after iteration 148000: 0.5345477132082933\n",
      "three-layer Loss after iteration 149000: 0.5318227383199212\n",
      "three-layer Loss after iteration 150000: 0.5290189041585536\n",
      "three-layer Loss after iteration 151000: 0.5265504867401566\n",
      "three-layer Loss after iteration 152000: 0.524387767667664\n",
      "three-layer Loss after iteration 153000: 0.5221374679598254\n",
      "three-layer Loss after iteration 154000: 0.5245293132909405\n",
      "three-layer Loss after iteration 155000: 0.5204463007366812\n",
      "three-layer Loss after iteration 156000: 0.5198895440954111\n",
      "three-layer Loss after iteration 157000: 0.5180999211811403\n",
      "three-layer Loss after iteration 158000: 0.4906081347105727\n",
      "three-layer Loss after iteration 159000: 0.5473397836002536\n",
      "three-layer Loss after iteration 160000: 0.5529075290477682\n",
      "three-layer Loss after iteration 161000: 0.5591304028957345\n",
      "three-layer Loss after iteration 162000: 0.5589944092948488\n",
      "three-layer Loss after iteration 163000: 0.5521123493804073\n",
      "three-layer Loss after iteration 164000: 0.5427268544061332\n",
      "three-layer Loss after iteration 165000: 0.5155990731893733\n",
      "three-layer Loss after iteration 166000: 0.5378988829911704\n",
      "three-layer Loss after iteration 167000: 0.5180358653288711\n",
      "three-layer Loss after iteration 168000: 0.515382437987152\n",
      "three-layer Loss after iteration 169000: 0.5119116094833763\n",
      "three-layer Loss after iteration 170000: 0.5058551573370883\n",
      "three-layer Loss after iteration 171000: 0.4794403014273985\n",
      "three-layer Loss after iteration 172000: 0.4831327373431654\n",
      "three-layer Loss after iteration 173000: 0.48251726451059046\n",
      "three-layer Loss after iteration 174000: 0.48105715549192707\n",
      "three-layer Loss after iteration 175000: 0.47965325369937967\n",
      "three-layer Loss after iteration 176000: 0.4787005944914683\n",
      "three-layer Loss after iteration 177000: 0.4776660298120348\n",
      "three-layer Loss after iteration 178000: 0.47710395760814567\n",
      "three-layer Loss after iteration 179000: 0.4667923484647944\n",
      "three-layer Loss after iteration 180000: 0.4793373807570554\n",
      "three-layer Loss after iteration 181000: 0.4790055163376598\n",
      "three-layer Loss after iteration 182000: 0.4700466834081339\n",
      "three-layer Loss after iteration 183000: 0.46833617049984405\n",
      "three-layer Loss after iteration 184000: 0.46413777666617545\n",
      "three-layer Loss after iteration 185000: 0.43413945981346197\n",
      "three-layer Loss after iteration 186000: 0.4537431867939721\n",
      "three-layer Loss after iteration 187000: 0.4531897161783611\n",
      "three-layer Loss after iteration 188000: 0.41427930511274835\n",
      "three-layer Loss after iteration 189000: 0.43381677998019047\n",
      "three-layer Loss after iteration 190000: 0.40241326066821237\n",
      "three-layer Loss after iteration 191000: 0.39178376969477946\n",
      "three-layer Loss after iteration 192000: 0.4627294335803948\n",
      "three-layer Loss after iteration 193000: 0.46777875726633017\n",
      "three-layer Loss after iteration 194000: 0.4893142202241781\n",
      "three-layer Loss after iteration 195000: 0.49296366048102497\n",
      "three-layer Loss after iteration 196000: 0.48740954519095997\n",
      "three-layer Loss after iteration 197000: 0.4834359263511104\n",
      "three-layer Loss after iteration 198000: 0.4809827434128183\n",
      "three-layer Loss after iteration 199000: 0.4708548079073579\n",
      "three-layer Loss after iteration 200000: 0.4670239478713262\n",
      "three-layer Loss after iteration 201000: 0.46403732159225686\n",
      "three-layer Loss after iteration 202000: 0.4603809380226024\n",
      "three-layer Loss after iteration 203000: 0.49862225339759714\n",
      "three-layer Loss after iteration 204000: 0.4896815661680816\n",
      "three-layer Loss after iteration 205000: 0.48246822239440085\n",
      "three-layer Loss after iteration 206000: 0.47679125541630596\n",
      "three-layer Loss after iteration 207000: 0.47438028067362414\n",
      "three-layer Loss after iteration 208000: 0.47191475591539384\n",
      "three-layer Loss after iteration 209000: 0.46563375233925264\n",
      "three-layer Loss after iteration 210000: 0.4670519253523707\n",
      "three-layer Loss after iteration 211000: 0.4640653041772603\n",
      "three-layer Loss after iteration 212000: 0.450073727471541\n",
      "three-layer Loss after iteration 213000: 0.45393192149073097\n",
      "three-layer Loss after iteration 214000: 0.4536661466187094\n",
      "three-layer Loss after iteration 215000: 0.45257469918116316\n",
      "three-layer Loss after iteration 216000: 0.41999734698627156\n",
      "three-layer Loss after iteration 217000: 0.4460313271533811\n",
      "three-layer Loss after iteration 218000: 0.43880034073519386\n",
      "three-layer Loss after iteration 219000: 0.43929869730768145\n",
      "three-layer Loss after iteration 220000: 0.3926030625676573\n",
      "three-layer Loss after iteration 221000: 0.4873232211715675\n",
      "three-layer Loss after iteration 222000: 0.47936431658024325\n",
      "three-layer Loss after iteration 223000: 0.4767329028192776\n",
      "three-layer Loss after iteration 224000: 0.4606953172869367\n",
      "three-layer Loss after iteration 225000: 0.4697807147838459\n",
      "three-layer Loss after iteration 226000: 0.4660813040958232\n",
      "three-layer Loss after iteration 227000: 0.4547309016093705\n",
      "three-layer Loss after iteration 228000: 0.4555307242010156\n",
      "three-layer Loss after iteration 229000: 0.4511076277004911\n",
      "three-layer Loss after iteration 230000: 0.44700099741799004\n",
      "three-layer Loss after iteration 231000: 0.443529535280113\n",
      "three-layer Loss after iteration 232000: 0.43977410561333624\n",
      "three-layer Loss after iteration 233000: 0.43650177411816293\n",
      "three-layer Loss after iteration 234000: 0.4326303925047028\n",
      "three-layer Loss after iteration 235000: 0.4298470739041298\n",
      "three-layer Loss after iteration 236000: 0.42694830427639013\n",
      "three-layer Loss after iteration 237000: 0.42421626738264084\n",
      "three-layer Loss after iteration 238000: 0.42147693036700107\n",
      "three-layer Loss after iteration 239000: 0.4188151156023149\n",
      "three-layer Loss after iteration 240000: 0.4166898184852925\n",
      "three-layer Loss after iteration 241000: 0.4147478975751967\n",
      "three-layer Loss after iteration 242000: 0.41301514354807684\n",
      "three-layer Loss after iteration 243000: 0.4092764055808338\n",
      "three-layer Loss after iteration 244000: 0.4074477115605508\n",
      "three-layer Loss after iteration 245000: 0.405663337720672\n",
      "three-layer Loss after iteration 246000: 0.40377225441170783\n",
      "three-layer Loss after iteration 247000: 0.401785794854145\n",
      "three-layer Loss after iteration 248000: 0.39967730463342604\n",
      "three-layer Loss after iteration 249000: 0.39790754668956246\n",
      "three-layer Loss after iteration 250000: 0.3960868742959885\n",
      "three-layer Loss after iteration 251000: 0.39459430254897376\n",
      "three-layer Loss after iteration 252000: 0.39307752645147576\n",
      "three-layer Loss after iteration 253000: 0.39141629496268016\n",
      "three-layer Loss after iteration 254000: 0.39134857461977113\n",
      "three-layer Loss after iteration 255000: 0.39022082379878786\n",
      "three-layer Loss after iteration 256000: 0.38888912644213963\n",
      "three-layer Loss after iteration 257000: 0.3882680778884843\n",
      "three-layer Loss after iteration 258000: 0.38458694735685395\n",
      "three-layer Loss after iteration 259000: 0.38597462004764516\n",
      "three-layer Loss after iteration 260000: 0.3455656015119641\n",
      "three-layer Loss after iteration 261000: 0.4363607353467212\n",
      "three-layer Loss after iteration 262000: 0.3714380159768261\n",
      "three-layer Loss after iteration 263000: 0.40723559389225095\n",
      "three-layer Loss after iteration 264000: 0.38294887067254135\n",
      "three-layer Loss after iteration 265000: 0.3819691953205008\n",
      "three-layer Loss after iteration 266000: 0.39399007407256714\n",
      "three-layer Loss after iteration 267000: 0.37483769883101026\n",
      "three-layer Loss after iteration 268000: 0.39442371696995904\n",
      "three-layer Loss after iteration 269000: 0.37652297441656785\n",
      "three-layer Loss after iteration 270000: 0.387330623030057\n",
      "three-layer Loss after iteration 271000: 0.37899440016220676\n",
      "three-layer Loss after iteration 272000: 0.3815776316354265\n",
      "three-layer Loss after iteration 273000: 0.3794880578763793\n",
      "three-layer Loss after iteration 274000: 0.37850204593649023\n",
      "three-layer Loss after iteration 275000: 0.3778740327726083\n",
      "three-layer Loss after iteration 276000: 0.3801681456095543\n",
      "three-layer Loss after iteration 277000: 0.37739961051648213\n",
      "three-layer Loss after iteration 278000: 0.37846943188899923\n",
      "three-layer Loss after iteration 279000: 0.37648179351550526\n",
      "three-layer Loss after iteration 280000: 0.37572672262806933\n",
      "three-layer Loss after iteration 281000: 0.3747531274299529\n",
      "three-layer Loss after iteration 282000: 0.3739389997972516\n",
      "three-layer Loss after iteration 283000: 0.373086039867851\n",
      "three-layer Loss after iteration 284000: 0.37248882783970516\n",
      "three-layer Loss after iteration 285000: 0.371623043747598\n",
      "three-layer Loss after iteration 286000: 0.37076169996254527\n",
      "three-layer Loss after iteration 287000: 0.3701088156987713\n",
      "three-layer Loss after iteration 288000: 0.36927181151662136\n",
      "three-layer Loss after iteration 289000: 0.3686033420915846\n",
      "three-layer Loss after iteration 290000: 0.3679548764427601\n",
      "three-layer Loss after iteration 291000: 0.36459943031847186\n",
      "three-layer Loss after iteration 292000: 0.36717426999068375\n",
      "three-layer Loss after iteration 293000: 0.36576724035294633\n",
      "three-layer Loss after iteration 294000: 0.3626162791938319\n",
      "three-layer Loss after iteration 295000: 0.364969138714938\n",
      "three-layer Loss after iteration 296000: 0.3649651324725452\n",
      "1.0976934671613372e-05 0.364969138714938 0.3649651324725452\n",
      "three-layer Loss after iteration 0: 1389.9084843555356\n",
      "three-layer Loss after iteration 1000: 9.54468005650569\n",
      "three-layer Loss after iteration 2000: 6.828137537804336\n",
      "three-layer Loss after iteration 3000: 6.053696844355245\n",
      "three-layer Loss after iteration 4000: 4.146191463409134\n",
      "three-layer Loss after iteration 5000: 2.7375484416945275\n",
      "three-layer Loss after iteration 6000: 2.710880784315626\n",
      "three-layer Loss after iteration 7000: 1.883261487399119\n",
      "three-layer Loss after iteration 8000: 2.590723927831795\n",
      "three-layer Loss after iteration 9000: 2.231208203217038\n",
      "three-layer Loss after iteration 10000: 1.496011189688248\n",
      "three-layer Loss after iteration 11000: 1.0159634021766675\n",
      "three-layer Loss after iteration 12000: 1.0623578180137476\n",
      "three-layer Loss after iteration 13000: 0.8251321825794283\n",
      "three-layer Loss after iteration 14000: 0.8167087223128543\n",
      "three-layer Loss after iteration 15000: 0.8650301170948903\n",
      "three-layer Loss after iteration 16000: 1.2646951787473282\n",
      "three-layer Loss after iteration 17000: 0.908124356176948\n",
      "three-layer Loss after iteration 18000: 1.2190785900089212\n",
      "three-layer Loss after iteration 19000: 0.7996509309329985\n",
      "three-layer Loss after iteration 20000: 0.9996277210589694\n",
      "three-layer Loss after iteration 21000: 0.7500309374172064\n",
      "three-layer Loss after iteration 22000: 0.8354857489757045\n",
      "three-layer Loss after iteration 23000: 0.82481556878376\n",
      "three-layer Loss after iteration 24000: 0.8114716567506404\n",
      "three-layer Loss after iteration 25000: 0.7960739561782926\n",
      "three-layer Loss after iteration 26000: 0.8078416171439055\n",
      "three-layer Loss after iteration 27000: 0.7886982487033628\n",
      "three-layer Loss after iteration 28000: 0.7687880875483198\n",
      "three-layer Loss after iteration 29000: 0.7314284114341116\n",
      "three-layer Loss after iteration 30000: 0.7344151503405038\n",
      "three-layer Loss after iteration 31000: 0.7365534382898611\n",
      "three-layer Loss after iteration 32000: 0.714853865774745\n",
      "three-layer Loss after iteration 33000: 0.7042794140382055\n",
      "three-layer Loss after iteration 34000: 0.6898349597141304\n",
      "three-layer Loss after iteration 35000: 0.697529914451548\n",
      "three-layer Loss after iteration 36000: 0.6270243317457014\n",
      "three-layer Loss after iteration 37000: 0.6742521485863574\n",
      "three-layer Loss after iteration 38000: 0.5413579050635993\n",
      "three-layer Loss after iteration 39000: 0.6500212497060476\n",
      "three-layer Loss after iteration 40000: 0.6848414243859967\n",
      "three-layer Loss after iteration 41000: 0.6435207142157846\n",
      "three-layer Loss after iteration 42000: 0.6275520255566045\n",
      "three-layer Loss after iteration 43000: 0.6268211515322492\n",
      "three-layer Loss after iteration 44000: 0.6737222407808278\n",
      "three-layer Loss after iteration 45000: 0.6464629970894561\n",
      "three-layer Loss after iteration 46000: 0.5812804317526722\n",
      "three-layer Loss after iteration 47000: 0.6523667934869474\n",
      "three-layer Loss after iteration 48000: 0.6277794570353471\n",
      "three-layer Loss after iteration 49000: 0.6311094029149682\n",
      "three-layer Loss after iteration 50000: 0.6276289004529154\n",
      "three-layer Loss after iteration 51000: 0.6254165168497187\n",
      "three-layer Loss after iteration 52000: 0.6209827852974527\n",
      "three-layer Loss after iteration 53000: 0.6181030908148725\n",
      "three-layer Loss after iteration 54000: 0.6166075254255275\n",
      "three-layer Loss after iteration 55000: 0.6136792961887022\n",
      "three-layer Loss after iteration 56000: 0.610658362225413\n",
      "three-layer Loss after iteration 57000: 0.6129976105213336\n",
      "three-layer Loss after iteration 58000: 0.6096411910025856\n",
      "three-layer Loss after iteration 59000: 0.5957811559192243\n",
      "three-layer Loss after iteration 60000: 0.6115149843064187\n",
      "three-layer Loss after iteration 61000: 0.6134604764023708\n",
      "three-layer Loss after iteration 62000: 0.6064692385166851\n",
      "three-layer Loss after iteration 63000: 0.6047239112588265\n",
      "three-layer Loss after iteration 64000: 0.5984442012885035\n",
      "three-layer Loss after iteration 65000: 0.5961348749549362\n",
      "three-layer Loss after iteration 66000: 0.5911944283847644\n",
      "three-layer Loss after iteration 67000: 0.5881020947713421\n",
      "three-layer Loss after iteration 68000: 0.5844854535889031\n",
      "three-layer Loss after iteration 69000: 0.5202504928839768\n",
      "three-layer Loss after iteration 70000: 0.5744816735393721\n",
      "three-layer Loss after iteration 71000: 0.5562235610974647\n",
      "three-layer Loss after iteration 72000: 0.5586897463346459\n",
      "three-layer Loss after iteration 73000: 0.5415866833466552\n",
      "three-layer Loss after iteration 74000: 0.5351496212714867\n",
      "three-layer Loss after iteration 75000: 0.45797414875555786\n",
      "three-layer Loss after iteration 76000: 0.45829983480035125\n",
      "three-layer Loss after iteration 77000: 0.45444694584413736\n",
      "three-layer Loss after iteration 78000: 0.5027230816906806\n",
      "three-layer Loss after iteration 79000: 0.4558789246670324\n",
      "three-layer Loss after iteration 80000: 0.4539477814469734\n",
      "three-layer Loss after iteration 81000: 0.45246958585713276\n",
      "three-layer Loss after iteration 82000: 0.46577881445307107\n",
      "three-layer Loss after iteration 83000: 0.45373934376562314\n",
      "three-layer Loss after iteration 84000: 0.45154030615777124\n",
      "three-layer Loss after iteration 85000: 0.44991801629042194\n",
      "three-layer Loss after iteration 86000: 0.7149891071581419\n",
      "three-layer Loss after iteration 87000: 0.4714718868444394\n",
      "three-layer Loss after iteration 88000: 0.6169819681970944\n",
      "three-layer Loss after iteration 89000: 0.4615118678197003\n",
      "three-layer Loss after iteration 90000: 0.46447008214783236\n",
      "three-layer Loss after iteration 91000: 0.4702829543931916\n",
      "three-layer Loss after iteration 92000: 0.4678266595763873\n",
      "three-layer Loss after iteration 93000: 0.4754689751980152\n",
      "three-layer Loss after iteration 94000: 0.47802144720135925\n",
      "three-layer Loss after iteration 95000: 0.47128769770623624\n",
      "three-layer Loss after iteration 96000: 0.4682886798017015\n",
      "three-layer Loss after iteration 97000: 0.46552772964683004\n",
      "three-layer Loss after iteration 98000: 0.46323713816759615\n",
      "three-layer Loss after iteration 99000: 0.4612145939832384\n",
      "three-layer Loss after iteration 100000: 0.4585390110787637\n",
      "three-layer Loss after iteration 101000: 0.45776023487346823\n",
      "three-layer Loss after iteration 102000: 0.45674935835622543\n",
      "three-layer Loss after iteration 103000: 0.45575000758582923\n",
      "three-layer Loss after iteration 104000: 0.4548812887129422\n",
      "three-layer Loss after iteration 105000: 0.4541858370465391\n",
      "three-layer Loss after iteration 106000: 0.45391293784284636\n",
      "three-layer Loss after iteration 107000: 0.4537505570153401\n",
      "three-layer Loss after iteration 108000: 0.45354879346676163\n",
      "three-layer Loss after iteration 109000: 0.4531270720013472\n",
      "three-layer Loss after iteration 110000: 0.45267808085226824\n",
      "three-layer Loss after iteration 111000: 0.4522750018436616\n",
      "three-layer Loss after iteration 112000: 0.4489420289164281\n",
      "three-layer Loss after iteration 113000: 0.4480571719279685\n",
      "three-layer Loss after iteration 114000: 0.4474324607770293\n",
      "three-layer Loss after iteration 115000: 0.4464772183619871\n",
      "three-layer Loss after iteration 116000: 0.4459545810424303\n",
      "three-layer Loss after iteration 117000: 0.4392051705556378\n",
      "three-layer Loss after iteration 118000: 0.4399151091439104\n",
      "three-layer Loss after iteration 119000: 0.43994327390185517\n",
      "6.402316574115689e-05 0.4399151091439104 0.43994327390185517\n",
      "three-layer Loss after iteration 0: 1364.2284480572544\n",
      "three-layer Loss after iteration 1000: 13.505989423699454\n",
      "three-layer Loss after iteration 2000: 8.95563568356288\n",
      "three-layer Loss after iteration 3000: 4.249194288691475\n",
      "three-layer Loss after iteration 4000: 2.7786882008900617\n",
      "three-layer Loss after iteration 5000: 2.7261856798869455\n",
      "three-layer Loss after iteration 6000: 2.2790501803155085\n",
      "three-layer Loss after iteration 7000: 2.1284411524028366\n",
      "three-layer Loss after iteration 8000: 1.71218469082814\n",
      "three-layer Loss after iteration 9000: 1.6876905035784058\n",
      "three-layer Loss after iteration 10000: 1.4098240222050058\n",
      "three-layer Loss after iteration 11000: 2.024557998120811\n",
      "three-layer Loss after iteration 12000: 2.454650572953527\n",
      "three-layer Loss after iteration 13000: 1.4695927114621885\n",
      "three-layer Loss after iteration 14000: 1.3326864999689956\n",
      "three-layer Loss after iteration 15000: 1.2076794163596614\n",
      "three-layer Loss after iteration 16000: 1.2774962335110414\n",
      "three-layer Loss after iteration 17000: 1.1945513659885751\n",
      "three-layer Loss after iteration 18000: 1.1336603232339273\n",
      "three-layer Loss after iteration 19000: 0.843112528242488\n",
      "three-layer Loss after iteration 20000: 1.0193502846333822\n",
      "three-layer Loss after iteration 21000: 1.0590212235909906\n",
      "three-layer Loss after iteration 22000: 0.9978577618102842\n",
      "three-layer Loss after iteration 23000: 0.9416815147820972\n",
      "three-layer Loss after iteration 24000: 0.9417586937289876\n",
      "8.195865128378305e-05 0.9416815147820972 0.9417586937289876\n",
      "three-layer Loss after iteration 0: 1317.1752012624697\n",
      "three-layer Loss after iteration 1000: 14.887893859200409\n",
      "three-layer Loss after iteration 2000: 7.889015430964533\n",
      "three-layer Loss after iteration 3000: 5.5306514297309075\n",
      "three-layer Loss after iteration 4000: 4.091418455605122\n",
      "three-layer Loss after iteration 5000: 2.5856391689005633\n",
      "three-layer Loss after iteration 6000: 1.5741387608057156\n",
      "three-layer Loss after iteration 7000: 2.022442381253144\n",
      "three-layer Loss after iteration 8000: 1.5086387018333993\n",
      "three-layer Loss after iteration 9000: 1.9423162512293184\n",
      "three-layer Loss after iteration 10000: 1.5475377176928082\n",
      "three-layer Loss after iteration 11000: 1.0939705577218066\n",
      "three-layer Loss after iteration 12000: 1.5628442242672584\n",
      "three-layer Loss after iteration 13000: 1.4997561030951543\n",
      "three-layer Loss after iteration 14000: 0.9622950480984046\n",
      "three-layer Loss after iteration 15000: 1.1513940314870796\n",
      "three-layer Loss after iteration 16000: 1.7916861119330425\n",
      "three-layer Loss after iteration 17000: 1.3962757056699917\n",
      "three-layer Loss after iteration 18000: 1.0914689624474903\n",
      "three-layer Loss after iteration 19000: 1.6836683091536346\n",
      "three-layer Loss after iteration 20000: 1.0404695042789034\n",
      "three-layer Loss after iteration 21000: 2.1393392810465652\n",
      "three-layer Loss after iteration 22000: 0.7337564089011791\n",
      "three-layer Loss after iteration 23000: 1.7087206008311857\n",
      "three-layer Loss after iteration 24000: 1.3009732287979596\n",
      "three-layer Loss after iteration 25000: 1.9261571304990224\n",
      "three-layer Loss after iteration 26000: 1.1863555312865508\n",
      "three-layer Loss after iteration 27000: 0.8924970153091943\n",
      "three-layer Loss after iteration 28000: 0.924948518818212\n",
      "three-layer Loss after iteration 29000: 0.845913715817183\n",
      "three-layer Loss after iteration 30000: 0.9097877228862575\n",
      "three-layer Loss after iteration 31000: 0.8188974805550816\n",
      "three-layer Loss after iteration 32000: 0.6142781889248725\n",
      "three-layer Loss after iteration 33000: 0.7477446462879428\n",
      "three-layer Loss after iteration 34000: 0.7135651670398697\n",
      "three-layer Loss after iteration 35000: 0.7039746252598608\n",
      "three-layer Loss after iteration 36000: 0.653165748667688\n",
      "three-layer Loss after iteration 37000: 0.6269259898448741\n",
      "three-layer Loss after iteration 38000: 0.5867667446440433\n",
      "three-layer Loss after iteration 39000: 0.9455506795010552\n",
      "three-layer Loss after iteration 40000: 0.5866859232507733\n",
      "three-layer Loss after iteration 41000: 0.6227886973277418\n",
      "three-layer Loss after iteration 42000: 0.6372262437307376\n",
      "three-layer Loss after iteration 43000: 0.6827116512546291\n",
      "three-layer Loss after iteration 44000: 0.5938113255498996\n",
      "three-layer Loss after iteration 45000: 0.8725475918570459\n",
      "three-layer Loss after iteration 46000: 1.2468915057653587\n",
      "three-layer Loss after iteration 47000: 0.7908052057958683\n",
      "three-layer Loss after iteration 48000: 0.7009182801731852\n",
      "three-layer Loss after iteration 49000: 0.6949912064630348\n",
      "three-layer Loss after iteration 50000: 0.6963534406664027\n",
      "three-layer Loss after iteration 51000: 0.652422518136475\n",
      "three-layer Loss after iteration 52000: 0.7053075895826415\n",
      "three-layer Loss after iteration 53000: 0.6606410265366791\n",
      "three-layer Loss after iteration 54000: 0.6226353596054437\n",
      "three-layer Loss after iteration 55000: 0.6405139189316088\n",
      "three-layer Loss after iteration 56000: 0.6274637484460868\n",
      "three-layer Loss after iteration 57000: 0.6206610895644306\n",
      "three-layer Loss after iteration 58000: 0.6173066202386225\n",
      "three-layer Loss after iteration 59000: 0.6095949254406602\n",
      "three-layer Loss after iteration 60000: 0.6044717629678773\n",
      "three-layer Loss after iteration 61000: 0.6005611245975024\n",
      "three-layer Loss after iteration 62000: 0.5932986310320055\n",
      "three-layer Loss after iteration 63000: 0.5853001122461562\n",
      "three-layer Loss after iteration 64000: 0.594077234285458\n",
      "three-layer Loss after iteration 65000: 0.5813812580072661\n",
      "three-layer Loss after iteration 66000: 0.577076258194969\n",
      "three-layer Loss after iteration 67000: 0.57306318299144\n",
      "three-layer Loss after iteration 68000: 0.568889529065521\n",
      "three-layer Loss after iteration 69000: 0.574067040471821\n",
      "three-layer Loss after iteration 70000: 0.5697945220286478\n",
      "three-layer Loss after iteration 71000: 0.5649833449840432\n",
      "three-layer Loss after iteration 72000: 0.5601368080394344\n",
      "three-layer Loss after iteration 73000: 0.557422935058713\n",
      "three-layer Loss after iteration 74000: 0.5524999445820205\n",
      "three-layer Loss after iteration 75000: 0.5489489855784323\n",
      "three-layer Loss after iteration 76000: 0.5461709630067053\n",
      "three-layer Loss after iteration 77000: 0.5456385745213382\n",
      "three-layer Loss after iteration 78000: 0.5430493241619931\n",
      "three-layer Loss after iteration 79000: 0.5096803640611683\n",
      "three-layer Loss after iteration 80000: 0.6334217759066393\n",
      "three-layer Loss after iteration 81000: 0.5679043089154734\n",
      "three-layer Loss after iteration 82000: 0.5410129039972911\n",
      "three-layer Loss after iteration 83000: 0.5367877906081369\n",
      "three-layer Loss after iteration 84000: 0.533045275688046\n",
      "three-layer Loss after iteration 85000: 0.5289692561562628\n",
      "three-layer Loss after iteration 86000: 0.5254934560385212\n",
      "three-layer Loss after iteration 87000: 0.5229029518869768\n",
      "three-layer Loss after iteration 88000: 0.5183483375086972\n",
      "three-layer Loss after iteration 89000: 0.5175927615963818\n",
      "three-layer Loss after iteration 90000: 0.5147310471893304\n",
      "three-layer Loss after iteration 91000: 0.5146135347095363\n",
      "three-layer Loss after iteration 92000: 0.5140823751473041\n",
      "three-layer Loss after iteration 93000: 0.48037175118135383\n",
      "three-layer Loss after iteration 94000: 0.497508785583687\n",
      "three-layer Loss after iteration 95000: 0.5060922767412896\n",
      "three-layer Loss after iteration 96000: 0.5031154302172265\n",
      "three-layer Loss after iteration 97000: 0.4974019302867642\n",
      "three-layer Loss after iteration 98000: 0.5193125477486795\n",
      "three-layer Loss after iteration 99000: 0.5236150931976363\n",
      "three-layer Loss after iteration 100000: 0.5237057051797662\n",
      "three-layer Loss after iteration 101000: 0.5121553465616061\n",
      "three-layer Loss after iteration 102000: 0.5124940537751235\n",
      "three-layer Loss after iteration 103000: 0.49871994763975575\n",
      "three-layer Loss after iteration 104000: 0.5011408718943681\n",
      "three-layer Loss after iteration 105000: 0.5009146923055156\n",
      "three-layer Loss after iteration 106000: 0.4976290995809449\n",
      "three-layer Loss after iteration 107000: 0.4978003907085046\n",
      "three-layer Loss after iteration 108000: 0.5002939626120015\n",
      "three-layer Loss after iteration 109000: 0.49969965950040823\n",
      "three-layer Loss after iteration 110000: 0.4955294372729558\n",
      "three-layer Loss after iteration 111000: 0.4922849776353905\n",
      "three-layer Loss after iteration 112000: 0.49073348442258824\n",
      "three-layer Loss after iteration 113000: 0.48861271368002845\n",
      "three-layer Loss after iteration 114000: 0.48628353242332245\n",
      "three-layer Loss after iteration 115000: 0.48412602310376796\n",
      "three-layer Loss after iteration 116000: 0.48272886176480256\n",
      "three-layer Loss after iteration 117000: 0.48207492427552034\n",
      "three-layer Loss after iteration 118000: 0.5540612480726108\n",
      "three-layer Loss after iteration 119000: 0.43128392278145977\n",
      "three-layer Loss after iteration 120000: 0.45592963369400624\n",
      "three-layer Loss after iteration 121000: 0.4364983521634762\n",
      "three-layer Loss after iteration 122000: 0.5002090380456498\n",
      "three-layer Loss after iteration 123000: 0.4234572808801945\n",
      "three-layer Loss after iteration 124000: 0.40022285610526626\n",
      "three-layer Loss after iteration 125000: 0.4109851262283697\n",
      "three-layer Loss after iteration 126000: 0.4074570283608029\n",
      "three-layer Loss after iteration 127000: 0.4120218873721616\n",
      "three-layer Loss after iteration 128000: 0.3876098658558012\n",
      "three-layer Loss after iteration 129000: 0.3882748055210933\n",
      "three-layer Loss after iteration 130000: 0.3632863509587691\n",
      "three-layer Loss after iteration 131000: 0.3524648084075836\n",
      "three-layer Loss after iteration 132000: 0.3510941132275207\n",
      "three-layer Loss after iteration 133000: 0.361749425225455\n",
      "three-layer Loss after iteration 134000: 0.34897778745002694\n",
      "three-layer Loss after iteration 135000: 0.39763162030447485\n",
      "three-layer Loss after iteration 136000: 0.3969275236373218\n",
      "three-layer Loss after iteration 137000: 0.3709443059411525\n",
      "three-layer Loss after iteration 138000: 0.375062463985028\n",
      "three-layer Loss after iteration 139000: 0.48549191358590765\n",
      "three-layer Loss after iteration 140000: 0.3491058549158228\n",
      "three-layer Loss after iteration 141000: 0.3502035191182966\n",
      "three-layer Loss after iteration 142000: 0.4546503425949619\n",
      "three-layer Loss after iteration 143000: 0.34494395255167226\n",
      "three-layer Loss after iteration 144000: 0.3401399171769432\n",
      "three-layer Loss after iteration 145000: 0.4931490432229952\n",
      "three-layer Loss after iteration 146000: 0.3419711751557237\n",
      "three-layer Loss after iteration 147000: 0.49399037595051165\n",
      "three-layer Loss after iteration 148000: 0.35151505199374816\n",
      "three-layer Loss after iteration 149000: 0.35451158113695774\n",
      "three-layer Loss after iteration 150000: 0.3500963262879766\n",
      "three-layer Loss after iteration 151000: 0.3284005915475895\n",
      "three-layer Loss after iteration 152000: 0.4796400654087582\n",
      "three-layer Loss after iteration 153000: 0.3955976771264419\n",
      "three-layer Loss after iteration 154000: 0.3921974314953461\n",
      "three-layer Loss after iteration 155000: 0.45962830393615606\n",
      "three-layer Loss after iteration 156000: 0.3757069423320602\n",
      "three-layer Loss after iteration 157000: 0.399825740243445\n",
      "three-layer Loss after iteration 158000: 0.47760534514999325\n",
      "three-layer Loss after iteration 159000: 0.42137785404334066\n",
      "three-layer Loss after iteration 160000: 0.40513775174899264\n",
      "three-layer Loss after iteration 161000: 0.39759495168111203\n",
      "three-layer Loss after iteration 162000: 0.3973632315179072\n",
      "three-layer Loss after iteration 163000: 0.3606050732062915\n",
      "three-layer Loss after iteration 164000: 0.3825663279606233\n",
      "three-layer Loss after iteration 165000: 0.3846613099138578\n",
      "three-layer Loss after iteration 166000: 0.38277426279994503\n",
      "three-layer Loss after iteration 167000: 0.3811481726162855\n",
      "three-layer Loss after iteration 168000: 0.39840026437605186\n",
      "three-layer Loss after iteration 169000: 0.3926198329244239\n",
      "three-layer Loss after iteration 170000: 0.39242075438622265\n",
      "three-layer Loss after iteration 171000: 0.4130248206154683\n",
      "three-layer Loss after iteration 172000: 0.35023066427618743\n",
      "three-layer Loss after iteration 173000: 0.3457948565425594\n",
      "three-layer Loss after iteration 174000: 0.3675117437482262\n",
      "three-layer Loss after iteration 175000: 0.3592417478352718\n",
      "three-layer Loss after iteration 176000: 0.3965926805386299\n",
      "three-layer Loss after iteration 177000: 0.37351589136574453\n",
      "three-layer Loss after iteration 178000: 0.3814499796612748\n",
      "three-layer Loss after iteration 179000: 0.37896263991057405\n",
      "three-layer Loss after iteration 180000: 0.3731994795487099\n",
      "three-layer Loss after iteration 181000: 0.3747224610105877\n",
      "three-layer Loss after iteration 182000: 0.3682059379812165\n",
      "three-layer Loss after iteration 183000: 0.3619313374909194\n",
      "three-layer Loss after iteration 184000: 0.3566817540771502\n",
      "three-layer Loss after iteration 185000: 0.35459305507409544\n",
      "three-layer Loss after iteration 186000: 0.35294971422232907\n",
      "three-layer Loss after iteration 187000: 0.3510653765129235\n",
      "three-layer Loss after iteration 188000: 0.3489895784428163\n",
      "three-layer Loss after iteration 189000: 0.3469255077639102\n",
      "three-layer Loss after iteration 190000: 0.34521736581371454\n",
      "three-layer Loss after iteration 191000: 0.3431440583660431\n",
      "three-layer Loss after iteration 192000: 0.3413803601365333\n",
      "three-layer Loss after iteration 193000: 0.34012308165161587\n",
      "three-layer Loss after iteration 194000: 0.3378999595360134\n",
      "three-layer Loss after iteration 195000: 0.2978559528949145\n",
      "three-layer Loss after iteration 196000: 0.27356746331978893\n",
      "three-layer Loss after iteration 197000: 0.26690066340525503\n",
      "three-layer Loss after iteration 198000: 0.3219465084402969\n",
      "three-layer Loss after iteration 199000: 0.30140328882353173\n",
      "three-layer Loss after iteration 200000: 0.3209630599438671\n",
      "three-layer Loss after iteration 201000: 0.3265318539283731\n",
      "three-layer Loss after iteration 202000: 0.3561567165870051\n",
      "three-layer Loss after iteration 203000: 0.2934691203962033\n",
      "three-layer Loss after iteration 204000: 0.3199911976773071\n",
      "three-layer Loss after iteration 205000: 0.2743901857020709\n",
      "three-layer Loss after iteration 206000: 0.3615598216745514\n",
      "three-layer Loss after iteration 207000: 0.2890898304477552\n",
      "three-layer Loss after iteration 208000: 0.2932855303439986\n",
      "three-layer Loss after iteration 209000: 0.33089132561092394\n",
      "three-layer Loss after iteration 210000: 0.2983727025842198\n",
      "three-layer Loss after iteration 211000: 0.29907747089849895\n",
      "three-layer Loss after iteration 212000: 0.2825266489860477\n",
      "three-layer Loss after iteration 213000: 0.39329554616322804\n",
      "three-layer Loss after iteration 214000: 0.29374459759243043\n",
      "three-layer Loss after iteration 215000: 0.28724672709032967\n",
      "three-layer Loss after iteration 216000: 0.30256771912537767\n",
      "three-layer Loss after iteration 217000: 0.27528007630442997\n",
      "three-layer Loss after iteration 218000: 0.290645567261315\n",
      "three-layer Loss after iteration 219000: 0.2914434571960666\n",
      "three-layer Loss after iteration 220000: 0.2849900834695157\n",
      "three-layer Loss after iteration 221000: 0.2902735358868887\n",
      "three-layer Loss after iteration 222000: 0.2958073206753391\n",
      "three-layer Loss after iteration 223000: 0.28581057022578354\n",
      "three-layer Loss after iteration 224000: 0.2874202354151554\n",
      "three-layer Loss after iteration 225000: 0.3114253434184476\n",
      "three-layer Loss after iteration 226000: 0.2793162533088156\n",
      "three-layer Loss after iteration 227000: 0.2953215704315385\n",
      "three-layer Loss after iteration 228000: 0.29327368613039356\n",
      "three-layer Loss after iteration 229000: 0.2962623967896597\n",
      "three-layer Loss after iteration 230000: 0.30286105824448595\n",
      "three-layer Loss after iteration 231000: 0.3088135298570813\n",
      "three-layer Loss after iteration 232000: 0.3023250982629245\n",
      "three-layer Loss after iteration 233000: 0.297196322743841\n",
      "three-layer Loss after iteration 234000: 0.29494469188375366\n",
      "three-layer Loss after iteration 235000: 0.2935814156128948\n",
      "three-layer Loss after iteration 236000: 0.2924325107422082\n",
      "three-layer Loss after iteration 237000: 0.291740706408038\n",
      "three-layer Loss after iteration 238000: 0.29070813783500593\n",
      "three-layer Loss after iteration 239000: 0.2901020137810074\n",
      "three-layer Loss after iteration 240000: 0.2894114560332821\n",
      "three-layer Loss after iteration 241000: 0.2887274098048903\n",
      "three-layer Loss after iteration 242000: 0.28787341091147534\n",
      "three-layer Loss after iteration 243000: 0.28731940544375373\n",
      "three-layer Loss after iteration 244000: 0.2863769780428192\n",
      "three-layer Loss after iteration 245000: 0.2867985031873361\n",
      "three-layer Loss after iteration 246000: 0.286110740093102\n",
      "three-layer Loss after iteration 247000: 0.28534101335656625\n",
      "three-layer Loss after iteration 248000: 0.284830654250503\n",
      "three-layer Loss after iteration 249000: 0.28470888972398634\n",
      "three-layer Loss after iteration 250000: 0.2842650294849827\n",
      "three-layer Loss after iteration 251000: 0.2836770078774568\n",
      "three-layer Loss after iteration 252000: 0.28350400266950243\n",
      "three-layer Loss after iteration 253000: 0.2829540525583014\n",
      "three-layer Loss after iteration 254000: 0.2824065076752391\n",
      "three-layer Loss after iteration 255000: 0.27723351790196826\n",
      "three-layer Loss after iteration 256000: 0.28044498549099206\n",
      "three-layer Loss after iteration 257000: 0.28152505228930463\n",
      "three-layer Loss after iteration 258000: 0.2807656665781359\n",
      "three-layer Loss after iteration 259000: 0.2700653199349771\n",
      "three-layer Loss after iteration 260000: 0.2762398211487177\n",
      "three-layer Loss after iteration 261000: 0.2762788181074983\n",
      "three-layer Loss after iteration 262000: 0.276755639975602\n",
      "three-layer Loss after iteration 263000: 0.2768071086067409\n",
      "three-layer Loss after iteration 264000: 0.27670117129039934\n",
      "three-layer Loss after iteration 265000: 0.27681993710726954\n",
      "three-layer Loss after iteration 266000: 0.276114993144695\n",
      "three-layer Loss after iteration 267000: 0.27585881066168955\n",
      "three-layer Loss after iteration 268000: 0.2758216838378364\n",
      "three-layer Loss after iteration 269000: 0.27807835120390134\n",
      "three-layer Loss after iteration 270000: 0.2766480929911297\n",
      "three-layer Loss after iteration 271000: 0.2762507409249014\n",
      "three-layer Loss after iteration 272000: 0.27598451075837777\n",
      "three-layer Loss after iteration 273000: 0.2760016329281512\n",
      "6.204032873585403e-05 0.27598451075837777 0.2760016329281512\n",
      "three-layer Loss after iteration 0: 1718.2099415726204\n",
      "three-layer Loss after iteration 1000: 9.540563534281182\n",
      "three-layer Loss after iteration 2000: 8.415981454354988\n",
      "three-layer Loss after iteration 3000: 8.601475582419608\n",
      "three-layer Loss after iteration 4000: 6.097176918414231\n",
      "three-layer Loss after iteration 5000: 5.760843106312611\n",
      "three-layer Loss after iteration 6000: 5.086245722877094\n",
      "three-layer Loss after iteration 7000: 4.771497475677307\n",
      "three-layer Loss after iteration 8000: 5.907907396068021\n",
      "three-layer Loss after iteration 9000: 3.4210156553129725\n",
      "three-layer Loss after iteration 10000: 4.391836767374327\n",
      "three-layer Loss after iteration 11000: 4.004576882913796\n",
      "three-layer Loss after iteration 12000: 3.829173984260376\n",
      "three-layer Loss after iteration 13000: 3.3757689147899206\n",
      "three-layer Loss after iteration 14000: 3.7344740846411275\n",
      "three-layer Loss after iteration 15000: 3.7909568294922713\n",
      "three-layer Loss after iteration 16000: 3.0889298587197564\n",
      "three-layer Loss after iteration 17000: 2.410330467615539\n",
      "three-layer Loss after iteration 18000: 2.671753732066204\n",
      "three-layer Loss after iteration 19000: 3.0622966431679335\n",
      "three-layer Loss after iteration 20000: 2.965837758530869\n",
      "three-layer Loss after iteration 21000: 2.813634425129243\n",
      "three-layer Loss after iteration 22000: 2.9518356420092804\n",
      "three-layer Loss after iteration 23000: 2.6823097067365147\n",
      "three-layer Loss after iteration 24000: 2.5563052136487263\n",
      "three-layer Loss after iteration 25000: 2.3240447588906026\n",
      "three-layer Loss after iteration 26000: 4.1975547415060355\n",
      "three-layer Loss after iteration 27000: 2.4142166000617276\n",
      "three-layer Loss after iteration 28000: 2.2846596457934334\n",
      "three-layer Loss after iteration 29000: 2.136499103345077\n",
      "three-layer Loss after iteration 30000: 2.222858559445382\n",
      "three-layer Loss after iteration 31000: 1.9378627307544154\n",
      "three-layer Loss after iteration 32000: 1.9697326288568542\n",
      "three-layer Loss after iteration 33000: 1.8656090845708577\n",
      "three-layer Loss after iteration 34000: 1.8337894784829225\n",
      "three-layer Loss after iteration 35000: 1.826454277858996\n",
      "three-layer Loss after iteration 36000: 1.7351389048580086\n",
      "three-layer Loss after iteration 37000: 1.6803749029495798\n",
      "three-layer Loss after iteration 38000: 1.6910928812434394\n",
      "three-layer Loss after iteration 39000: 1.6335249263275136\n",
      "three-layer Loss after iteration 40000: 1.5985727334619453\n",
      "three-layer Loss after iteration 41000: 1.5499379794604793\n",
      "three-layer Loss after iteration 42000: 1.406288735010219\n",
      "three-layer Loss after iteration 43000: 1.4628531238395062\n",
      "three-layer Loss after iteration 44000: 1.4676292866582628\n",
      "three-layer Loss after iteration 45000: 1.5311046845418432\n",
      "three-layer Loss after iteration 46000: 1.4869182634440914\n",
      "three-layer Loss after iteration 47000: 1.4742677512337559\n",
      "three-layer Loss after iteration 48000: 1.4599729425429777\n",
      "three-layer Loss after iteration 49000: 1.433407417159308\n",
      "three-layer Loss after iteration 50000: 1.4230086953289518\n",
      "three-layer Loss after iteration 51000: 1.414895742915084\n",
      "three-layer Loss after iteration 52000: 1.4183899309803099\n",
      "three-layer Loss after iteration 53000: 1.4556953477182784\n",
      "three-layer Loss after iteration 54000: 1.4540284323911843\n",
      "three-layer Loss after iteration 55000: 1.452544531372558\n",
      "three-layer Loss after iteration 56000: 1.4488241758147367\n",
      "three-layer Loss after iteration 57000: 1.4403335542645643\n",
      "three-layer Loss after iteration 58000: 1.4327882544147144\n",
      "three-layer Loss after iteration 59000: 1.4226604446261615\n",
      "three-layer Loss after iteration 60000: 1.4147840007290158\n",
      "three-layer Loss after iteration 61000: 1.4069296041518684\n",
      "three-layer Loss after iteration 62000: 1.3942574166963722\n",
      "three-layer Loss after iteration 63000: 1.3899493671291785\n",
      "three-layer Loss after iteration 64000: 1.3835412240437717\n",
      "three-layer Loss after iteration 65000: 1.3737047911951388\n",
      "three-layer Loss after iteration 66000: 1.3677205486068633\n",
      "three-layer Loss after iteration 67000: 1.3651088136892002\n",
      "three-layer Loss after iteration 68000: 1.3610286751807064\n",
      "three-layer Loss after iteration 69000: 1.351766463004513\n",
      "three-layer Loss after iteration 70000: 1.350927784337266\n",
      "three-layer Loss after iteration 71000: 1.345549189391398\n",
      "three-layer Loss after iteration 72000: 1.3387871457849854\n",
      "three-layer Loss after iteration 73000: 1.3370359950352464\n",
      "three-layer Loss after iteration 74000: 1.328277388758235\n",
      "three-layer Loss after iteration 75000: 1.3290776483105247\n",
      "three-layer Loss after iteration 76000: 1.322532455779394\n",
      "three-layer Loss after iteration 77000: 1.3193384137529327\n",
      "three-layer Loss after iteration 78000: 1.3157366815562712\n",
      "three-layer Loss after iteration 79000: 1.3130731838452787\n",
      "three-layer Loss after iteration 80000: 1.3089706879062548\n",
      "three-layer Loss after iteration 81000: 1.306688039399087\n",
      "three-layer Loss after iteration 82000: 1.3052594367023806\n",
      "three-layer Loss after iteration 83000: 1.3048903988757148\n",
      "three-layer Loss after iteration 84000: 1.2982267155037845\n",
      "three-layer Loss after iteration 85000: 1.2973688236065894\n",
      "three-layer Loss after iteration 86000: 1.292749806870606\n",
      "three-layer Loss after iteration 87000: 1.2913937230074128\n",
      "three-layer Loss after iteration 88000: 1.2957201876903095\n",
      "three-layer Loss after iteration 89000: 1.2911675959274038\n",
      "three-layer Loss after iteration 90000: 1.2852191921242133\n",
      "three-layer Loss after iteration 91000: 1.2793862056693555\n",
      "three-layer Loss after iteration 92000: 1.249657720920243\n",
      "three-layer Loss after iteration 93000: 1.2649289550445288\n",
      "three-layer Loss after iteration 94000: 1.266498642460675\n",
      "three-layer Loss after iteration 95000: 1.2656272173342555\n",
      "three-layer Loss after iteration 96000: 1.2748032180221058\n",
      "three-layer Loss after iteration 97000: 1.2660574579398576\n",
      "three-layer Loss after iteration 98000: 1.2642854628048796\n",
      "three-layer Loss after iteration 99000: 1.2596202393784846\n",
      "three-layer Loss after iteration 100000: 1.2602006475773047\n",
      "three-layer Loss after iteration 101000: 1.2630685201282772\n",
      "three-layer Loss after iteration 102000: 1.2544509989429706\n",
      "three-layer Loss after iteration 103000: 1.2455195926368121\n",
      "three-layer Loss after iteration 104000: 1.243852599686197\n",
      "three-layer Loss after iteration 105000: 1.2522474194662514\n",
      "three-layer Loss after iteration 106000: 1.2444740395021445\n",
      "three-layer Loss after iteration 107000: 1.2455390085639402\n",
      "three-layer Loss after iteration 108000: 1.2471906357655684\n",
      "three-layer Loss after iteration 109000: 1.2424288557836953\n",
      "three-layer Loss after iteration 110000: 1.2273343332900792\n",
      "three-layer Loss after iteration 111000: 1.235211687822195\n",
      "three-layer Loss after iteration 112000: 1.2336286970179322\n",
      "three-layer Loss after iteration 113000: 1.2269690081713442\n",
      "three-layer Loss after iteration 114000: 1.2408589404386416\n",
      "three-layer Loss after iteration 115000: 1.2197939565323652\n",
      "three-layer Loss after iteration 116000: 1.2239393931839828\n",
      "three-layer Loss after iteration 117000: 1.2226699560577778\n",
      "three-layer Loss after iteration 118000: 1.2189007519796584\n",
      "three-layer Loss after iteration 119000: 1.2187291929948016\n",
      "three-layer Loss after iteration 120000: 1.3239074227634755\n",
      "three-layer Loss after iteration 121000: 1.3289202846851162\n",
      "three-layer Loss after iteration 122000: 1.306011840352605\n",
      "three-layer Loss after iteration 123000: 1.3430016980359576\n",
      "three-layer Loss after iteration 124000: 1.2587171606023075\n",
      "three-layer Loss after iteration 125000: 1.2673095104042476\n",
      "three-layer Loss after iteration 126000: 1.2587353113222532\n",
      "three-layer Loss after iteration 127000: 1.265696518659013\n",
      "three-layer Loss after iteration 128000: 1.2553911596284433\n",
      "three-layer Loss after iteration 129000: 1.2448731160727309\n",
      "three-layer Loss after iteration 130000: 1.2406650311035066\n",
      "three-layer Loss after iteration 131000: 1.2337552485115149\n",
      "three-layer Loss after iteration 132000: 1.2220351947883172\n",
      "three-layer Loss after iteration 133000: 1.2187273028320704\n",
      "three-layer Loss after iteration 134000: 1.210665077233816\n",
      "three-layer Loss after iteration 135000: 1.2027722001754162\n",
      "three-layer Loss after iteration 136000: 1.1974970310246418\n",
      "three-layer Loss after iteration 137000: 1.1892382973034705\n",
      "three-layer Loss after iteration 138000: 1.1804086084698038\n",
      "three-layer Loss after iteration 139000: 1.1752048650258893\n",
      "three-layer Loss after iteration 140000: 1.1668510240031629\n",
      "three-layer Loss after iteration 141000: 1.1606098941598748\n",
      "three-layer Loss after iteration 142000: 1.1542955508021744\n",
      "three-layer Loss after iteration 143000: 1.136194368404027\n",
      "three-layer Loss after iteration 144000: 1.1290453918687504\n",
      "three-layer Loss after iteration 145000: 1.1210878512408684\n",
      "three-layer Loss after iteration 146000: 1.1175353038616085\n",
      "three-layer Loss after iteration 147000: 1.108971636103068\n",
      "three-layer Loss after iteration 148000: 1.1032433098380798\n",
      "three-layer Loss after iteration 149000: 1.097288797279478\n",
      "three-layer Loss after iteration 150000: 1.0928578160919131\n",
      "three-layer Loss after iteration 151000: 1.087040518476074\n",
      "three-layer Loss after iteration 152000: 1.0827591798182488\n",
      "three-layer Loss after iteration 153000: 1.0793149981638106\n",
      "three-layer Loss after iteration 154000: 1.076016769897852\n",
      "three-layer Loss after iteration 155000: 1.0725616634905843\n",
      "three-layer Loss after iteration 156000: 1.0760994233730639\n",
      "three-layer Loss after iteration 157000: 1.0689341953416966\n",
      "three-layer Loss after iteration 158000: 1.0646402156650139\n",
      "three-layer Loss after iteration 159000: 1.0542288447460746\n",
      "three-layer Loss after iteration 160000: 1.0521801761795324\n",
      "three-layer Loss after iteration 161000: 1.0469579036148622\n",
      "three-layer Loss after iteration 162000: 1.046234396376147\n",
      "three-layer Loss after iteration 163000: 1.0452471029257806\n",
      "three-layer Loss after iteration 164000: 1.0432232834160313\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "numDataOne = 15\n",
    "numData = numDataOne*numDataOne\n",
    "# create data for regression\n",
    "xs=np.linspace(-8,8,numDataOne)\n",
    "ys=np.linspace(-8,8,numDataOne)\n",
    "counter=0\n",
    "\n",
    "X = np.empty((0, 2))\n",
    "y = np.empty((0, 1))\n",
    "\n",
    "for r in np.arange(0,numDataOne):\n",
    "    for c in np.arange(0,numDataOne):\n",
    "        X = np.vstack((X, [xs[r], ys[c]]))\n",
    "        y = np.vstack((y, xs[r]**2 + ys[c]**2 + 1))\n",
    "\n",
    "# training set size\n",
    "num_examples = len(X)\n",
    "# input layer dimensionality\n",
    "nn_input_dim = 2\n",
    "# output layer dimensionality\n",
    "nn_output_dim = 1\n",
    "# learning rate for gradient descent\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Parameters\n",
    "num_runs = 20\n",
    "tolerance = 0.0001\n",
    "max_iterations = 1000000\n",
    "\n",
    "# Data structures to store results\n",
    "results_old = {\"8\": {\"losses\": [], \"iterations\": []}}\n",
    "results_new = {\"4+4\": {\"losses\": [], \"iterations\": []}}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# Test the old (2-layer) network with 8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 8, 1)\n",
    "    model_old, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"8\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"8\"][\"iterations\"].append(iterations)\n",
    "\n",
    "# Test the new (3-layer) network with 4+4 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [4,4], 1)\n",
    "    model_new, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"4+4\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"4+4\"][\"iterations\"].append(iterations)\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"16\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"8+8\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 16, 1)\n",
    "    model_old, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"16\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 8+8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [8, 8], 1)\n",
    "    model_new, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"8+8\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"8+8\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "'''\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"32\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"16+16\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 32 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 32, 1)\n",
    "    _, losses, iterations = old_train(model_old, X, y, \n",
    "                                      num_passes=max_iterations, \n",
    "                                      learning_rate=learning_rate, \n",
    "                                      tolerance=tolerance)\n",
    "    results_old[\"32\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"32\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 16+16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [16, 16], 1)\n",
    "    _, losses, iterations = train(model_new, X, y, \n",
    "                                  num_passes=max_iterations, \n",
    "                                  learning_rate=learning_rate, \n",
    "                                  tolerance=tolerance)\n",
    "    results_new[\"16+16\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"16+16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "'''\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-27T04:40:54.518509Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate average losses for old and new models for different configurations\n",
    "avg_losses_old = {config: np.mean(data['losses']) for config, data in results_old.items()}\n",
    "avg_losses_new = {config: np.mean(data['losses']) for config, data in results_new.items()}\n",
    "\n",
    "# Bar chart for average loss values\n",
    "labels = [f\"{old_key} & {new_key}\" for old_key, new_key in zip(avg_losses_old.keys(), avg_losses_new.keys())]\n",
    "\n",
    "\n",
    "old_vals = list(avg_losses_old.values())\n",
    "new_vals = list(avg_losses_new.values())\n",
    "bar_width = 0.35\n",
    "\n",
    "\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "print(old_vals, new_vals)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(r1[:len(old_vals)], old_vals, color='b', width=bar_width, edgecolor='grey', label='2-layer Model')\n",
    "plt.bar(r2[:len(new_vals)], new_vals, color='r', width=bar_width, edgecolor='grey', label='3-layer Model')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Average Losses for 2-layer vs 3-layer Models', fontweight='bold')\n",
    "plt.xlabel('Model Configuration', fontweight='bold')\n",
    "plt.ylabel('Average Loss', fontweight='bold')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xticks(r1 + 0.5 * bar_width, labels)  # This ensures labels are centered between the old and new model bars\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(avg_losses_old, avg_losses_new)\n",
    "\n",
    "# Calculate average iterations for old and new models for different configurations\n",
    "avg_iterations_old = {config: np.mean(data[\"iterations\"]) for config, data in results_old.items()}\n",
    "avg_iterations_new = {config: np.mean(data[\"iterations\"]) for config, data in results_new.items()}\n",
    "\n",
    "# Bar chart for average iteration values\n",
    "old_it_vals = list(avg_iterations_old.values())\n",
    "new_it_vals = list(avg_iterations_new.values())\n",
    "\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(r1[:len(old_it_vals)], old_it_vals, color='b', width=bar_width, edgecolor='grey', label='2-layer Model')\n",
    "plt.bar(r2[:len(new_it_vals)], new_it_vals, color='r', width=bar_width, edgecolor='grey', label='3-layer Model')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Average Iterations for Convergence for 2-layer vs 3-layer Models', fontweight='bold')\n",
    "plt.xlabel('Model Configuration', fontweight='bold')\n",
    "plt.ylabel('Average Iterations', fontweight='bold')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xticks([r + 0.5*bar_width for r in range(len(labels))], labels)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(avg_iterations_old, avg_iterations_new)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion\n",
    "\n",
    "### \n",
    "<p>\n",
    "When we compare the 2-layer model with the 3-layer model, it's evident that the 2-layer model consistently demonstrates superior performance in terms of loss. \n",
    "\n",
    "This essentially means that, on average, the 2-layer model's predictions are closer to the actual values than those of the 3-layer model. \n",
    "\n",
    "Such a result can be counterintuitive because one might often assume that adding more layers to a neural network, thereby increasing its depth, would inherently lead to improved performance. \n",
    "\n",
    "However, in some scenarios, simpler models with fewer layers can capture the underlying patterns in the data more effectively without being susceptible to overfitting. \n",
    "\n",
    "\n",
    "\n",
    "When we shift our focus from loss values to the number of iterations required for convergence, the distinction between the two models becomes less clear-cut. \n",
    "\n",
    "The iterations to convergence it gives us an idea about the efficiency and speed of the learning process. \n",
    "\n",
    "In this case, the difference between the iterations to converge varies only on the number of neurons and not the number of layers.\n",
    "\n",
    "Specifically, as the number of neurons in the model increases, there is a corresponding increase in the number of iterations required for convergence. \n",
    "\n",
    "This seems to suggests that while models with a greater number of neurons have a higher capacity to learn complex patterns and nuances in data, this enhanced capacity comes with the trade-off of potentially longer training times. \n",
    "\n",
    "This observation underscores an essential principle in neural network design: increasing model complexity doesn't always lead to faster or better learning. \n",
    "\n",
    "Instead, it might introduce challenges in terms of training efficiency and risk of overfitting.\n",
    "\n",
    "In conclusion, the exploration of these models underscores the multifaceted nature of machine learning model evaluation. \n",
    "\n",
    "While the 2-layer model exhibits better loss values, the intricacies of model convergence, especially in relation to model complexity, emphasize the importance of a holistic approach to model assessment and selection.\n",
    "\n",
    "\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus: arbitrary number of layers (20 points):\n",
    "Change the functions such that they can accept an arbitrary number of layers, but\n",
    "keep the overall call-logic and training loops the same - do NOT use classes! For this,\n",
    "you will need to play around with the dictionaries in create_model, forward,\n",
    "backprop.\n"
   ],
   "metadata": {
    "id": "t8NWou_MVkxi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a multi-layer neural network\n",
    "def create_multilayer_model(X, layer_sizes):\n",
    "    model = {}\n",
    "    # using ReLU as the default activation function\n",
    "    model['activation_function'] = 'relu'  \n",
    "\n",
    "    # Create weights and biases for each layer based on layer_sizes\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        model[f'W{i+1}'] = np.random.randn(layer_sizes[i], layer_sizes[i+1]) / np.sqrt(layer_sizes[i])\n",
    "        model[f'b{i+1}'] = np.zeros((1, layer_sizes[i+1]))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define the forward pass given a model and data\n",
    "def feed_forward_multilayer(model, x):\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "\n",
    "    z = {}\n",
    "    a = {}\n",
    "    a[0] = x  # the input layer\n",
    "\n",
    "    # Compute activations and outputs for each layer\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "    for i in range(1, num_layers+1):\n",
    "        z[i] = a[i-1].dot(model[f'W{i}']) + model[f'b{i}']\n",
    "        a[i] = act_func(z[i])\n",
    "\n",
    "    return z, a\n",
    "\n",
    "# define the regression loss\n",
    "def calculate_loss_multilayer(model, X, y):\n",
    "    z, a = feed_forward_multilayer(model, X)\n",
    "    out = a[len(a) - 1]\n",
    "    \n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "    # data_loss = np.mean((y - output) ** 2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# back-propagation for the multi-layer network\n",
    "def backprop_multilayer(X, y, model, z, a):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "\n",
    "    # Initialize the gradients\n",
    "    dW = {}\n",
    "    db = {}\n",
    "    delta = {}\n",
    "\n",
    "    # Compute the error for the last layer\n",
    "    delta[num_layers] = a[num_layers] - y\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    for i in reversed(range(1, num_layers+1)):\n",
    "        dW[i] = a[i-1].T.dot(delta[i]) / m\n",
    "        db[i] = np.sum(delta[i], axis=0, keepdims=True) / m\n",
    "        \n",
    "        if i > 1:  # Skip delta computation for the input layer\n",
    "            delta[i-1] = delta[i].dot(model[f'W{i}'].T) * act_func_derivative(a[i-1])\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "# training loop\n",
    "def train_multilayer(model, X, y, num_passes=100000, learning_rate=0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z, a = feed_forward_multilayer(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW, db = backprop_multilayer(X, y, model, z, a)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for key in dW:\n",
    "            model[f'W{key}'] -= learning_rate * dW[key]\n",
    "            model[f'b{key}'] -= learning_rate * db[key]\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss_multilayer(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(f\"Loss after iteration {i}: {loss}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "id": "bk4EXE1lVkg1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# architectures in lists of layer sizes\n",
    "architectures = [[2, 8, 1], [2, 4, 4, 1], [2, 16, 1], [2, 8, 8, 1]]\n",
    "\n",
    "# re-run the model training process with the provided architectures\n",
    "results = {}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# create and train multiple models with different layer sizes\n",
    "for arch in architectures:\n",
    "    model = create_multilayer_model(X, arch)\n",
    "    trained_model, losses, iterations = train_multilayer(model, X, y, num_passes=1000000, learning_rate=0.001, tolerance=0.00001)\n",
    "    results[str(arch)] = losses\n",
    "    \n",
    "    #iterations\n",
    "    results[str(arch)+'iter'] = iterations\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extracting data for plotting\n",
    "print(results.keys())\n",
    "\n",
    "configs_two_layer = ['[2, 8, 1]', '[2, 16, 1]']\n",
    "configs_three_layer = ['[2, 4, 4, 1]', '[2, 8, 8, 1]']\n",
    "\n",
    "# Compute Average losses\n",
    "average_losses_two_layer = [np.mean(results[config]) for config in configs_two_layer]\n",
    "average_losses_three_layer = [np.mean(results[config]) for config in configs_three_layer]\n",
    "\n",
    "iterations_of_convergence_two_layer = [np.mean(results[config+'iter']) for config in configs_two_layer]\n",
    "iterations_of_convergence_three_layer = [np.mean(results[config+'iter']) for config in configs_three_layer]\n",
    "\n",
    "# Create the combined bar + line graph\n",
    "fig, ax1 = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Combine the results\n",
    "configs_combined = configs_two_layer + configs_three_layer\n",
    "median_losses_combined = average_losses_two_layer + average_losses_three_layer\n",
    "iterations_of_convergence_combined = iterations_of_convergence_two_layer + iterations_of_convergence_three_layer\n",
    "\n",
    "# Bar graph for Average Loss\n",
    "bars = ax1.bar(configs_combined, median_losses_combined,\n",
    "               color=['blue', 'blue', 'green', 'green'],\n",
    "               width=0.4, align='center')\n",
    "ax1.set_title('Average Loss and Average # of Iterations to Convergence for Each Configuration')\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Average Loss', color='black')\n",
    "ax1.tick_params('y', colors='black')\n",
    "\n",
    "# Line graph for Iterations using twin axes\n",
    "ax2 = ax1.twinx()\n",
    "line, = ax2.plot(configs_combined, iterations_of_convergence_combined, color='red', marker='o', linestyle='-')\n",
    "ax2.set_ylabel('Iterations to Convergence', color='black')\n",
    "ax2.tick_params('y', colors='black')\n",
    "\n",
    "# Legend\n",
    "ax1.legend([bars[0], bars[2], line], [\"Two-layer Average Loss\", \"Three-layer Average Loss\", \"Iterations to Convergence\"], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(iterations_of_convergence_combined)\n",
    "# print(median_losses_combined)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part2 Pytorch version (20 points):\n",
    "\n",
    "<p>\n",
    "Add all code to the same threelayer.ipynb.\n",
    "Given that everything is easier with pytorch, adapt the code from class to solve the\n",
    "exact same regression problem with three layers and the same number of\n",
    "parameters. Use the ‘“nn” layers. Visualize the network architecture as well.\n",
    "Test the network 20 times with ADAM optimizer and 20 times with SGD optimizer,\n",
    "using a suitably high number of iterations. Record, plot, and compare the loss\n",
    "evaluation of the two optimizer runs. What can you say about the optimizers?\n",
    "</p>"
   ],
   "metadata": {
    "id": "xJhRwYl8YCuL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "Yf_A3FKBYCNW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model_pytorch(model, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "    # Choose the optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Convert data to torch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Print every 1000 epochs\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the models with the new data using nn.Sequential\n",
    "losses_adam_seq = []\n",
    "losses_sgd_seq = []\n",
    "input_dim = X.shape[1]\n",
    "hidden_nodes = [8, 8]\n",
    "output_dim = 1\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(20):\n",
    "    # build model with adam optimizer\n",
    "    model_adam_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_adam_seq.append(train_model_pytorch(model_adam_seq, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    # build model with sgd optimizer\n",
    "    model_sgd_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_sgd_seq.append(train_model_pytorch(model_sgd_seq, X, y, optimizer_type='sgd', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    print(f'difference of optimizers: {losses_sgd_seq[-1][-1] - losses_adam_seq[-1][-1]}')\n",
    "    print(f'Done with run {_}')\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the loss evaluations for ADAM and SGD optimizers using the provided models\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Average loss for better visualization\n",
    "avg_losses_adam_seq = np.mean(losses_adam_seq, axis=0)\n",
    "avg_losses_sgd_seq = np.mean(losses_sgd_seq, axis=0)\n",
    "\n",
    "plt.plot(avg_losses_adam_seq, label='ADAM', color='blue')\n",
    "plt.plot(avg_losses_sgd_seq, label='SGD', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evaluation for ADAM vs. SGD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion\n",
    "\n",
    "<p>\n",
    "SGD’s Better Performance in this Specific Task:\n",
    "\n",
    "The graph suggests that SGD is performing better because the dataset is relatively small, and the model is not complex. SGD can be more effective in such scenarios because it may generalize better, avoiding overfitting which is a common concern in small datasets. In contrast, Adam, with its adaptive learning rates, might overfit or converge too quickly to suboptimal solutions in such settings.\n",
    "\n",
    " Adam is known for its efficiency in handling large datasets and its effectiveness in training complex models. It does this by adapting the learning rate for each parameter, which helps in navigating through the parameter space more efficiently, especially when dealing with sparse gradients or non-stationary objectives.\n",
    " \n",
    "Fast Convergence and Adaptability: \n",
    "\n",
    "Adam generally converges faster than traditional SGD because of its adaptive learning rate mechanism. This can be particularly beneficial when training deep networks or dealing with challenging optimization landscapes.\n",
    "\n",
    "In simpler scenarios or with smaller datasets, SGD's straightforward approach to optimization can sometimes lead to finding better (or more generalized) solutions. Since Adam adjusts learning rates based on recent gradient updates, it might miss certain nuances that a more steady, consistent update rule like SGD captures.\n",
    "\n",
    "Learning Rate, Noise, Regularization Factors: \n",
    "\n",
    "These factors can also influence the performance of the optimizers. For example, the inherent noise in SGD updates can help escape local minima, potentially leading to better solutions in some landscapes. Regularization factors might also interact differently with these optimizers, affecting their performance.\n",
    "\n",
    "While Adam is often a preferred choice for many deep learning tasks, it’s not universally superior. The choice of optimizer and its tuning should depend on the specific characteristics of the problem at hand.\n",
    "This emphasizes the importance of empirical testing and validation in choosing an optimizer. Different problems might benefit from different optimizers, and the best way to determine the right choice is often through experimentation and performance evaluations.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
