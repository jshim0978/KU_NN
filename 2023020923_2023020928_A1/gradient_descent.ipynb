{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "from IPython import display\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part1 Better gradient descent for optimization (40 points):\n",
    "\n",
    "# 더 좋은 2D graient_descent를 만들거임.\n",
    "\n",
    "def gradient_descent(gradient, X, Y, initial, learning_rate=0.001, max_iter=100, stop_tolerance=1e-03, online_loss_plot=True):\n",
    "    # Your code here\n",
    "    \"\"\"\n",
    "    Gradient Descent Algorithm for Optimization\n",
    "    \n",
    "    Parameters:\n",
    "    - gradient: Function to compute the gradient\n",
    "    - X: Input data\n",
    "    - Y: Output data\n",
    "    - initial: Initial values for parameters\n",
    "    - learning_rate: Learning rate for the update step\n",
    "    - max_iter: Maximum number of iterations\n",
    "    - stop_tolerance: Tolerance for stopping criterion\n",
    "    - online_loss_plot: If True, plot the loss after each iteration\n",
    "    \n",
    "    Returns:\n",
    "    - theta: Optimized parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    theta = initial\n",
    "    iteration = 0\n",
    "    losses = []\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        grad = gradient(X, Y, theta)\n",
    "        theta = theta - learning_rate * grad\n",
    "        \n",
    "        # Compute the loss (mean squared error for linear regression)\n",
    "        loss = ((X @ theta - Y) ** 2).mean()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Print the loss at each iteration\n",
    "        print(f\"Iteration {iteration}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        # If the change in loss is below the tolerance, stop\n",
    "        if iteration > 0 and abs(losses[-1] - losses[-2]) < stop_tolerance:\n",
    "            print(f\"Stopping criterion reached: Change in loss below {stop_tolerance}\")\n",
    "            break\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Optionally plot the loss\n",
    "        if online_loss_plot:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(losses)\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss over iterations')\n",
    "            plt.show()\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part2 Fitting a line (40 points):\n",
    "\n",
    "# test the function with meaningful parameters to fit data from the WHOon how life expectancy may depend on GDP\n",
    "\n",
    "# Colab 쓰면 아래 명령어 치면 받아짐\n",
    "# !gdown 1ls8UVNIxToijFweUfhjkLsjS45z9-WP_\n",
    "\n",
    "# Colab 안쓰면 아래 링크에서 받아서 쓰면됨\n",
    "# download the data https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who \n",
    "\n",
    "# 제공 코드\n",
    "\n",
    "# load the data\n",
    "data = np.genfromtxt('content\\Life Expectancy Data.csv', delimiter=',')\n",
    "print(data.shape)\n",
    "# life expectancy\n",
    "Y = data[1:, 3]\n",
    "# GDP\n",
    "X = data[1:, 16]\n",
    "# we will focus on higher-income countries only!\n",
    "ind = X>10000\n",
    "Y = Y[ind]\n",
    "X = X[ind]\n",
    "\n",
    "# Your code here\n",
    "\n",
    "def gradient(X, Y, theta):\n",
    "    m = len(Y)\n",
    "    h = X @ theta\n",
    "    return (1/m) * X.T @ (h - Y)\n",
    "\n",
    "# Augment X with a column of ones for the bias term\n",
    "X_augmented = np.column_stack((np.ones(X.shape[0]), X))\n",
    "\n",
    "# Initial parameters\n",
    "theta_initial = np.array([0, 0])\n",
    "\n",
    "# Standardize the GDP values\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "X_standardized = (X - X_mean) / X_std\n",
    "\n",
    "# Augment the standardized X values with a column of ones for the bias term\n",
    "X_augmented = np.column_stack((np.ones(X_standardized.shape[0]), X_standardized))\n",
    "\n",
    "# Run gradient descent with standardized features\n",
    "theta_optimal = gradient_descent(gradient, X_augmented, Y, theta_initial, learning_rate=0.001, max_iter=4707, online_loss_plot=False)\n",
    "\n",
    "# Sort X and Y for plotting\n",
    "sorted_indices = np.argsort(X_standardized)\n",
    "X_sorted = X_standardized[sorted_indices]\n",
    "X_augmented_sorted = X_augmented[sorted_indices]\n",
    "Y_sorted = Y[sorted_indices]\n",
    "\n",
    "# Plotting the data and regression line\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X_sorted, Y_sorted, label='Data')\n",
    "plt.plot(X_sorted, X_augmented_sorted @ theta_optimal, color='red', label='Regression Line')\n",
    "plt.xlabel('GDP (Standardized)')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.legend()\n",
    "plt.title('Life Expectancy vs. GDP (Standardized)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 1. 데이터랑 결과값 선이랑 Loss 선 Plot으로 그리기 (x축 y축도 넣어야됨) - 그러기 위해선 Part 1을 잘 작성해야된대\n",
    "\n",
    "# 2. learning_rate 조절하면서 실험해서 최소의 iteration으로 optimal이되는 learning_rate 및 최소 iteration 찾아서 쓰래 \n",
    "\n",
    "# 3. 저거 데이터가 GDP 높은 나라들만 잰건데 낮은 GDP도 포함되면 어떻게 될지 디스커션해서 쓰래\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion : \n",
    "\n",
    "We filtered the data to focus only on higher-income countries. If you look at all data, what can you say about the dependency of life expectancy on GDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part3 Exponential optimization (40 points):\n",
    "\n",
    "# load the data\n",
    "response = requests.get('https://covidtracking.com/data/download/national-history.csv')\n",
    "# not strictly necessary but good practice\n",
    "response.raise_for_status()\n",
    "# interpret results as BYTES, so that numpy can read from it!\n",
    "data = np.genfromtxt(io.BytesIO(response.content),delimiter=',')\n",
    "print(data.shape)\n",
    "\n",
    "X = np.arange(20)\n",
    "Y = data[340:360, 2]\n",
    "Y = np.flip(Y)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(X,Y,'ko-')\n",
    "plt.grid()\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Number of COVID-related deaths in the US')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# 1. 강의에서 쓴 역겨운 exponential을 log로 transform 해주기\n",
    "\n",
    "# 2. 기존거랑 같은 결과 나오는지 loss plot을 통해 확인하기\n",
    "\n",
    "# 3. log로 바꿔도 되는 이유 설명하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Why can you take the logarithm of the loss function and optimize this instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Bonus: online loss function plot (20 points):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# load data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgenfromtxt(\u001b[39m'\u001b[39m\u001b[39mhttps://raw.githubusercontent.com/kaustubholpadkar/Linear_Regression-Gradient_Descent-Octave/master/data.csv\u001b[39m\u001b[39m'\u001b[39m, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m X \u001b[39m=\u001b[39m data[:, \u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Bonus: online loss function plot (20 points):\n",
    "\n",
    "# load data\n",
    "data = np.genfromtxt('https://raw.githubusercontent.com/kaustubholpadkar/Linear_Regression-Gradient_Descent-Octave/master/data.csv', delimiter=',')\n",
    "print(data.shape)\n",
    "X = data[:, 0]\n",
    "Y = data[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.scatter(X,Y)\n",
    "plt.title('Student Data : Expected Grades vs Hours of Study', fontsize=18)\n",
    "plt.xlabel('study time [hours]', fontsize=14)\n",
    "plt.ylabel('expected grade [points]', fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 위에 로드한 학생 데이터를 가지고 \n",
    "# online_surf_plot = True 면 곡면이 나오게 하래\n",
    "# 그 plot 위에 iteration 마다 점/선이 찍히게 하고\n",
    "# wait-time 0.01s로 해야 렉 안걸린대\n",
    "# 결과물은 과제 페이지 에 나온거랑 비슷하게 나오면 됨\n",
    "\n",
    "# 이를 하기 위해선 충분히 큰 parameter space를 고려해야됨 (걍 눈에 보이는 그래프가 좋게 나오게 하면 됨)\n",
    "# 의미있는 학생 데이터를 골라서 결과물에 추가하라고 함\n",
    "\n",
    "# 아래 코드는 파라메터가 추가된 gradient_descent 함수임\n",
    "\n",
    "def compute_loss(X, Y, theta):\n",
    "    m = len(Y)\n",
    "    h = X @ theta\n",
    "    return ((h - Y) ** 2).mean()\n",
    "\n",
    "def gradient(X, Y, theta):\n",
    "    m = len(Y)\n",
    "    h = X @ theta\n",
    "    return (1/m) * X.T @ (h - Y)\n",
    "\n",
    "def gradient_descent(gradient, X, Y, initial, learning_rate=0.001, max_iter=100, stop_tolerance=1e-06, online_loss_plot=False, online_surf_plot=False):\n",
    "    # Your code here\n",
    "    theta = initial\n",
    "    iteration = 0\n",
    "    losses = []\n",
    "    \n",
    "    if online_surf_plot:\n",
    "        # Create a meshgrid for theta values\n",
    "        theta0_vals = np.linspace(-50, 50, 100)\n",
    "        theta1_vals = np.linspace(-50, 50, 100)\n",
    "        T0, T1 = np.meshgrid(theta0_vals, theta1_vals)\n",
    "        Z = np.zeros(T0.shape)\n",
    "        \n",
    "        # Compute the loss for each combination of theta values\n",
    "        for i in range(T0.shape[0]):\n",
    "            for j in range(T0.shape[1]):\n",
    "                Z[i, j] = compute_loss(X, Y, np.array([T0[i, j], T1[i, j]]))\n",
    "        \n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_surface(T0, T1, Z, cmap='viridis')\n",
    "        ax.set_xlabel('Theta 0')\n",
    "        ax.set_ylabel('Theta 1')\n",
    "        ax.set_zlabel('Loss')\n",
    "        ax.set_title('Loss Surface')\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        grad = gradient(X, Y, theta)\n",
    "        theta = theta - learning_rate * grad\n",
    "        \n",
    "        loss = compute_loss(X, Y, theta)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if online_surf_plot:\n",
    "            ax.scatter(theta[0], theta[1], loss, color='r', s=100)  # plot the current theta on the surface\n",
    "            plt.pause(0.01)\n",
    "        \n",
    "        if iteration > 0 and abs(losses[-1] - losses[-2]) < stop_tolerance:\n",
    "            break\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    if online_surf_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    return theta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
