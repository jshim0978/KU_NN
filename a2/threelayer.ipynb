{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Part1 Three layer network (60 points)**\n",
        "* 수업에서 작성한 두 개의 hidden layer를 갖도록 한 개 이상의 layer로 regression problem을 해결하는 fully-connected two layer perceptron을 확장하여 사용.\n",
        "* hidden layer 수와 관계없이 internal derivative의 derivation은 항상 동일함을 유의.\n",
        "* 아래 조건에 따라 기존의 two-layer version과 새로운 three-layer로 x^2 + y^2 + 1에 대한 여러 테스트를 진행할 것\n",
        "-----------------------------\n",
        "    조건\n",
        "      - lr=0.0001\n",
        "      - tolerance_threshold=0.0001\n",
        "      - max(iter)=100,000\n",
        "      - No sgd, No regularization\n",
        "      - act_func=ReLU\n",
        "-----------------------------\n",
        "    1st Test\n",
        "      - two-layer version: 8 neurons\n",
        "      - three-layer version: 4+4 neurons\n",
        "      - 각 네트워크마다 20번 실행 후 loss와 iter 수 기록\n",
        "-----------------------------\n",
        "    2nd Test\n",
        "      - two-layer version: 16 neurons\n",
        "      - three-layer version: 8+8 neurons\n",
        "      - 각 네트워크마다 20번 실행 후 loss와 iter 수 기록\n",
        "-----------------------------\n",
        "\n",
        "1과 2 테스트 중 어떤 구조가 더 좋은가?\n",
        "- 하나의 그래프에 iter 수와 error에 대한 결과 plot하고 결과 기술"
      ],
      "metadata": {
        "id": "89fqwYoOQ-kY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw7wVooSQtvB"
      },
      "outputs": [],
      "source": [
        "# create a two-layer neural network\n",
        "def create_model(X, hidden_nodes, output_dim = 2):\n",
        "    # this will hold a dictionary of layers\n",
        "    model = {}\n",
        "    # input dimensionality\n",
        "    input_dim = X.shape[1]\n",
        "    # first set of weights from input to hidden layer 1\n",
        "    model['W1'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n",
        "    # set of biases\n",
        "    model['b1'] = np.zeros((1, hidden_nodes))\n",
        "\n",
        "    # second set of weights from hidden layer 1 to output\n",
        "    model['W2'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n",
        "    # set of biases\n",
        "    model['b2'] = np.zeros((1, output_dim))\n",
        "    return model\n",
        "\n",
        "# defines the forward pass given a model and data\n",
        "def feed_forward(model, x):\n",
        "    # get weights and biases\n",
        "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
        "    # first layer\n",
        "    z1 = x.dot(W1) + b1\n",
        "\n",
        "    # activation function\n",
        "    #a1 = logistic(z1)\n",
        "    #a1 = tanh(z1)\n",
        "    a1 = relu(z1)\n",
        "\n",
        "    # second layer\n",
        "    z2 = a1.dot(W2) + b2\n",
        "\n",
        "    # no activation function as this is simply a linear layer!!\n",
        "    out = z2\n",
        "    return z1, a1, z2, out\n",
        "\n",
        "# define the regression loss\n",
        "def calculate_loss(model,X,y,reg_lambda):\n",
        "    num_examples = X.shape[0]\n",
        "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
        "\n",
        "    # what are the current predictions\n",
        "    z1, a1, z2, out = feed_forward(model, X)\n",
        "\n",
        "    # calculate L2 loss\n",
        "    loss = 0.5 * np.sum((out - y) ** 2)\n",
        "\n",
        "    # add regulatization term to loss\n",
        "    loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
        "\n",
        "    # return per-item loss\n",
        "    return 1./num_examples * loss\n",
        "\n",
        "# back-propagation for the two-layer network\n",
        "def backprop(X,y,model,z1,a1,z2,output,reg_lambda):\n",
        "    num_examples = X.shape[0]\n",
        "    # derivative of loss function\n",
        "    delta3 = (output-y)/num_examples\n",
        "    # multiply this by activation outputs of hidden layer\n",
        "    dW2 = (a1.T).dot(delta3)\n",
        "    # and over all neurons\n",
        "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
        "\n",
        "    # derivative of activation function\n",
        "    #delta2 = delta3.dot(model['W2'].T) * logistic_derivative(a1) #if logistic\n",
        "    #delta2 = delta3.dot(model['W2'].T) * tanh_derivative(a1) #if tanh\n",
        "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1) #if ReLU\n",
        "\n",
        "    # multiply by input data\n",
        "    dW1 = np.dot(X.T, delta2)\n",
        "    # and sum over all neurons\n",
        "    db1 = np.sum(delta2, axis=0)\n",
        "\n",
        "    # add regularization terms on the two weights\n",
        "    dW2 += reg_lambda * model['W2']\n",
        "    dW1 += reg_lambda * model['W1']\n",
        "\n",
        "    return dW1, dW2, db1, db2\n",
        "\n",
        "# simple training loop\n",
        "def train(model, X, y, num_passes=100000, reg_lambda = 0.1, learning_rate = 0.001):\n",
        "    # whether to do stochastic gradient descent\n",
        "    sgd = True\n",
        "\n",
        "    # variable that checks whether we break iteration\n",
        "    done = False\n",
        "\n",
        "    # keeping track of losses\n",
        "    previous_loss = float('inf')\n",
        "    losses = []\n",
        "\n",
        "    # iteration counter\n",
        "    i = 0\n",
        "    while done == False:\n",
        "        if sgd:\n",
        "            # choose a random set of points\n",
        "            randinds = np.random.choice(np.arange(len(y)),30,False)\n",
        "            # get predictions\n",
        "            z1,a1,z2,output = feed_forward(model, X[randinds,:])\n",
        "            # feed this into backprop\n",
        "            dW1, dW2, db1, db2 = backprop(X[randinds,:],y[randinds],model,z1,a1,z2,output,reg_lambda)\n",
        "        else:\n",
        "            # get predictions\n",
        "            z1,a1,z2,output = feed_forward(model, X)\n",
        "            # feed this into backprop\n",
        "            dW1, dW2, db1, db2 = backprop(X,y,model,z1,a1,z2,output,reg_lambda)\n",
        "\n",
        "        # given the results of backprop, update both weights and biases\n",
        "        model['W1'] -= learning_rate * dW1\n",
        "        model['b1'] -= learning_rate * db1\n",
        "        model['W2'] -= learning_rate * dW2\n",
        "        model['b2'] -= learning_rate * db2\n",
        "\n",
        "        # do some book-keeping every once in a while\n",
        "        if i % 1000 == 0:\n",
        "            loss = calculate_loss(model, X, y, reg_lambda)\n",
        "            losses.append(loss)\n",
        "            print(\"Loss after iteration {}: {}\".format(i, loss))\n",
        "            # very crude method to break optimization\n",
        "            if np.abs((previous_loss-loss)/previous_loss) < 0.001:\n",
        "                done = True\n",
        "            previous_loss = loss\n",
        "        i += 1\n",
        "        if i>=num_passes:\n",
        "            done = True\n",
        "    return model, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus: arbitrary number of layers (20 points)**\n",
        "위의 코드에 대해 전체 call-logic과 training loop를 유지하되, 함수가 임의의 수의 레이어를 사용할 수 있도록 코드 수정.\n",
        "  - create_model, forward, backprop의 딕셔너리 수정해야함"
      ],
      "metadata": {
        "id": "t8NWou_MVkxi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk4EXE1lVkg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part2 Pytorch version (20 points)**\n",
        "- pytorch의 nn layer를 사용하여 같은 수의 파라미터와 세 개의 layer로 동일한 regression 문제를 해결할 수 있도록 코드를 수정하고 시각화\n",
        "- 충분히 큰 iter 수를 사용하여 ADAM optimizer와 SGD optimizer로 각각 20번씩 테스트\n",
        "- 각 optimizer에 대해 loss 평가와 plot 비교 후, 각 optimizer에 대해 설명"
      ],
      "metadata": {
        "id": "xJhRwYl8YCuL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yf_A3FKBYCNW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}