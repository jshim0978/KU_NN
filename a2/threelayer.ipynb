{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Part1 Three layer network (60 points):\n",
    "\n",
    "Create a notebook called threelayer.ipynb.\n",
    "Extend the fully-connected two layer perceptron shown in class for the regression\n",
    "problem by one more layer to have two hidden layers.\n",
    "For this, take a long look at the derivation of the backpropagation. As you can see,\n",
    "the derivation of the internal derivatives is always the same, no matter the number\n",
    "of hidden layers.\n",
    "Armed with this knowledge, it should be easy to extend the different functions in\n",
    "the notebook.\n",
    "The create_model function should now of course receive a list of values for the\n",
    "number of hidden_nodes in each layer - in addition, please extend the functionality,\n",
    "so that I can pass the activation function type as a string already here:\n",
    "```python\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function = 'relu') :\n",
    "    return\n",
    "```\n",
    "forward, calculate_loss, backprop should of course be extended\n",
    "accordingly to deal with the added number of layers and the activation function\n",
    "choice.\n",
    "Next, do a series of tests for the x^2+y^2+1 function using a learning rate of 0.001, a\n",
    "tolerance threshold of 0.0001, maximum iterations of 100,000, NO sgd, and NO\n",
    "regularization, and a “relu” function, comparing the “old” two-layer version with\n",
    "your “new” three-layer version as follows:\n",
    "- take 8 neurons for the two-layer version and 4 + 4 neurons for the three-layer\n",
    "version, and run each network 20 times, recording the loss and the number\n",
    "of iterations it needs\n",
    "- repeat this with 16 neurons for the two-layer version and 8+8 neurons for the\n",
    "three-layer version and 20 runs each\n",
    "- which network architecture converges “better” (earlier? lower error?). Plot\n",
    "the results nicely in one graph for errors and in another for number of\n",
    "iterations and comment on the results."
   ],
   "metadata": {
    "id": "89fqwYoOQ-kY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# numpy, matplotlib imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T00:56:58.766305900Z",
     "start_time": "2023-10-25T00:56:58.762784200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1. * (X > 0)\n",
    "\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return 1.-tanh(X)**2\n",
    "\n",
    "def logistic(X):\n",
    "    return 1./(1. + np.exp(-X))\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return logistic(X)*(1. - logistic(X))\n",
    "\n",
    "# Activation functions mapping\n",
    "activation_functions = {\n",
    "    'relu': relu,\n",
    "    'tanh': tanh,\n",
    "    'logistic': logistic\n",
    "}\n",
    "\n",
    "# Activation functions derivatives mapping\n",
    "activation_derivatives = {\n",
    "    'relu': relu_derivative,\n",
    "    'tanh': tanh_derivative,\n",
    "    'logistic': logistic_derivative\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T00:56:58.782107500Z",
     "start_time": "2023-10-25T00:56:58.769308100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def old_create_model(X, hidden_nodes, output_dim = 2):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b1'] = np.zeros((1, hidden_nodes))\n",
    "\n",
    "    # second set of weights from hidden layer 1 to output\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b2'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def old_feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "\n",
    "    # activation function\n",
    "    #a1 = logistic(z1)\n",
    "    #a1 = tanh(z1)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "\n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = z2\n",
    "    return z1, a1, z2, out\n",
    "\n",
    "# define the regression loss\n",
    "def old_calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out = old_feed_forward(model, X)\n",
    "\n",
    "    # calculate L2 loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def old_backprop(X,y,model,z1,a1,z2,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # derivative of loss function\n",
    "    delta3 = (output-y)/num_examples\n",
    "    \n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    \n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    #delta2 = delta3.dot(model['W2'].T) * logistic_derivative(a1) #if logistic\n",
    "    #delta2 = delta3.dot(model['W2'].T) * tanh_derivative(a1) #if tanh\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1) #if ReLU\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = np.dot(X.T, delta2)\n",
    "    \n",
    "    # and sum over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop\n",
    "def old_train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1,a1,z2,output = old_feed_forward(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW1, dW2, db1, db2 = old_backprop(X, y, model, z1, a1, z2, output)\n",
    "\n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "\n",
    "        loss = old_calculate_loss(model, X, y)\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"two-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "        if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n",
    "            done = True\n",
    "        previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "            \n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T00:56:58.834129200Z",
     "start_time": "2023-10-25T00:56:58.785107100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create a three-layer neural network\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function='relu'):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    #save activation function to model\n",
    "    model['activation_function'] = activation_function\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # [i -> 1]weights and biases from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes[0]) / np.sqrt(input_dim)\n",
    "    model['b1'] = np.zeros((1, hidden_nodes[0]))\n",
    "    \n",
    "    # [1 -> 2]weights and biases  from  hidden layer 1 to hidden layer 2\n",
    "    model['W2'] = np.random.randn(hidden_nodes[0], hidden_nodes[1]) / np.sqrt(hidden_nodes[0])\n",
    "    model['b2'] = np.zeros((1, hidden_nodes[1]))\n",
    "\n",
    "    # [2 -> o]weights and biases from hidden layer 2 to output\n",
    "    model['W3'] = np.random.randn(hidden_nodes[1], output_dim) / np.sqrt(hidden_nodes[1])\n",
    "    model['b3'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = act_func(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = act_func(z2)\n",
    "    \n",
    "    # third layer\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    out = z3\n",
    "    \n",
    "    return z1, a1, z2, a2, z3, out\n",
    "    \n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, a2, z3, out = feed_forward(model, X)\n",
    "\n",
    "    # calculate L2 loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,a2,z3,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "\n",
    "    # Derivative of loss function for output layer\n",
    "    delta4 = (output - y) / num_examples\n",
    "\n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW3 = a2.T.dot(delta4)\n",
    "\n",
    "    # and over all neurons\n",
    "    db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta3 = delta4.dot(model['W3'].T) * act_func_derivative(a2)\n",
    "\n",
    "    # multiply this by hidden layer outputs\n",
    "    dW2 = a1.T.dot(delta3)\n",
    "\n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta2 = delta3.dot(model['W2'].T) * act_func_derivative(a1)\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = X.T.dot(delta2)\n",
    "\n",
    "    # and over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, dW3, db1, db2, db3\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance = 0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1, a1, z2, a2, z3, output = feed_forward(model, X)\n",
    "\n",
    "        # feed this into backprop\n",
    "        dW1, dW2, dW3, db1, db2, db3 = backprop(X, y, model, z1, a1, z2, a2, z3, output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        model['W3'] -= learning_rate * dW3\n",
    "        model['b3'] -= learning_rate * db3\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"three-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n",
    "            done = True\n",
    "        previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T00:56:58.836133700Z",
     "start_time": "2023-10-25T00:56:58.806050300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0073581   7.2114289 ]\n",
      " [ 3.71190307  1.57853575]\n",
      " [-5.50370175 -5.50408767]\n",
      " ...\n",
      " [ 4.02200138  2.5112825 ]\n",
      " [ 7.30583394 -6.89667174]\n",
      " [-7.08712446 -3.4850068 ]] [[ 57.03419335]\n",
      " [ 17.2699995 ]\n",
      " [ 61.58571412]\n",
      " [ 85.32001627]\n",
      " [ 14.70072147]\n",
      " [116.36755104]\n",
      " [ 50.47632555]\n",
      " [ 52.5758223 ]\n",
      " [ 10.96709897]\n",
      " [ 13.34348861]\n",
      " [ 37.47379112]\n",
      " [ 16.63212904]\n",
      " [ 22.31332486]\n",
      " [ 24.14200483]\n",
      " [ 55.84740625]\n",
      " [ 31.75078355]\n",
      " [101.01365563]\n",
      " [ 80.85206132]\n",
      " [ 52.2111378 ]\n",
      " [ 10.60602421]\n",
      " [ 37.57685968]\n",
      " [ 99.39033365]\n",
      " [ 22.65775109]\n",
      " [ 10.17899375]\n",
      " [ 26.98363249]\n",
      " [ 76.82919504]\n",
      " [ 90.35627681]\n",
      " [ 49.01594266]\n",
      " [ 68.01179925]\n",
      " [ 61.7558943 ]\n",
      " [ 17.55654684]\n",
      " [ 33.91851759]\n",
      " [ 13.75203701]\n",
      " [ 57.38615741]\n",
      " [108.02490722]\n",
      " [ 43.21170047]\n",
      " [ 89.07021811]\n",
      " [ 25.37996639]\n",
      " [ 66.28654329]\n",
      " [ 43.90266353]\n",
      " [ 38.64391065]\n",
      " [ 57.08365865]\n",
      " [ 17.96986972]\n",
      " [ 19.34011009]\n",
      " [ 39.58066391]\n",
      " [ 49.68656051]\n",
      " [ 19.37151655]\n",
      " [ 19.80619705]\n",
      " [  2.4763739 ]\n",
      " [ 98.0178914 ]\n",
      " [ 61.97060053]\n",
      " [  9.84151258]\n",
      " [ 59.61499316]\n",
      " [ 19.77442685]\n",
      " [ 65.63908652]\n",
      " [ 41.69771975]\n",
      " [ 72.57202963]\n",
      " [ 40.87956027]\n",
      " [ 49.75649492]\n",
      " [ 40.8464971 ]\n",
      " [ 65.36038832]\n",
      " [ 48.40665259]\n",
      " [ 21.30913287]\n",
      " [ 60.20254593]\n",
      " [ 63.26219676]\n",
      " [ 22.51552292]\n",
      " [ 44.74301991]\n",
      " [ 59.22110952]\n",
      " [ 11.64186499]\n",
      " [ 62.74085604]\n",
      " [ 71.52019949]\n",
      " [ 11.15219666]\n",
      " [ 67.75643197]\n",
      " [  4.07494923]\n",
      " [ 65.04318678]\n",
      " [ 61.03431486]\n",
      " [ 33.30998941]\n",
      " [ 78.41231271]\n",
      " [ 26.10732027]\n",
      " [ 31.95469058]\n",
      " [  9.95645   ]\n",
      " [  5.89216299]\n",
      " [ 72.75430043]\n",
      " [ 34.37994993]\n",
      " [ 57.10214342]\n",
      " [ 68.89540601]\n",
      " [ 20.18740558]\n",
      " [ 33.54076458]\n",
      " [ 13.61724201]\n",
      " [ 83.46378214]\n",
      " [ 45.71363163]\n",
      " [ 83.6239122 ]\n",
      " [ 22.55191055]\n",
      " [ 27.5413258 ]\n",
      " [ 18.28496551]\n",
      " [ 83.77650328]\n",
      " [ 46.58095832]\n",
      " [ 13.4541851 ]\n",
      " [ 54.44064701]\n",
      " [ 59.41055258]\n",
      " [ 50.43681465]\n",
      " [ 70.97518549]\n",
      " [ 65.56695779]\n",
      " [ 48.50278647]\n",
      " [ 93.16389472]\n",
      " [ 11.03487978]\n",
      " [ 26.37462735]\n",
      " [ 30.19886438]\n",
      " [ 24.35827922]\n",
      " [ 37.9528307 ]\n",
      " [  8.55401961]\n",
      " [ 47.74542158]\n",
      " [ 31.89183509]\n",
      " [ 61.20277283]\n",
      " [ 44.74984721]\n",
      " [ 23.25168869]\n",
      " [  2.52836925]\n",
      " [ 37.4446157 ]\n",
      " [ 71.23003056]\n",
      " [ 33.10744377]\n",
      " [103.41405889]\n",
      " [ 49.3766337 ]\n",
      " [108.06920487]\n",
      " [ 58.06860459]\n",
      " [ 87.92714465]\n",
      " [ 15.19617023]\n",
      " [ 41.14451566]\n",
      " [ 29.79012554]\n",
      " [ 59.53661906]\n",
      " [ 43.79688825]\n",
      " [ 65.86513618]\n",
      " [ 34.24812601]\n",
      " [ 52.29727045]\n",
      " [ 21.43264166]\n",
      " [ 16.96084821]\n",
      " [ 50.11990837]\n",
      " [ 79.21051938]\n",
      " [  1.03352299]\n",
      " [ 29.53611375]\n",
      " [ 33.84068424]\n",
      " [ 46.65753238]\n",
      " [ 47.16454831]\n",
      " [ 57.69820255]\n",
      " [  1.76852767]\n",
      " [ 14.77672148]\n",
      " [112.22603872]\n",
      " [ 32.64615129]\n",
      " [ 36.7321277 ]\n",
      " [ 40.33547702]\n",
      " [ 48.88193122]\n",
      " [ 52.70493237]\n",
      " [  6.25777688]\n",
      " [ 72.05343813]\n",
      " [  9.09218863]\n",
      " [ 36.75110314]\n",
      " [ 47.44441854]\n",
      " [113.46988093]\n",
      " [ 39.72986389]\n",
      " [ 30.44669891]\n",
      " [ 47.18601358]\n",
      " [ 13.40951669]\n",
      " [ 19.96762604]\n",
      " [ 68.45532583]\n",
      " [  4.95158737]\n",
      " [ 18.94657373]\n",
      " [ 23.33049241]\n",
      " [ 99.10275872]\n",
      " [107.76317032]\n",
      " [ 43.96416012]\n",
      " [ 42.57551448]\n",
      " [  1.19815437]\n",
      " [ 29.46019127]\n",
      " [  7.07296874]\n",
      " [ 58.59967694]\n",
      " [  9.08005499]\n",
      " [ 33.53627981]\n",
      " [ 36.53199851]\n",
      " [ 53.40177959]\n",
      " [ 60.27654128]\n",
      " [ 52.07174835]\n",
      " [  9.45759461]\n",
      " [  1.97887043]\n",
      " [ 54.21312935]\n",
      " [ 97.51350505]\n",
      " [ 72.16613705]\n",
      " [101.22103385]\n",
      " [ 51.67980575]\n",
      " [ 56.4621434 ]\n",
      " [ 89.62940861]\n",
      " [ 38.49830805]\n",
      " [ 48.23292444]\n",
      " [ 41.73251296]\n",
      " [ 38.72392211]\n",
      " [ 39.79786892]\n",
      " [ 41.18055961]\n",
      " [ 64.54712697]\n",
      " [ 24.75257478]\n",
      " [ 54.98921891]\n",
      " [ 35.1763965 ]\n",
      " [ 33.69846205]\n",
      " [ 82.80723358]\n",
      " [ 28.29012854]\n",
      " [ 49.33811721]\n",
      " [ 65.39835423]\n",
      " [ 85.83356861]\n",
      " [ 61.21205395]\n",
      " [ 54.36062062]\n",
      " [  6.12996217]\n",
      " [ 19.46211148]\n",
      " [ 24.92461332]\n",
      " [ 44.23763389]\n",
      " [ 43.7887573 ]\n",
      " [ 51.74108442]\n",
      " [ 40.35570319]\n",
      " [ 44.22918487]\n",
      " [ 51.13574389]\n",
      " [ 45.30812928]\n",
      " [ 55.61906957]\n",
      " [ 74.2480887 ]\n",
      " [ 56.75800313]\n",
      " [105.74951757]\n",
      " [  9.33056945]\n",
      " [ 77.25596397]\n",
      " [ 77.90176993]\n",
      " [ 49.32853585]\n",
      " [ 21.53669617]\n",
      " [ 44.74174468]\n",
      " [ 39.71277743]\n",
      " [ 62.37789846]\n",
      " [ 88.59464042]\n",
      " [ 44.14877086]\n",
      " [ 18.277039  ]\n",
      " [ 59.69077413]\n",
      " [ 47.53686458]\n",
      " [ 75.48944167]\n",
      " [118.4475183 ]\n",
      " [ 93.59412517]\n",
      " [ 63.40810026]\n",
      " [ 48.24471096]\n",
      " [ 57.51926537]\n",
      " [ 15.09575854]\n",
      " [  5.69442012]\n",
      " [ 44.01738306]\n",
      " [ 66.17660607]\n",
      " [ 91.92513946]\n",
      " [  5.19793014]\n",
      " [ 38.58785247]\n",
      " [  6.84278396]\n",
      " [ 48.43843376]\n",
      " [119.13153833]\n",
      " [ 11.38617922]\n",
      " [ 35.4952431 ]\n",
      " [ 38.87595155]\n",
      " [ 70.86020455]\n",
      " [ 65.83693017]\n",
      " [  6.00408519]\n",
      " [ 83.24171814]\n",
      " [111.37550102]\n",
      " [ 29.59639577]\n",
      " [ 93.06947231]\n",
      " [  6.90018038]\n",
      " [ 87.4934141 ]\n",
      " [ 30.55428625]\n",
      " [ 16.00161849]\n",
      " [ 85.4352286 ]\n",
      " [ 89.99194739]\n",
      " [ 64.92853041]\n",
      " [ 70.1637207 ]\n",
      " [ 48.63808755]\n",
      " [ 36.83196721]\n",
      " [ 56.66345942]\n",
      " [ 27.39765812]\n",
      " [  9.72245651]\n",
      " [ 46.76642457]\n",
      " [ 20.08984555]\n",
      " [ 32.85622424]\n",
      " [  2.64038561]\n",
      " [ 39.70297434]\n",
      " [ 92.1335985 ]\n",
      " [ 21.36976829]\n",
      " [ 32.75375983]\n",
      " [ 95.18050186]\n",
      " [  8.78477261]\n",
      " [  4.97330719]\n",
      " [ 48.71088733]\n",
      " [ 21.64761279]\n",
      " [  7.55579092]\n",
      " [ 86.34431962]\n",
      " [ 79.47273723]\n",
      " [ 15.97802973]\n",
      " [ 60.4178969 ]\n",
      " [  8.5214089 ]\n",
      " [ 22.84194237]\n",
      " [ 82.31127738]\n",
      " [ 67.43753797]\n",
      " [ 39.47462526]\n",
      " [ 33.00483024]\n",
      " [ 41.47847383]\n",
      " [ 35.67516481]\n",
      " [ 79.20484492]\n",
      " [ 41.60831499]\n",
      " [ 71.01288525]\n",
      " [ 38.23659452]\n",
      " [ 27.79361422]\n",
      " [ 10.28303834]\n",
      " [ 78.28125222]\n",
      " [ 49.96870835]\n",
      " [ 82.07380888]\n",
      " [ 63.06365215]\n",
      " [ 40.78037699]\n",
      " [ 48.10522959]\n",
      " [  5.35635562]\n",
      " [  8.97896317]\n",
      " [ 17.03725126]\n",
      " [ 71.64317978]\n",
      " [ 52.63686153]\n",
      " [  1.29891548]\n",
      " [ 41.21229769]\n",
      " [ 18.25064386]\n",
      " [ 18.48681295]\n",
      " [ 68.0687945 ]\n",
      " [ 29.93297054]\n",
      " [ 77.05565271]\n",
      " [ 11.45432015]\n",
      " [ 22.40734259]\n",
      " [ 45.03500642]\n",
      " [ 18.35217031]\n",
      " [ 37.70191321]\n",
      " [ 17.76782502]\n",
      " [  7.73316089]\n",
      " [ 66.89574134]\n",
      " [ 21.54094628]\n",
      " [ 72.44549795]\n",
      " [ 46.99669919]\n",
      " [ 37.97262513]\n",
      " [ 68.61154827]\n",
      " [ 46.26195622]\n",
      " [ 62.62377182]\n",
      " [ 42.20250677]\n",
      " [ 91.62347438]\n",
      " [ 41.74130927]\n",
      " [ 36.00941203]\n",
      " [ 49.02195611]\n",
      " [ 14.70561959]\n",
      " [ 44.66542243]\n",
      " [ 10.38207651]\n",
      " [ 18.00112994]\n",
      " [ 42.11039113]\n",
      " [ 14.12683306]\n",
      " [ 31.67413206]\n",
      " [ 52.69259859]\n",
      " [ 42.04980233]\n",
      " [111.25982611]\n",
      " [ 33.66056412]\n",
      " [ 24.8449783 ]\n",
      " [ 20.67751879]\n",
      " [  3.19646399]\n",
      " [ 54.07803817]\n",
      " [ 84.34316044]\n",
      " [ 50.77980605]\n",
      " [ 76.20120683]\n",
      " [ 63.99255135]\n",
      " [ 32.33264824]\n",
      " [ 56.53836188]\n",
      " [ 52.75131671]\n",
      " [ 55.18872552]\n",
      " [  8.1224667 ]\n",
      " [ 42.52552453]\n",
      " [ 29.62163456]\n",
      " [  5.8248916 ]\n",
      " [ 11.4559481 ]\n",
      " [ 16.69735825]\n",
      " [ 60.23068934]\n",
      " [  4.93679365]\n",
      " [ 47.24759209]\n",
      " [ 88.09616849]\n",
      " [ 12.15276874]\n",
      " [ 56.44331507]\n",
      " [  4.97750052]\n",
      " [ 62.04056763]\n",
      " [ 41.13541611]\n",
      " [ 32.42848165]\n",
      " [ 57.95399872]\n",
      " [ 54.97424813]\n",
      " [ 33.80497405]\n",
      " [ 70.53902969]\n",
      " [ 39.22453693]\n",
      " [ 55.0012396 ]\n",
      " [ 32.41145885]\n",
      " [ 18.49471531]\n",
      " [ 11.09699256]\n",
      " [ 22.865839  ]\n",
      " [ 69.66224   ]\n",
      " [  5.29686605]\n",
      " [ 52.58552188]\n",
      " [ 33.66581341]\n",
      " [  4.18892112]\n",
      " [ 95.89766363]\n",
      " [ 49.88794667]\n",
      " [ 56.59738205]\n",
      " [ 42.90130387]\n",
      " [  5.40525185]\n",
      " [ 16.78548009]\n",
      " [ 95.66451525]\n",
      " [ 39.91240361]\n",
      " [ 90.58026271]\n",
      " [ 82.40441422]\n",
      " [ 36.11761691]\n",
      " [  5.7441047 ]\n",
      " [ 58.70166974]\n",
      " [ 87.30006885]\n",
      " [ 10.75158804]\n",
      " [ 46.48535546]\n",
      " [ 13.04041622]\n",
      " [ 16.02463181]\n",
      " [ 20.99475281]\n",
      " [ 60.82951995]\n",
      " [  2.68339121]\n",
      " [ 49.90433552]\n",
      " [  7.14101096]\n",
      " [ 68.53829894]\n",
      " [ 81.95914913]\n",
      " [ 34.46609129]\n",
      " [108.2176435 ]\n",
      " [  3.33039862]\n",
      " [ 64.97517138]\n",
      " [ 63.10610746]\n",
      " [ 47.74987975]\n",
      " [ 56.18501528]\n",
      " [ 50.36021976]\n",
      " [ 20.99293453]\n",
      " [  2.05060797]\n",
      " [  4.43166444]\n",
      " [ 57.32742456]\n",
      " [ 85.45640733]\n",
      " [ 18.56378476]\n",
      " [  8.44072191]\n",
      " [ 88.93322181]\n",
      " [ 19.40661067]\n",
      " [ 78.04004584]\n",
      " [ 27.07817737]\n",
      " [100.1764435 ]\n",
      " [ 50.4804973 ]\n",
      " [ 24.52118518]\n",
      " [ 58.96037666]\n",
      " [ 46.036177  ]\n",
      " [ 31.6217947 ]\n",
      " [ 30.51344105]\n",
      " [ 52.16670529]\n",
      " [105.69639795]\n",
      " [ 80.23256164]\n",
      " [ 28.74509641]\n",
      " [ 42.05348453]\n",
      " [ 33.20279427]\n",
      " [ 19.52185626]\n",
      " [ 16.85859875]\n",
      " [  7.04651522]\n",
      " [  2.7379147 ]\n",
      " [ 63.29880129]\n",
      " [ 51.94567449]\n",
      " [ 35.79721575]\n",
      " [ 63.86966637]\n",
      " [ 60.80891684]\n",
      " [ 15.1423544 ]\n",
      " [ 20.56731086]\n",
      " [ 63.01403004]\n",
      " [ 61.23819728]\n",
      " [ 41.86744189]\n",
      " [104.31930664]\n",
      " [ 39.90406545]\n",
      " [ 14.82049261]\n",
      " [ 34.70439453]\n",
      " [ 84.28026861]\n",
      " [ 75.71794163]\n",
      " [ 39.21647426]\n",
      " [ 10.55923025]\n",
      " [ 71.94299332]\n",
      " [  6.18523679]\n",
      " [ 53.633015  ]\n",
      " [ 44.01485888]\n",
      " [ 48.28251115]\n",
      " [ 20.73824947]\n",
      " [ 17.76305962]\n",
      " [ 23.82002095]\n",
      " [  2.27568883]\n",
      " [ 56.70776871]\n",
      " [ 35.64928465]\n",
      " [ 71.7347552 ]\n",
      " [ 54.82620511]\n",
      " [  3.40428584]\n",
      " [ 26.82233853]\n",
      " [ 86.82984677]\n",
      " [ 14.33515179]\n",
      " [ 11.17419233]\n",
      " [ 43.53138378]\n",
      " [ 33.65448559]\n",
      " [ 16.3752094 ]\n",
      " [ 48.6501658 ]\n",
      " [ 79.34913974]\n",
      " [ 53.64103714]\n",
      " [ 26.82962224]\n",
      " [ 50.41235056]\n",
      " [ 31.5131251 ]\n",
      " [ 41.68041737]\n",
      " [ 17.07115192]\n",
      " [ 81.77933091]\n",
      " [105.78498797]\n",
      " [ 58.11290445]\n",
      " [ 28.30205811]\n",
      " [100.79824034]\n",
      " [ 21.4907998 ]\n",
      " [103.91404825]\n",
      " [ 51.69763071]\n",
      " [ 64.94602731]\n",
      " [ 37.07296715]\n",
      " [ 58.24923387]\n",
      " [ 56.51736732]\n",
      " [ 56.67945655]\n",
      " [ 20.49699298]\n",
      " [ 58.09638906]\n",
      " [ 23.03635731]\n",
      " [ 13.46701923]\n",
      " [ 42.57519373]\n",
      " [ 77.16134623]\n",
      " [ 94.73597528]\n",
      " [ 37.15710752]\n",
      " [ 73.20527766]\n",
      " [ 70.17768716]\n",
      " [ 21.73782463]\n",
      " [  5.26563426]\n",
      " [ 96.73667605]\n",
      " [ 21.15562924]\n",
      " [ 17.03329821]\n",
      " [ 57.31175581]\n",
      " [ 17.99890271]\n",
      " [ 29.7885343 ]\n",
      " [114.18379259]\n",
      " [ 63.23100151]\n",
      " [ 12.68728851]\n",
      " [ 73.15256032]\n",
      " [ 65.14765883]\n",
      " [ 39.53404663]\n",
      " [ 51.90540368]\n",
      " [ 32.67450528]\n",
      " [ 81.19115396]\n",
      " [ 56.91181911]\n",
      " [  4.01266311]\n",
      " [ 46.35625329]\n",
      " [ 46.17092423]\n",
      " [ 26.76125844]\n",
      " [107.38345633]\n",
      " [ 85.54748705]\n",
      " [ 90.92718372]\n",
      " [ 20.82792422]\n",
      " [ 41.50230435]\n",
      " [ 45.97499169]\n",
      " [ 20.87681736]\n",
      " [  4.85321784]\n",
      " [ 80.19608495]\n",
      " [ 50.2038486 ]\n",
      " [ 92.8382995 ]\n",
      " [ 59.0957726 ]\n",
      " [  7.83175544]\n",
      " [ 46.81395218]\n",
      " [ 19.92517328]\n",
      " [ 13.78299053]\n",
      " [ 65.9778467 ]\n",
      " [ 55.59169862]\n",
      " [ 32.58268228]\n",
      " [  3.28251639]\n",
      " [ 33.91642031]\n",
      " [ 10.21867307]\n",
      " [ 51.59494233]\n",
      " [ 30.07372089]\n",
      " [ 67.00002018]\n",
      " [ 41.19023707]\n",
      " [ 19.29009301]\n",
      " [ 20.95119589]\n",
      " [ 67.97293039]\n",
      " [ 93.12818679]\n",
      " [ 63.43689734]\n",
      " [ 23.13940815]\n",
      " [ 56.3438404 ]\n",
      " [ 29.25443308]\n",
      " [ 74.98596208]\n",
      " [ 63.77428279]\n",
      " [ 20.21769615]\n",
      " [  6.00642792]\n",
      " [ 60.59108184]\n",
      " [ 18.87865508]\n",
      " [ 62.27769792]\n",
      " [ 53.28463373]\n",
      " [ 90.3868202 ]\n",
      " [ 57.84885814]\n",
      " [ 22.23548676]\n",
      " [ 50.64799656]\n",
      " [ 10.82338421]\n",
      " [ 89.16381393]\n",
      " [114.67379126]\n",
      " [ 51.98553007]\n",
      " [ 75.93567446]\n",
      " [ 67.42668923]\n",
      " [ 19.75178207]\n",
      " [ 10.35007034]\n",
      " [ 77.38467965]\n",
      " [  2.99997992]\n",
      " [ 52.21577055]\n",
      " [ 30.56349791]\n",
      " [ 53.98831081]\n",
      " [ 20.2729491 ]\n",
      " [ 84.20986794]\n",
      " [ 52.55165793]\n",
      " [  7.56208431]\n",
      " [ 16.73521464]\n",
      " [ 63.06839965]\n",
      " [ 59.42910649]\n",
      " [ 78.34890473]\n",
      " [ 11.31376594]\n",
      " [ 35.16950869]\n",
      " [ 82.43341753]\n",
      " [ 55.37154789]\n",
      " [ 50.2221748 ]\n",
      " [ 22.52066392]\n",
      " [ 91.75510095]\n",
      " [ 49.82437296]\n",
      " [ 23.59860184]\n",
      " [ 74.45631043]\n",
      " [ 11.05868649]\n",
      " [ 23.97755929]\n",
      " [ 51.33808928]\n",
      " [ 37.32779919]\n",
      " [ 10.71976235]\n",
      " [108.85693716]\n",
      " [  4.06178196]\n",
      " [ 14.67259934]\n",
      " [ 43.93275568]\n",
      " [ 75.3084549 ]\n",
      " [ 41.05379662]\n",
      " [ 78.6656009 ]\n",
      " [ 33.7195323 ]\n",
      " [ 15.06792751]\n",
      " [ 63.4339424 ]\n",
      " [  8.78992124]\n",
      " [ 50.19458207]\n",
      " [102.2468277 ]\n",
      " [ 38.25376725]\n",
      " [ 11.35743762]\n",
      " [ 47.14713337]\n",
      " [ 69.33521306]\n",
      " [ 16.67883229]\n",
      " [105.88132185]\n",
      " [ 52.8233902 ]\n",
      " [ 47.41904424]\n",
      " [ 36.03950096]\n",
      " [ 32.97211979]\n",
      " [ 86.03903594]\n",
      " [ 27.35338473]\n",
      " [  6.85935374]\n",
      " [ 14.90868662]\n",
      " [ 95.29088307]\n",
      " [ 47.04584937]\n",
      " [ 56.56833415]\n",
      " [ 65.78098546]\n",
      " [ 48.35340661]\n",
      " [  7.44900184]\n",
      " [ 14.08532285]\n",
      " [ 82.94155104]\n",
      " [ 64.49474785]\n",
      " [ 32.42578129]\n",
      " [104.24512639]\n",
      " [ 16.38934423]\n",
      " [ 48.93098867]\n",
      " [ 17.50117085]\n",
      " [ 11.33395896]\n",
      " [116.02122313]\n",
      " [ 37.0708964 ]\n",
      " [ 14.95432457]\n",
      " [ 36.74339376]\n",
      " [ 43.67253686]\n",
      " [ 15.93361031]\n",
      " [ 65.96253381]\n",
      " [  9.30179128]\n",
      " [ 49.66151728]\n",
      " [ 55.7553327 ]\n",
      " [ 44.90052276]\n",
      " [ 83.94119408]\n",
      " [ 33.16599703]\n",
      " [ 52.54499047]\n",
      " [  1.82203369]\n",
      " [  8.2845069 ]\n",
      " [ 33.02033049]\n",
      " [ 72.38492537]\n",
      " [ 15.11197329]\n",
      " [ 55.02670204]\n",
      " [ 58.05400855]\n",
      " [ 65.4030419 ]\n",
      " [ 29.01941768]\n",
      " [ 35.49145161]\n",
      " [ 18.3877891 ]\n",
      " [ 15.45881992]\n",
      " [ 57.66254758]\n",
      " [ 70.26317551]\n",
      " [  8.03373203]\n",
      " [ 55.24321033]\n",
      " [ 37.58473051]\n",
      " [ 61.70675853]\n",
      " [ 64.79194738]\n",
      " [ 50.04859393]\n",
      " [ 91.8813544 ]\n",
      " [ 49.77631072]\n",
      " [  9.66583557]\n",
      " [ 86.45739465]\n",
      " [117.00493526]\n",
      " [115.99988785]\n",
      " [ 40.60786651]\n",
      " [  1.58667949]\n",
      " [  8.15982828]\n",
      " [ 90.79866737]\n",
      " [ 21.16899122]\n",
      " [ 31.45636427]\n",
      " [ 99.74789369]\n",
      " [  9.43661544]\n",
      " [ 62.37881035]\n",
      " [ 39.76523688]\n",
      " [ 60.20298087]\n",
      " [  4.55736422]\n",
      " [ 37.11561572]\n",
      " [105.00079152]\n",
      " [ 18.17180537]\n",
      " [ 28.15781417]\n",
      " [  9.7388615 ]\n",
      " [ 46.9991511 ]\n",
      " [117.10276642]\n",
      " [ 61.50850045]\n",
      " [ 56.60311873]\n",
      " [ 16.70539639]\n",
      " [ 40.03055567]\n",
      " [ 15.84929794]\n",
      " [ 42.87202967]\n",
      " [ 24.76767457]\n",
      " [ 63.78649615]\n",
      " [ 64.04096807]\n",
      " [ 13.4422791 ]\n",
      " [ 13.54080995]\n",
      " [  4.85322013]\n",
      " [ 22.74673704]\n",
      " [ 66.67515538]\n",
      " [ 10.98327221]\n",
      " [  4.99632067]\n",
      " [ 38.49447669]\n",
      " [  1.20416224]\n",
      " [ 65.0557317 ]\n",
      " [  7.29501937]\n",
      " [  2.93452542]\n",
      " [ 21.94250273]\n",
      " [ 38.25766375]\n",
      " [113.53803058]\n",
      " [ 86.77161116]\n",
      " [ 47.12533996]\n",
      " [ 64.52891766]\n",
      " [ 98.81745393]\n",
      " [ 47.30558283]\n",
      " [ 16.19545477]\n",
      " [ 22.35470744]\n",
      " [ 24.89972489]\n",
      " [ 13.231252  ]\n",
      " [ 26.19576551]\n",
      " [ 39.51363437]\n",
      " [ 35.42417007]\n",
      " [ 27.14136888]\n",
      " [ 51.75099984]\n",
      " [ 17.33090898]\n",
      " [ 55.93044162]\n",
      " [ 25.61546139]\n",
      " [ 58.90755942]\n",
      " [ 55.80702447]\n",
      " [ 64.87101042]\n",
      " [ 41.15584887]\n",
      " [  6.17957455]\n",
      " [ 71.53419634]\n",
      " [ 24.22493306]\n",
      " [ 34.1281629 ]\n",
      " [ 29.13333823]\n",
      " [ 50.06615068]\n",
      " [  5.52287722]\n",
      " [ 31.13519159]\n",
      " [ 49.05910797]\n",
      " [ 63.38286374]\n",
      " [ 64.25148253]\n",
      " [  4.49434392]\n",
      " [ 22.44098842]\n",
      " [  3.28767516]\n",
      " [ 95.74835642]\n",
      " [ 13.09249155]\n",
      " [ 14.44104523]\n",
      " [ 72.26121566]\n",
      " [ 76.4739133 ]\n",
      " [ 15.45693789]\n",
      " [ 59.34021676]\n",
      " [ 84.72877614]\n",
      " [ 33.29269777]\n",
      " [ 25.5101552 ]\n",
      " [ 48.55433032]\n",
      " [ 71.9562693 ]\n",
      " [ 19.56261341]\n",
      " [ 43.80936788]\n",
      " [  9.17788057]\n",
      " [ 15.49323239]\n",
      " [ 70.21360171]\n",
      " [ 15.47387679]\n",
      " [ 50.8394645 ]\n",
      " [ 55.60525934]\n",
      " [ 37.23168967]\n",
      " [ 24.22541608]\n",
      " [ 43.27403014]\n",
      " [ 23.74664737]\n",
      " [ 30.30855839]\n",
      " [ 65.74130283]\n",
      " [ 34.52368963]\n",
      " [109.76667559]\n",
      " [ 31.35733676]\n",
      " [  5.02244073]\n",
      " [ 60.89932029]\n",
      " [ 63.69884154]\n",
      " [ 32.51723291]\n",
      " [ 65.99984303]\n",
      " [ 20.37343464]\n",
      " [ 93.40848471]\n",
      " [ 11.42316156]\n",
      " [  4.43049717]\n",
      " [  3.65345648]\n",
      " [ 46.03010499]\n",
      " [ 94.6240672 ]\n",
      " [ 70.21685993]\n",
      " [ 64.47495887]\n",
      " [ 41.22153855]\n",
      " [ 59.63835586]\n",
      " [ 32.30058053]\n",
      " [ 48.37062222]\n",
      " [ 38.01559536]\n",
      " [ 64.97193148]\n",
      " [ 32.70991117]\n",
      " [ 69.11970599]\n",
      " [ 46.79610524]\n",
      " [ 21.84196286]\n",
      " [ 28.06854479]\n",
      " [ 35.64205705]\n",
      " [ 53.22970462]\n",
      " [ 92.63724394]\n",
      " [ 56.53376807]\n",
      " [ 57.7159865 ]\n",
      " [ 10.0754279 ]\n",
      " [ 41.74474932]\n",
      " [ 26.71938936]\n",
      " [ 10.43658166]\n",
      " [ 81.25621823]\n",
      " [ 21.59342698]\n",
      " [113.66864729]\n",
      " [ 60.80077382]\n",
      " [ 78.30491412]\n",
      " [ 70.46172885]\n",
      " [ 51.65346879]\n",
      " [ 59.49969951]\n",
      " [ 64.17991453]\n",
      " [ 29.63432936]\n",
      " [ 14.53928645]\n",
      " [  2.58589617]\n",
      " [ 74.17443714]\n",
      " [ 25.5189099 ]\n",
      " [  7.3644899 ]\n",
      " [ 42.43532847]\n",
      " [ 80.77358083]\n",
      " [  2.88881733]\n",
      " [ 74.94963491]\n",
      " [ 52.05668037]\n",
      " [ 22.08333545]\n",
      " [ 79.08072297]\n",
      " [ 82.24939788]\n",
      " [ 55.10770336]\n",
      " [ 40.57912248]\n",
      " [ 31.88978638]\n",
      " [ 42.38902598]\n",
      " [ 40.22034373]\n",
      " [ 15.56985915]\n",
      " [  7.79397723]\n",
      " [ 22.21702268]\n",
      " [ 93.43138049]\n",
      " [ 30.7953708 ]\n",
      " [ 11.05773767]\n",
      " [ 46.15326396]\n",
      " [ 96.86251102]\n",
      " [  6.19890713]\n",
      " [ 47.4669956 ]\n",
      " [  9.10482272]\n",
      " [ 49.16110316]\n",
      " [ 46.76166976]\n",
      " [  9.3536455 ]\n",
      " [ 98.9101184 ]\n",
      " [ 59.85635167]\n",
      " [108.99588696]\n",
      " [ 68.78975935]\n",
      " [ 17.11082721]\n",
      " [ 26.09406242]\n",
      " [ 33.03642998]\n",
      " [ 69.47763955]\n",
      " [ 25.47100582]\n",
      " [ 73.88240927]\n",
      " [101.72922308]\n",
      " [ 30.26692925]\n",
      " [  8.54668905]\n",
      " [ 59.87186739]\n",
      " [ 23.69483425]\n",
      " [  6.53642329]\n",
      " [110.89834661]\n",
      " [ 34.83113645]\n",
      " [ 62.6323521 ]\n",
      " [ 18.49345202]\n",
      " [ 32.15160456]\n",
      " [ 43.10436441]\n",
      " [ 67.32759407]\n",
      " [  6.36443382]\n",
      " [  5.20533877]\n",
      " [ 68.47045573]\n",
      " [ 61.10504004]\n",
      " [ 44.7687479 ]\n",
      " [ 38.28854301]\n",
      " [ 65.16907806]\n",
      " [ 61.64229188]\n",
      " [ 18.05050027]\n",
      " [ 44.30777593]\n",
      " [ 26.80728582]\n",
      " [ 28.15638328]\n",
      " [110.11054774]\n",
      " [ 83.18607192]\n",
      " [ 48.7927977 ]\n",
      " [ 47.86419279]\n",
      " [ 30.72487867]\n",
      " [ 15.33101844]\n",
      " [  3.1075466 ]\n",
      " [ 12.57490921]\n",
      " [ 45.81220015]\n",
      " [ 23.21050631]\n",
      " [ 30.95945501]\n",
      " [ 79.06132659]\n",
      " [ 76.04945578]\n",
      " [ 44.61256081]\n",
      " [ 46.47519791]\n",
      " [ 15.68328465]\n",
      " [ 29.35998487]\n",
      " [ 26.97155838]\n",
      " [ 64.42099204]\n",
      " [ 73.7172387 ]\n",
      " [ 39.11914472]\n",
      " [ 59.18136534]\n",
      " [ 40.22746026]\n",
      " [ 46.99181246]\n",
      " [ 68.50467954]\n",
      " [ 14.30083042]\n",
      " [ 41.39793382]\n",
      " [  5.96681391]\n",
      " [ 73.97498953]\n",
      " [ 10.00707319]\n",
      " [ 65.56542438]\n",
      " [  3.08426773]\n",
      " [ 54.65989109]\n",
      " [  3.60550677]\n",
      " [  4.68367211]\n",
      " [ 32.78476104]\n",
      " [ 41.6117427 ]\n",
      " [ 46.79398192]\n",
      " [ 57.92267957]\n",
      " [ 32.8202005 ]\n",
      " [ 22.40078544]\n",
      " [ 33.73366548]\n",
      " [  9.67102053]\n",
      " [ 64.31066558]\n",
      " [ 17.68743982]\n",
      " [112.59997437]\n",
      " [  7.79083654]\n",
      " [ 26.63612712]\n",
      " [ 40.36880437]\n",
      " [ 34.01385635]\n",
      " [  4.29448552]\n",
      " [ 31.34478294]\n",
      " [ 65.48027458]\n",
      " [ 87.12260002]\n",
      " [ 14.26686696]\n",
      " [ 61.0856555 ]\n",
      " [ 17.09887086]\n",
      " [ 66.49667935]\n",
      " [ 49.67207497]\n",
      " [  1.52182931]\n",
      " [ 80.41939735]\n",
      " [ 14.30402588]\n",
      " [ 15.50543636]\n",
      " [ 48.83122883]\n",
      " [ 38.1497546 ]\n",
      " [ 23.48303487]\n",
      " [101.93929059]\n",
      " [ 63.37260556]]\n",
      "two-layer Loss after iteration 0: 1345.7130824339488\n",
      "two-layer Loss after iteration 1000: 19.107678554201275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_35640\\41365746.py:112: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-layer Loss after iteration 2000: 8.8475399422964\n",
      "two-layer Loss after iteration 3000: 6.717682967680359\n",
      "two-layer Loss after iteration 0: 1263.4814371327614\n",
      "two-layer Loss after iteration 1000: 20.560788533342777\n",
      "two-layer Loss after iteration 2000: 9.33151562433909\n",
      "two-layer Loss after iteration 3000: 6.805840488122652\n",
      "two-layer Loss after iteration 0: 1334.413215990809\n",
      "two-layer Loss after iteration 1000: 20.538915918032448\n",
      "two-layer Loss after iteration 2000: 8.979107458271443\n",
      "two-layer Loss after iteration 3000: 6.234711435606511\n",
      "two-layer Loss after iteration 4000: 5.240775832494115\n",
      "two-layer Loss after iteration 0: 1227.2707100262523\n",
      "two-layer Loss after iteration 1000: 20.992417409761334\n",
      "two-layer Loss after iteration 2000: 9.119168869614052\n",
      "two-layer Loss after iteration 3000: 6.562950401320199\n",
      "two-layer Loss after iteration 4000: 4.819073592702401\n",
      "two-layer Loss after iteration 5000: 4.2306578542007935\n",
      "two-layer Loss after iteration 0: 1267.3731236053886\n",
      "two-layer Loss after iteration 1000: 19.736142522560186\n",
      "two-layer Loss after iteration 2000: 8.673362179824498\n",
      "two-layer Loss after iteration 3000: 6.35138235452596\n",
      "two-layer Loss after iteration 4000: 5.43841977735747\n",
      "two-layer Loss after iteration 5000: 4.868673354289362\n",
      "two-layer Loss after iteration 0: 1298.4146668332796\n",
      "two-layer Loss after iteration 1000: 19.768745687514496\n",
      "two-layer Loss after iteration 2000: 8.197065325118857\n",
      "two-layer Loss after iteration 3000: 5.766438029611479\n",
      "two-layer Loss after iteration 4000: 4.843286825261528\n",
      "two-layer Loss after iteration 0: 1362.945960584009\n",
      "two-layer Loss after iteration 1000: 19.397003988270917\n",
      "two-layer Loss after iteration 2000: 8.877640227434076\n",
      "two-layer Loss after iteration 3000: 6.646540825283791\n",
      "two-layer Loss after iteration 4000: 5.487142166288184\n",
      "two-layer Loss after iteration 0: 1299.886808015325\n",
      "two-layer Loss after iteration 1000: 19.14739952153084\n",
      "two-layer Loss after iteration 2000: 8.297687644325583\n",
      "two-layer Loss after iteration 3000: 5.9454632499103734\n",
      "two-layer Loss after iteration 4000: 5.139407292419801\n",
      "two-layer Loss after iteration 0: 1098.6669201831269\n",
      "two-layer Loss after iteration 1000: 20.138228003352214\n",
      "two-layer Loss after iteration 2000: 8.70689263891078\n",
      "two-layer Loss after iteration 3000: 6.630990062244676\n",
      "two-layer Loss after iteration 4000: 5.54690314735255\n",
      "two-layer Loss after iteration 5000: 4.622677801423148\n",
      "two-layer Loss after iteration 6000: 3.9255825278641816\n",
      "two-layer Loss after iteration 0: 1420.9364002839595\n",
      "two-layer Loss after iteration 1000: 22.149219264175922\n",
      "two-layer Loss after iteration 2000: 9.487936684094336\n",
      "two-layer Loss after iteration 3000: 6.696277512730673\n",
      "two-layer Loss after iteration 4000: 5.5839635066934274\n",
      "two-layer Loss after iteration 0: 1277.0298935752228\n",
      "two-layer Loss after iteration 1000: 20.709394982462086\n",
      "two-layer Loss after iteration 2000: 8.856735383510538\n",
      "two-layer Loss after iteration 3000: 6.66390489712697\n",
      "two-layer Loss after iteration 0: 1255.9539845923657\n",
      "two-layer Loss after iteration 1000: 20.605649189694944\n",
      "two-layer Loss after iteration 2000: 8.72203263249672\n",
      "two-layer Loss after iteration 3000: 6.4021399271046535\n",
      "two-layer Loss after iteration 4000: 5.421579474350682\n",
      "two-layer Loss after iteration 5000: 4.515359947964671\n",
      "two-layer Loss after iteration 0: 1238.3554425800507\n",
      "two-layer Loss after iteration 1000: 19.346827080709925\n",
      "two-layer Loss after iteration 2000: 8.319956566290298\n",
      "two-layer Loss after iteration 3000: 6.546668050808405\n",
      "two-layer Loss after iteration 4000: 5.400976570509247\n",
      "two-layer Loss after iteration 0: 1266.1805139103353\n",
      "two-layer Loss after iteration 1000: 19.876743120427722\n",
      "two-layer Loss after iteration 2000: 8.382184526422375\n",
      "two-layer Loss after iteration 3000: 5.959982269341148\n",
      "two-layer Loss after iteration 4000: 4.820367963156348\n",
      "two-layer Loss after iteration 5000: 3.73885909139631\n",
      "two-layer Loss after iteration 6000: 3.148835043233367\n",
      "two-layer Loss after iteration 0: 1289.2808720461596\n",
      "two-layer Loss after iteration 1000: 32.01254781626964\n",
      "two-layer Loss after iteration 2000: 19.77994911100047\n",
      "two-layer Loss after iteration 3000: 16.99484445848099\n",
      "two-layer Loss after iteration 0: 1111.464168123416\n",
      "two-layer Loss after iteration 1000: 20.002417464209227\n",
      "two-layer Loss after iteration 2000: 9.030306092782231\n",
      "two-layer Loss after iteration 3000: 6.809150002051572\n",
      "two-layer Loss after iteration 4000: 5.826172939017498\n",
      "two-layer Loss after iteration 0: 1319.121569952824\n",
      "two-layer Loss after iteration 1000: 19.186492986711244\n",
      "two-layer Loss after iteration 2000: 8.942622119993466\n",
      "two-layer Loss after iteration 3000: 6.777874369860466\n",
      "two-layer Loss after iteration 4000: 5.409986141697674\n",
      "two-layer Loss after iteration 0: 1286.2885821557154\n",
      "two-layer Loss after iteration 1000: 21.421660949246252\n",
      "two-layer Loss after iteration 2000: 9.137374195942515\n",
      "two-layer Loss after iteration 3000: 6.881506430224096\n",
      "two-layer Loss after iteration 0: 1440.7703116242049\n",
      "two-layer Loss after iteration 1000: 21.03437320962921\n",
      "two-layer Loss after iteration 2000: 8.634446455075325\n",
      "two-layer Loss after iteration 3000: 6.134582518804437\n",
      "two-layer Loss after iteration 0: 1194.02133495885\n",
      "two-layer Loss after iteration 1000: 36.40674794610618\n",
      "three-layer Loss after iteration 0: 1385.050406512403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_35640\\2251317011.py:131: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three-layer Loss after iteration 1000: 17.23523398763797\n",
      "three-layer Loss after iteration 0: 1459.0708119913807\n",
      "three-layer Loss after iteration 1000: 17.753719313131775\n",
      "three-layer Loss after iteration 0: 1413.6442349131614\n",
      "three-layer Loss after iteration 1000: 11.112263626387747\n",
      "three-layer Loss after iteration 0: 1309.8941923315815\n",
      "three-layer Loss after iteration 1000: 18.086897610800694\n",
      "three-layer Loss after iteration 0: 1319.4665920303419\n",
      "three-layer Loss after iteration 0: 1300.5810240239553\n",
      "three-layer Loss after iteration 1000: 15.969369226215779\n",
      "three-layer Loss after iteration 0: 1259.5357851137194\n",
      "three-layer Loss after iteration 0: 1443.6664081885497\n",
      "three-layer Loss after iteration 1000: 68.88445279002465\n",
      "three-layer Loss after iteration 2000: 44.458214170496284\n",
      "three-layer Loss after iteration 0: 1422.1633875185107\n",
      "three-layer Loss after iteration 1000: 18.85158397515922\n",
      "three-layer Loss after iteration 0: 1198.5224091580426\n",
      "three-layer Loss after iteration 0: 1392.288447296058\n",
      "three-layer Loss after iteration 1000: 7.55360566475711\n",
      "three-layer Loss after iteration 0: 1459.0971565076127\n",
      "three-layer Loss after iteration 0: 1400.5386013136037\n",
      "three-layer Loss after iteration 0: 1288.6890809852675\n",
      "three-layer Loss after iteration 0: 1420.5114860761319\n",
      "three-layer Loss after iteration 1000: 8.8286082389801\n",
      "three-layer Loss after iteration 0: 1327.823277390412\n",
      "three-layer Loss after iteration 1000: 8.780677244859687\n",
      "three-layer Loss after iteration 0: 1313.466686997807\n",
      "three-layer Loss after iteration 1000: 8.588282851350577\n",
      "three-layer Loss after iteration 0: 1376.0469413975356\n",
      "three-layer Loss after iteration 1000: 8.919303030703873\n",
      "three-layer Loss after iteration 0: 1403.5284277972044\n",
      "three-layer Loss after iteration 0: 1312.9126192741642\n",
      "three-layer Loss after iteration 1000: 7.425714069543393\n",
      "{'8': {'losses': [6.717682967680359, 6.805840488122652, 5.240775832494115, 4.2306578542007935, 4.868673354289362, 4.843286825261528, 5.487142166288184, 5.139407292419801, 3.9255825278641816, 5.5839635066934274, 6.66390489712697, 4.515359947964671, 5.400976570509247, 3.148835043233367, 16.99484445848099, 5.826172939017498, 5.409986141697674, 6.881506430224096, 6.134582518804437, 36.40674794610618], 'iterations': [3829, 3525, 4325, 5067, 5007, 4986, 4804, 4040, 6285, 4139, 3840, 5154, 4607, 6627, 3046, 4581, 4751, 3560, 3973, 1548]}} {'4+4': {'losses': [17.23523398763797, 17.753719313131775, 11.112263626387747, 18.086897610800694, 1319.4665920303419, 15.969369226215779, 1259.5357851137194, 44.458214170496284, 18.85158397515922, 1198.5224091580426, 7.55360566475711, 1459.0971565076127, 1400.5386013136037, 1288.6890809852675, 8.8286082389801, 8.780677244859687, 8.588282851350577, 8.919303030703873, 1403.5284277972044, 7.425714069543393], 'iterations': [1350, 1963, 1757, 1487, 479, 1953, 405, 2731, 1594, 883, 1689, 255, 566, 358, 1117, 1269, 1507, 1216, 370, 1399]}}\n",
      "two-layer Loss after iteration 0: 1070.2980371055364\n",
      "two-layer Loss after iteration 1000: 20.89895370208518\n",
      "two-layer Loss after iteration 2000: 8.63325045451727\n",
      "two-layer Loss after iteration 3000: 5.963700147450032\n",
      "two-layer Loss after iteration 4000: 4.7286419204872105\n",
      "two-layer Loss after iteration 5000: 3.748784314373\n",
      "two-layer Loss after iteration 6000: 2.8558986613053756\n",
      "two-layer Loss after iteration 7000: 2.3169122453032123\n",
      "two-layer Loss after iteration 8000: 2.026333431441469\n",
      "two-layer Loss after iteration 9000: 1.8094866951964572\n",
      "two-layer Loss after iteration 0: 1319.1769636470103\n",
      "two-layer Loss after iteration 1000: 19.756169468591217\n",
      "two-layer Loss after iteration 2000: 8.445939175034688\n",
      "two-layer Loss after iteration 3000: 5.721788855929995\n",
      "two-layer Loss after iteration 4000: 4.120847715211503\n",
      "two-layer Loss after iteration 5000: 3.0940362904226095\n",
      "two-layer Loss after iteration 6000: 2.4572494366879494\n",
      "two-layer Loss after iteration 7000: 2.052982409423204\n",
      "two-layer Loss after iteration 8000: 1.7965765662341473\n",
      "two-layer Loss after iteration 0: 1081.016992504317\n",
      "two-layer Loss after iteration 1000: 19.450363924310878\n",
      "two-layer Loss after iteration 2000: 8.25946269574987\n",
      "two-layer Loss after iteration 3000: 5.714757745800054\n",
      "two-layer Loss after iteration 4000: 4.298311771154483\n",
      "two-layer Loss after iteration 5000: 3.1887575389439182\n",
      "two-layer Loss after iteration 6000: 2.5939127929039816\n",
      "two-layer Loss after iteration 7000: 2.199200541467321\n",
      "two-layer Loss after iteration 8000: 1.8836715579593915\n",
      "two-layer Loss after iteration 9000: 1.5085375775159036\n",
      "two-layer Loss after iteration 10000: 1.2999236173506825\n",
      "two-layer Loss after iteration 11000: 1.1288757508116374\n",
      "two-layer Loss after iteration 12000: 1.0003711118573113\n",
      "two-layer Loss after iteration 0: 1374.953190661336\n",
      "two-layer Loss after iteration 1000: 19.376467781326102\n",
      "two-layer Loss after iteration 2000: 8.100774050204636\n",
      "two-layer Loss after iteration 3000: 5.486886922402486\n",
      "two-layer Loss after iteration 4000: 3.847585477268861\n",
      "two-layer Loss after iteration 5000: 3.108596627746946\n",
      "two-layer Loss after iteration 6000: 2.591360857934332\n",
      "two-layer Loss after iteration 7000: 2.163765990190503\n",
      "two-layer Loss after iteration 8000: 1.5338835663912143\n",
      "two-layer Loss after iteration 9000: 1.1755023002325238\n",
      "two-layer Loss after iteration 10000: 0.925922529722825\n",
      "two-layer Loss after iteration 0: 1233.5899390202096\n",
      "two-layer Loss after iteration 1000: 20.81599198199602\n",
      "two-layer Loss after iteration 2000: 8.881976452594769\n",
      "two-layer Loss after iteration 3000: 6.589008586175609\n",
      "two-layer Loss after iteration 4000: 5.366354616874581\n",
      "two-layer Loss after iteration 5000: 4.445406617650889\n",
      "two-layer Loss after iteration 6000: 3.6522262641825605\n",
      "two-layer Loss after iteration 7000: 3.107563494514784\n",
      "two-layer Loss after iteration 8000: 2.330916778067638\n",
      "two-layer Loss after iteration 9000: 1.9018917018749129\n",
      "two-layer Loss after iteration 10000: 1.570482488473973\n",
      "two-layer Loss after iteration 0: 1289.028093662954\n",
      "two-layer Loss after iteration 1000: 19.692387457107287\n",
      "two-layer Loss after iteration 2000: 8.01554401147664\n",
      "two-layer Loss after iteration 3000: 5.332124015160278\n",
      "two-layer Loss after iteration 4000: 4.162930815719077\n",
      "two-layer Loss after iteration 5000: 3.199316718238775\n",
      "two-layer Loss after iteration 6000: 2.392083671508676\n",
      "two-layer Loss after iteration 7000: 1.9135711508462436\n",
      "two-layer Loss after iteration 8000: 1.5935431350938833\n",
      "two-layer Loss after iteration 9000: 1.335841275070725\n",
      "two-layer Loss after iteration 10000: 1.1613698556247571\n",
      "two-layer Loss after iteration 11000: 1.030159498608969\n",
      "two-layer Loss after iteration 0: 1008.3880703561765\n",
      "two-layer Loss after iteration 1000: 22.45413886284957\n",
      "two-layer Loss after iteration 2000: 9.288943378864989\n",
      "two-layer Loss after iteration 3000: 6.285432522715345\n",
      "two-layer Loss after iteration 4000: 5.057984944749168\n",
      "two-layer Loss after iteration 5000: 4.2602807861001715\n",
      "two-layer Loss after iteration 6000: 3.639568902156397\n",
      "two-layer Loss after iteration 7000: 2.8410028680916377\n",
      "two-layer Loss after iteration 8000: 2.4666044635106017\n",
      "two-layer Loss after iteration 9000: 2.18732804136931\n",
      "two-layer Loss after iteration 10000: 1.965337365825783\n",
      "two-layer Loss after iteration 0: 1319.2836689137532\n",
      "two-layer Loss after iteration 1000: 20.140378010083396\n",
      "two-layer Loss after iteration 2000: 8.52148748233546\n",
      "two-layer Loss after iteration 3000: 5.513967473495815\n",
      "two-layer Loss after iteration 4000: 3.6169061496126416\n",
      "two-layer Loss after iteration 5000: 2.7418575639603833\n",
      "two-layer Loss after iteration 6000: 2.0870089806436893\n",
      "two-layer Loss after iteration 7000: 1.6759881959248177\n",
      "two-layer Loss after iteration 8000: 1.3906898362095987\n",
      "two-layer Loss after iteration 9000: 1.1822373766142942\n",
      "two-layer Loss after iteration 10000: 1.0299149870462105\n",
      "two-layer Loss after iteration 11000: 0.90754764083666\n",
      "two-layer Loss after iteration 12000: 0.735985923955636\n",
      "two-layer Loss after iteration 13000: 0.6259106645954415\n",
      "two-layer Loss after iteration 0: 1304.8883120846078\n",
      "two-layer Loss after iteration 1000: 19.161389846277963\n",
      "two-layer Loss after iteration 2000: 8.166549204360441\n",
      "two-layer Loss after iteration 3000: 5.880878271179385\n",
      "two-layer Loss after iteration 4000: 4.9835653049894715\n",
      "two-layer Loss after iteration 5000: 4.251465074196908\n",
      "two-layer Loss after iteration 6000: 3.760801266008483\n",
      "two-layer Loss after iteration 0: 1228.359064681102\n",
      "two-layer Loss after iteration 1000: 20.906410204927553\n",
      "two-layer Loss after iteration 2000: 8.35895105738903\n",
      "two-layer Loss after iteration 3000: 5.502740354695616\n",
      "two-layer Loss after iteration 4000: 4.037595118488709\n",
      "two-layer Loss after iteration 5000: 3.207627490468319\n",
      "two-layer Loss after iteration 6000: 2.2563199440559347\n",
      "two-layer Loss after iteration 7000: 1.8235900871331432\n",
      "two-layer Loss after iteration 8000: 1.4856483947726074\n",
      "two-layer Loss after iteration 9000: 1.0850232525078796\n",
      "two-layer Loss after iteration 10000: 0.8918356926634462\n",
      "two-layer Loss after iteration 11000: 0.7856989738161627\n",
      "two-layer Loss after iteration 12000: 0.7010323640660631\n",
      "two-layer Loss after iteration 0: 1210.6586106506084\n",
      "two-layer Loss after iteration 1000: 19.777378951158507\n",
      "two-layer Loss after iteration 2000: 8.410425380515735\n",
      "two-layer Loss after iteration 3000: 5.529045170540565\n",
      "two-layer Loss after iteration 4000: 4.076837444200109\n",
      "two-layer Loss after iteration 5000: 2.9810947870542384\n",
      "two-layer Loss after iteration 6000: 2.195984957046969\n",
      "two-layer Loss after iteration 7000: 1.6772872989687957\n",
      "two-layer Loss after iteration 8000: 1.3998381398850328\n",
      "two-layer Loss after iteration 9000: 1.1916431727575263\n",
      "two-layer Loss after iteration 10000: 1.038571080996438\n",
      "two-layer Loss after iteration 11000: 0.9135104848571771\n",
      "two-layer Loss after iteration 12000: 0.8145700492888918\n",
      "two-layer Loss after iteration 0: 1387.6812250667713\n",
      "two-layer Loss after iteration 1000: 19.420000306086855\n",
      "two-layer Loss after iteration 2000: 8.282207803771108\n",
      "two-layer Loss after iteration 3000: 5.497201194255028\n",
      "two-layer Loss after iteration 4000: 4.048967524329539\n",
      "two-layer Loss after iteration 5000: 3.2908347019404363\n",
      "two-layer Loss after iteration 6000: 2.806143889091844\n",
      "two-layer Loss after iteration 7000: 2.251023063629032\n",
      "two-layer Loss after iteration 8000: 1.9213874535280115\n",
      "two-layer Loss after iteration 9000: 1.7051691603179497\n",
      "two-layer Loss after iteration 10000: 1.5312525760644375\n",
      "two-layer Loss after iteration 0: 1038.3128703758973\n",
      "two-layer Loss after iteration 1000: 19.873923609534696\n",
      "two-layer Loss after iteration 2000: 8.151226883260719\n",
      "two-layer Loss after iteration 3000: 5.349168191059368\n",
      "two-layer Loss after iteration 4000: 3.557745783492044\n",
      "two-layer Loss after iteration 5000: 2.6964081550648524\n",
      "two-layer Loss after iteration 6000: 2.1293247350379043\n",
      "two-layer Loss after iteration 7000: 1.7028126575079654\n",
      "two-layer Loss after iteration 8000: 1.3907421111251157\n",
      "two-layer Loss after iteration 9000: 1.1162778495885568\n",
      "two-layer Loss after iteration 10000: 0.937423081173812\n",
      "two-layer Loss after iteration 11000: 0.784067784656756\n",
      "two-layer Loss after iteration 12000: 0.6953817901318258\n",
      "two-layer Loss after iteration 0: 1078.8948221935555\n",
      "two-layer Loss after iteration 1000: 20.367167673153077\n",
      "two-layer Loss after iteration 2000: 8.505705100987518\n",
      "two-layer Loss after iteration 3000: 5.947229560927364\n",
      "two-layer Loss after iteration 4000: 4.281299041061302\n",
      "two-layer Loss after iteration 5000: 2.985818041820301\n",
      "two-layer Loss after iteration 6000: 2.1514168349594858\n",
      "two-layer Loss after iteration 7000: 1.6423420606098906\n",
      "two-layer Loss after iteration 8000: 1.2679722984886344\n",
      "two-layer Loss after iteration 9000: 1.0035790605728903\n",
      "two-layer Loss after iteration 10000: 0.8377481528770577\n",
      "two-layer Loss after iteration 11000: 0.7196482389933743\n",
      "two-layer Loss after iteration 0: 1169.3896492852793\n",
      "two-layer Loss after iteration 1000: 19.833829426848016\n",
      "two-layer Loss after iteration 2000: 8.327734757408017\n",
      "two-layer Loss after iteration 3000: 5.913005214667164\n",
      "two-layer Loss after iteration 4000: 4.854949452708283\n",
      "two-layer Loss after iteration 5000: 3.5987115284848805\n",
      "two-layer Loss after iteration 6000: 2.5581771742969064\n",
      "two-layer Loss after iteration 7000: 1.8184125396374413\n",
      "two-layer Loss after iteration 8000: 1.522205017306857\n",
      "two-layer Loss after iteration 9000: 1.275396860198713\n",
      "two-layer Loss after iteration 10000: 1.023634042436437\n",
      "two-layer Loss after iteration 11000: 0.8533891085401725\n",
      "two-layer Loss after iteration 12000: 0.6970770127807078\n",
      "two-layer Loss after iteration 13000: 0.5852439257022206\n",
      "two-layer Loss after iteration 14000: 0.5086002169254643\n",
      "two-layer Loss after iteration 15000: 0.45329272034532675\n",
      "two-layer Loss after iteration 0: 1184.3649224687233\n",
      "two-layer Loss after iteration 1000: 20.48047158662677\n",
      "two-layer Loss after iteration 2000: 8.655028222895831\n",
      "two-layer Loss after iteration 3000: 5.822228263799913\n",
      "two-layer Loss after iteration 4000: 4.095323353848548\n",
      "two-layer Loss after iteration 5000: 2.7963883974701007\n",
      "two-layer Loss after iteration 6000: 2.0900537832758124\n",
      "two-layer Loss after iteration 7000: 1.5355546962003703\n",
      "two-layer Loss after iteration 8000: 1.273648357925486\n",
      "two-layer Loss after iteration 9000: 1.1044775835352416\n",
      "two-layer Loss after iteration 10000: 0.9782354919627921\n",
      "two-layer Loss after iteration 0: 1054.77650292779\n",
      "two-layer Loss after iteration 1000: 20.406480471391244\n",
      "two-layer Loss after iteration 2000: 8.398300575651444\n",
      "two-layer Loss after iteration 3000: 5.8444602137683\n",
      "two-layer Loss after iteration 4000: 4.471799757201775\n",
      "two-layer Loss after iteration 5000: 3.139856875044827\n",
      "two-layer Loss after iteration 6000: 2.3043314111611157\n",
      "two-layer Loss after iteration 7000: 1.7262706291330072\n",
      "two-layer Loss after iteration 8000: 1.2313017691610857\n",
      "two-layer Loss after iteration 9000: 0.8947371344386823\n",
      "two-layer Loss after iteration 10000: 0.7597188047773654\n",
      "two-layer Loss after iteration 0: 1300.5653603011378\n",
      "two-layer Loss after iteration 1000: 19.966627109417388\n",
      "two-layer Loss after iteration 2000: 8.280087635508242\n",
      "two-layer Loss after iteration 3000: 5.827387594125832\n",
      "two-layer Loss after iteration 4000: 4.469167883045204\n",
      "two-layer Loss after iteration 5000: 3.2997927996721783\n",
      "two-layer Loss after iteration 6000: 2.5137943069422186\n",
      "two-layer Loss after iteration 7000: 2.0530234126690194\n",
      "two-layer Loss after iteration 8000: 1.7476765064954616\n",
      "two-layer Loss after iteration 9000: 1.5226504488540296\n",
      "two-layer Loss after iteration 0: 1182.3860952353075\n",
      "two-layer Loss after iteration 1000: 20.702016284265362\n",
      "two-layer Loss after iteration 2000: 8.873484675094161\n",
      "two-layer Loss after iteration 3000: 6.182316732661425\n",
      "two-layer Loss after iteration 4000: 4.647917042889513\n",
      "two-layer Loss after iteration 5000: 3.7853985432619126\n",
      "two-layer Loss after iteration 6000: 3.2786325112459997\n",
      "two-layer Loss after iteration 7000: 2.6735307820424037\n",
      "two-layer Loss after iteration 8000: 1.9380656722243796\n",
      "two-layer Loss after iteration 9000: 1.422627455155698\n",
      "two-layer Loss after iteration 10000: 1.0863670449823775\n",
      "two-layer Loss after iteration 11000: 0.8989201663470132\n",
      "two-layer Loss after iteration 12000: 0.7893530329654193\n",
      "two-layer Loss after iteration 0: 1099.8750761632166\n",
      "two-layer Loss after iteration 1000: 21.573266311504696\n",
      "two-layer Loss after iteration 2000: 8.683642456497525\n",
      "two-layer Loss after iteration 3000: 5.644698530795378\n",
      "two-layer Loss after iteration 4000: 4.5315495153867795\n",
      "two-layer Loss after iteration 5000: 3.5114551019822495\n",
      "two-layer Loss after iteration 6000: 2.730133415414091\n",
      "two-layer Loss after iteration 7000: 2.007930783615776\n",
      "two-layer Loss after iteration 8000: 1.64555356993214\n",
      "two-layer Loss after iteration 9000: 1.37377793156168\n",
      "two-layer Loss after iteration 10000: 1.1496999278224114\n",
      "two-layer Loss after iteration 11000: 0.9476553411248161\n",
      "two-layer Loss after iteration 12000: 0.8041889646625746\n",
      "two-layer Loss after iteration 13000: 0.7007388472874952\n",
      "two-layer Loss after iteration 14000: 0.6222222738689897\n",
      "three-layer Loss after iteration 0: 1279.034909539777\n",
      "three-layer Loss after iteration 1000: 4.921187554664212\n",
      "three-layer Loss after iteration 0: 1407.7320334570225\n",
      "three-layer Loss after iteration 0: 1379.5409000766938\n",
      "three-layer Loss after iteration 0: 1380.0052633588652\n",
      "three-layer Loss after iteration 1000: 5.437098539517157\n",
      "three-layer Loss after iteration 0: 1374.7836875299472\n",
      "three-layer Loss after iteration 1000: 6.585152419554444\n",
      "three-layer Loss after iteration 0: 1394.4055809274248\n",
      "three-layer Loss after iteration 0: 1370.9631366549013\n",
      "three-layer Loss after iteration 1000: 6.3118143827862605\n",
      "three-layer Loss after iteration 2000: 4.508851302907055\n",
      "three-layer Loss after iteration 0: 1311.6817258182743\n",
      "three-layer Loss after iteration 1000: 6.170956088048799\n",
      "three-layer Loss after iteration 0: 1413.7607437137547\n",
      "three-layer Loss after iteration 1000: 7.294963571542917\n",
      "three-layer Loss after iteration 0: 1455.9925613508738\n",
      "three-layer Loss after iteration 0: 1421.7035476513668\n",
      "three-layer Loss after iteration 0: 1203.8656084125053\n",
      "three-layer Loss after iteration 0: 1306.5171888060943\n",
      "three-layer Loss after iteration 1000: 5.991672405098221\n",
      "three-layer Loss after iteration 0: 1348.02765368506\n",
      "three-layer Loss after iteration 1000: 6.021822751183245\n",
      "three-layer Loss after iteration 0: 1125.523459177685\n",
      "three-layer Loss after iteration 0: 1209.8768223179409\n",
      "three-layer Loss after iteration 1000: 6.582421623729276\n",
      "three-layer Loss after iteration 0: 1207.713796596048\n",
      "three-layer Loss after iteration 1000: 5.886956898994658\n",
      "three-layer Loss after iteration 0: 1308.0141941415193\n",
      "three-layer Loss after iteration 1000: 5.2838370064612175\n",
      "three-layer Loss after iteration 0: 1227.8942236964904\n",
      "three-layer Loss after iteration 1000: 6.136439246420483\n",
      "three-layer Loss after iteration 0: 1249.8586698773677\n",
      "{'8': {'losses': [6.717682967680359, 6.805840488122652, 5.240775832494115, 4.2306578542007935, 4.868673354289362, 4.843286825261528, 5.487142166288184, 5.139407292419801, 3.9255825278641816, 5.5839635066934274, 6.66390489712697, 4.515359947964671, 5.400976570509247, 3.148835043233367, 16.99484445848099, 5.826172939017498, 5.409986141697674, 6.881506430224096, 6.134582518804437, 36.40674794610618], 'iterations': [3829, 3525, 4325, 5067, 5007, 4986, 4804, 4040, 6285, 4139, 3840, 5154, 4607, 6627, 3046, 4581, 4751, 3560, 3973, 1548]}, '16': {'losses': [1.8094866951964572, 1.7965765662341473, 1.0003711118573113, 0.925922529722825, 1.570482488473973, 1.030159498608969, 1.965337365825783, 0.6259106645954415, 3.760801266008483, 0.7010323640660631, 0.8145700492888918, 1.5312525760644375, 0.6953817901318258, 0.7196482389933743, 0.45329272034532675, 0.9782354919627921, 0.7597188047773654, 1.5226504488540296, 0.7893530329654193, 0.6222222738689897], 'iterations': [9766, 8759, 12125, 10715, 10462, 11799, 10146, 13926, 6367, 12593, 12550, 10121, 12577, 11418, 15482, 10641, 10723, 9406, 12315, 14407]}} {'4+4': {'losses': [17.23523398763797, 17.753719313131775, 11.112263626387747, 18.086897610800694, 1319.4665920303419, 15.969369226215779, 1259.5357851137194, 44.458214170496284, 18.85158397515922, 1198.5224091580426, 7.55360566475711, 1459.0971565076127, 1400.5386013136037, 1288.6890809852675, 8.8286082389801, 8.780677244859687, 8.588282851350577, 8.919303030703873, 1403.5284277972044, 7.425714069543393], 'iterations': [1350, 1963, 1757, 1487, 479, 1953, 405, 2731, 1594, 883, 1689, 255, 566, 358, 1117, 1269, 1507, 1216, 370, 1399]}, '8+8': {'losses': [4.921187554664212, 1407.7320334570225, 1379.5409000766938, 5.437098539517157, 6.585152419554444, 1394.4055809274248, 4.508851302907055, 6.170956088048799, 7.294963571542917, 1455.9925613508738, 1421.7035476513668, 1203.8656084125053, 5.991672405098221, 6.021822751183245, 1125.523459177685, 6.582421623729276, 5.886956898994658, 5.2838370064612175, 6.136439246420483, 1249.8586698773677], 'iterations': [1183, 820, 852, 1140, 1196, 709, 2019, 1780, 1027, 540, 698, 458, 1314, 1322, 849, 1469, 1396, 1041, 1687, 821]}}\n",
      "--- 198.9060037136078 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate the dataset based on the function f(x, y) = x^2 + y^2 + 1\n",
    "# Randomly generate data points for x and y in the range [-1, 1]\n",
    "np.random.seed(42)  # for reproducibility\n",
    "num_samples = 1000\n",
    "X = np.random.uniform(-8, 8, (num_samples, 2))\n",
    "y = (X[:, 0] ** 2 + X[:, 1] ** 2 + 1).reshape(-1, 1)\n",
    "print(X, y)\n",
    "\n",
    "# Parameters\n",
    "num_runs = 20\n",
    "learning_rate = 0.001\n",
    "tolerance = 0.0001\n",
    "max_iterations = 100000\n",
    "\n",
    "# Data structures to store results\n",
    "results_old = {\"8\": {\"losses\": [], \"iterations\": []}}\n",
    "results_new = {\"4+4\": {\"losses\": [], \"iterations\": []}}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# Test the old (2-layer) network with 8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 8, 1)\n",
    "    _, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"8\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"8\"][\"iterations\"].append(iterations)\n",
    "\n",
    "# Test the new (3-layer) network with 4+4 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [4,4], 1)\n",
    "    _, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"4+4\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"4+4\"][\"iterations\"].append(iterations)\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"16\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"8+8\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 16, 1)\n",
    "    _, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"16\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 8+8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [8, 8], 1)\n",
    "    _, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"8+8\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"8+8\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "'''\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"32\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"16+16\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 32 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 32, 1)\n",
    "    _, losses, iterations = old_train(model_old, X, y, \n",
    "                                      num_passes=max_iterations, \n",
    "                                      learning_rate=learning_rate, \n",
    "                                      tolerance=tolerance)\n",
    "    results_old[\"32\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"32\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 16+16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [16, 16], 1)\n",
    "    _, losses, iterations = train(model_new, X, y, \n",
    "                                  num_passes=max_iterations, \n",
    "                                  learning_rate=learning_rate, \n",
    "                                  tolerance=tolerance)\n",
    "    results_new[\"16+16\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"16+16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "'''\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T01:00:17.747177Z",
     "start_time": "2023-10-25T00:56:58.820135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x700 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4384.7, 11314.9, 1217.4, 1116.05]\n",
      "[5.448564153992929, 0.9520790108428085, 17.920308461966236, 6.583787021641861]\n"
     ]
    }
   ],
   "source": [
    "# Using Median Loss instead of Average Loss because the distributions are skewed\n",
    "# Combining the results\n",
    "configs_old = list(results_old.keys())\n",
    "\n",
    "median_losses_old = [np.median(results_old[config]['losses']) for config in configs_old]\n",
    "iterations_of_convergence_old = [np.mean(results_old[config]['iterations']) for config in configs_old]\n",
    "\n",
    "configs_new = list(results_new.keys())\n",
    "\n",
    "median_losses_new = [np.median(results_new[config]['losses']) for config in configs_new]\n",
    "iterations_of_convergence_new = [np.mean(results_new[config]['iterations']) for config in configs_new]\n",
    "\n",
    "configs_combined = configs_old + configs_new\n",
    "median_losses_combined = median_losses_old + median_losses_new\n",
    "iterations_of_convergence_combined = iterations_of_convergence_old + iterations_of_convergence_new\n",
    "\n",
    "# Create the combined bar + line graph\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Bar graph for Median Loss\n",
    "bars = ax1.bar(configs_combined, median_losses_combined, color=['blue', 'blue', 'green', 'green'], width=0.4, align='center')\n",
    "ax1.set_title('Median Loss and Average # of Iterations to Convergence for Each Configuration')\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Median Loss', color='black')\n",
    "ax1.tick_params('y', colors='black')\n",
    "\n",
    "# Line graph for Iterations using twin axes\n",
    "ax2 = ax1.twinx()\n",
    "line, = ax2.plot(configs_combined, iterations_of_convergence_combined, color='red', marker='o', linestyle='-')\n",
    "ax2.set_ylabel('Iterations to Convergence', color='black')\n",
    "ax2.tick_params('y', colors='black')\n",
    "\n",
    "# Legend\n",
    "ax1.legend([bars[0], bars[2], line], [\"Two-layer Median Loss\", \"Three-layer Median Loss\", \"Iterations to Convergence\"], loc=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(iterations_of_convergence_combined)\n",
    "print(median_losses_combined)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T01:00:18.219178800Z",
     "start_time": "2023-10-25T01:00:17.748178700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Median Loss:**\n",
    "The two-layer configurations (8 and 16 neurons) generally exhibit lower median loss values compared to the three-layer configurations (4+4 and 8+8).\n",
    "In particular, the configuration with 16 neurons in a single layer has the lowest median loss among all the tested configurations, indicating better performance in approximating the target function.\n",
    "The 4+4 configuration for the three-layer network shows the highest median loss, suggesting it may not be the optimal configuration for this specific problem.\n",
    "\n",
    "**Median Convergence Iterations:**\n",
    "The three-layer configurations tend to converge faster (in fewer iterations) than the two-layer configurations.\n",
    "(Two-layer: [8: 4384.7, 16: 11314.9], Three-layer: [4+4: 1132.65, 8+8: 1116.05])\n",
    "Among all configurations, the 8+8 configuration for the three-layer network takes the shortest time to converge.\n",
    "The two-layer network with 8 neurons also demonstrates relatively fast convergence.\n",
    "\n",
    "**General Trends:**\n",
    "Increasing the number of neurons in the two-layer configurations appears to improve performance in terms of loss. However, it leads to slower convergence.\n",
    "For the three-layer configurations, simply increasing the number of neurons does not guarantee better performance. Notably, when additional experiments were conducted with 16+16 neurons, the loss diverged significantly, and the number of iterations required was greater than that of the 8+8 configuration.\n",
    "In conclusion, for this specific problem and dataset, a two-layer configuration with an appropriate number of neurons appears to be more efficient and performant than the tested three-layer configurations when considering the trade-off between performance and convergence speed.\n",
    "\n",
    "\n",
    "**Additional experiments with activation functions:**\n",
    "In our case, when using a simple linear regression model, we observed that ReLU performed better (around zero) than tanh and the Logistic function (over 1370). Based on the results, we can conclude as follows:\n",
    "\n",
    ">ReLU is a piecewise linear activation function, and it only introduces non-linearity for positive inputs. This means that ReLU can approximate linear functions well and doesn't introduce strong non-linearities that might lead to convergence issues in a linear regression scenario. It's computationally efficient and less likely to cause gradient vanishing/exploding problems. Tanh is a scaled and shifted version of the sigmoid function. It introduces stronger non-linearity than ReLU across its entire range. When you apply tanh to the outputs of a linear regression model, it can introduce oscillations and non-linearities that might make convergence more challenging. The logistic function is also highly non-linear across its entire range. Applying this function to the outputs of a linear regression model can introduce even stronger non-linearities and may lead to convergence issues. It can suffer from the vanishing gradient problem for extreme input values, which can hinder convergence.\n",
    "\n",
    "\n",
    ">Additionally, tanh and logistic activations are prone to the vanishing gradient problem, especially for inputs far from zero. When gradients become too small during backpropagation, it can cause slow convergence or loss divergence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus: arbitrary number of layers (20 points):\n",
    "Change the functions such that they can accept an arbitrary number of layers, but\n",
    "keep the overall call-logic and training loops the same - do NOT use classes! For this,\n",
    "you will need to play around with the dictionaries in create_model, forward,\n",
    "backprop.\n"
   ],
   "metadata": {
    "id": "t8NWou_MVkxi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a multi-layer neural network\n",
    "def create_multilayer_model(X, layer_sizes):\n",
    "    model = {}\n",
    "    # using ReLU as the default activation function\n",
    "    model['activation_function'] = 'relu'  \n",
    "\n",
    "    # Create weights and biases for each layer based on layer_sizes\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        model[f'W{i+1}'] = np.random.randn(layer_sizes[i], layer_sizes[i+1]) / np.sqrt(layer_sizes[i])\n",
    "        model[f'b{i+1}'] = np.zeros((1, layer_sizes[i+1]))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define the forward pass given a model and data\n",
    "def feed_forward_multilayer(model, x):\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "\n",
    "    z = {}\n",
    "    a = {}\n",
    "    a[0] = x  # the input layer\n",
    "\n",
    "    # Compute activations and outputs for each layer\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "    for i in range(1, num_layers+1):\n",
    "        z[i] = a[i-1].dot(model[f'W{i}']) + model[f'b{i}']\n",
    "        a[i] = act_func(z[i])\n",
    "\n",
    "    return z, a\n",
    "\n",
    "# define the regression loss\n",
    "def calculate_loss_multilayer(model, X, y):\n",
    "    z, a = feed_forward_multilayer(model, X)\n",
    "    out = a[len(a) - 1]\n",
    "    \n",
    "    # calculate L2 loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "    # data_loss = np.mean((y - output) ** 2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# back-propagation for the multi-layer network\n",
    "def backprop_multilayer(X, y, model, z, a):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "\n",
    "    # Initialize the gradients\n",
    "    dW = {}\n",
    "    db = {}\n",
    "    delta = {}\n",
    "\n",
    "    # Compute the error for the last layer\n",
    "    delta[num_layers] = a[num_layers] - y\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    for i in reversed(range(1, num_layers+1)):\n",
    "        dW[i] = a[i-1].T.dot(delta[i]) / m\n",
    "        db[i] = np.sum(delta[i], axis=0, keepdims=True) / m\n",
    "        \n",
    "        if i > 1:  # Skip delta computation for the input layer\n",
    "            delta[i-1] = delta[i].dot(model[f'W{i}'].T) * act_func_derivative(a[i-1])\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "# training loop\n",
    "def train_multilayer(model, X, y, num_passes=100000, learning_rate=0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z, a = feed_forward_multilayer(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW, db = backprop_multilayer(X, y, model, z, a)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for key in dW:\n",
    "            model[f'W{key}'] -= learning_rate * dW[key]\n",
    "            model[f'b{key}'] -= learning_rate * db[key]\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss_multilayer(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(f\"Loss after iteration {i}: {loss}\")\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n",
    "            done = True\n",
    "        previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "id": "bk4EXE1lVkg1",
    "ExecuteTime": {
     "end_time": "2023-10-25T01:00:18.238784500Z",
     "start_time": "2023-10-25T01:00:18.224176500Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 999899.4990100244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_35640\\3937941782.py:101: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 1000: 48820.35809641017\n",
      "Loss after iteration 2000: 20742.28462854798\n",
      "Loss after iteration 3000: 14316.208809044201\n",
      "Loss after iteration 4000: 12764.239795143196\n",
      "Loss after iteration 5000: 12909.987705012747\n",
      "Loss after iteration 6000: 11278.611772227137\n",
      "Loss after iteration 7000: 10692.211717642487\n",
      "Loss after iteration 8000: 10820.37431815486\n",
      "Loss after iteration 9000: 9707.871204943285\n",
      "Loss after iteration 0: 1171545.5423141685\n",
      "Loss after iteration 0: 134638.75346603192\n",
      "Loss after iteration 1000: 34906.60622008264\n",
      "Loss after iteration 2000: 16150.1139372382\n",
      "Loss after iteration 3000: 11761.479808645363\n",
      "Loss after iteration 4000: 10189.52084274988\n",
      "Loss after iteration 5000: 10268.164518481304\n",
      "Loss after iteration 6000: 9931.716683331013\n",
      "Loss after iteration 7000: 9900.282587144673\n",
      "Loss after iteration 8000: 9136.480318451617\n",
      "Loss after iteration 9000: 8397.079958203536\n",
      "Loss after iteration 0: 574967.0114531331\n",
      "--- 10.560997486114502 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# architectures in lists of layer sizes\n",
    "architectures = [[2, 8, 1], [2, 4, 4, 1], [2, 16, 1], [2, 8, 8, 1]]\n",
    "\n",
    "# re-run the model training process with the provided architectures\n",
    "results = {}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# create and train multiple models with different layer sizes\n",
    "for arch in architectures:\n",
    "    model = create_multilayer_model(X, arch)\n",
    "    trained_model, losses, iterations = train_multilayer(model, X, y, num_passes=10000, learning_rate=0.01, tolerance=0.00001)\n",
    "    results[str(arch)] = losses\n",
    "    \n",
    "    #iterations\n",
    "    results[str(arch)+'iter'] = iterations\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T01:00:28.801997400Z",
     "start_time": "2023-10-25T01:00:18.231791200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[2, 8, 2]'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m configs_three_layer \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[2, 4, 4, 2]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[2, 8, 8, 2]\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Compute median losses\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m median_losses_two_layer \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmedian(results[config]) \u001B[38;5;28;01mfor\u001B[39;00m config \u001B[38;5;129;01min\u001B[39;00m configs_two_layer]\n\u001B[0;32m      7\u001B[0m median_losses_three_layer \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmedian(results[config]) \u001B[38;5;28;01mfor\u001B[39;00m config \u001B[38;5;129;01min\u001B[39;00m configs_three_layer]\n\u001B[0;32m      9\u001B[0m iterations_of_convergence_two_layer \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmean(results[config\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124miter\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m config \u001B[38;5;129;01min\u001B[39;00m configs_two_layer]\n",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      3\u001B[0m configs_three_layer \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[2, 4, 4, 2]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[2, 8, 8, 2]\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Compute median losses\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m median_losses_two_layer \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmedian(results[config]) \u001B[38;5;28;01mfor\u001B[39;00m config \u001B[38;5;129;01min\u001B[39;00m configs_two_layer]\n\u001B[0;32m      7\u001B[0m median_losses_three_layer \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmedian(results[config]) \u001B[38;5;28;01mfor\u001B[39;00m config \u001B[38;5;129;01min\u001B[39;00m configs_three_layer]\n\u001B[0;32m      9\u001B[0m iterations_of_convergence_two_layer \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmean(results[config\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124miter\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m config \u001B[38;5;129;01min\u001B[39;00m configs_two_layer]\n",
      "\u001B[1;31mKeyError\u001B[0m: '[2, 8, 2]'"
     ]
    }
   ],
   "source": [
    "# Extracting data for plotting\n",
    "configs_two_layer = ['[2, 8, 2]', '[2, 16, 2]']\n",
    "configs_three_layer = ['[2, 4, 4, 2]', '[2, 8, 8, 2]']\n",
    "\n",
    "# Compute median losses\n",
    "median_losses_two_layer = [np.median(results[config]) for config in configs_two_layer]\n",
    "median_losses_three_layer = [np.median(results[config]) for config in configs_three_layer]\n",
    "\n",
    "iterations_of_convergence_two_layer = [np.mean(results[config+'iter']) for config in configs_two_layer]\n",
    "iterations_of_convergence_three_layer = [np.mean(results[config+'iter']) for config in configs_three_layer]\n",
    "\n",
    "# Create the combined bar + line graph\n",
    "fig, ax1 = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Combine the results\n",
    "configs_combined = configs_two_layer + configs_three_layer\n",
    "median_losses_combined = median_losses_two_layer + median_losses_three_layer\n",
    "iterations_of_convergence_combined = iterations_of_convergence_two_layer + iterations_of_convergence_three_layer\n",
    "\n",
    "# Bar graph for Average Loss\n",
    "bars = ax1.bar(configs_combined, median_losses_combined,\n",
    "               color=['blue', 'blue', 'green', 'green'],\n",
    "               width=0.4, align='center')\n",
    "ax1.set_title('Median Loss and Average # of Iterations to Convergence for Each Configuration')\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Median Loss', color='black')\n",
    "ax1.tick_params('y', colors='black')\n",
    "\n",
    "# Line graph for Iterations using twin axes\n",
    "ax2 = ax1.twinx()\n",
    "line, = ax2.plot(configs_combined, iterations_of_convergence_combined, color='red', marker='o', linestyle='-')\n",
    "ax2.set_ylabel('Iterations to Convergence', color='black')\n",
    "ax2.tick_params('y', colors='black')\n",
    "\n",
    "# Legend\n",
    "ax1.legend([bars[0], bars[2], line], [\"Two-layer Median Loss\", \"Three-layer Median Loss\", \"Iterations to Convergence\"], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(iterations_of_convergence_combined)\n",
    "# print(median_losses_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T01:00:30.084392700Z",
     "start_time": "2023-10-25T01:00:28.804994100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part2 Pytorch version (20 points):\n",
    "\n",
    "Add all code to the same threelayer.ipynb.\n",
    "Given that everything is easier with pytorch, adapt the code from class to solve the\n",
    "exact same regression problem with three layers and the same number of\n",
    "parameters. Use the ‘“nn” layers. Visualize the network architecture as well.\n",
    "Test the network 20 times with ADAM optimizer and 20 times with SGD optimizer,\n",
    "using a suitably high number of iterations. Record, plot, and compare the loss\n",
    "evaluation of the two optimizer runs. What can you say about the optimizers?"
   ],
   "metadata": {
    "id": "xJhRwYl8YCuL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "Yf_A3FKBYCNW",
    "ExecuteTime": {
     "start_time": "2023-10-25T01:00:30.083392Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model_pytorch(model, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "    # Choose the optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Convert data to torch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Print every 1000 epochs\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T01:00:30.088390600Z",
     "start_time": "2023-10-25T01:00:30.085392900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the models with the new data using nn.Sequential\n",
    "losses_adam_seq = []\n",
    "losses_sgd_seq = []\n",
    "input_dim = X.shape[1]\n",
    "hidden_nodes = [8, 8]\n",
    "output_dim = 1\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(20):\n",
    "    # build model with adam optimizer\n",
    "    model_adam_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_adam_seq.append(train_model_pytorch(model_adam_seq, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    # build model with sgd optimizer\n",
    "    model_sgd_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_sgd_seq.append(train_model_pytorch(model_sgd_seq, X, y, optimizer_type='sgd', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    print(f'difference of optimizers: {losses_sgd_seq[-1][-1] - losses_adam_seq[-1][-1]}')\n",
    "    print(f'Done with run {_}')\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T01:00:30.086391600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the loss evaluations for ADAM and SGD optimizers using the provided models\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Average loss for better visualization\n",
    "avg_losses_adam_seq = np.mean(losses_adam_seq, axis=0)\n",
    "avg_losses_sgd_seq = np.mean(losses_sgd_seq, axis=0)\n",
    "\n",
    "plt.plot(avg_losses_adam_seq, label='ADAM', color='blue')\n",
    "plt.plot(avg_losses_sgd_seq, label='SGD', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evaluation for ADAM vs. SGD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T01:00:30.104390Z",
     "start_time": "2023-10-25T01:00:30.089392700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The performance of ADAM and SGD optimizers on the dataset and model architecture provided gives us some insights into their characteristics:\n",
    "\n",
    "Convergence Speed: ADAM tends to converge faster than SGD. This is evident from the lower loss achieved by ADAM within the same number of epochs compared to SGD. This rapid convergence can be attributed to the adaptive learning rates for each parameter in ADAM, which means it can adjust itself during training to move faster in the more relevant directions in the parameter space.\n",
    "\n",
    "Adaptive Learning Rates: ADAM uses momentums of first and second order, which allows it to have an adaptive learning rate. It combines the benefits of two extensions of SGD: AdaGrad and RMSProp. This often helps in navigating the loss landscapes more effectively than standard SGD.\n",
    "\n",
    "Stability: SGD, being more straightforward, sometimes offers more stability in terms of finding a minimum. However, that minimum might not always be as low (in terms of loss) as the one found by algorithms like ADAM. In our case, SGD didn't perform as well as ADAM, but in some scenarios, especially with proper learning rate annealing and larger datasets, SGD can outperform ADAM.\n",
    "\n",
    "Hyperparameters: ADAM has more hyperparameters to tune than SGD \n",
    "While default values often work well, there's still a tuning aspect involved. SGD is simpler with mainly the learning rate to consider.\n",
    "\n",
    "Generalization: Some studies suggest that while adaptive optimizers like ADAM can converge faster and achieve lower training losses, SGD (especially with momentum) might generalize better to the test set. This wasn't specifically evaluated in our experiment, but it's an important consideration in real-world scenarios.\n",
    "\n",
    "Noise: SGD inherently introduces a lot of noise in the optimization process. This noise can be beneficial as it can help escape shallow local minima. ADAM tries to dampen this noise, which can be both good (faster convergence) and bad (might get stuck in poor local minima).\n",
    "\n",
    "In conclusion, the choice of optimizer often depends on the specific problem, dataset size, and model architecture. While ADAM might be a good default choice to start with (especially for rapid prototyping), one should not disregard SGD, especially with momentum, when fine-tuning models or when aiming for the best generalization on a test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T01:00:30.091392800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T01:00:30.093390700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T01:00:30.094390100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T01:00:30.096392400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-25T01:00:30.099390800Z"
    }
   }
  }
 ]
}
