{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Part1 Three layer network (60 points)**\n",
    "* 수업에서 작성한 두 개의 hidden layer를 갖도록 한 개 이상의 layer로 regression problem을 해결하는 fully-connected two layer perceptron을 확장하여 사용.\n",
    "* hidden layer 수와 관계없이 internal derivative의 derivation은 항상 동일함을 유의.\n",
    "* 아래 조건에 따라 기존의 two-layer version과 새로운 three-layer로 x^2 + y^2 + 1에 대한 여러 테스트를 진행할 것\n",
    "-----------------------------\n",
    "    조건\n",
    "      - lr=0.0001\n",
    "      - tolerance_threshold=0.0001\n",
    "      - max(iter)=100,000\n",
    "      - No sgd, No regularization\n",
    "      - act_func=ReLU\n",
    "-----------------------------\n",
    "    1st Test\n",
    "      - two-layer version: 8 neurons\n",
    "      - three-layer version: 4+4 neurons\n",
    "      - 각 네트워크마다 20번 실행 후 loss와 iter 수 기록\n",
    "-----------------------------\n",
    "    2nd Test\n",
    "      - two-layer version: 16 neurons\n",
    "      - three-layer version: 8+8 neurons\n",
    "      - 각 네트워크마다 20번 실행 후 loss와 iter 수 기록\n",
    "-----------------------------\n",
    "\n",
    "1과 2 테스트 중 어떤 구조가 더 좋은가?\n",
    "- 하나의 그래프에 iter 수와 error에 대한 결과 plot하고 결과 기술"
   ],
   "metadata": {
    "id": "89fqwYoOQ-kY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "import sys\n",
    "import time\n",
    "from IPython import display\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn import datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:48:35.911175100Z",
     "start_time": "2023-10-23T02:48:35.701060800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1. * (X > 0)\n",
    "\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return 1.-tanh(X)**2\n",
    "\n",
    "def logistic(X):\n",
    "    return 1./(1. + np.exp(-X))\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return logistic(X)*(1. - logistic(X))\n",
    "\n",
    "# Activation functions mapping\n",
    "activation_functions = {\n",
    "    'relu': relu,\n",
    "    'tanh': tanh,\n",
    "    'logistic': logistic\n",
    "}\n",
    "\n",
    "# Activation functions derivatives mapping\n",
    "activation_derivatives = {\n",
    "    'relu': relu_derivative,\n",
    "    'tanh': tanh_derivative,\n",
    "    'logistic': logistic_derivative\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:48:35.948160200Z",
     "start_time": "2023-10-23T02:48:35.709057100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def old_create_model(X, hidden_nodes, output_dim = 2):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n",
    "    # set of biases\n",
    "    model['b1'] = np.zeros((1, hidden_nodes))\n",
    "\n",
    "    # second set of weights from hidden layer 1 to output\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n",
    "    # set of biases\n",
    "    model['b2'] = np.zeros((1, output_dim))\n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def old_feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "\n",
    "    # activation function\n",
    "    #a1 = logistic(z1)\n",
    "    #a1 = tanh(z1)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "\n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = z2\n",
    "    return z1, a1, z2, out\n",
    "\n",
    "# define the regression loss\n",
    "def old_calculate_loss(model,X,y,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "\n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out = old_feed_forward(model, X)\n",
    "\n",
    "    # calculate L2 loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # add regulatization term to loss\n",
    "    loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def old_backprop(X,y,model,z1,a1,z2,output,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    # derivative of loss function\n",
    "    delta3 = (output-y)/num_examples\n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    #delta2 = delta3.dot(model['W2'].T) * logistic_derivative(a1) #if logistic\n",
    "    #delta2 = delta3.dot(model['W2'].T) * tanh_derivative(a1) #if tanh\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1) #if ReLU\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = np.dot(X.T, delta2)\n",
    "    # and sum over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    # add regularization terms on the two weights\n",
    "    dW2 += reg_lambda * model['W2']\n",
    "    dW1 += reg_lambda * model['W1']\n",
    "\n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop\n",
    "def old_train(model, X, y, num_passes=100000, reg_lambda = 0.1, learning_rate = 0.001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while done == False:\n",
    "        # get predictions\n",
    "        z1,a1,z2,output = old_feed_forward(model, X)\n",
    "        # feed this into backprop\n",
    "        dW1, dW2, db1, db2 = old_backprop(X,y,model,z1,a1,z2,output,reg_lambda)\n",
    "\n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "\n",
    "        # do some book-keeping every once in a while\n",
    "        if i % 1000 == 0:\n",
    "            loss = old_calculate_loss(model, X, y, reg_lambda)\n",
    "            losses.append(loss)\n",
    "            print(\"Two Layer Loss after iteration {}: {}\".format(i, loss))\n",
    "            # very crude method to break optimization\n",
    "            if previous_loss != 0 and np.abs((previous_loss-loss)/previous_loss) < 0.001:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "        i += 1\n",
    "        if i>=num_passes:\n",
    "            done = True\n",
    "    return model, losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:48:35.991749600Z",
     "start_time": "2023-10-23T02:48:35.726057400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# create a three-layer neural network\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function='relu'):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    #save activation function to model\n",
    "    model['activation_function'] = activation_function\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # [i -> 1]weights and biases from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes[0]) / np.sqrt(input_dim)\n",
    "    model['b1'] = np.zeros((1, hidden_nodes[0]))\n",
    "    \n",
    "    # [1 -> 2]weights and biases  from  hidden layer 1 to hidden layer 2\n",
    "    model['W2'] = np.random.randn(hidden_nodes[0], hidden_nodes[1]) / np.sqrt(hidden_nodes[0])\n",
    "    model['b2'] = np.zeros((1, hidden_nodes[1]))\n",
    "\n",
    "    # [2 -> o]weights and biases from hidden layer 2 to output\n",
    "    model['W3'] = np.random.randn(hidden_nodes[1], output_dim) / np.sqrt(hidden_nodes[1])\n",
    "    model['b3'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = act_func(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = act_func(z2)\n",
    "    \n",
    "    # third layer\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    out = z3\n",
    "    \n",
    "    return z1, a1, z2, a2, z3, out\n",
    "    \n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "\n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, a2, z3, out = feed_forward(model, X)\n",
    "\n",
    "    # calculate L2 loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # add regulatization term to loss\n",
    "    loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,a2,z3,output,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "    \n",
    "    delta4 = (output-y)/num_examples\n",
    "    dW3 = a2.T.dot(delta4)\n",
    "    db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "\n",
    "    delta3 = delta4.dot(model['W3'].T) * act_func_derivative(a2)\n",
    "    dW2 = np.dot(a1.T, delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    \n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1)\n",
    "    dW1 = np.dot(X.T, delta2)\n",
    "    db1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "\n",
    "    # add regularization terms on the two weights\n",
    "    dW3 += reg_lambda * model['W3']\n",
    "    dW2 += reg_lambda * model['W2']\n",
    "    dW1 += reg_lambda * model['W1']\n",
    "\n",
    "    return dW1, dW2, dW3, db1, db2, db3\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, reg_lambda = 0, learning_rate = 0.001, tolerance = 0.0001):\n",
    "    \n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    iterations = 0\n",
    "    while done == False:\n",
    "            # choose a random set of points\n",
    "            randinds = np.random.choice(np.arange(len(y)),30,False)\n",
    "            # get predictions\n",
    "            z1,a1,z2,a2,z3,output = feed_forward(model, X[randinds,:])\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, dW3, db1, db2, db3 = backprop(X[randinds,:],y[randinds],model,z1,a1,z2,a2,z3,output,reg_lambda)\n",
    "        else:\n",
    "            # get predictions\n",
    "            z1,a1,z2,a2,z3,output = feed_forward(model, X)\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, dW3, db1, db2, db3 = backprop(X,y,model,z1,a1,z2,a2,z3,output,reg_lambda)\n",
    "\n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        model['W3'] -= learning_rate * dW3\n",
    "        model['b3'] -= learning_rate * db3\n",
    "\n",
    "        # do some book-keeping every once in a while\n",
    "        if i % 1000 == 0:\n",
    "            loss = calculate_loss(model, X, y, reg_lambda)\n",
    "            losses.append(loss)\n",
    "            print(\"Three Layer Loss after iteration {}: {}\".format(i, loss))\n",
    "            # very crude method to break optimization\n",
    "            if previous_loss != 0 and np.abs((previous_loss-loss)/previous_loss) < 0.001:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "        i += 1\n",
    "        if i>=num_passes:\n",
    "            done = True\n",
    "    return model, losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:48:36.036209800Z",
     "start_time": "2023-10-23T02:48:35.751063400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Layer Loss after iteration 0: 1521.6570461719425\n",
      "Two Layer Loss after iteration 1000: 26.98206088806651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_22460\\2629070055.py:120: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if previous_loss != 0 and np.abs((previous_loss-loss)/previous_loss) < 0.001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Layer Loss after iteration 2000: 10.345953168112125\n",
      "Two Layer Loss after iteration 3000: 7.2135242945218065\n",
      "Two Layer Loss after iteration 4000: 6.424877598812335\n",
      "Two Layer Loss after iteration 5000: 5.890511861048182\n",
      "Two Layer Loss after iteration 6000: 5.667468946774807\n",
      "Two Layer Loss after iteration 7000: 5.1882792944397575\n",
      "Two Layer Loss after iteration 8000: 4.818835784523701\n",
      "Two Layer Loss after iteration 9000: 4.429588956156372\n",
      "Two Layer Loss after iteration 10000: 4.185377656494947\n",
      "Two Layer Loss after iteration 11000: 4.2752520753480425\n",
      "Two Layer Loss after iteration 12000: 3.6288564134878216\n",
      "Two Layer Loss after iteration 13000: 3.6151854153028444\n",
      "Two Layer Loss after iteration 14000: 3.2394202797021907\n",
      "Two Layer Loss after iteration 15000: 3.3376810824305143\n",
      "Two Layer Loss after iteration 16000: 3.050671681755592\n",
      "Two Layer Loss after iteration 17000: 3.134164294691153\n",
      "Two Layer Loss after iteration 18000: 3.126843193146604\n",
      "Two Layer Loss after iteration 19000: 2.9240619606172986\n",
      "Two Layer Loss after iteration 20000: 3.2349951828390946\n",
      "Two Layer Loss after iteration 21000: 2.7855895688701815\n",
      "Two Layer Loss after iteration 22000: 2.7659970081567025\n",
      "Two Layer Loss after iteration 23000: 2.77638721007705\n",
      "Two Layer Loss after iteration 24000: 2.9968487439339575\n",
      "Two Layer Loss after iteration 25000: 2.836343697487105\n",
      "Two Layer Loss after iteration 26000: 2.6547790127833415\n",
      "Two Layer Loss after iteration 27000: 2.7816897179708713\n",
      "Two Layer Loss after iteration 28000: 2.6897224412221576\n",
      "Two Layer Loss after iteration 29000: 2.7261536465614706\n",
      "Two Layer Loss after iteration 30000: 2.8259440475028947\n",
      "Two Layer Loss after iteration 31000: 2.6129814047907884\n",
      "Two Layer Loss after iteration 32000: 2.7151356497599974\n",
      "Two Layer Loss after iteration 33000: 2.655227978658757\n",
      "Two Layer Loss after iteration 34000: 2.689148997785734\n",
      "Two Layer Loss after iteration 35000: 2.7257575903665567\n",
      "Two Layer Loss after iteration 36000: 2.7080247938590905\n",
      "Two Layer Loss after iteration 37000: 2.6875569094058176\n",
      "Two Layer Loss after iteration 38000: 2.7800818338602005\n",
      "Two Layer Loss after iteration 39000: 2.5972561278522703\n",
      "Two Layer Loss after iteration 40000: 2.6023839609885844\n",
      "Two Layer Loss after iteration 41000: 2.6031428939654955\n",
      "Three Layer Loss after iteration 0: 1285.540723483767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_22460\\1964550964.py:132: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if previous_loss != 0 and np.abs((previous_loss-loss)/previous_loss) < 0.001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Layer Loss after iteration 1000: 12.515808653998098\n",
      "Three Layer Loss after iteration 2000: 10.671005198877367\n",
      "Three Layer Loss after iteration 3000: 7.656088862221957\n",
      "Three Layer Loss after iteration 4000: 12.389931348327366\n",
      "Three Layer Loss after iteration 5000: 5.494603818369914\n",
      "Three Layer Loss after iteration 6000: 6.261388295479025\n",
      "Three Layer Loss after iteration 7000: 4.5642718091018315\n",
      "Three Layer Loss after iteration 8000: 3.837694082379573\n",
      "Three Layer Loss after iteration 9000: 4.619846334538207\n",
      "Three Layer Loss after iteration 10000: 3.4797042823925883\n",
      "Three Layer Loss after iteration 11000: 3.4928060685296347\n",
      "Three Layer Loss after iteration 12000: 3.749459332923351\n",
      "Three Layer Loss after iteration 13000: 3.132621707762737\n",
      "Three Layer Loss after iteration 14000: 3.5209541011191194\n",
      "Three Layer Loss after iteration 15000: 3.563093603242017\n",
      "Three Layer Loss after iteration 16000: 2.5654773594224607\n",
      "Three Layer Loss after iteration 17000: 3.711897649277704\n",
      "Three Layer Loss after iteration 18000: 2.631623135290177\n",
      "Three Layer Loss after iteration 19000: 2.790077369713285\n",
      "Three Layer Loss after iteration 20000: 2.569815886822683\n",
      "Three Layer Loss after iteration 21000: 3.6871343333674638\n",
      "Three Layer Loss after iteration 22000: 2.434153232907224\n",
      "Three Layer Loss after iteration 23000: 3.4082028067254133\n",
      "Three Layer Loss after iteration 24000: 2.361413300637935\n",
      "Three Layer Loss after iteration 25000: 2.3920786933302263\n",
      "Three Layer Loss after iteration 26000: 2.7586552593283717\n",
      "Three Layer Loss after iteration 27000: 2.375696669590932\n",
      "Three Layer Loss after iteration 28000: 3.0112914919606153\n",
      "Three Layer Loss after iteration 29000: 3.266879274786276\n",
      "Three Layer Loss after iteration 30000: 2.5073486247168497\n",
      "Three Layer Loss after iteration 31000: 2.807581991025142\n",
      "Three Layer Loss after iteration 32000: 3.460769154188205\n",
      "Three Layer Loss after iteration 33000: 2.6322040665994098\n",
      "Three Layer Loss after iteration 34000: 2.5945567166043166\n",
      "Three Layer Loss after iteration 35000: 2.6548887077620087\n",
      "Three Layer Loss after iteration 36000: 3.172469363077095\n",
      "Three Layer Loss after iteration 37000: 2.8296034814950723\n",
      "Three Layer Loss after iteration 38000: 2.711752299443956\n",
      "Three Layer Loss after iteration 39000: 2.213243095440806\n",
      "Three Layer Loss after iteration 40000: 2.234855315438549\n",
      "Three Layer Loss after iteration 41000: 2.5056438190651766\n",
      "Three Layer Loss after iteration 42000: 2.578053292318636\n",
      "Three Layer Loss after iteration 43000: 2.535174823911469\n",
      "Three Layer Loss after iteration 44000: 2.4387050435537008\n",
      "Three Layer Loss after iteration 45000: 2.2636623007354837\n",
      "Three Layer Loss after iteration 46000: 2.2148077683276286\n",
      "Three Layer Loss after iteration 47000: 2.2994739990942814\n",
      "Three Layer Loss after iteration 48000: 2.205904536835257\n",
      "Three Layer Loss after iteration 49000: 2.3129768092254746\n",
      "Three Layer Loss after iteration 50000: 2.0955312191502764\n",
      "Three Layer Loss after iteration 51000: 2.4286141037909394\n",
      "Three Layer Loss after iteration 52000: 4.457340403639089\n",
      "Three Layer Loss after iteration 53000: 2.5359184586164605\n",
      "Three Layer Loss after iteration 54000: 2.08036384063115\n",
      "Three Layer Loss after iteration 55000: 2.117751604752048\n",
      "Three Layer Loss after iteration 56000: 2.1171725490746214\n"
     ]
    }
   ],
   "source": [
    "# data creation \n",
    "np.random.seed(0)  # For reproducibility\n",
    "num_samples = 1000\n",
    "X_train = np.random.uniform(-5, 5, (num_samples, 2))\n",
    "y_train = (X_train[:, 0]**2 + X_train[:, 1]**2 + 1).reshape(-1, 1)\n",
    "\n",
    "# create the model\n",
    "old_model = old_create_model(X,8,1)\n",
    "model = create_model(X,[4,4],1, activation_function='relu')\n",
    "\n",
    "\n",
    "# Training configurations\n",
    "configs = [\n",
    "    {\"neurons\": [8], \"label\": \"Two-layer (8)\"},\n",
    "    {\"neurons\": [4, 4], \"label\": \"Three-layer (4+4)\"},\n",
    "    {\"neurons\": [16], \"label\": \"Two-layer (16)\"},\n",
    "    {\"neurons\": [8, 8], \"label\": \"Three-layer (8+8)\"}\n",
    "]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Training loop for each configuration\n",
    "for config in configs:\n",
    "    config_results = {\"label\": config[\"label\"], \"losses\": [], \"iterations\": []}\n",
    "    for _ in range(20):  # 20 runs for each configuration\n",
    "        model = create_model(X_train, *config[\"neurons\"])\n",
    "        trained_model, losses, iters = modified_train(model, X_train, y_train)\n",
    "        config_results[\"losses\"].append(losses[-1])\n",
    "        config_results[\"iterations\"].append(iters)\n",
    "    results.append(config_results)\n",
    "\n",
    "results\n",
    "\n",
    "# training\n",
    "old_model, old_losses = old_train(old_model,X, y, reg_lambda=reg_lambda)\n",
    "\n",
    "new_model, new_losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n",
    "\n",
    "# determine predictions of the trained model\n",
    "old_output = old_feed_forward(model, X)\n",
    "output = feed_forward(model, X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-23T02:54:45.122292800Z",
     "start_time": "2023-10-23T02:54:31.231937Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bonus: arbitrary number of layers (20 points)**\n",
    "위의 코드에 대해 전체 call-logic과 training loop를 유지하되, 함수가 임의의 수의 레이어를 사용할 수 있도록 코드 수정.\n",
    "  - create_model, forward, backprop의 딕셔너리 수정해야함"
   ],
   "metadata": {
    "id": "t8NWou_MVkxi"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "bk4EXE1lVkg1",
    "ExecuteTime": {
     "end_time": "2023-10-23T02:48:40.724964300Z",
     "start_time": "2023-10-23T02:48:40.720721200Z"
    }
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part2 Pytorch version (20 points)**\n",
    "- pytorch의 nn layer를 사용하여 같은 수의 파라미터와 세 개의 layer로 동일한 regression 문제를 해결할 수 있도록 코드를 수정하고 시각화\n",
    "- 충분히 큰 iter 수를 사용하여 ADAM optimizer와 SGD optimizer로 각각 20번씩 테스트\n",
    "- 각 optimizer에 대해 loss 평가와 plot 비교 후, 각 optimizer에 대해 설명"
   ],
   "metadata": {
    "id": "xJhRwYl8YCuL"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Yf_A3FKBYCNW",
    "ExecuteTime": {
     "end_time": "2023-10-23T02:48:40.728959800Z",
     "start_time": "2023-10-23T02:48:40.724384300Z"
    }
   },
   "execution_count": 51,
   "outputs": []
  }
 ]
}
