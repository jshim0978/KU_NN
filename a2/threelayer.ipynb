{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Part1 Three layer network (60 points):\n",
    "\n",
    "<p>\n",
    "Create a notebook called threelayer.ipynb.\n",
    "Extend the fully-connected two layer perceptron shown in class for the regression\n",
    "problem by one more layer to have two hidden layers.\n",
    "For this, take a long look at the derivation of the backpropagation. As you can see,\n",
    "the derivation of the internal derivatives is always the same, no matter the number\n",
    "of hidden layers.\n",
    "Armed with this knowledge, it should be easy to extend the different functions in\n",
    "the notebook.\n",
    "The create_model function should now of course receive a list of values for the\n",
    "number of hidden_nodes in each layer - in addition, please extend the functionality,\n",
    "so that I can pass the activation function type as a string already here:\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "```python\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function = 'relu') :\n",
    "    return\n",
    "```\n",
    "\n",
    "<p>\n",
    "forward, calculate_loss, backprop should of course be extended\n",
    "accordingly to deal with the added number of layers and the activation function\n",
    "choice.\n",
    "Next, do a series of tests for the x^2+y^2+1 function using a learning rate of 0.001, a\n",
    "tolerance threshold of 0.0001, maximum iterations of 100,000, NO sgd, and NO\n",
    "regularization, and a “relu” function, comparing the “old” two-layer version with\n",
    "your “new” three-layer version as follows:\n",
    "- take 8 neurons for the two-layer version and 4 + 4 neurons for the three-layer\n",
    "version, and run each network 20 times, recording the loss and the number\n",
    "of iterations it needs\n",
    "- repeat this with 16 neurons for the two-layer version and 8+8 neurons for the\n",
    "three-layer version and 20 runs each\n",
    "- which network architecture converges “better” (earlier? lower error?). Plot\n",
    "the results nicely in one graph for errors and in another for number of\n",
    "iterations and comment on the results.\n",
    "</p>"
   ],
   "metadata": {
    "id": "89fqwYoOQ-kY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# numpy, matplotlib imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:00:37.521882900Z",
     "start_time": "2023-10-26T04:00:37.519365100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1. * (X > 0)\n",
    "\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return 1.-tanh(X)**2\n",
    "\n",
    "def logistic(X):\n",
    "    return 1./(1. + np.exp(-X))\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return logistic(X)*(1. - logistic(X))\n",
    "\n",
    "# Activation functions mapping\n",
    "activation_functions = {\n",
    "    'relu': relu,\n",
    "    'tanh': tanh,\n",
    "    'logistic': logistic\n",
    "}\n",
    "\n",
    "# Activation functions derivatives mapping\n",
    "activation_derivatives = {\n",
    "    'relu': relu_derivative,\n",
    "    'tanh': tanh_derivative,\n",
    "    'logistic': logistic_derivative\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:00:37.548045600Z",
     "start_time": "2023-10-26T04:00:37.524882800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def old_create_model(X, hidden_nodes, output_dim = 2):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b1'] = np.zeros((1, hidden_nodes))\n",
    "\n",
    "    # second set of weights from hidden layer 1 to output\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b2'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def old_feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "\n",
    "    # activation function\n",
    "    #a1 = logistic(z1)\n",
    "    #a1 = tanh(z1)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "\n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = z2\n",
    "    return z1, a1, z2, out\n",
    "\n",
    "# define the regression loss\n",
    "def old_calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out = old_feed_forward(model, X)\n",
    "\n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def old_backprop(X,y,model,z1,a1,z2,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # derivative of loss function\n",
    "    delta3 = (output-y)/num_examples\n",
    "    \n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    \n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    #delta2 = delta3.dot(model['W2'].T) * logistic_derivative(a1) #if logistic\n",
    "    #delta2 = delta3.dot(model['W2'].T) * tanh_derivative(a1) #if tanh\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1) #if ReLU\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = np.dot(X.T, delta2)\n",
    "    \n",
    "    # and sum over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop\n",
    "def old_train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1,a1,z2,output = old_feed_forward(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW1, dW2, db1, db2 = old_backprop(X, y, model, z1, a1, z2, output)\n",
    "\n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "\n",
    "        loss = old_calculate_loss(model, X, y)\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"two-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n",
    "                print(np.abs((previous_loss - loss) / previous_loss), previous_loss, loss)\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "            \n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:00:37.562045200Z",
     "start_time": "2023-10-26T04:00:37.541419400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create a three-layer neural network\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function='relu'):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    #save activation function to model\n",
    "    model['activation_function'] = activation_function\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # [i -> 1]weights and biases from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes[0]) / np.sqrt(input_dim)\n",
    "    model['b1'] = np.zeros((1, hidden_nodes[0]))\n",
    "    \n",
    "    # [1 -> 2]weights and biases  from  hidden layer 1 to hidden layer 2\n",
    "    model['W2'] = np.random.randn(hidden_nodes[0], hidden_nodes[1]) / np.sqrt(hidden_nodes[0])\n",
    "    model['b2'] = np.zeros((1, hidden_nodes[1]))\n",
    "\n",
    "    # [2 -> o]weights and biases from hidden layer 2 to output\n",
    "    model['W3'] = np.random.randn(hidden_nodes[1], output_dim) / np.sqrt(hidden_nodes[1])\n",
    "    model['b3'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = relu(z2)\n",
    "    \n",
    "    # third layer\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    out = z3\n",
    "    \n",
    "    return z1, a1, z2, a2, z3, out\n",
    "    \n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, a2, z3, out = feed_forward(model, X)\n",
    "\n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,a2,z3,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "\n",
    "    # Derivative of loss function for output layer\n",
    "    delta4 = (output - y) / num_examples\n",
    "\n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW3 = a2.T.dot(delta4)\n",
    "\n",
    "    # and over all neurons\n",
    "    db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta3 = delta4.dot(model['W3'].T) * relu_derivative(a2)\n",
    "\n",
    "    # multiply this by hidden layer outputs\n",
    "    dW2 = a1.T.dot(delta3)\n",
    "\n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1)\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = X.T.dot(delta2)\n",
    "\n",
    "    # and over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, dW3, db1, db2, db3\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance = 0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1, a1, z2, a2, z3, output = feed_forward(model, X)\n",
    "\n",
    "        # feed this into backprop\n",
    "        dW1, dW2, dW3, db1, db2, db3 = backprop(X, y, model, z1, a1, z2, a2, z3, output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        model['W3'] -= learning_rate * dW3\n",
    "        model['b3'] -= learning_rate * db3\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"three-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "\n",
    "        # Check for convergence\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n",
    "                print(np.abs((previous_loss - loss) / previous_loss), previous_loss, loss)\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:00:37.595523700Z",
     "start_time": "2023-10-26T04:00:37.559048700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-layer Loss after iteration 0: 1522.656689053422\n",
      "two-layer Loss after iteration 1000: 24.11791281948347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_32504\\3437914713.py:112: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-layer Loss after iteration 2000: 9.369516502494285\n",
      "two-layer Loss after iteration 3000: 7.965212996431529\n",
      "two-layer Loss after iteration 4000: 7.691576454560319\n",
      "two-layer Loss after iteration 5000: 7.286443122573906\n",
      "two-layer Loss after iteration 6000: 6.282086687075516\n",
      "two-layer Loss after iteration 7000: 5.937977844128537\n",
      "two-layer Loss after iteration 8000: 5.5619831601963385\n",
      "two-layer Loss after iteration 9000: 5.1451295226795155\n",
      "two-layer Loss after iteration 10000: 4.870613843309535\n",
      "two-layer Loss after iteration 11000: 4.687903007434956\n",
      "two-layer Loss after iteration 12000: 4.434173461507116\n",
      "two-layer Loss after iteration 13000: 3.778048399968018\n",
      "two-layer Loss after iteration 14000: 3.6688333515338045\n",
      "two-layer Loss after iteration 15000: 3.6424384790594213\n",
      "two-layer Loss after iteration 16000: 3.6333028663736067\n",
      "two-layer Loss after iteration 17000: 3.629236633593514\n",
      "two-layer Loss after iteration 18000: 3.6269431101102487\n",
      "two-layer Loss after iteration 19000: 3.62442734950004\n",
      "two-layer Loss after iteration 20000: 3.622324826800096\n",
      "two-layer Loss after iteration 21000: 3.620781185009317\n",
      "two-layer Loss after iteration 22000: 3.619812556973983\n",
      "two-layer Loss after iteration 23000: 3.618673512067705\n",
      "two-layer Loss after iteration 24000: 3.618127014966309\n",
      "two-layer Loss after iteration 25000: 3.61772324426115\n",
      "two-layer Loss after iteration 26000: 3.6174003731743847\n",
      "8.924703880478334e-05 3.61772324426115 3.6174003731743847\n",
      "two-layer Loss after iteration 0: 1577.2893820330282\n",
      "two-layer Loss after iteration 1000: 24.364335544205726\n",
      "two-layer Loss after iteration 2000: 9.759825132571427\n",
      "two-layer Loss after iteration 3000: 7.791814536174247\n",
      "two-layer Loss after iteration 4000: 6.924864421817193\n",
      "two-layer Loss after iteration 5000: 5.622826923028747\n",
      "two-layer Loss after iteration 6000: 4.8537561838764445\n",
      "two-layer Loss after iteration 7000: 4.540819619901784\n",
      "two-layer Loss after iteration 8000: 4.414733633510556\n",
      "two-layer Loss after iteration 9000: 4.328919099843561\n",
      "two-layer Loss after iteration 10000: 4.272470125707836\n",
      "two-layer Loss after iteration 11000: 4.2301783720838735\n",
      "two-layer Loss after iteration 12000: 4.195931837956435\n",
      "two-layer Loss after iteration 13000: 4.158635215022267\n",
      "two-layer Loss after iteration 14000: 4.136353811977476\n",
      "two-layer Loss after iteration 15000: 4.116897341936483\n",
      "two-layer Loss after iteration 16000: 4.103664173525523\n",
      "two-layer Loss after iteration 17000: 4.093841905321096\n",
      "two-layer Loss after iteration 18000: 4.086749746923175\n",
      "two-layer Loss after iteration 19000: 4.081476987295216\n",
      "two-layer Loss after iteration 20000: 4.075081257241654\n",
      "two-layer Loss after iteration 21000: 4.070557666284413\n",
      "two-layer Loss after iteration 22000: 4.067088986758138\n",
      "two-layer Loss after iteration 23000: 4.064309481995209\n",
      "two-layer Loss after iteration 24000: 4.062005769796828\n",
      "two-layer Loss after iteration 25000: 4.060025203493089\n",
      "two-layer Loss after iteration 26000: 4.058284229686808\n",
      "two-layer Loss after iteration 27000: 4.056730957102829\n",
      "two-layer Loss after iteration 28000: 4.055331616205555\n",
      "two-layer Loss after iteration 29000: 4.05406572462271\n",
      "two-layer Loss after iteration 30000: 4.052914051451889\n",
      "two-layer Loss after iteration 31000: 4.051857662064803\n",
      "two-layer Loss after iteration 32000: 4.050887303716069\n",
      "two-layer Loss after iteration 33000: 4.049995366905525\n",
      "two-layer Loss after iteration 34000: 4.049174449014434\n",
      "two-layer Loss after iteration 35000: 4.048416075595378\n",
      "two-layer Loss after iteration 36000: 4.047484965074827\n",
      "two-layer Loss after iteration 37000: 4.0418349940596\n",
      "two-layer Loss after iteration 38000: 4.040260335980477\n",
      "two-layer Loss after iteration 39000: 4.039011919058107\n",
      "two-layer Loss after iteration 40000: 4.037896465530066\n",
      "two-layer Loss after iteration 41000: 4.036891143736417\n",
      "two-layer Loss after iteration 42000: 4.035981162008638\n",
      "two-layer Loss after iteration 43000: 4.035154997598312\n",
      "two-layer Loss after iteration 44000: 4.034406667908482\n",
      "two-layer Loss after iteration 45000: 4.033719978293683\n",
      "two-layer Loss after iteration 46000: 4.033090755515102\n",
      "two-layer Loss after iteration 47000: 4.032515639830328\n",
      "two-layer Loss after iteration 48000: 4.031984357200569\n",
      "two-layer Loss after iteration 49000: 4.031499855606007\n",
      "two-layer Loss after iteration 50000: 4.031051748896035\n",
      "two-layer Loss after iteration 51000: 4.030639065334765\n",
      "two-layer Loss after iteration 52000: 4.0302617318691345\n",
      "9.361628752019611e-05 4.030639065334765 4.0302617318691345\n",
      "two-layer Loss after iteration 0: 1545.9214317146107\n",
      "two-layer Loss after iteration 1000: 24.540063249872627\n",
      "two-layer Loss after iteration 2000: 10.81721222370477\n",
      "two-layer Loss after iteration 3000: 8.558947250383996\n",
      "two-layer Loss after iteration 4000: 7.955362769227712\n",
      "two-layer Loss after iteration 5000: 7.532945099103912\n",
      "two-layer Loss after iteration 6000: 7.120104830495885\n",
      "two-layer Loss after iteration 7000: 6.606714769109516\n",
      "two-layer Loss after iteration 8000: 6.301430202326414\n",
      "two-layer Loss after iteration 9000: 6.129359728855461\n",
      "two-layer Loss after iteration 10000: 5.992238631173306\n",
      "two-layer Loss after iteration 11000: 5.8840143608792665\n",
      "two-layer Loss after iteration 12000: 5.812160330997546\n",
      "two-layer Loss after iteration 13000: 5.745033186365539\n",
      "two-layer Loss after iteration 14000: 5.682797434068703\n",
      "two-layer Loss after iteration 15000: 5.626475341304265\n",
      "two-layer Loss after iteration 16000: 5.561622347950054\n",
      "two-layer Loss after iteration 17000: 5.479649358286606\n",
      "two-layer Loss after iteration 18000: 5.397840703334313\n",
      "two-layer Loss after iteration 19000: 4.9190381206432425\n",
      "two-layer Loss after iteration 20000: 4.760972686799129\n",
      "two-layer Loss after iteration 21000: 4.68341068243797\n",
      "two-layer Loss after iteration 22000: 4.621149954076597\n",
      "two-layer Loss after iteration 23000: 4.589346338905372\n",
      "two-layer Loss after iteration 24000: 4.566629492284929\n",
      "two-layer Loss after iteration 25000: 4.548006798105493\n",
      "two-layer Loss after iteration 26000: 4.530333003525654\n",
      "two-layer Loss after iteration 27000: 4.507557060432813\n",
      "two-layer Loss after iteration 28000: 4.493963197743313\n",
      "two-layer Loss after iteration 29000: 4.482849842787727\n",
      "two-layer Loss after iteration 30000: 4.47274690451763\n",
      "two-layer Loss after iteration 31000: 4.463612520651023\n",
      "two-layer Loss after iteration 32000: 4.4216723202911465\n",
      "two-layer Loss after iteration 33000: 4.342189599206866\n",
      "two-layer Loss after iteration 34000: 4.150117650802612\n",
      "two-layer Loss after iteration 35000: 4.0568012013195265\n",
      "two-layer Loss after iteration 36000: 4.001667816153785\n",
      "two-layer Loss after iteration 37000: 3.9709956245979052\n",
      "two-layer Loss after iteration 38000: 3.9321547843001343\n",
      "two-layer Loss after iteration 39000: 3.9165074012483294\n",
      "two-layer Loss after iteration 40000: 3.9093891862861594\n",
      "two-layer Loss after iteration 41000: 3.9045197095911943\n",
      "two-layer Loss after iteration 42000: 3.900958532259672\n",
      "two-layer Loss after iteration 43000: 3.8983481443050607\n",
      "two-layer Loss after iteration 44000: 3.8965015688248816\n",
      "two-layer Loss after iteration 45000: 3.881109185721202\n",
      "two-layer Loss after iteration 46000: 3.8237802771290927\n",
      "two-layer Loss after iteration 47000: 3.679484651348799\n",
      "two-layer Loss after iteration 48000: 3.556630119180353\n",
      "two-layer Loss after iteration 49000: 3.5337772129835514\n",
      "two-layer Loss after iteration 50000: 3.4194035320819047\n",
      "two-layer Loss after iteration 51000: 3.1115827672882728\n",
      "two-layer Loss after iteration 52000: 2.974509973562121\n",
      "two-layer Loss after iteration 53000: 2.811040969846954\n",
      "two-layer Loss after iteration 54000: 2.7543496489853725\n",
      "two-layer Loss after iteration 55000: 2.708153102999388\n",
      "two-layer Loss after iteration 56000: 2.6872667036738265\n",
      "two-layer Loss after iteration 57000: 2.6751948345001466\n",
      "two-layer Loss after iteration 58000: 2.6640939353830326\n",
      "two-layer Loss after iteration 59000: 2.6535539120888814\n",
      "two-layer Loss after iteration 60000: 2.6432370466382706\n",
      "two-layer Loss after iteration 61000: 2.6330636630237017\n",
      "two-layer Loss after iteration 62000: 2.6148446314304956\n",
      "two-layer Loss after iteration 63000: 2.595523857751422\n",
      "two-layer Loss after iteration 64000: 2.5788337213633055\n",
      "two-layer Loss after iteration 65000: 2.55663970243123\n",
      "two-layer Loss after iteration 66000: 2.5230851676402715\n",
      "two-layer Loss after iteration 67000: 2.4914198307701856\n",
      "two-layer Loss after iteration 68000: 2.4618314358239455\n",
      "two-layer Loss after iteration 69000: 2.4383794886209453\n",
      "two-layer Loss after iteration 70000: 2.4202589479936747\n",
      "two-layer Loss after iteration 71000: 2.407074914552005\n",
      "two-layer Loss after iteration 72000: 2.3945510026768666\n",
      "two-layer Loss after iteration 73000: 2.3862058015710934\n",
      "two-layer Loss after iteration 74000: 2.380706040929406\n",
      "two-layer Loss after iteration 75000: 2.366046152290132\n",
      "two-layer Loss after iteration 76000: 2.3599827828220175\n",
      "two-layer Loss after iteration 77000: 2.3572586601854137\n",
      "two-layer Loss after iteration 78000: 2.3558882533972154\n",
      "two-layer Loss after iteration 79000: 2.3551277598592777\n",
      "two-layer Loss after iteration 80000: 2.3546888009813616\n",
      "two-layer Loss after iteration 81000: 2.354429073920237\n",
      "two-layer Loss after iteration 82000: 2.3542526157535666\n",
      "7.494732741162824e-05 2.354429073920237 2.3542526157535666\n",
      "two-layer Loss after iteration 0: 1428.5125385881329\n",
      "two-layer Loss after iteration 1000: 22.521960110788683\n",
      "two-layer Loss after iteration 2000: 9.030875879548276\n",
      "two-layer Loss after iteration 3000: 6.708819703099293\n",
      "two-layer Loss after iteration 4000: 5.297081397396582\n",
      "two-layer Loss after iteration 5000: 4.7420738397991675\n",
      "two-layer Loss after iteration 6000: 4.308965756251964\n",
      "two-layer Loss after iteration 7000: 3.9049901426409943\n",
      "two-layer Loss after iteration 8000: 3.6710067631856984\n",
      "two-layer Loss after iteration 9000: 3.437189856292018\n",
      "two-layer Loss after iteration 10000: 3.192305236536786\n",
      "two-layer Loss after iteration 11000: 3.0549014694418752\n",
      "two-layer Loss after iteration 12000: 2.9340539887586967\n",
      "two-layer Loss after iteration 13000: 2.8628179937507565\n",
      "two-layer Loss after iteration 14000: 2.8033885268875074\n",
      "two-layer Loss after iteration 15000: 2.742086599711355\n",
      "two-layer Loss after iteration 16000: 2.6861825622771973\n",
      "two-layer Loss after iteration 17000: 2.6513756036889506\n",
      "two-layer Loss after iteration 18000: 2.6274943024534414\n",
      "two-layer Loss after iteration 19000: 2.61123749812309\n",
      "two-layer Loss after iteration 20000: 2.599501088315308\n",
      "two-layer Loss after iteration 21000: 2.586776053474054\n",
      "two-layer Loss after iteration 22000: 2.575550933212231\n",
      "two-layer Loss after iteration 23000: 2.5647700486075196\n",
      "two-layer Loss after iteration 24000: 2.5584059336824074\n",
      "two-layer Loss after iteration 25000: 2.554025610887412\n",
      "two-layer Loss after iteration 26000: 2.5506733566865254\n",
      "two-layer Loss after iteration 27000: 2.5480123884680292\n",
      "two-layer Loss after iteration 28000: 2.545873568190402\n",
      "two-layer Loss after iteration 29000: 2.5441249319725947\n",
      "two-layer Loss after iteration 30000: 2.542684544901275\n",
      "two-layer Loss after iteration 31000: 2.541483404188925\n",
      "two-layer Loss after iteration 32000: 2.539898931764222\n",
      "two-layer Loss after iteration 33000: 2.5373882397354\n",
      "two-layer Loss after iteration 34000: 2.535189965252169\n",
      "two-layer Loss after iteration 35000: 2.5336774416465473\n",
      "two-layer Loss after iteration 36000: 2.5324821999838725\n",
      "two-layer Loss after iteration 37000: 2.531522052037274\n",
      "two-layer Loss after iteration 38000: 2.530716589682209\n",
      "two-layer Loss after iteration 39000: 2.5300632424605607\n",
      "two-layer Loss after iteration 40000: 2.5295248553115584\n",
      "two-layer Loss after iteration 41000: 2.52906082763975\n",
      "two-layer Loss after iteration 42000: 2.5286763787096036\n",
      "two-layer Loss after iteration 43000: 2.5283495310926156\n",
      "two-layer Loss after iteration 44000: 2.5280750228470756\n",
      "two-layer Loss after iteration 45000: 2.527842325041258\n",
      "9.204545107032579e-05 2.5280750228470756 2.527842325041258\n",
      "two-layer Loss after iteration 0: 1638.5846690993005\n",
      "two-layer Loss after iteration 1000: 24.27675439355667\n",
      "two-layer Loss after iteration 2000: 9.056086926911805\n",
      "two-layer Loss after iteration 3000: 7.022446960977355\n",
      "two-layer Loss after iteration 4000: 5.771489707852867\n",
      "two-layer Loss after iteration 5000: 5.230511159121573\n",
      "two-layer Loss after iteration 6000: 4.595320308735568\n",
      "two-layer Loss after iteration 7000: 4.174729768899514\n",
      "two-layer Loss after iteration 8000: 3.9042555597508564\n",
      "two-layer Loss after iteration 9000: 3.75358696028262\n",
      "two-layer Loss after iteration 10000: 3.6383205199732918\n",
      "two-layer Loss after iteration 11000: 3.188852765989557\n",
      "two-layer Loss after iteration 12000: 2.9334073602945234\n",
      "two-layer Loss after iteration 13000: 2.772520182868584\n",
      "two-layer Loss after iteration 14000: 2.668765447792376\n",
      "two-layer Loss after iteration 15000: 2.593940957519421\n",
      "two-layer Loss after iteration 16000: 2.543894084140278\n",
      "two-layer Loss after iteration 17000: 2.5014749406764927\n",
      "two-layer Loss after iteration 18000: 2.465430970596639\n",
      "two-layer Loss after iteration 19000: 2.4174098396360395\n",
      "two-layer Loss after iteration 20000: 2.377481180051258\n",
      "two-layer Loss after iteration 21000: 2.3493878420822276\n",
      "two-layer Loss after iteration 22000: 2.3257338604173516\n",
      "two-layer Loss after iteration 23000: 2.3118542668224578\n",
      "two-layer Loss after iteration 24000: 2.302864684596597\n",
      "two-layer Loss after iteration 25000: 2.291351581483513\n",
      "two-layer Loss after iteration 26000: 2.284783608933855\n",
      "two-layer Loss after iteration 27000: 2.275919671461586\n",
      "two-layer Loss after iteration 28000: 2.2712138290706747\n",
      "two-layer Loss after iteration 29000: 2.2678812952142153\n",
      "two-layer Loss after iteration 30000: 2.2652868162651667\n",
      "two-layer Loss after iteration 31000: 2.263207407872458\n",
      "two-layer Loss after iteration 32000: 2.261512290075538\n",
      "two-layer Loss after iteration 33000: 2.260113820116478\n",
      "two-layer Loss after iteration 34000: 2.2589772278136064\n",
      "two-layer Loss after iteration 35000: 2.256118671759285\n",
      "two-layer Loss after iteration 36000: 2.2545311353144006\n",
      "two-layer Loss after iteration 37000: 2.253171509322693\n",
      "two-layer Loss after iteration 38000: 2.2519998331687225\n",
      "two-layer Loss after iteration 39000: 2.250987661807919\n",
      "two-layer Loss after iteration 40000: 2.250098872014408\n",
      "two-layer Loss after iteration 41000: 2.2493052130257167\n",
      "two-layer Loss after iteration 42000: 2.248588564132625\n",
      "two-layer Loss after iteration 43000: 2.2479346690993456\n",
      "two-layer Loss after iteration 44000: 2.2473324399966894\n",
      "two-layer Loss after iteration 45000: 2.246771518178741\n",
      "two-layer Loss after iteration 46000: 2.246246201991846\n",
      "two-layer Loss after iteration 47000: 2.245752742068154\n",
      "two-layer Loss after iteration 48000: 2.2453030424496943\n",
      "two-layer Loss after iteration 49000: 2.2449177152322015\n",
      "two-layer Loss after iteration 50000: 2.244556438285153\n",
      "two-layer Loss after iteration 51000: 2.2442170941364274\n",
      "two-layer Loss after iteration 52000: 2.2438979651692086\n",
      "two-layer Loss after iteration 53000: 2.243597575708754\n",
      "two-layer Loss after iteration 54000: 2.2433146267426687\n",
      "two-layer Loss after iteration 55000: 2.2430479574270277\n",
      "two-layer Loss after iteration 56000: 2.242801657281061\n",
      "two-layer Loss after iteration 57000: 2.2425777626677728\n",
      "9.982809338559558e-05 2.242801657281061 2.2425777626677728\n",
      "two-layer Loss after iteration 0: 1534.9339875972958\n",
      "two-layer Loss after iteration 1000: 23.530589029365267\n",
      "two-layer Loss after iteration 2000: 9.476831792807472\n",
      "two-layer Loss after iteration 3000: 7.402598262945753\n",
      "two-layer Loss after iteration 4000: 6.60672202257266\n",
      "two-layer Loss after iteration 5000: 5.505034489523767\n",
      "two-layer Loss after iteration 6000: 5.0086789325241785\n",
      "two-layer Loss after iteration 7000: 4.753668774274533\n",
      "two-layer Loss after iteration 8000: 4.5370071630763915\n",
      "two-layer Loss after iteration 9000: 4.3617360950217385\n",
      "two-layer Loss after iteration 10000: 4.220591459002712\n",
      "two-layer Loss after iteration 11000: 4.0938669091343165\n",
      "two-layer Loss after iteration 12000: 4.000200712687656\n",
      "two-layer Loss after iteration 13000: 3.9107756786689\n",
      "two-layer Loss after iteration 14000: 3.8516159713211082\n",
      "two-layer Loss after iteration 15000: 3.8003661105114124\n",
      "two-layer Loss after iteration 16000: 3.7240526728078245\n",
      "two-layer Loss after iteration 17000: 3.6379382181612234\n",
      "two-layer Loss after iteration 18000: 3.4548855346960994\n",
      "two-layer Loss after iteration 19000: 3.338490591179515\n",
      "two-layer Loss after iteration 20000: 3.3068885427269934\n",
      "two-layer Loss after iteration 21000: 3.2928259692831174\n",
      "two-layer Loss after iteration 22000: 3.2821151413689322\n",
      "two-layer Loss after iteration 23000: 3.2723411421464452\n",
      "two-layer Loss after iteration 24000: 3.265491544161378\n",
      "two-layer Loss after iteration 25000: 3.2543122802823636\n",
      "two-layer Loss after iteration 26000: 3.2455183889677564\n",
      "two-layer Loss after iteration 27000: 3.2382503460610113\n",
      "two-layer Loss after iteration 28000: 3.2317901099253143\n",
      "two-layer Loss after iteration 29000: 3.2292349601258743\n",
      "two-layer Loss after iteration 30000: 3.2275499968833086\n",
      "two-layer Loss after iteration 31000: 3.226356203221676\n",
      "two-layer Loss after iteration 32000: 3.2256196840632034\n",
      "two-layer Loss after iteration 33000: 3.224994551744753\n",
      "two-layer Loss after iteration 34000: 3.224457773135528\n",
      "two-layer Loss after iteration 35000: 3.223979678194994\n",
      "two-layer Loss after iteration 36000: 3.223539063428866\n",
      "two-layer Loss after iteration 37000: 3.2231292356940333\n",
      "two-layer Loss after iteration 38000: 3.2227427578034855\n",
      "two-layer Loss after iteration 39000: 3.2223801525555973\n",
      "two-layer Loss after iteration 40000: 3.22203310401332\n",
      "two-layer Loss after iteration 41000: 3.2217036592916957\n",
      "two-layer Loss after iteration 42000: 3.2213845196611124\n",
      "9.90592755677101e-05 3.2217036592916957 3.2213845196611124\n",
      "two-layer Loss after iteration 0: 1678.4620353755117\n",
      "two-layer Loss after iteration 1000: 22.938231094901777\n",
      "two-layer Loss after iteration 2000: 9.197671214011834\n",
      "two-layer Loss after iteration 3000: 7.777624277784186\n",
      "two-layer Loss after iteration 4000: 6.883945998305437\n",
      "two-layer Loss after iteration 5000: 5.899948926587202\n",
      "two-layer Loss after iteration 6000: 5.438114941363957\n",
      "two-layer Loss after iteration 7000: 5.093746016579951\n",
      "two-layer Loss after iteration 8000: 4.8494616254949525\n",
      "two-layer Loss after iteration 9000: 4.671012456389524\n",
      "two-layer Loss after iteration 10000: 4.537949686659832\n",
      "two-layer Loss after iteration 11000: 4.429315059070375\n",
      "two-layer Loss after iteration 12000: 4.338531321072846\n",
      "two-layer Loss after iteration 13000: 4.263850219046389\n",
      "two-layer Loss after iteration 14000: 4.1991536479008635\n",
      "two-layer Loss after iteration 15000: 4.141789970343489\n",
      "two-layer Loss after iteration 16000: 4.090334550716033\n",
      "two-layer Loss after iteration 17000: 4.043182724046582\n",
      "two-layer Loss after iteration 18000: 3.9727571780635613\n",
      "two-layer Loss after iteration 19000: 3.9304949293377325\n",
      "two-layer Loss after iteration 20000: 3.8928028624246114\n",
      "two-layer Loss after iteration 21000: 3.8586386322115223\n",
      "two-layer Loss after iteration 22000: 3.823239360110786\n",
      "two-layer Loss after iteration 23000: 3.792078821940541\n",
      "two-layer Loss after iteration 24000: 3.7653727118655778\n",
      "two-layer Loss after iteration 25000: 3.7422744985742047\n",
      "two-layer Loss after iteration 26000: 3.722194480801686\n",
      "two-layer Loss after iteration 27000: 3.7050228861919186\n",
      "two-layer Loss after iteration 28000: 3.689738539731739\n",
      "two-layer Loss after iteration 29000: 3.672105736230673\n",
      "two-layer Loss after iteration 30000: 3.659130432364985\n",
      "two-layer Loss after iteration 31000: 3.635267676311112\n",
      "two-layer Loss after iteration 32000: 3.620604070785463\n",
      "two-layer Loss after iteration 33000: 3.6097224737107916\n",
      "two-layer Loss after iteration 34000: 3.600953595667778\n",
      "two-layer Loss after iteration 35000: 3.5937442950625074\n",
      "two-layer Loss after iteration 36000: 3.5866356561647494\n",
      "two-layer Loss after iteration 37000: 3.5804241369512946\n",
      "two-layer Loss after iteration 38000: 3.5754727087641167\n",
      "two-layer Loss after iteration 39000: 3.571469336616636\n",
      "two-layer Loss after iteration 40000: 3.5682303201647523\n",
      "two-layer Loss after iteration 41000: 3.5655646147579096\n",
      "two-layer Loss after iteration 42000: 3.563498071645365\n",
      "two-layer Loss after iteration 43000: 3.5618082448824597\n",
      "two-layer Loss after iteration 44000: 3.5604434706557533\n",
      "two-layer Loss after iteration 45000: 3.559325026424168\n",
      "two-layer Loss after iteration 46000: 3.5178846240386887\n",
      "two-layer Loss after iteration 47000: 3.488391080430368\n",
      "two-layer Loss after iteration 48000: 3.4759272175076332\n",
      "two-layer Loss after iteration 49000: 3.4653810107949146\n",
      "two-layer Loss after iteration 50000: 3.462614845128612\n",
      "two-layer Loss after iteration 51000: 3.461086689498406\n",
      "two-layer Loss after iteration 52000: 3.460039314538067\n",
      "two-layer Loss after iteration 53000: 3.4592729475365913\n",
      "two-layer Loss after iteration 54000: 3.4586794424796325\n",
      "two-layer Loss after iteration 55000: 3.4582113773818257\n",
      "two-layer Loss after iteration 56000: 3.4578367930208183\n",
      "two-layer Loss after iteration 57000: 3.4575327594329943\n",
      "8.792595082497494e-05 3.4578367930208183 3.4575327594329943\n",
      "two-layer Loss after iteration 0: 1614.6859925110648\n",
      "two-layer Loss after iteration 1000: 24.058798156151187\n",
      "two-layer Loss after iteration 2000: 10.198188453102352\n",
      "two-layer Loss after iteration 3000: 7.93227342799577\n",
      "two-layer Loss after iteration 4000: 6.446647258457003\n",
      "two-layer Loss after iteration 5000: 6.009526969664132\n",
      "two-layer Loss after iteration 6000: 5.662120705049468\n",
      "two-layer Loss after iteration 7000: 5.370168641398035\n",
      "two-layer Loss after iteration 8000: 4.947181732336004\n",
      "two-layer Loss after iteration 9000: 4.413232666373686\n",
      "two-layer Loss after iteration 10000: 4.2805911598609185\n",
      "two-layer Loss after iteration 11000: 4.169065693549631\n",
      "two-layer Loss after iteration 12000: 3.878374923441564\n",
      "two-layer Loss after iteration 13000: 3.4852494714295417\n",
      "two-layer Loss after iteration 14000: 3.4504166146792783\n",
      "two-layer Loss after iteration 15000: 3.4395045924790235\n",
      "two-layer Loss after iteration 16000: 3.423332724704081\n",
      "two-layer Loss after iteration 17000: 3.4154694840827284\n",
      "two-layer Loss after iteration 18000: 3.4090117778990363\n",
      "two-layer Loss after iteration 19000: 3.4026234939475986\n",
      "two-layer Loss after iteration 20000: 3.3925157303833173\n",
      "two-layer Loss after iteration 21000: 3.3852276470354434\n",
      "two-layer Loss after iteration 22000: 3.3789864300818784\n",
      "two-layer Loss after iteration 23000: 3.3743150633886034\n",
      "two-layer Loss after iteration 24000: 3.37124754787141\n",
      "two-layer Loss after iteration 25000: 3.3689653476929875\n",
      "two-layer Loss after iteration 26000: 3.367175438086521\n",
      "two-layer Loss after iteration 27000: 3.365707305678386\n",
      "two-layer Loss after iteration 28000: 3.3645044597593223\n",
      "two-layer Loss after iteration 29000: 3.363471854391927\n",
      "two-layer Loss after iteration 30000: 3.3626307320295794\n",
      "two-layer Loss after iteration 31000: 3.3458972768367596\n",
      "two-layer Loss after iteration 32000: 3.34088075965575\n",
      "two-layer Loss after iteration 33000: 3.335476169541703\n",
      "two-layer Loss after iteration 34000: 3.331794490869087\n",
      "two-layer Loss after iteration 35000: 3.328728744793203\n",
      "two-layer Loss after iteration 36000: 3.326852259371004\n",
      "two-layer Loss after iteration 37000: 3.3206212537874733\n",
      "two-layer Loss after iteration 38000: 3.3179325956460475\n",
      "two-layer Loss after iteration 39000: 3.3163909763813324\n",
      "two-layer Loss after iteration 40000: 3.3153417957307627\n",
      "two-layer Loss after iteration 41000: 3.3146050010376693\n",
      "two-layer Loss after iteration 42000: 3.3136402703706835\n",
      "two-layer Loss after iteration 43000: 3.31210595422914\n",
      "two-layer Loss after iteration 44000: 3.3113498800220316\n",
      "two-layer Loss after iteration 45000: 3.3107828240834443\n",
      "two-layer Loss after iteration 46000: 3.3103362459369845\n",
      "two-layer Loss after iteration 47000: 3.3099730849869893\n",
      "two-layer Loss after iteration 48000: 3.3096772983501146\n",
      "8.93622483567255e-05 3.3099730849869893 3.3096772983501146\n",
      "two-layer Loss after iteration 0: 1636.7802041957361\n",
      "two-layer Loss after iteration 1000: 24.23798960355505\n",
      "two-layer Loss after iteration 2000: 10.150725745858287\n",
      "two-layer Loss after iteration 3000: 8.633584577679626\n",
      "two-layer Loss after iteration 4000: 7.857129773529025\n",
      "two-layer Loss after iteration 5000: 7.085052454656341\n",
      "two-layer Loss after iteration 6000: 5.839616832218515\n",
      "two-layer Loss after iteration 7000: 5.616031977220583\n",
      "two-layer Loss after iteration 8000: 5.536040772589186\n",
      "two-layer Loss after iteration 9000: 5.480641374022855\n",
      "two-layer Loss after iteration 10000: 5.442916840364313\n",
      "two-layer Loss after iteration 11000: 5.391833825369827\n",
      "two-layer Loss after iteration 12000: 5.358770111089725\n",
      "two-layer Loss after iteration 13000: 5.3169275711041175\n",
      "two-layer Loss after iteration 14000: 5.287484975736289\n",
      "two-layer Loss after iteration 15000: 5.258206332648057\n",
      "two-layer Loss after iteration 16000: 5.23899284033176\n",
      "two-layer Loss after iteration 17000: 5.194579478570175\n",
      "two-layer Loss after iteration 18000: 5.170305601826901\n",
      "two-layer Loss after iteration 19000: 5.148312271118654\n",
      "two-layer Loss after iteration 20000: 5.127785088851582\n",
      "two-layer Loss after iteration 21000: 5.114042178261927\n",
      "two-layer Loss after iteration 22000: 5.102228864034303\n",
      "two-layer Loss after iteration 23000: 5.08789358024162\n",
      "two-layer Loss after iteration 24000: 5.077934709018197\n",
      "two-layer Loss after iteration 25000: 5.069411941490343\n",
      "two-layer Loss after iteration 26000: 5.06198011190865\n",
      "two-layer Loss after iteration 27000: 5.0554443684333545\n",
      "two-layer Loss after iteration 28000: 5.049694254256145\n",
      "two-layer Loss after iteration 29000: 5.044618381501009\n",
      "two-layer Loss after iteration 30000: 5.0401399979590416\n",
      "two-layer Loss after iteration 31000: 5.036172192591621\n",
      "two-layer Loss after iteration 32000: 5.0326583463472705\n",
      "two-layer Loss after iteration 33000: 5.0295511013846435\n",
      "two-layer Loss after iteration 34000: 5.026786851110577\n",
      "two-layer Loss after iteration 35000: 5.020503540639981\n",
      "two-layer Loss after iteration 36000: 5.0156071444984045\n",
      "two-layer Loss after iteration 37000: 5.012454480772101\n",
      "two-layer Loss after iteration 38000: 5.009787199890201\n",
      "two-layer Loss after iteration 39000: 5.007504481339654\n",
      "two-layer Loss after iteration 40000: 5.005525895737597\n",
      "two-layer Loss after iteration 41000: 5.0038187774733585\n",
      "two-layer Loss after iteration 42000: 5.00231678124837\n",
      "two-layer Loss after iteration 43000: 5.001006445242786\n",
      "two-layer Loss after iteration 44000: 4.804416968746219\n",
      "two-layer Loss after iteration 45000: 4.610273924982844\n",
      "two-layer Loss after iteration 46000: 4.427399664989948\n",
      "two-layer Loss after iteration 47000: 4.38210543965991\n",
      "two-layer Loss after iteration 48000: 4.354076955331287\n",
      "two-layer Loss after iteration 49000: 4.322798068586942\n",
      "two-layer Loss after iteration 50000: 4.245017482083906\n",
      "two-layer Loss after iteration 51000: 4.066089231041436\n",
      "two-layer Loss after iteration 52000: 3.9864620538760454\n",
      "two-layer Loss after iteration 53000: 3.9222661291486896\n",
      "two-layer Loss after iteration 54000: 3.8836077360406747\n",
      "two-layer Loss after iteration 55000: 3.8554614339165982\n",
      "two-layer Loss after iteration 56000: 3.8325750413026687\n",
      "two-layer Loss after iteration 57000: 3.813829548116852\n",
      "two-layer Loss after iteration 58000: 3.7984195642955787\n",
      "two-layer Loss after iteration 59000: 3.785714396254874\n",
      "two-layer Loss after iteration 60000: 3.775113307539007\n",
      "two-layer Loss after iteration 61000: 3.766322078245732\n",
      "two-layer Loss after iteration 62000: 3.7590267726245274\n",
      "two-layer Loss after iteration 63000: 3.753114347302692\n",
      "two-layer Loss after iteration 64000: 3.748291766492896\n",
      "two-layer Loss after iteration 65000: 3.7442735238383973\n",
      "two-layer Loss after iteration 66000: 3.741018840094699\n",
      "two-layer Loss after iteration 67000: 3.738448218848328\n",
      "two-layer Loss after iteration 68000: 3.7363246587521326\n",
      "two-layer Loss after iteration 69000: 3.7345448942355\n",
      "two-layer Loss after iteration 70000: 3.7330532281655158\n",
      "two-layer Loss after iteration 71000: 3.7318122343801003\n",
      "two-layer Loss after iteration 72000: 3.730762063862464\n",
      "two-layer Loss after iteration 73000: 3.729871570280678\n",
      "two-layer Loss after iteration 74000: 3.7290848483229033\n",
      "two-layer Loss after iteration 75000: 3.728290277343572\n",
      "two-layer Loss after iteration 76000: 3.7276025642159367\n",
      "two-layer Loss after iteration 77000: 3.726999864703379\n",
      "two-layer Loss after iteration 78000: 3.72647751504203\n",
      "two-layer Loss after iteration 79000: 3.7260384482209887\n",
      "two-layer Loss after iteration 80000: 3.725686206897967\n",
      "9.45350746957796e-05 3.7260384482209887 3.725686206897967\n",
      "two-layer Loss after iteration 0: 1303.8154762186255\n",
      "two-layer Loss after iteration 1000: 23.67000964789745\n",
      "two-layer Loss after iteration 2000: 10.050477100460476\n",
      "two-layer Loss after iteration 3000: 7.700548418336444\n",
      "two-layer Loss after iteration 4000: 6.408621918951521\n",
      "two-layer Loss after iteration 5000: 5.56317697316024\n",
      "two-layer Loss after iteration 6000: 5.195005991495588\n",
      "two-layer Loss after iteration 7000: 4.982968212674477\n",
      "two-layer Loss after iteration 8000: 4.831521357207744\n",
      "two-layer Loss after iteration 9000: 4.662867450498827\n",
      "two-layer Loss after iteration 10000: 4.5156149529206315\n",
      "two-layer Loss after iteration 11000: 4.415813183411106\n",
      "two-layer Loss after iteration 12000: 4.341905997751454\n",
      "two-layer Loss after iteration 13000: 4.158802864021128\n",
      "two-layer Loss after iteration 14000: 3.8373951380092137\n",
      "two-layer Loss after iteration 15000: 3.744229731464764\n",
      "two-layer Loss after iteration 16000: 3.6940916106524724\n",
      "two-layer Loss after iteration 17000: 3.6613706491793354\n",
      "two-layer Loss after iteration 18000: 3.6374827617062846\n",
      "two-layer Loss after iteration 19000: 3.6189086262106844\n",
      "two-layer Loss after iteration 20000: 3.6028082175506304\n",
      "two-layer Loss after iteration 21000: 3.5882604176168655\n",
      "two-layer Loss after iteration 22000: 3.5768106610642545\n",
      "two-layer Loss after iteration 23000: 3.567171241822695\n",
      "two-layer Loss after iteration 24000: 3.5588756444264185\n",
      "two-layer Loss after iteration 25000: 3.551616592982946\n",
      "two-layer Loss after iteration 26000: 3.54519791792821\n",
      "two-layer Loss after iteration 27000: 3.5394997576944274\n",
      "two-layer Loss after iteration 28000: 3.524673630611189\n",
      "two-layer Loss after iteration 29000: 3.51658884820124\n",
      "two-layer Loss after iteration 30000: 3.509878577722894\n",
      "two-layer Loss after iteration 31000: 3.5039496323513797\n",
      "two-layer Loss after iteration 32000: 3.497760125555536\n",
      "two-layer Loss after iteration 33000: 3.489925647278423\n",
      "two-layer Loss after iteration 34000: 3.483431771083666\n",
      "two-layer Loss after iteration 35000: 3.4567641198596792\n",
      "two-layer Loss after iteration 36000: 3.4313055487570394\n",
      "two-layer Loss after iteration 37000: 3.419040907906442\n",
      "two-layer Loss after iteration 38000: 3.407507060884885\n",
      "two-layer Loss after iteration 39000: 3.3965336481359527\n",
      "two-layer Loss after iteration 40000: 3.387484857071925\n",
      "two-layer Loss after iteration 41000: 3.380218870669334\n",
      "two-layer Loss after iteration 42000: 3.3741941997513534\n",
      "two-layer Loss after iteration 43000: 3.369093136850618\n",
      "two-layer Loss after iteration 44000: 3.3646042999611794\n",
      "two-layer Loss after iteration 45000: 3.3605940090164106\n",
      "two-layer Loss after iteration 46000: 3.357007213877091\n",
      "two-layer Loss after iteration 47000: 3.3537958491996265\n",
      "two-layer Loss after iteration 48000: 3.350917447906302\n",
      "two-layer Loss after iteration 49000: 3.3444950606276094\n",
      "two-layer Loss after iteration 50000: 3.3361285375835026\n",
      "two-layer Loss after iteration 51000: 3.3295169478521074\n",
      "two-layer Loss after iteration 52000: 3.3241300354357275\n",
      "two-layer Loss after iteration 53000: 3.3195564438248737\n",
      "two-layer Loss after iteration 54000: 3.3156602063138143\n",
      "two-layer Loss after iteration 55000: 3.312402522832025\n",
      "two-layer Loss after iteration 56000: 3.309648621928237\n",
      "two-layer Loss after iteration 57000: 3.3072798435896544\n",
      "two-layer Loss after iteration 58000: 3.2793976539641263\n",
      "two-layer Loss after iteration 59000: 2.9584319832943433\n",
      "two-layer Loss after iteration 60000: 2.6589118500205267\n",
      "two-layer Loss after iteration 61000: 2.606981764888163\n",
      "two-layer Loss after iteration 62000: 2.576868946760553\n",
      "two-layer Loss after iteration 63000: 2.5531509137837802\n",
      "two-layer Loss after iteration 64000: 2.5337109510906557\n",
      "two-layer Loss after iteration 65000: 2.517789296312607\n",
      "two-layer Loss after iteration 66000: 2.5055114720416602\n",
      "two-layer Loss after iteration 67000: 2.4958655716341447\n",
      "two-layer Loss after iteration 68000: 2.4881041163414355\n",
      "two-layer Loss after iteration 69000: 2.482007813462115\n",
      "two-layer Loss after iteration 70000: 2.4771282577123714\n",
      "two-layer Loss after iteration 71000: 2.473205850967379\n",
      "two-layer Loss after iteration 72000: 2.469050009165337\n",
      "two-layer Loss after iteration 73000: 2.4632552727767263\n",
      "two-layer Loss after iteration 74000: 2.458689554438399\n",
      "two-layer Loss after iteration 75000: 2.4551021389791288\n",
      "two-layer Loss after iteration 76000: 2.4525695334217876\n",
      "two-layer Loss after iteration 77000: 2.4507403513790043\n",
      "two-layer Loss after iteration 78000: 2.449340454624996\n",
      "two-layer Loss after iteration 79000: 2.448168894843532\n",
      "two-layer Loss after iteration 80000: 2.4471802771013014\n",
      "two-layer Loss after iteration 81000: 2.4463470944276806\n",
      "two-layer Loss after iteration 82000: 2.445642615210941\n",
      "two-layer Loss after iteration 83000: 2.4450902723560017\n",
      "two-layer Loss after iteration 84000: 2.444650211638492\n",
      "two-layer Loss after iteration 85000: 2.444275100819063\n",
      "two-layer Loss after iteration 86000: 2.443947006036339\n",
      "two-layer Loss after iteration 87000: 2.4436552687031217\n",
      "two-layer Loss after iteration 88000: 2.4433956600724245\n",
      "two-layer Loss after iteration 89000: 2.4431653297436964\n",
      "9.426648843326255e-05 2.4433956600724245 2.4431653297436964\n",
      "two-layer Loss after iteration 0: 1220.2698411824476\n",
      "two-layer Loss after iteration 1000: 24.843509746088092\n",
      "two-layer Loss after iteration 2000: 9.523523062713107\n",
      "two-layer Loss after iteration 3000: 7.244309243917214\n",
      "two-layer Loss after iteration 4000: 6.734248797276495\n",
      "two-layer Loss after iteration 5000: 6.469258108046167\n",
      "two-layer Loss after iteration 6000: 6.288931695718254\n",
      "two-layer Loss after iteration 7000: 6.159723415773993\n",
      "two-layer Loss after iteration 8000: 6.037202888269331\n",
      "two-layer Loss after iteration 9000: 5.9383670256312\n",
      "two-layer Loss after iteration 10000: 5.85032309840179\n",
      "two-layer Loss after iteration 11000: 5.77411734689008\n",
      "two-layer Loss after iteration 12000: 5.679268193312178\n",
      "two-layer Loss after iteration 13000: 5.551173541355596\n",
      "two-layer Loss after iteration 14000: 5.2475751690062316\n",
      "two-layer Loss after iteration 15000: 5.164795690858251\n",
      "two-layer Loss after iteration 16000: 5.1152673431003866\n",
      "two-layer Loss after iteration 17000: 5.072699083847955\n",
      "two-layer Loss after iteration 18000: 5.02806141741357\n",
      "two-layer Loss after iteration 19000: 4.988132844786464\n",
      "two-layer Loss after iteration 20000: 4.951591611701412\n",
      "two-layer Loss after iteration 21000: 4.91155213850578\n",
      "two-layer Loss after iteration 22000: 4.877491267249407\n",
      "two-layer Loss after iteration 23000: 4.844679856314667\n",
      "two-layer Loss after iteration 24000: 4.7465576377367205\n",
      "two-layer Loss after iteration 25000: 4.61304201336447\n",
      "two-layer Loss after iteration 26000: 4.565468541996144\n",
      "two-layer Loss after iteration 27000: 4.485076441681393\n",
      "two-layer Loss after iteration 28000: 4.329954749476185\n",
      "two-layer Loss after iteration 29000: 4.229689869430522\n",
      "two-layer Loss after iteration 30000: 4.169131736988108\n",
      "two-layer Loss after iteration 31000: 4.1071636857436635\n",
      "two-layer Loss after iteration 32000: 4.061966684181327\n",
      "two-layer Loss after iteration 33000: 4.024874894140781\n",
      "two-layer Loss after iteration 34000: 3.9899814134320826\n",
      "two-layer Loss after iteration 35000: 3.95941797082877\n",
      "two-layer Loss after iteration 36000: 3.926854434223478\n",
      "two-layer Loss after iteration 37000: 3.8968377667386\n",
      "two-layer Loss after iteration 38000: 3.8738890296254724\n",
      "two-layer Loss after iteration 39000: 3.8462593053013894\n",
      "two-layer Loss after iteration 40000: 3.8158159036694843\n",
      "two-layer Loss after iteration 41000: 3.7878347217415618\n",
      "two-layer Loss after iteration 42000: 3.7683763215223434\n",
      "two-layer Loss after iteration 43000: 3.7547911688353994\n",
      "two-layer Loss after iteration 44000: 3.743258313136526\n",
      "two-layer Loss after iteration 45000: 3.720712526508761\n",
      "two-layer Loss after iteration 46000: 3.7056196539283475\n",
      "two-layer Loss after iteration 47000: 3.694146012244186\n",
      "two-layer Loss after iteration 48000: 3.6595617625258625\n",
      "two-layer Loss after iteration 49000: 3.6238784384358294\n",
      "two-layer Loss after iteration 50000: 3.5997569593153984\n",
      "two-layer Loss after iteration 51000: 3.581737163573954\n",
      "two-layer Loss after iteration 52000: 3.5676168776336623\n",
      "two-layer Loss after iteration 53000: 3.555239631727786\n",
      "two-layer Loss after iteration 54000: 3.5275648957556243\n",
      "two-layer Loss after iteration 55000: 3.4985825042631324\n",
      "two-layer Loss after iteration 56000: 3.44514144894851\n",
      "two-layer Loss after iteration 57000: 3.4042939690735343\n",
      "two-layer Loss after iteration 58000: 3.371619379981982\n",
      "two-layer Loss after iteration 59000: 3.3393057989488546\n",
      "two-layer Loss after iteration 60000: 3.305230677474199\n",
      "two-layer Loss after iteration 61000: 3.2715128979984627\n",
      "two-layer Loss after iteration 62000: 3.241634923123709\n",
      "two-layer Loss after iteration 63000: 3.217027655936492\n",
      "two-layer Loss after iteration 64000: 2.837928831886205\n",
      "two-layer Loss after iteration 65000: 2.6730155996777807\n",
      "two-layer Loss after iteration 66000: 2.607295801101902\n",
      "two-layer Loss after iteration 67000: 2.5716649909750453\n",
      "two-layer Loss after iteration 68000: 2.5459618562154667\n",
      "two-layer Loss after iteration 69000: 2.5259066986518746\n",
      "two-layer Loss after iteration 70000: 2.5096236819734785\n",
      "two-layer Loss after iteration 71000: 2.4959960555151492\n",
      "two-layer Loss after iteration 72000: 2.4823543218918283\n",
      "two-layer Loss after iteration 73000: 2.454856186556327\n",
      "two-layer Loss after iteration 74000: 2.4393149937057044\n",
      "two-layer Loss after iteration 75000: 2.427116293135026\n",
      "two-layer Loss after iteration 76000: 2.417008207376493\n",
      "two-layer Loss after iteration 77000: 2.4091164574026505\n",
      "two-layer Loss after iteration 78000: 2.4025308385843647\n",
      "two-layer Loss after iteration 79000: 2.396862132986363\n",
      "two-layer Loss after iteration 80000: 2.3921013917540486\n",
      "two-layer Loss after iteration 81000: 2.3880471596188975\n",
      "two-layer Loss after iteration 82000: 2.3847758486298134\n",
      "two-layer Loss after iteration 83000: 2.3820945193614795\n",
      "two-layer Loss after iteration 84000: 2.3798995107556604\n",
      "two-layer Loss after iteration 85000: 2.3779655753425026\n",
      "two-layer Loss after iteration 86000: 2.376323185881248\n",
      "two-layer Loss after iteration 87000: 2.3748968497375893\n",
      "two-layer Loss after iteration 88000: 2.3652498160644315\n",
      "two-layer Loss after iteration 89000: 2.3625122845354016\n",
      "two-layer Loss after iteration 90000: 2.3603928737832147\n",
      "two-layer Loss after iteration 91000: 2.358637934683694\n",
      "two-layer Loss after iteration 92000: 2.3571758122604423\n",
      "two-layer Loss after iteration 93000: 2.3559603188187963\n",
      "two-layer Loss after iteration 94000: 2.3549433590160804\n",
      "two-layer Loss after iteration 95000: 2.3540866613442146\n",
      "two-layer Loss after iteration 96000: 2.353363600718226\n",
      "two-layer Loss after iteration 97000: 2.342046954605008\n",
      "two-layer Loss after iteration 98000: 2.340058315530603\n",
      "two-layer Loss after iteration 99000: 2.3393728189569836\n",
      "two-layer Loss after iteration 0: 1810.8962494577731\n",
      "two-layer Loss after iteration 1000: 27.471491636079357\n",
      "two-layer Loss after iteration 2000: 10.57941762940559\n",
      "two-layer Loss after iteration 3000: 7.710993222487181\n",
      "two-layer Loss after iteration 4000: 6.865459419118742\n",
      "two-layer Loss after iteration 5000: 6.600252075393364\n",
      "two-layer Loss after iteration 6000: 6.453093505561914\n",
      "two-layer Loss after iteration 7000: 6.304506941828974\n",
      "two-layer Loss after iteration 8000: 6.14953275500594\n",
      "two-layer Loss after iteration 9000: 6.02751325090729\n",
      "two-layer Loss after iteration 10000: 5.953478030649211\n",
      "two-layer Loss after iteration 11000: 5.891101663817394\n",
      "two-layer Loss after iteration 12000: 5.827241440106833\n",
      "two-layer Loss after iteration 13000: 5.767173933697802\n",
      "two-layer Loss after iteration 14000: 5.670151632579207\n",
      "two-layer Loss after iteration 15000: 5.5984130645412025\n",
      "two-layer Loss after iteration 16000: 5.464874730982658\n",
      "two-layer Loss after iteration 17000: 5.353506188689678\n",
      "two-layer Loss after iteration 18000: 5.208302388383608\n",
      "two-layer Loss after iteration 19000: 4.910610886179092\n",
      "two-layer Loss after iteration 20000: 4.537188421225599\n",
      "two-layer Loss after iteration 21000: 4.360371306193059\n",
      "two-layer Loss after iteration 22000: 4.263966810801173\n",
      "two-layer Loss after iteration 23000: 4.212397025609345\n",
      "two-layer Loss after iteration 24000: 4.183632847330952\n",
      "two-layer Loss after iteration 25000: 4.161131693303793\n",
      "two-layer Loss after iteration 26000: 4.144366682884432\n",
      "two-layer Loss after iteration 27000: 4.130000896693371\n",
      "two-layer Loss after iteration 28000: 4.11739421566304\n",
      "two-layer Loss after iteration 29000: 4.105885426852427\n",
      "two-layer Loss after iteration 30000: 4.089023970681863\n",
      "two-layer Loss after iteration 31000: 4.075763794691492\n",
      "two-layer Loss after iteration 32000: 4.063311696682207\n",
      "two-layer Loss after iteration 33000: 4.051778470425884\n",
      "two-layer Loss after iteration 34000: 4.04090186870288\n",
      "two-layer Loss after iteration 35000: 4.030613649779764\n",
      "two-layer Loss after iteration 36000: 4.020881147963719\n",
      "two-layer Loss after iteration 37000: 4.011651445954452\n",
      "two-layer Loss after iteration 38000: 4.002885088018677\n",
      "two-layer Loss after iteration 39000: 3.994541466949786\n",
      "two-layer Loss after iteration 40000: 3.9865862921479707\n",
      "two-layer Loss after iteration 41000: 3.978959515933742\n",
      "two-layer Loss after iteration 42000: 3.971602481930288\n",
      "two-layer Loss after iteration 43000: 3.964496807731524\n",
      "two-layer Loss after iteration 44000: 3.957657807929909\n",
      "two-layer Loss after iteration 45000: 3.9510413115110654\n",
      "two-layer Loss after iteration 46000: 3.9446360393347804\n",
      "two-layer Loss after iteration 47000: 3.938386447431051\n",
      "two-layer Loss after iteration 48000: 3.932034763557427\n",
      "two-layer Loss after iteration 49000: 3.9260713500765867\n",
      "two-layer Loss after iteration 50000: 3.920350536394546\n",
      "two-layer Loss after iteration 51000: 3.914818589612523\n",
      "two-layer Loss after iteration 52000: 3.9094500776334584\n",
      "two-layer Loss after iteration 53000: 3.904249979738104\n",
      "two-layer Loss after iteration 54000: 3.8992116733147952\n",
      "two-layer Loss after iteration 55000: 3.8940634009724575\n",
      "two-layer Loss after iteration 56000: 3.889299688513557\n",
      "two-layer Loss after iteration 57000: 3.884801174484329\n",
      "two-layer Loss after iteration 58000: 3.8804756319448055\n",
      "two-layer Loss after iteration 59000: 3.87632184094533\n",
      "two-layer Loss after iteration 60000: 3.8723219729545777\n",
      "two-layer Loss after iteration 61000: 3.868474002678649\n",
      "two-layer Loss after iteration 62000: 3.864766385068301\n",
      "two-layer Loss after iteration 63000: 3.861189744050688\n",
      "two-layer Loss after iteration 64000: 3.857729477378804\n",
      "two-layer Loss after iteration 65000: 3.854380286100502\n",
      "two-layer Loss after iteration 66000: 3.8511327618698368\n",
      "two-layer Loss after iteration 67000: 3.847980847907924\n",
      "two-layer Loss after iteration 68000: 3.8345385902605615\n",
      "two-layer Loss after iteration 69000: 3.7915296483861276\n",
      "two-layer Loss after iteration 70000: 3.775128040587725\n",
      "two-layer Loss after iteration 71000: 3.757954887849717\n",
      "two-layer Loss after iteration 72000: 3.742464317678174\n",
      "two-layer Loss after iteration 73000: 3.728665687563417\n",
      "two-layer Loss after iteration 74000: 3.714941114168214\n",
      "two-layer Loss after iteration 75000: 3.7019728884889576\n",
      "two-layer Loss after iteration 76000: 3.6898171543924434\n",
      "two-layer Loss after iteration 77000: 3.679030154351657\n",
      "two-layer Loss after iteration 78000: 3.669936593004367\n",
      "two-layer Loss after iteration 79000: 3.661506160736553\n",
      "two-layer Loss after iteration 80000: 3.6536092666187017\n",
      "two-layer Loss after iteration 81000: 3.6461582638151295\n",
      "two-layer Loss after iteration 82000: 3.6376906410020293\n",
      "two-layer Loss after iteration 83000: 3.6244623416530977\n",
      "two-layer Loss after iteration 84000: 3.6081833204901512\n",
      "two-layer Loss after iteration 85000: 3.5858230941986324\n",
      "two-layer Loss after iteration 86000: 3.563376863123573\n",
      "two-layer Loss after iteration 87000: 3.542736400210989\n",
      "two-layer Loss after iteration 88000: 3.521991603475037\n",
      "two-layer Loss after iteration 89000: 3.497126907454216\n",
      "two-layer Loss after iteration 90000: 3.449042780054267\n",
      "two-layer Loss after iteration 91000: 3.4001351340978085\n",
      "two-layer Loss after iteration 92000: 3.3648805868833134\n",
      "two-layer Loss after iteration 93000: 3.335933626303726\n",
      "two-layer Loss after iteration 94000: 3.307619099610103\n",
      "two-layer Loss after iteration 95000: 3.2768304144460845\n",
      "two-layer Loss after iteration 96000: 3.2491857314783448\n",
      "two-layer Loss after iteration 97000: 3.2244831281837247\n",
      "two-layer Loss after iteration 98000: 3.203508013389047\n",
      "two-layer Loss after iteration 99000: 3.184104310011226\n",
      "two-layer Loss after iteration 0: 1597.8125574203802\n",
      "two-layer Loss after iteration 1000: 25.41514043720271\n",
      "two-layer Loss after iteration 2000: 9.94887956370445\n",
      "two-layer Loss after iteration 3000: 7.469705899932811\n",
      "two-layer Loss after iteration 4000: 6.3209716101201066\n",
      "two-layer Loss after iteration 5000: 5.628444459764121\n",
      "two-layer Loss after iteration 6000: 5.318259790024364\n",
      "two-layer Loss after iteration 7000: 5.115513458262628\n",
      "two-layer Loss after iteration 8000: 4.9376577028448025\n",
      "two-layer Loss after iteration 9000: 4.814615907249921\n",
      "two-layer Loss after iteration 10000: 4.710841472273898\n",
      "two-layer Loss after iteration 11000: 4.62612381735725\n",
      "two-layer Loss after iteration 12000: 4.508865582520267\n",
      "two-layer Loss after iteration 13000: 4.408170236577643\n",
      "two-layer Loss after iteration 14000: 4.318393613904435\n",
      "two-layer Loss after iteration 15000: 4.220786992338571\n",
      "two-layer Loss after iteration 16000: 4.1539973417064315\n",
      "two-layer Loss after iteration 17000: 4.099424622828336\n",
      "two-layer Loss after iteration 18000: 4.045522354547336\n",
      "two-layer Loss after iteration 19000: 3.9999623890300953\n",
      "two-layer Loss after iteration 20000: 3.954397018936874\n",
      "two-layer Loss after iteration 21000: 3.930938932848123\n",
      "two-layer Loss after iteration 22000: 3.914983968714149\n",
      "two-layer Loss after iteration 23000: 3.82533181693374\n",
      "two-layer Loss after iteration 24000: 3.719955836407468\n",
      "two-layer Loss after iteration 25000: 3.6818398288323646\n",
      "two-layer Loss after iteration 26000: 3.6666447785476337\n",
      "two-layer Loss after iteration 27000: 3.641613247522598\n",
      "two-layer Loss after iteration 28000: 3.6263181749861277\n",
      "two-layer Loss after iteration 29000: 3.615339699419315\n",
      "two-layer Loss after iteration 30000: 3.6056747756622056\n",
      "two-layer Loss after iteration 31000: 3.596884842808667\n",
      "two-layer Loss after iteration 32000: 3.5878409097899566\n",
      "two-layer Loss after iteration 33000: 3.579306162330715\n",
      "two-layer Loss after iteration 34000: 3.571727065995616\n",
      "two-layer Loss after iteration 35000: 3.563569391341419\n",
      "two-layer Loss after iteration 36000: 3.5566772431331577\n",
      "two-layer Loss after iteration 37000: 3.5505484935054548\n",
      "two-layer Loss after iteration 38000: 3.544702650782499\n",
      "two-layer Loss after iteration 39000: 3.5393740418705413\n",
      "two-layer Loss after iteration 40000: 3.534692657174588\n",
      "two-layer Loss after iteration 41000: 3.5305782939977557\n",
      "two-layer Loss after iteration 42000: 3.5268729192743846\n",
      "two-layer Loss after iteration 43000: 3.5227555684696523\n",
      "two-layer Loss after iteration 44000: 3.51943191109873\n",
      "two-layer Loss after iteration 45000: 3.5165194606124777\n",
      "two-layer Loss after iteration 46000: 3.5139531891714055\n",
      "two-layer Loss after iteration 47000: 3.511634478740018\n",
      "two-layer Loss after iteration 48000: 3.5095807461600077\n",
      "two-layer Loss after iteration 49000: 3.5077348109657396\n",
      "two-layer Loss after iteration 50000: 3.506031795735261\n",
      "two-layer Loss after iteration 51000: 3.5045425913482\n",
      "two-layer Loss after iteration 52000: 3.5031971318189643\n",
      "two-layer Loss after iteration 53000: 3.5019502631984074\n",
      "two-layer Loss after iteration 54000: 3.5008218289003064\n",
      "two-layer Loss after iteration 55000: 3.4998149585328737\n",
      "two-layer Loss after iteration 56000: 3.4988892021452425\n",
      "two-layer Loss after iteration 57000: 3.4980392522514228\n",
      "two-layer Loss after iteration 58000: 3.497282089393234\n",
      "two-layer Loss after iteration 59000: 3.496601412368114\n",
      "two-layer Loss after iteration 60000: 3.4959419096883355\n",
      "two-layer Loss after iteration 61000: 3.4953652727051305\n",
      "two-layer Loss after iteration 62000: 3.4939541798674987\n",
      "two-layer Loss after iteration 63000: 3.485226255631568\n",
      "two-layer Loss after iteration 64000: 3.4834032592466913\n",
      "two-layer Loss after iteration 65000: 3.4819715890876504\n",
      "two-layer Loss after iteration 66000: 3.4806475505224075\n",
      "two-layer Loss after iteration 67000: 3.479462003536858\n",
      "two-layer Loss after iteration 68000: 3.4783653397986773\n",
      "two-layer Loss after iteration 69000: 3.477328599434868\n",
      "two-layer Loss after iteration 70000: 3.476380805418373\n",
      "two-layer Loss after iteration 71000: 3.475515842767908\n",
      "two-layer Loss after iteration 72000: 3.474726261558026\n",
      "two-layer Loss after iteration 73000: 3.4739795902396\n",
      "two-layer Loss after iteration 74000: 3.4732940759177215\n",
      "two-layer Loss after iteration 75000: 3.4726495213600033\n",
      "two-layer Loss after iteration 76000: 3.4720639268600033\n",
      "two-layer Loss after iteration 77000: 3.4715330154088133\n",
      "two-layer Loss after iteration 78000: 3.471017074760285\n",
      "two-layer Loss after iteration 79000: 3.470559767912086\n",
      "two-layer Loss after iteration 80000: 3.4701410453978836\n",
      "two-layer Loss after iteration 81000: 3.4697471274225755\n",
      "two-layer Loss after iteration 82000: 3.4693597565222234\n",
      "two-layer Loss after iteration 83000: 3.4690188981178847\n",
      "9.824821530771832e-05 3.4693597565222234 3.4690188981178847\n",
      "two-layer Loss after iteration 0: 1123.3241431329898\n",
      "two-layer Loss after iteration 1000: 25.986091879588113\n",
      "two-layer Loss after iteration 2000: 9.873697192022515\n",
      "two-layer Loss after iteration 3000: 7.241873227203594\n",
      "two-layer Loss after iteration 4000: 5.832380761164899\n",
      "two-layer Loss after iteration 5000: 4.908392469152642\n",
      "two-layer Loss after iteration 6000: 4.252184157829018\n",
      "two-layer Loss after iteration 7000: 3.9225854451167574\n",
      "two-layer Loss after iteration 8000: 3.7823015927146466\n",
      "two-layer Loss after iteration 9000: 3.677798732071276\n",
      "two-layer Loss after iteration 10000: 3.330120840834775\n",
      "two-layer Loss after iteration 11000: 3.1868354776394465\n",
      "two-layer Loss after iteration 12000: 3.0690900692794245\n",
      "two-layer Loss after iteration 13000: 2.9726796329890455\n",
      "two-layer Loss after iteration 14000: 2.865645743486638\n",
      "two-layer Loss after iteration 15000: 2.7746968643342336\n",
      "two-layer Loss after iteration 16000: 2.5782760146297714\n",
      "two-layer Loss after iteration 17000: 2.338621376587126\n",
      "two-layer Loss after iteration 18000: 2.2224507143467096\n",
      "two-layer Loss after iteration 19000: 2.151502321087405\n",
      "two-layer Loss after iteration 20000: 2.088020679272539\n",
      "two-layer Loss after iteration 21000: 2.0476801095694235\n",
      "two-layer Loss after iteration 22000: 2.0075392301809365\n",
      "two-layer Loss after iteration 23000: 1.9792987399839\n",
      "two-layer Loss after iteration 24000: 1.9541146170999106\n",
      "two-layer Loss after iteration 25000: 1.9371101896081078\n",
      "two-layer Loss after iteration 26000: 1.9250390436606297\n",
      "two-layer Loss after iteration 27000: 1.9161576571436627\n",
      "two-layer Loss after iteration 28000: 1.909425266462093\n",
      "two-layer Loss after iteration 29000: 1.904203668908365\n",
      "two-layer Loss after iteration 30000: 1.8993174486450635\n",
      "two-layer Loss after iteration 31000: 1.895154784976222\n",
      "two-layer Loss after iteration 32000: 1.891942549152763\n",
      "two-layer Loss after iteration 33000: 1.8892923971495386\n",
      "two-layer Loss after iteration 34000: 1.887055160301791\n",
      "two-layer Loss after iteration 35000: 1.8851326971631437\n",
      "two-layer Loss after iteration 36000: 1.883457432030574\n",
      "two-layer Loss after iteration 37000: 1.88198129817236\n",
      "two-layer Loss after iteration 38000: 1.880669164786479\n",
      "two-layer Loss after iteration 39000: 1.8794946989535841\n",
      "two-layer Loss after iteration 40000: 1.8784376725869552\n",
      "two-layer Loss after iteration 41000: 1.877482173811945\n",
      "two-layer Loss after iteration 42000: 1.8766154018655774\n",
      "two-layer Loss after iteration 43000: 1.8758268451516298\n",
      "two-layer Loss after iteration 44000: 1.87510771371971\n",
      "two-layer Loss after iteration 45000: 1.8744505421082942\n",
      "two-layer Loss after iteration 46000: 1.8738489071240625\n",
      "two-layer Loss after iteration 47000: 1.8732972237673367\n",
      "two-layer Loss after iteration 48000: 1.872790594756189\n",
      "two-layer Loss after iteration 49000: 1.8723247772097242\n",
      "two-layer Loss after iteration 50000: 1.8719005302465515\n",
      "two-layer Loss after iteration 51000: 1.871514966604701\n",
      "two-layer Loss after iteration 52000: 1.8711627547412502\n",
      "two-layer Loss after iteration 53000: 1.8708426578304058\n",
      "two-layer Loss after iteration 54000: 1.870549152297837\n",
      "two-layer Loss after iteration 55000: 1.8702817594515075\n",
      "two-layer Loss after iteration 56000: 1.8700385176215304\n",
      "two-layer Loss after iteration 57000: 1.8698136001152728\n",
      "two-layer Loss after iteration 58000: 1.8696084935845008\n",
      "two-layer Loss after iteration 59000: 1.8694220917511037\n",
      "9.970099838370035e-05 1.8696084935845008 1.8694220917511037\n",
      "two-layer Loss after iteration 0: 1470.564680745017\n",
      "two-layer Loss after iteration 1000: 25.192916466453955\n",
      "two-layer Loss after iteration 2000: 10.403416020247274\n",
      "two-layer Loss after iteration 3000: 8.154300524505786\n",
      "two-layer Loss after iteration 4000: 7.210007784116999\n",
      "two-layer Loss after iteration 5000: 6.637463876312562\n",
      "two-layer Loss after iteration 6000: 6.345268164355922\n",
      "two-layer Loss after iteration 7000: 6.082281226732719\n",
      "two-layer Loss after iteration 8000: 5.884341885155698\n",
      "two-layer Loss after iteration 9000: 5.749133535493596\n",
      "two-layer Loss after iteration 10000: 5.644811477058564\n",
      "two-layer Loss after iteration 11000: 5.522902409058758\n",
      "two-layer Loss after iteration 12000: 5.423616559456578\n",
      "two-layer Loss after iteration 13000: 5.3430825111717155\n",
      "two-layer Loss after iteration 14000: 5.268220361258601\n",
      "two-layer Loss after iteration 15000: 5.217879049600846\n",
      "two-layer Loss after iteration 16000: 4.777446282038207\n",
      "two-layer Loss after iteration 17000: 4.420838798060291\n",
      "two-layer Loss after iteration 18000: 4.3064906924847275\n",
      "two-layer Loss after iteration 19000: 4.28155517635968\n",
      "two-layer Loss after iteration 20000: 4.260967911037379\n",
      "two-layer Loss after iteration 21000: 4.24242459366021\n",
      "two-layer Loss after iteration 22000: 4.225962711195421\n",
      "two-layer Loss after iteration 23000: 4.210463536790447\n",
      "two-layer Loss after iteration 24000: 4.195840976363272\n",
      "two-layer Loss after iteration 25000: 4.1821041193953565\n",
      "two-layer Loss after iteration 26000: 4.169426556443712\n",
      "two-layer Loss after iteration 27000: 4.1575677390591705\n",
      "two-layer Loss after iteration 28000: 4.1465407798247575\n",
      "two-layer Loss after iteration 29000: 4.136241434517029\n",
      "two-layer Loss after iteration 30000: 4.125694442512122\n",
      "two-layer Loss after iteration 31000: 4.116225387418521\n",
      "two-layer Loss after iteration 32000: 4.10747537646259\n",
      "two-layer Loss after iteration 33000: 4.099156748554413\n",
      "two-layer Loss after iteration 34000: 4.091239890897897\n",
      "two-layer Loss after iteration 35000: 4.083638498803856\n",
      "two-layer Loss after iteration 36000: 4.076320414087335\n",
      "two-layer Loss after iteration 37000: 4.069265244867315\n",
      "two-layer Loss after iteration 38000: 4.0624573909983885\n",
      "two-layer Loss after iteration 39000: 4.055873658000157\n",
      "two-layer Loss after iteration 40000: 4.049425446333341\n",
      "two-layer Loss after iteration 41000: 4.043144090031751\n",
      "two-layer Loss after iteration 42000: 4.036865653108386\n",
      "two-layer Loss after iteration 43000: 4.0300488216242405\n",
      "two-layer Loss after iteration 44000: 4.019918529594693\n",
      "two-layer Loss after iteration 45000: 4.0077307692058\n",
      "two-layer Loss after iteration 46000: 3.9958651160128325\n",
      "two-layer Loss after iteration 47000: 3.9825189021700367\n",
      "two-layer Loss after iteration 48000: 3.9672687216964975\n",
      "two-layer Loss after iteration 49000: 3.9498241882498926\n",
      "two-layer Loss after iteration 50000: 3.930272910089472\n",
      "two-layer Loss after iteration 51000: 3.9086315198228982\n",
      "two-layer Loss after iteration 52000: 3.8851058430918526\n",
      "two-layer Loss after iteration 53000: 3.8599422783318773\n",
      "two-layer Loss after iteration 54000: 3.8338526835504885\n",
      "two-layer Loss after iteration 55000: 3.8074949755905023\n",
      "two-layer Loss after iteration 56000: 3.7808821973880224\n",
      "two-layer Loss after iteration 57000: 3.7506115209689748\n",
      "two-layer Loss after iteration 58000: 3.7208239900487015\n",
      "two-layer Loss after iteration 59000: 3.6913959114759733\n",
      "two-layer Loss after iteration 60000: 3.663782190598374\n",
      "two-layer Loss after iteration 61000: 3.6378396356914546\n",
      "two-layer Loss after iteration 62000: 3.612327975919078\n",
      "two-layer Loss after iteration 63000: 3.5881796632731766\n",
      "two-layer Loss after iteration 64000: 3.5643650494300183\n",
      "two-layer Loss after iteration 65000: 3.5406538680545863\n",
      "two-layer Loss after iteration 66000: 3.5173443347198674\n",
      "two-layer Loss after iteration 67000: 3.4945563996139386\n",
      "two-layer Loss after iteration 68000: 3.4722336047044844\n",
      "two-layer Loss after iteration 69000: 3.450384026440593\n",
      "two-layer Loss after iteration 70000: 3.4294858234973233\n",
      "two-layer Loss after iteration 71000: 3.409587828708907\n",
      "two-layer Loss after iteration 72000: 3.390729037309487\n",
      "two-layer Loss after iteration 73000: 3.3728907268229036\n",
      "two-layer Loss after iteration 74000: 3.356195926583678\n",
      "two-layer Loss after iteration 75000: 3.3408158927846747\n",
      "two-layer Loss after iteration 76000: 3.3265104990195975\n",
      "two-layer Loss after iteration 77000: 3.3130856117111134\n",
      "two-layer Loss after iteration 78000: 3.300486699692802\n",
      "two-layer Loss after iteration 79000: 3.288301404808121\n",
      "two-layer Loss after iteration 80000: 3.2771641981147908\n",
      "two-layer Loss after iteration 81000: 3.266418455841928\n",
      "two-layer Loss after iteration 82000: 3.2529069656221963\n",
      "two-layer Loss after iteration 83000: 3.242411334051298\n",
      "two-layer Loss after iteration 84000: 3.232867267260334\n",
      "two-layer Loss after iteration 85000: 3.223689351105862\n",
      "two-layer Loss after iteration 86000: 3.2149498950466024\n",
      "two-layer Loss after iteration 87000: 3.2065130393265506\n",
      "two-layer Loss after iteration 88000: 3.198335463930694\n",
      "two-layer Loss after iteration 89000: 3.190390823691342\n",
      "two-layer Loss after iteration 90000: 3.1826770787706953\n",
      "two-layer Loss after iteration 91000: 3.175180707221589\n",
      "two-layer Loss after iteration 92000: 3.1678886843741525\n",
      "two-layer Loss after iteration 93000: 3.1607952665732024\n",
      "two-layer Loss after iteration 94000: 3.153878491105169\n",
      "two-layer Loss after iteration 95000: 3.1471383196130347\n",
      "two-layer Loss after iteration 96000: 3.1405677570196797\n",
      "two-layer Loss after iteration 97000: 3.1341576005504193\n",
      "two-layer Loss after iteration 98000: 3.127902799298156\n",
      "two-layer Loss after iteration 99000: 3.1217868139570055\n",
      "two-layer Loss after iteration 0: 1681.7425849949148\n",
      "two-layer Loss after iteration 1000: 22.75747515779686\n",
      "two-layer Loss after iteration 2000: 9.765265916829376\n",
      "two-layer Loss after iteration 3000: 7.722630105119044\n",
      "two-layer Loss after iteration 4000: 6.7228623870349296\n",
      "two-layer Loss after iteration 5000: 5.673637947439248\n",
      "two-layer Loss after iteration 6000: 5.084530441530577\n",
      "two-layer Loss after iteration 7000: 4.622231485992098\n",
      "two-layer Loss after iteration 8000: 3.866019916411324\n",
      "two-layer Loss after iteration 9000: 3.4918524395524786\n",
      "two-layer Loss after iteration 10000: 3.3073327163501722\n",
      "two-layer Loss after iteration 11000: 3.024364578297943\n",
      "two-layer Loss after iteration 12000: 2.768038552351567\n",
      "two-layer Loss after iteration 13000: 2.6391825552168404\n",
      "two-layer Loss after iteration 14000: 2.5534452753005805\n",
      "two-layer Loss after iteration 15000: 2.5123259986332362\n",
      "two-layer Loss after iteration 16000: 2.489588910572933\n",
      "two-layer Loss after iteration 17000: 2.4749083907596527\n",
      "two-layer Loss after iteration 18000: 2.453503024515754\n",
      "two-layer Loss after iteration 19000: 2.4431194466507944\n",
      "two-layer Loss after iteration 20000: 2.435467675581879\n",
      "two-layer Loss after iteration 21000: 2.428816753870812\n",
      "two-layer Loss after iteration 22000: 2.423021144360388\n",
      "two-layer Loss after iteration 23000: 2.418767110859708\n",
      "two-layer Loss after iteration 24000: 2.415383803595337\n",
      "two-layer Loss after iteration 25000: 2.4096931161201676\n",
      "two-layer Loss after iteration 26000: 2.4022629720270126\n",
      "two-layer Loss after iteration 27000: 2.3984323064612565\n",
      "two-layer Loss after iteration 28000: 2.395515740861384\n",
      "two-layer Loss after iteration 29000: 2.391818564763108\n",
      "two-layer Loss after iteration 30000: 2.388027520649336\n",
      "two-layer Loss after iteration 31000: 2.3861616671924923\n",
      "two-layer Loss after iteration 32000: 2.3847929888265234\n",
      "two-layer Loss after iteration 33000: 2.3837747116932957\n",
      "two-layer Loss after iteration 34000: 2.383124097778257\n",
      "two-layer Loss after iteration 35000: 2.38270457967127\n",
      "two-layer Loss after iteration 36000: 2.38247654431638\n",
      "9.570441792724573e-05 2.38270457967127 2.38247654431638\n",
      "two-layer Loss after iteration 0: 1671.1767995831644\n",
      "two-layer Loss after iteration 1000: 24.780159489017418\n",
      "two-layer Loss after iteration 2000: 10.2882930524948\n",
      "two-layer Loss after iteration 3000: 7.947370650779507\n",
      "two-layer Loss after iteration 4000: 7.020763738067439\n",
      "two-layer Loss after iteration 5000: 5.8237097558532795\n",
      "two-layer Loss after iteration 6000: 5.363354547334657\n",
      "two-layer Loss after iteration 7000: 5.1258973159928365\n",
      "two-layer Loss after iteration 8000: 4.964272531335859\n",
      "two-layer Loss after iteration 9000: 4.840784806026117\n",
      "two-layer Loss after iteration 10000: 4.747540657692077\n",
      "two-layer Loss after iteration 11000: 4.651128668121268\n",
      "two-layer Loss after iteration 12000: 4.576507090820489\n",
      "two-layer Loss after iteration 13000: 4.505049716389413\n",
      "two-layer Loss after iteration 14000: 4.433114922534591\n",
      "two-layer Loss after iteration 15000: 4.3761171411875495\n",
      "two-layer Loss after iteration 16000: 4.3375349075960115\n",
      "two-layer Loss after iteration 17000: 3.9610284491610215\n",
      "two-layer Loss after iteration 18000: 3.8548523981497387\n",
      "two-layer Loss after iteration 19000: 3.805318464063518\n",
      "two-layer Loss after iteration 20000: 3.7761087568327367\n",
      "two-layer Loss after iteration 21000: 3.7566619442357716\n",
      "two-layer Loss after iteration 22000: 3.7427147261746896\n",
      "two-layer Loss after iteration 23000: 3.7322298463707826\n",
      "two-layer Loss after iteration 24000: 3.724198800152687\n",
      "two-layer Loss after iteration 25000: 3.7177512581951655\n",
      "two-layer Loss after iteration 26000: 3.712399084811765\n",
      "two-layer Loss after iteration 27000: 3.7078614876884735\n",
      "two-layer Loss after iteration 28000: 3.7039706895959785\n",
      "two-layer Loss after iteration 29000: 3.7006048073059326\n",
      "two-layer Loss after iteration 30000: 3.693506054941777\n",
      "two-layer Loss after iteration 31000: 3.6714057111552654\n",
      "two-layer Loss after iteration 32000: 3.638353002504844\n",
      "two-layer Loss after iteration 33000: 3.627750869908241\n",
      "two-layer Loss after iteration 34000: 3.618821035747354\n",
      "two-layer Loss after iteration 35000: 3.6052172820415804\n",
      "two-layer Loss after iteration 36000: 3.595144732145737\n",
      "two-layer Loss after iteration 37000: 3.587129714261249\n",
      "two-layer Loss after iteration 38000: 3.581069157947659\n",
      "two-layer Loss after iteration 39000: 3.576307881602489\n",
      "two-layer Loss after iteration 40000: 3.5722957899122827\n",
      "two-layer Loss after iteration 41000: 3.5687550960931547\n",
      "two-layer Loss after iteration 42000: 3.565628595916991\n",
      "two-layer Loss after iteration 43000: 3.562871228786481\n",
      "two-layer Loss after iteration 44000: 3.5604412245366155\n",
      "two-layer Loss after iteration 45000: 3.5582942610550767\n",
      "two-layer Loss after iteration 46000: 3.556484026537662\n",
      "two-layer Loss after iteration 47000: 3.5550268837670935\n",
      "two-layer Loss after iteration 48000: 3.5537612166863135\n",
      "two-layer Loss after iteration 49000: 3.5379963299715946\n",
      "two-layer Loss after iteration 50000: 3.5348040846543607\n",
      "two-layer Loss after iteration 51000: 3.532309169399192\n",
      "two-layer Loss after iteration 52000: 3.530272065200367\n",
      "two-layer Loss after iteration 53000: 3.528554649637034\n",
      "two-layer Loss after iteration 54000: 3.5271025621367302\n",
      "two-layer Loss after iteration 55000: 3.525872719862485\n",
      "two-layer Loss after iteration 56000: 3.524829545066543\n",
      "two-layer Loss after iteration 57000: 3.5239434740075226\n",
      "two-layer Loss after iteration 58000: 3.523189871680042\n",
      "two-layer Loss after iteration 59000: 3.522548166048889\n",
      "two-layer Loss after iteration 60000: 3.5220011420958377\n",
      "two-layer Loss after iteration 61000: 3.5215343635378713\n",
      "two-layer Loss after iteration 62000: 3.5194973989928764\n",
      "two-layer Loss after iteration 63000: 3.517674929821252\n",
      "two-layer Loss after iteration 64000: 3.5158770907162515\n",
      "two-layer Loss after iteration 65000: 3.514408573830653\n",
      "two-layer Loss after iteration 66000: 3.5133148996101142\n",
      "two-layer Loss after iteration 67000: 3.512590017282168\n",
      "two-layer Loss after iteration 68000: 3.5119851771384116\n",
      "two-layer Loss after iteration 69000: 3.511479987289643\n",
      "two-layer Loss after iteration 70000: 3.5110580043443123\n",
      "two-layer Loss after iteration 71000: 3.5107055032811907\n",
      "two-layer Loss after iteration 72000: 3.5102956613651664\n",
      "two-layer Loss after iteration 73000: 3.5099340306431843\n",
      "two-layer Loss after iteration 74000: 3.5096327922166832\n",
      "8.582452657832224e-05 3.5099340306431843 3.5096327922166832\n",
      "two-layer Loss after iteration 0: 1339.3426116782675\n",
      "two-layer Loss after iteration 1000: 24.860714622034347\n",
      "two-layer Loss after iteration 2000: 9.604249542270404\n",
      "two-layer Loss after iteration 3000: 7.591724879596256\n",
      "two-layer Loss after iteration 4000: 6.577528485195287\n",
      "two-layer Loss after iteration 5000: 5.729392466225182\n",
      "two-layer Loss after iteration 6000: 5.300579695448317\n",
      "two-layer Loss after iteration 7000: 5.033409763683707\n",
      "two-layer Loss after iteration 8000: 4.78519441913042\n",
      "two-layer Loss after iteration 9000: 4.550031053511247\n",
      "two-layer Loss after iteration 10000: 4.3402883508420995\n",
      "two-layer Loss after iteration 11000: 4.170098196166235\n",
      "two-layer Loss after iteration 12000: 4.03650926359302\n",
      "two-layer Loss after iteration 13000: 3.931632148715288\n",
      "two-layer Loss after iteration 14000: 3.815340403801019\n",
      "two-layer Loss after iteration 15000: 3.7487127163381935\n",
      "two-layer Loss after iteration 16000: 3.7196455663204806\n",
      "two-layer Loss after iteration 17000: 3.696114564130121\n",
      "two-layer Loss after iteration 18000: 3.6755099087639587\n",
      "two-layer Loss after iteration 19000: 3.654735842683954\n",
      "two-layer Loss after iteration 20000: 3.634975956063627\n",
      "two-layer Loss after iteration 21000: 3.622412473837891\n",
      "two-layer Loss after iteration 22000: 3.6112726837694784\n",
      "two-layer Loss after iteration 23000: 3.5987013146176636\n",
      "two-layer Loss after iteration 24000: 3.5875836730484245\n",
      "two-layer Loss after iteration 25000: 3.5795968109681744\n",
      "two-layer Loss after iteration 26000: 3.5730729067336457\n",
      "two-layer Loss after iteration 27000: 3.5673441821994833\n",
      "two-layer Loss after iteration 28000: 3.5623714999537026\n",
      "two-layer Loss after iteration 29000: 3.5579213169606363\n",
      "two-layer Loss after iteration 30000: 3.5538764311466244\n",
      "two-layer Loss after iteration 31000: 3.5485883378844245\n",
      "two-layer Loss after iteration 32000: 3.5442756283392343\n",
      "two-layer Loss after iteration 33000: 3.5405076392464108\n",
      "two-layer Loss after iteration 34000: 3.5371639485437285\n",
      "two-layer Loss after iteration 35000: 3.534195067873927\n",
      "two-layer Loss after iteration 36000: 3.531547753332821\n",
      "two-layer Loss after iteration 37000: 3.5292410704811594\n",
      "two-layer Loss after iteration 38000: 3.5266000307068848\n",
      "two-layer Loss after iteration 39000: 3.5240224686944885\n",
      "two-layer Loss after iteration 40000: 3.5217252071254834\n",
      "two-layer Loss after iteration 41000: 3.519671657951227\n",
      "two-layer Loss after iteration 42000: 3.517832877582722\n",
      "two-layer Loss after iteration 43000: 3.5161795370233833\n",
      "two-layer Loss after iteration 44000: 3.51469234731197\n",
      "two-layer Loss after iteration 45000: 3.513354599131231\n",
      "two-layer Loss after iteration 46000: 3.5121513378885316\n",
      "two-layer Loss after iteration 47000: 3.51106910995206\n",
      "two-layer Loss after iteration 48000: 3.5100957966374597\n",
      "two-layer Loss after iteration 49000: 3.5092204787674466\n",
      "two-layer Loss after iteration 50000: 3.5075203962968735\n",
      "two-layer Loss after iteration 51000: 3.5058591300793642\n",
      "two-layer Loss after iteration 52000: 3.504423803845646\n",
      "two-layer Loss after iteration 53000: 3.5031632771245618\n",
      "two-layer Loss after iteration 54000: 3.502065192031809\n",
      "two-layer Loss after iteration 55000: 3.5011870198145885\n",
      "two-layer Loss after iteration 56000: 3.5004749326090465\n",
      "two-layer Loss after iteration 57000: 3.4998841076555878\n",
      "two-layer Loss after iteration 58000: 3.498331636782636\n",
      "two-layer Loss after iteration 59000: 3.495135046966649\n",
      "two-layer Loss after iteration 60000: 3.4927446665158848\n",
      "two-layer Loss after iteration 61000: 3.4909435263265016\n",
      "two-layer Loss after iteration 62000: 3.485872753708149\n",
      "two-layer Loss after iteration 63000: 3.4803744369979124\n",
      "two-layer Loss after iteration 64000: 3.478407974599881\n",
      "two-layer Loss after iteration 65000: 3.476992232705181\n",
      "two-layer Loss after iteration 66000: 3.4760778409300563\n",
      "two-layer Loss after iteration 67000: 3.475311419098347\n",
      "two-layer Loss after iteration 68000: 3.474109327732944\n",
      "two-layer Loss after iteration 69000: 3.472119273583993\n",
      "two-layer Loss after iteration 70000: 3.4712599014477803\n",
      "two-layer Loss after iteration 71000: 3.4705712759333447\n",
      "two-layer Loss after iteration 72000: 3.4697161949658533\n",
      "two-layer Loss after iteration 73000: 3.4650276739247663\n",
      "two-layer Loss after iteration 74000: 3.4622837905430286\n",
      "two-layer Loss after iteration 75000: 3.460159584980818\n",
      "two-layer Loss after iteration 76000: 3.4584646946963535\n",
      "two-layer Loss after iteration 77000: 3.4521685642284146\n",
      "two-layer Loss after iteration 78000: 3.4494813865226743\n",
      "two-layer Loss after iteration 79000: 3.4431517876683078\n",
      "two-layer Loss after iteration 80000: 3.43725612480882\n",
      "two-layer Loss after iteration 81000: 3.4320946842328537\n",
      "two-layer Loss after iteration 82000: 3.4274607322401756\n",
      "two-layer Loss after iteration 83000: 3.423642583470685\n",
      "two-layer Loss after iteration 84000: 3.420478733520116\n",
      "two-layer Loss after iteration 85000: 3.4160569621679415\n",
      "two-layer Loss after iteration 86000: 3.411705804866801\n",
      "two-layer Loss after iteration 87000: 3.4083234049986406\n",
      "two-layer Loss after iteration 88000: 3.4053993398392746\n",
      "two-layer Loss after iteration 89000: 3.4019187037210576\n",
      "two-layer Loss after iteration 90000: 3.3991406009325136\n",
      "two-layer Loss after iteration 91000: 3.3960317574132923\n",
      "two-layer Loss after iteration 92000: 3.124136512732201\n",
      "two-layer Loss after iteration 93000: 2.952318465687414\n",
      "two-layer Loss after iteration 94000: 2.6924402120765882\n",
      "two-layer Loss after iteration 95000: 2.5977580838062346\n",
      "two-layer Loss after iteration 96000: 2.547292338485282\n",
      "two-layer Loss after iteration 97000: 2.515490186492775\n",
      "two-layer Loss after iteration 98000: 2.491409764511784\n",
      "two-layer Loss after iteration 99000: 2.470640460743997\n",
      "two-layer Loss after iteration 0: 1466.752205569474\n",
      "two-layer Loss after iteration 1000: 25.8439060355006\n",
      "two-layer Loss after iteration 2000: 10.246959743209539\n",
      "two-layer Loss after iteration 3000: 7.8386869247227\n",
      "two-layer Loss after iteration 4000: 6.483031178593333\n",
      "two-layer Loss after iteration 5000: 6.2485854966025745\n",
      "two-layer Loss after iteration 6000: 6.030444003162704\n",
      "two-layer Loss after iteration 7000: 5.844233860753826\n",
      "two-layer Loss after iteration 8000: 5.629145555301586\n",
      "two-layer Loss after iteration 9000: 5.436868583639801\n",
      "two-layer Loss after iteration 10000: 4.978798646433968\n",
      "two-layer Loss after iteration 11000: 4.074882056575743\n",
      "two-layer Loss after iteration 12000: 3.8872856679871366\n",
      "two-layer Loss after iteration 13000: 3.845534003224163\n",
      "two-layer Loss after iteration 14000: 3.815240567686916\n",
      "two-layer Loss after iteration 15000: 3.7912657732336164\n",
      "two-layer Loss after iteration 16000: 3.767143229145514\n",
      "two-layer Loss after iteration 17000: 3.7500692041871764\n",
      "two-layer Loss after iteration 18000: 3.737329235090505\n",
      "two-layer Loss after iteration 19000: 3.7151454105823722\n",
      "two-layer Loss after iteration 20000: 3.695795451290335\n",
      "two-layer Loss after iteration 21000: 3.6156596019842304\n",
      "two-layer Loss after iteration 22000: 3.3534665021005994\n",
      "two-layer Loss after iteration 23000: 3.0975881250091293\n",
      "two-layer Loss after iteration 24000: 3.0651981770942367\n",
      "two-layer Loss after iteration 25000: 3.0571727897060463\n",
      "two-layer Loss after iteration 26000: 3.0540103728485644\n",
      "two-layer Loss after iteration 27000: 3.0521343999986907\n",
      "two-layer Loss after iteration 28000: 3.0507888307260007\n",
      "two-layer Loss after iteration 29000: 3.0498538385349647\n",
      "two-layer Loss after iteration 30000: 3.0443229426401\n",
      "two-layer Loss after iteration 31000: 3.0419012126165024\n",
      "two-layer Loss after iteration 32000: 3.0399281238886937\n",
      "two-layer Loss after iteration 33000: 3.0385620522250933\n",
      "two-layer Loss after iteration 34000: 3.037554034824434\n",
      "two-layer Loss after iteration 35000: 3.036812791219819\n",
      "two-layer Loss after iteration 36000: 3.036180267130475\n",
      "two-layer Loss after iteration 37000: 3.0356384799168374\n",
      "two-layer Loss after iteration 38000: 3.0351598898778875\n",
      "two-layer Loss after iteration 39000: 3.0347513742521004\n",
      "two-layer Loss after iteration 40000: 3.034397937930676\n",
      "two-layer Loss after iteration 41000: 3.034087413383298\n",
      "two-layer Loss after iteration 42000: 3.033816853544125\n",
      "8.91733830671313e-05 3.034087413383298 3.033816853544125\n",
      "two-layer Loss after iteration 0: 1567.5293514438652\n",
      "two-layer Loss after iteration 1000: 24.822701188042796\n",
      "two-layer Loss after iteration 2000: 9.606092792053605\n",
      "two-layer Loss after iteration 3000: 7.706078128836878\n",
      "two-layer Loss after iteration 4000: 6.0491137880499\n",
      "two-layer Loss after iteration 5000: 5.597590896604525\n",
      "two-layer Loss after iteration 6000: 5.405976182854647\n",
      "two-layer Loss after iteration 7000: 5.226869711570113\n",
      "two-layer Loss after iteration 8000: 5.073853394945105\n",
      "two-layer Loss after iteration 9000: 4.93768597766837\n",
      "two-layer Loss after iteration 10000: 4.837833087656314\n",
      "two-layer Loss after iteration 11000: 4.745521479544388\n",
      "two-layer Loss after iteration 12000: 4.676414204622321\n",
      "two-layer Loss after iteration 13000: 4.627341149789978\n",
      "two-layer Loss after iteration 14000: 4.564011975240444\n",
      "two-layer Loss after iteration 15000: 4.072310130561109\n",
      "two-layer Loss after iteration 16000: 3.8769718936672106\n",
      "two-layer Loss after iteration 17000: 3.807385895436022\n",
      "two-layer Loss after iteration 18000: 3.7670803005632205\n",
      "two-layer Loss after iteration 19000: 3.7371898601187845\n",
      "two-layer Loss after iteration 20000: 3.7157220124095014\n",
      "two-layer Loss after iteration 21000: 3.700817464770966\n",
      "two-layer Loss after iteration 22000: 3.6903077113686917\n",
      "two-layer Loss after iteration 23000: 3.6828820090696315\n",
      "two-layer Loss after iteration 24000: 3.677677752989786\n",
      "two-layer Loss after iteration 25000: 3.6740176540994254\n",
      "two-layer Loss after iteration 26000: 3.6712643944319057\n",
      "two-layer Loss after iteration 27000: 3.6691199291441623\n",
      "two-layer Loss after iteration 28000: 3.667406589115411\n",
      "two-layer Loss after iteration 29000: 3.666043992209434\n",
      "two-layer Loss after iteration 30000: 3.664977237814032\n",
      "two-layer Loss after iteration 31000: 3.664089605328247\n",
      "two-layer Loss after iteration 32000: 3.6633418913478675\n",
      "two-layer Loss after iteration 33000: 3.6627052328439063\n",
      "two-layer Loss after iteration 34000: 3.662155465009934\n",
      "two-layer Loss after iteration 35000: 3.6616762340407734\n",
      "two-layer Loss after iteration 36000: 3.661250616456462\n",
      "two-layer Loss after iteration 37000: 3.6608795667280094\n",
      "two-layer Loss after iteration 38000: 3.6605498237904737\n",
      "9.007205277458837e-05 3.6608795667280094 3.6605498237904737\n",
      "three-layer Loss after iteration 0: 1710.573761552976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_32504\\2957246329.py:131: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three-layer Loss after iteration 1000: 28.133052274011895\n",
      "three-layer Loss after iteration 2000: 27.95932095615113\n",
      "three-layer Loss after iteration 3000: 22.999374261043357\n",
      "three-layer Loss after iteration 4000: 23.195466151943993\n",
      "three-layer Loss after iteration 5000: 17.829892288349836\n",
      "three-layer Loss after iteration 6000: 16.5618634667434\n",
      "three-layer Loss after iteration 7000: 13.403136278797788\n",
      "three-layer Loss after iteration 8000: 12.584806317980897\n",
      "three-layer Loss after iteration 9000: 9.045121186239907\n",
      "three-layer Loss after iteration 10000: 5.52857603874525\n",
      "three-layer Loss after iteration 11000: 8.67943360360814\n",
      "three-layer Loss after iteration 12000: 6.2439791985456194\n",
      "three-layer Loss after iteration 13000: 7.930443030537683\n",
      "three-layer Loss after iteration 14000: 6.004183664140997\n",
      "three-layer Loss after iteration 15000: 6.675961711043526\n",
      "three-layer Loss after iteration 16000: 5.997402835461987\n",
      "three-layer Loss after iteration 17000: 6.010892599273585\n",
      "three-layer Loss after iteration 18000: 5.215196439466422\n",
      "three-layer Loss after iteration 19000: 5.783289095259987\n",
      "three-layer Loss after iteration 20000: 5.770118601215881\n",
      "three-layer Loss after iteration 21000: 5.198284612187338\n",
      "three-layer Loss after iteration 22000: 5.133008461801224\n",
      "three-layer Loss after iteration 23000: 6.116701794611219\n",
      "three-layer Loss after iteration 24000: 5.005853246368368\n",
      "three-layer Loss after iteration 25000: 5.189010579371203\n",
      "three-layer Loss after iteration 26000: 5.165696804318694\n",
      "three-layer Loss after iteration 27000: 5.192180465985383\n",
      "three-layer Loss after iteration 28000: 5.1696457734001955\n",
      "three-layer Loss after iteration 29000: 5.140991462927072\n",
      "three-layer Loss after iteration 30000: 5.168741753744601\n",
      "three-layer Loss after iteration 31000: 5.180996192814589\n",
      "three-layer Loss after iteration 32000: 5.24445688032746\n",
      "three-layer Loss after iteration 33000: 5.217152700406196\n",
      "three-layer Loss after iteration 34000: 5.256657691682322\n",
      "three-layer Loss after iteration 35000: 5.501192619659268\n",
      "three-layer Loss after iteration 36000: 5.444581040235759\n",
      "three-layer Loss after iteration 37000: 5.217741135082567\n",
      "three-layer Loss after iteration 38000: 5.1617662231165\n",
      "three-layer Loss after iteration 39000: 5.105873448102486\n",
      "three-layer Loss after iteration 40000: 5.070785275474025\n",
      "three-layer Loss after iteration 41000: 5.063886127903948\n",
      "three-layer Loss after iteration 42000: 5.0703539118932754\n",
      "three-layer Loss after iteration 43000: 5.229292701999278\n",
      "three-layer Loss after iteration 44000: 5.0741883826235625\n",
      "three-layer Loss after iteration 45000: 5.15112135082053\n",
      "three-layer Loss after iteration 46000: 5.10219341411632\n",
      "three-layer Loss after iteration 47000: 5.08789908835528\n",
      "three-layer Loss after iteration 48000: 5.074883545145225\n",
      "three-layer Loss after iteration 49000: 5.063139290828792\n",
      "three-layer Loss after iteration 50000: 5.053807221326004\n",
      "three-layer Loss after iteration 51000: 5.042963392624163\n",
      "three-layer Loss after iteration 52000: 5.026637420557268\n",
      "three-layer Loss after iteration 53000: 4.997873916438875\n",
      "three-layer Loss after iteration 54000: 4.909006866334113\n",
      "three-layer Loss after iteration 55000: 4.909573386537238\n",
      "three-layer Loss after iteration 56000: 4.909367626583326\n",
      "4.190994567385672e-05 4.909573386537238 4.909367626583326\n",
      "three-layer Loss after iteration 0: 1413.208141474445\n",
      "three-layer Loss after iteration 1000: 14.153904082334051\n",
      "three-layer Loss after iteration 2000: 11.62155134807322\n",
      "three-layer Loss after iteration 3000: 11.10404604882167\n",
      "three-layer Loss after iteration 4000: 8.924346730727189\n",
      "three-layer Loss after iteration 5000: 6.185820883945566\n",
      "three-layer Loss after iteration 6000: 5.057356150684831\n",
      "three-layer Loss after iteration 7000: 4.578605224833212\n",
      "three-layer Loss after iteration 8000: 4.701141305029198\n",
      "three-layer Loss after iteration 9000: 4.855710796893667\n",
      "three-layer Loss after iteration 10000: 4.405059661360473\n",
      "three-layer Loss after iteration 11000: 3.8552791539331146\n",
      "three-layer Loss after iteration 12000: 3.7243025775350143\n",
      "three-layer Loss after iteration 13000: 3.6926549231168946\n",
      "three-layer Loss after iteration 14000: 3.681642087266959\n",
      "three-layer Loss after iteration 15000: 3.6783453109594535\n",
      "three-layer Loss after iteration 16000: 3.6766982822642964\n",
      "three-layer Loss after iteration 17000: 3.675867332966609\n",
      "three-layer Loss after iteration 18000: 3.678509814395351\n",
      "three-layer Loss after iteration 19000: 3.6874114602207455\n",
      "three-layer Loss after iteration 20000: 3.6851050940682346\n",
      "three-layer Loss after iteration 21000: 3.680349098195447\n",
      "three-layer Loss after iteration 22000: 3.6786278361615263\n",
      "three-layer Loss after iteration 23000: 3.6780964212691045\n",
      "three-layer Loss after iteration 24000: 3.6774758972972803\n",
      "three-layer Loss after iteration 25000: 3.676463420069842\n",
      "three-layer Loss after iteration 26000: 3.6753429675819067\n",
      "three-layer Loss after iteration 27000: 3.6742721619346694\n",
      "three-layer Loss after iteration 28000: 3.6732593128487596\n",
      "three-layer Loss after iteration 29000: 3.6722838269973814\n",
      "three-layer Loss after iteration 30000: 3.6713510765696182\n",
      "three-layer Loss after iteration 31000: 3.670515081953589\n",
      "three-layer Loss after iteration 32000: 3.6689048461113876\n",
      "three-layer Loss after iteration 33000: 3.6676810690963273\n",
      "three-layer Loss after iteration 34000: 3.6669290373204118\n",
      "three-layer Loss after iteration 35000: 3.6661086518114554\n",
      "three-layer Loss after iteration 36000: 3.665258521711481\n",
      "three-layer Loss after iteration 37000: 3.6643623566813655\n",
      "three-layer Loss after iteration 38000: 3.663517575266544\n",
      "three-layer Loss after iteration 39000: 3.66269510870742\n",
      "three-layer Loss after iteration 40000: 3.6618720607370396\n",
      "three-layer Loss after iteration 41000: 3.661095071283752\n",
      "three-layer Loss after iteration 42000: 3.660300110703286\n",
      "three-layer Loss after iteration 43000: 3.6595470106695656\n",
      "three-layer Loss after iteration 44000: 3.6588171739197777\n",
      "three-layer Loss after iteration 45000: 3.6580647945157025\n",
      "three-layer Loss after iteration 46000: 3.6573889989359616\n",
      "three-layer Loss after iteration 47000: 3.6566756111049736\n",
      "three-layer Loss after iteration 48000: 3.6559853265579316\n",
      "three-layer Loss after iteration 49000: 3.6552976971652784\n",
      "three-layer Loss after iteration 50000: 3.65467581023765\n",
      "three-layer Loss after iteration 51000: 3.6540098887474817\n",
      "three-layer Loss after iteration 52000: 3.6533715485580847\n",
      "three-layer Loss after iteration 53000: 3.6527576374504465\n",
      "three-layer Loss after iteration 54000: 3.652142951175375\n",
      "three-layer Loss after iteration 55000: 3.651550975091935\n",
      "three-layer Loss after iteration 56000: 3.65097761897424\n",
      "three-layer Loss after iteration 57000: 3.6505298608351433\n",
      "three-layer Loss after iteration 58000: 3.6500886540614172\n",
      "three-layer Loss after iteration 59000: 3.6496145021746087\n",
      "three-layer Loss after iteration 60000: 3.6490810658176653\n",
      "three-layer Loss after iteration 61000: 3.6485868281432037\n",
      "three-layer Loss after iteration 62000: 3.648059607641999\n",
      "three-layer Loss after iteration 63000: 3.64754120428668\n",
      "three-layer Loss after iteration 64000: 3.647062456060443\n",
      "three-layer Loss after iteration 65000: 3.6465296010912613\n",
      "three-layer Loss after iteration 66000: 3.645535265254077\n",
      "three-layer Loss after iteration 67000: 3.6445150261318444\n",
      "three-layer Loss after iteration 68000: 3.643609629024886\n",
      "three-layer Loss after iteration 69000: 3.6429773951232365\n",
      "three-layer Loss after iteration 70000: 3.6424166845171695\n",
      "three-layer Loss after iteration 71000: 3.6418891434396423\n",
      "three-layer Loss after iteration 72000: 3.6413621105131186\n",
      "three-layer Loss after iteration 73000: 3.640835788233473\n",
      "three-layer Loss after iteration 74000: 3.640317006870194\n",
      "three-layer Loss after iteration 75000: 3.6397832854216885\n",
      "three-layer Loss after iteration 76000: 3.6392464913586657\n",
      "three-layer Loss after iteration 77000: 3.638685070241843\n",
      "three-layer Loss after iteration 78000: 3.638258920898396\n",
      "three-layer Loss after iteration 79000: 3.6378994430040743\n",
      "9.880492349157088e-05 3.638258920898396 3.6378994430040743\n",
      "three-layer Loss after iteration 0: 1472.7740427126507\n",
      "three-layer Loss after iteration 1000: 29.86162117609544\n",
      "three-layer Loss after iteration 2000: 25.86030120789585\n",
      "three-layer Loss after iteration 3000: 21.549905532056727\n",
      "three-layer Loss after iteration 4000: 16.591060506932862\n",
      "three-layer Loss after iteration 5000: 15.377578959677546\n",
      "three-layer Loss after iteration 6000: 13.578571508447196\n",
      "three-layer Loss after iteration 7000: 12.474762590471551\n",
      "three-layer Loss after iteration 8000: 11.64663213202291\n",
      "three-layer Loss after iteration 9000: 11.075197167010556\n",
      "three-layer Loss after iteration 10000: 10.649977502688586\n",
      "three-layer Loss after iteration 11000: 10.508068806447028\n",
      "three-layer Loss after iteration 12000: 10.36580636036467\n",
      "three-layer Loss after iteration 13000: 9.933141405526463\n",
      "three-layer Loss after iteration 14000: 12.355005140911462\n",
      "three-layer Loss after iteration 15000: 12.29777786503772\n",
      "three-layer Loss after iteration 16000: 14.076977601467423\n",
      "three-layer Loss after iteration 17000: 13.01206240760657\n",
      "three-layer Loss after iteration 18000: 12.90883854868857\n",
      "three-layer Loss after iteration 19000: 12.894940821815172\n",
      "three-layer Loss after iteration 20000: 12.585966287063387\n",
      "three-layer Loss after iteration 21000: 12.21323793943258\n",
      "three-layer Loss after iteration 22000: 11.945332728403901\n",
      "three-layer Loss after iteration 23000: 11.873269886734702\n",
      "three-layer Loss after iteration 24000: 11.683516285533612\n",
      "three-layer Loss after iteration 25000: 11.204799855478491\n",
      "three-layer Loss after iteration 26000: 10.746630653072433\n",
      "three-layer Loss after iteration 27000: 10.439181732044016\n",
      "three-layer Loss after iteration 28000: 10.233359371581622\n",
      "three-layer Loss after iteration 29000: 10.22462051822334\n",
      "three-layer Loss after iteration 30000: 9.806025113294861\n",
      "three-layer Loss after iteration 31000: 9.681839376657345\n",
      "three-layer Loss after iteration 32000: 9.440582033887607\n",
      "three-layer Loss after iteration 33000: 9.461041112554865\n",
      "three-layer Loss after iteration 34000: 9.30616189911717\n",
      "three-layer Loss after iteration 35000: 8.970928900529735\n",
      "three-layer Loss after iteration 36000: 8.883942848676956\n",
      "three-layer Loss after iteration 37000: 8.794450251234633\n",
      "three-layer Loss after iteration 38000: 8.613759138403084\n",
      "three-layer Loss after iteration 39000: 8.331985616997667\n",
      "three-layer Loss after iteration 40000: 8.996916741059826\n",
      "three-layer Loss after iteration 41000: 7.49837494486172\n",
      "three-layer Loss after iteration 42000: 7.157842354214592\n",
      "three-layer Loss after iteration 43000: 7.4440840739991385\n",
      "three-layer Loss after iteration 44000: 7.481511848363762\n",
      "three-layer Loss after iteration 45000: 7.903218061930676\n",
      "three-layer Loss after iteration 46000: 8.312018590715514\n",
      "three-layer Loss after iteration 47000: 8.25292299555618\n",
      "three-layer Loss after iteration 48000: 8.271271701157353\n",
      "three-layer Loss after iteration 49000: 8.194376467346972\n",
      "three-layer Loss after iteration 50000: 8.113169780506208\n",
      "three-layer Loss after iteration 51000: 8.040108378796493\n",
      "three-layer Loss after iteration 52000: 8.03106311837616\n",
      "three-layer Loss after iteration 53000: 7.972646938847791\n",
      "three-layer Loss after iteration 54000: 7.886005157352985\n",
      "three-layer Loss after iteration 55000: 7.872337523223071\n",
      "three-layer Loss after iteration 56000: 7.831512983090486\n",
      "three-layer Loss after iteration 57000: 7.763848341393548\n",
      "three-layer Loss after iteration 58000: 7.678900054453461\n",
      "three-layer Loss after iteration 59000: 7.67363684311247\n",
      "three-layer Loss after iteration 60000: 7.586135182512977\n",
      "three-layer Loss after iteration 61000: 7.57844722174357\n",
      "three-layer Loss after iteration 62000: 7.530497013788596\n",
      "three-layer Loss after iteration 63000: 7.509965842289842\n",
      "three-layer Loss after iteration 64000: 7.438958078652239\n",
      "three-layer Loss after iteration 65000: 7.433883947268158\n",
      "three-layer Loss after iteration 66000: 7.346479432083495\n",
      "three-layer Loss after iteration 67000: 7.324433044539027\n",
      "three-layer Loss after iteration 68000: 7.297616598727224\n",
      "three-layer Loss after iteration 69000: 7.291546393610406\n",
      "three-layer Loss after iteration 70000: 7.207691895213014\n",
      "three-layer Loss after iteration 71000: 7.356251752267581\n",
      "three-layer Loss after iteration 72000: 7.297558515984662\n",
      "three-layer Loss after iteration 73000: 7.275335621569061\n",
      "three-layer Loss after iteration 74000: 7.275345492905362\n",
      "1.356822119807874e-06 7.275335621569061 7.275345492905362\n",
      "three-layer Loss after iteration 0: 1210.8907398796903\n",
      "three-layer Loss after iteration 1000: 23.262247138912578\n",
      "three-layer Loss after iteration 2000: 12.216999137224452\n",
      "three-layer Loss after iteration 3000: 9.918635109437867\n",
      "three-layer Loss after iteration 4000: 10.554587938492348\n",
      "three-layer Loss after iteration 5000: 8.755943646708026\n",
      "three-layer Loss after iteration 6000: 8.806271562360061\n",
      "three-layer Loss after iteration 7000: 7.8496325732942225\n",
      "three-layer Loss after iteration 8000: 6.421550734851491\n",
      "three-layer Loss after iteration 9000: 6.552543116692858\n",
      "three-layer Loss after iteration 10000: 6.2063748088256965\n",
      "three-layer Loss after iteration 11000: 5.9371738086630685\n",
      "three-layer Loss after iteration 12000: 5.182200616119946\n",
      "three-layer Loss after iteration 13000: 6.108082854184134\n",
      "three-layer Loss after iteration 14000: 5.3647669839795755\n",
      "three-layer Loss after iteration 15000: 4.505103668510721\n",
      "three-layer Loss after iteration 16000: 4.830940618129854\n",
      "three-layer Loss after iteration 17000: 6.831996470347268\n",
      "three-layer Loss after iteration 18000: 3.5495091276987183\n",
      "three-layer Loss after iteration 19000: 3.5218995701458384\n",
      "three-layer Loss after iteration 20000: 6.069808195474045\n",
      "three-layer Loss after iteration 21000: 3.515926683489412\n",
      "three-layer Loss after iteration 22000: 3.4923495102993027\n",
      "three-layer Loss after iteration 23000: 3.4831278170110256\n",
      "three-layer Loss after iteration 24000: 3.4697420476737135\n",
      "three-layer Loss after iteration 25000: 3.8131408803787923\n",
      "three-layer Loss after iteration 26000: 4.498441739954404\n",
      "three-layer Loss after iteration 27000: 4.141567368548164\n",
      "three-layer Loss after iteration 28000: 4.051111319314151\n",
      "three-layer Loss after iteration 29000: 4.052422332825143\n",
      "three-layer Loss after iteration 30000: 4.0484426174794095\n",
      "three-layer Loss after iteration 31000: 3.9195173758650936\n",
      "three-layer Loss after iteration 32000: 3.8898510047169497\n",
      "three-layer Loss after iteration 33000: 3.8615456613111614\n",
      "three-layer Loss after iteration 34000: 3.7770173096261437\n",
      "three-layer Loss after iteration 35000: 3.826647172391074\n",
      "three-layer Loss after iteration 36000: 3.8161600988024484\n",
      "three-layer Loss after iteration 37000: 3.801459722195717\n",
      "three-layer Loss after iteration 38000: 3.877970764632051\n",
      "three-layer Loss after iteration 39000: 3.540941883950338\n",
      "three-layer Loss after iteration 40000: 3.6504057890922343\n",
      "three-layer Loss after iteration 41000: 3.625018155724387\n",
      "three-layer Loss after iteration 42000: 3.4967996345750567\n",
      "three-layer Loss after iteration 43000: 3.3043801543744995\n",
      "three-layer Loss after iteration 44000: 3.6033245164920737\n",
      "three-layer Loss after iteration 45000: 3.4728652518285688\n",
      "three-layer Loss after iteration 46000: 3.404771703240844\n",
      "three-layer Loss after iteration 47000: 3.3603767723750857\n",
      "three-layer Loss after iteration 48000: 3.383981583594241\n",
      "three-layer Loss after iteration 49000: 3.1541711180837972\n",
      "three-layer Loss after iteration 50000: 3.1251808535389514\n",
      "three-layer Loss after iteration 51000: 3.1339803530408448\n",
      "three-layer Loss after iteration 52000: 3.273297951372552\n",
      "three-layer Loss after iteration 53000: 3.5707834223539887\n",
      "three-layer Loss after iteration 54000: 3.176936032832259\n",
      "three-layer Loss after iteration 55000: 3.494826356273683\n",
      "three-layer Loss after iteration 56000: 3.3789298815958926\n",
      "three-layer Loss after iteration 57000: 3.31931612755512\n",
      "three-layer Loss after iteration 58000: 3.3576372420063287\n",
      "three-layer Loss after iteration 59000: 3.2331002746866577\n",
      "three-layer Loss after iteration 60000: 3.3331607151281673\n",
      "three-layer Loss after iteration 61000: 3.322986318091059\n",
      "three-layer Loss after iteration 62000: 3.3270889162975297\n",
      "three-layer Loss after iteration 63000: 3.3269489985532434\n",
      "4.205410429545635e-05 3.3270889162975297 3.3269489985532434\n",
      "three-layer Loss after iteration 0: 1755.095328402296\n",
      "three-layer Loss after iteration 1000: 22.189584358827677\n",
      "three-layer Loss after iteration 2000: 18.05148093770286\n",
      "three-layer Loss after iteration 3000: 16.490889676471134\n",
      "three-layer Loss after iteration 4000: 14.932505712402728\n",
      "three-layer Loss after iteration 5000: 13.304625359906353\n",
      "three-layer Loss after iteration 6000: 12.475001981440316\n",
      "three-layer Loss after iteration 7000: 11.7733837840763\n",
      "three-layer Loss after iteration 8000: 10.210405701974361\n",
      "three-layer Loss after iteration 9000: 9.061358464688396\n",
      "three-layer Loss after iteration 10000: 9.640981991208912\n",
      "three-layer Loss after iteration 11000: 7.325591450756896\n",
      "three-layer Loss after iteration 12000: 6.962204827389521\n",
      "three-layer Loss after iteration 13000: 7.818445729142117\n",
      "three-layer Loss after iteration 14000: 7.323210335395011\n",
      "three-layer Loss after iteration 15000: 6.424641220005637\n",
      "three-layer Loss after iteration 16000: 5.622388529131438\n",
      "three-layer Loss after iteration 17000: 4.979885651933347\n",
      "three-layer Loss after iteration 18000: 4.9576196417699085\n",
      "three-layer Loss after iteration 19000: 6.161491352834011\n",
      "three-layer Loss after iteration 20000: 5.725352430156897\n",
      "three-layer Loss after iteration 21000: 12.007976571851518\n",
      "three-layer Loss after iteration 22000: 10.153368765741828\n",
      "three-layer Loss after iteration 23000: 4.977102662543378\n",
      "three-layer Loss after iteration 24000: 4.727430356875346\n",
      "three-layer Loss after iteration 25000: 4.757654531661837\n",
      "three-layer Loss after iteration 26000: 5.589776771696856\n",
      "three-layer Loss after iteration 27000: 9.219855194416885\n",
      "three-layer Loss after iteration 28000: 5.43690079066568\n",
      "three-layer Loss after iteration 29000: 4.75926608770069\n",
      "three-layer Loss after iteration 30000: 4.679144348469199\n",
      "three-layer Loss after iteration 31000: 4.719922937079494\n",
      "three-layer Loss after iteration 32000: 6.766309372312684\n",
      "three-layer Loss after iteration 33000: 4.642292021431592\n",
      "three-layer Loss after iteration 34000: 7.066729853650641\n",
      "three-layer Loss after iteration 35000: 4.630823351634879\n",
      "three-layer Loss after iteration 36000: 6.892316042812051\n",
      "three-layer Loss after iteration 37000: 4.763980537927851\n",
      "three-layer Loss after iteration 38000: 4.706694042226608\n",
      "three-layer Loss after iteration 39000: 5.579624350446881\n",
      "three-layer Loss after iteration 40000: 4.992773405531279\n",
      "three-layer Loss after iteration 41000: 4.715191073911567\n",
      "three-layer Loss after iteration 42000: 5.072767117733585\n",
      "three-layer Loss after iteration 43000: 5.091917280608376\n",
      "three-layer Loss after iteration 44000: 4.811556214293305\n",
      "three-layer Loss after iteration 45000: 4.809048845267865\n",
      "three-layer Loss after iteration 46000: 4.964480937467998\n",
      "three-layer Loss after iteration 47000: 4.84142662541994\n",
      "three-layer Loss after iteration 48000: 4.844241359274598\n",
      "three-layer Loss after iteration 49000: 4.789698881117536\n",
      "three-layer Loss after iteration 50000: 4.887703631679212\n",
      "three-layer Loss after iteration 51000: 4.770838666927843\n",
      "three-layer Loss after iteration 52000: 4.808365736190193\n",
      "three-layer Loss after iteration 53000: 4.783146334437614\n",
      "three-layer Loss after iteration 54000: 4.810798544326185\n",
      "three-layer Loss after iteration 55000: 4.756874327137856\n",
      "three-layer Loss after iteration 56000: 4.739679032602306\n",
      "three-layer Loss after iteration 57000: 4.794966305574306\n",
      "three-layer Loss after iteration 58000: 4.700533565492665\n",
      "three-layer Loss after iteration 59000: 4.785885799875283\n",
      "three-layer Loss after iteration 60000: 4.696632740365399\n",
      "three-layer Loss after iteration 61000: 4.686634858549686\n",
      "three-layer Loss after iteration 62000: 4.763648264287478\n",
      "three-layer Loss after iteration 63000: 4.680511071869947\n",
      "three-layer Loss after iteration 64000: 4.670929735288849\n",
      "three-layer Loss after iteration 65000: 4.71340372528839\n",
      "three-layer Loss after iteration 66000: 4.738202630892747\n",
      "three-layer Loss after iteration 67000: 4.6623508304721755\n",
      "three-layer Loss after iteration 68000: 4.656618216452348\n",
      "three-layer Loss after iteration 69000: 4.657598673964876\n",
      "three-layer Loss after iteration 70000: 4.666302166493185\n",
      "three-layer Loss after iteration 71000: 4.7061147200078715\n",
      "three-layer Loss after iteration 72000: 4.714881646597335\n",
      "three-layer Loss after iteration 73000: 4.683339564398197\n",
      "three-layer Loss after iteration 74000: 4.661631060631831\n",
      "three-layer Loss after iteration 75000: 4.6538862264296625\n",
      "three-layer Loss after iteration 76000: 4.653638691089553\n",
      "5.318895393362638e-05 4.6538862264296625 4.653638691089553\n",
      "three-layer Loss after iteration 0: 1680.8512384984658\n",
      "three-layer Loss after iteration 1000: 41.16560119459017\n",
      "three-layer Loss after iteration 2000: 18.2421127051985\n",
      "three-layer Loss after iteration 3000: 16.40720422447821\n",
      "three-layer Loss after iteration 4000: 15.120725241118391\n",
      "three-layer Loss after iteration 5000: 12.881746708337033\n",
      "three-layer Loss after iteration 6000: 12.741649881051623\n",
      "three-layer Loss after iteration 7000: 11.343610360849775\n",
      "three-layer Loss after iteration 8000: 10.717848533154195\n",
      "three-layer Loss after iteration 9000: 8.209673216388245\n",
      "three-layer Loss after iteration 10000: 8.922622703561395\n",
      "three-layer Loss after iteration 11000: 8.159672994129101\n",
      "three-layer Loss after iteration 12000: 8.003177047627574\n",
      "three-layer Loss after iteration 13000: 7.470907456661551\n",
      "three-layer Loss after iteration 14000: 7.230144579551592\n",
      "three-layer Loss after iteration 15000: 10.926007346505015\n",
      "three-layer Loss after iteration 16000: 4.670004034081636\n",
      "three-layer Loss after iteration 17000: 5.697280211415\n",
      "three-layer Loss after iteration 18000: 5.46033430377388\n",
      "three-layer Loss after iteration 19000: 5.2231248886906485\n",
      "three-layer Loss after iteration 20000: 6.824057798881751\n",
      "three-layer Loss after iteration 21000: 6.3403672104468365\n",
      "three-layer Loss after iteration 22000: 9.19891552612436\n",
      "three-layer Loss after iteration 23000: 7.443477880452877\n",
      "three-layer Loss after iteration 24000: 6.771876984663053\n",
      "three-layer Loss after iteration 25000: 5.284693885980092\n",
      "three-layer Loss after iteration 26000: 5.611928163178179\n",
      "three-layer Loss after iteration 27000: 4.4014930977742255\n",
      "three-layer Loss after iteration 28000: 4.46302858838839\n",
      "three-layer Loss after iteration 29000: 4.311461272853429\n",
      "three-layer Loss after iteration 30000: 4.238837724844692\n",
      "three-layer Loss after iteration 31000: 4.248284072456139\n",
      "three-layer Loss after iteration 32000: 4.145163330312329\n",
      "three-layer Loss after iteration 33000: 4.113708153868454\n",
      "three-layer Loss after iteration 34000: 3.5410424003590397\n",
      "three-layer Loss after iteration 35000: 3.177937771038679\n",
      "three-layer Loss after iteration 36000: 2.9709296544159205\n",
      "three-layer Loss after iteration 37000: 3.219067837559062\n",
      "three-layer Loss after iteration 38000: 3.530394246408091\n",
      "three-layer Loss after iteration 39000: 3.9502524535480714\n",
      "three-layer Loss after iteration 40000: 3.765281716190878\n",
      "three-layer Loss after iteration 41000: 3.7934618658853267\n",
      "three-layer Loss after iteration 42000: 3.673662314235645\n",
      "three-layer Loss after iteration 43000: 3.687219330737293\n",
      "three-layer Loss after iteration 44000: 3.5802258739513135\n",
      "three-layer Loss after iteration 45000: 3.5388150748660743\n",
      "three-layer Loss after iteration 46000: 3.5579959676702173\n",
      "three-layer Loss after iteration 47000: 3.539756648865925\n",
      "three-layer Loss after iteration 48000: 3.57200815365489\n",
      "three-layer Loss after iteration 49000: 3.4145145846572977\n",
      "three-layer Loss after iteration 50000: 3.453286843825916\n",
      "three-layer Loss after iteration 51000: 3.089413451218735\n",
      "three-layer Loss after iteration 52000: 3.477895685478926\n",
      "three-layer Loss after iteration 53000: 3.2857621632126444\n",
      "three-layer Loss after iteration 54000: 3.040595849901322\n",
      "three-layer Loss after iteration 55000: 3.5943305918641073\n",
      "three-layer Loss after iteration 56000: 3.2157820432041566\n",
      "three-layer Loss after iteration 57000: 2.9299363207029465\n",
      "three-layer Loss after iteration 58000: 3.046975308331698\n",
      "three-layer Loss after iteration 59000: 3.0873415106502105\n",
      "three-layer Loss after iteration 60000: 3.1774856532632554\n",
      "three-layer Loss after iteration 61000: 3.0006635163070374\n",
      "three-layer Loss after iteration 62000: 3.2678834678652127\n",
      "three-layer Loss after iteration 63000: 2.6956139585703855\n",
      "three-layer Loss after iteration 64000: 3.5465467860327515\n",
      "three-layer Loss after iteration 65000: 3.3078067271621148\n",
      "three-layer Loss after iteration 66000: 2.8996033252171025\n",
      "three-layer Loss after iteration 67000: 2.9697494402571762\n",
      "three-layer Loss after iteration 68000: 3.026399424546488\n",
      "three-layer Loss after iteration 69000: 2.7938752237284588\n",
      "three-layer Loss after iteration 70000: 2.9585638334548365\n",
      "three-layer Loss after iteration 71000: 2.824917298287984\n",
      "three-layer Loss after iteration 72000: 2.788071550650245\n",
      "three-layer Loss after iteration 73000: 3.008438557337559\n",
      "three-layer Loss after iteration 74000: 3.204289039778418\n",
      "three-layer Loss after iteration 75000: 2.8542971208089534\n",
      "three-layer Loss after iteration 76000: 2.5766405660694685\n",
      "three-layer Loss after iteration 77000: 2.475332282320546\n",
      "three-layer Loss after iteration 78000: 2.436331533957363\n",
      "three-layer Loss after iteration 79000: 2.6580861774012243\n",
      "three-layer Loss after iteration 80000: 2.676816321252164\n",
      "three-layer Loss after iteration 81000: 2.409035778450744\n",
      "three-layer Loss after iteration 82000: 2.7882284220385047\n",
      "three-layer Loss after iteration 83000: 2.781193730058514\n",
      "three-layer Loss after iteration 84000: 2.6025044642396487\n",
      "three-layer Loss after iteration 85000: 2.9751362831609813\n",
      "three-layer Loss after iteration 86000: 2.4167575178364626\n",
      "three-layer Loss after iteration 87000: 2.6611372292222115\n",
      "three-layer Loss after iteration 88000: 2.167388814540807\n",
      "three-layer Loss after iteration 89000: 2.3583017463120153\n",
      "three-layer Loss after iteration 90000: 2.407162912775213\n",
      "three-layer Loss after iteration 91000: 2.828519054457883\n",
      "three-layer Loss after iteration 92000: 2.3956275944618333\n",
      "three-layer Loss after iteration 93000: 2.1835063781178086\n",
      "three-layer Loss after iteration 94000: 3.1938508953898754\n",
      "three-layer Loss after iteration 95000: 2.210524792642487\n",
      "three-layer Loss after iteration 96000: 2.972377323139958\n",
      "three-layer Loss after iteration 97000: 2.2251718904100732\n",
      "three-layer Loss after iteration 98000: 2.7046223054561516\n",
      "three-layer Loss after iteration 99000: 2.6738579400352975\n",
      "three-layer Loss after iteration 0: 1668.1444758375046\n",
      "three-layer Loss after iteration 1000: 11.52753987924189\n",
      "three-layer Loss after iteration 2000: 9.387832315205928\n",
      "three-layer Loss after iteration 3000: 9.031788145479043\n",
      "three-layer Loss after iteration 4000: 6.228692288634188\n",
      "three-layer Loss after iteration 5000: 5.887376860044965\n",
      "three-layer Loss after iteration 6000: 5.410266458804367\n",
      "three-layer Loss after iteration 7000: 5.374285085996831\n",
      "three-layer Loss after iteration 8000: 4.802264343609293\n",
      "three-layer Loss after iteration 9000: 4.586977317090698\n",
      "three-layer Loss after iteration 10000: 4.477319536062883\n",
      "three-layer Loss after iteration 11000: 4.325664332735458\n",
      "three-layer Loss after iteration 12000: 4.264894202045409\n",
      "three-layer Loss after iteration 13000: 4.22660296854524\n",
      "three-layer Loss after iteration 14000: 4.202330356517274\n",
      "three-layer Loss after iteration 15000: 4.199548221728694\n",
      "three-layer Loss after iteration 16000: 4.168469043342252\n",
      "three-layer Loss after iteration 17000: 4.17612199616514\n",
      "three-layer Loss after iteration 18000: 4.166138715395476\n",
      "three-layer Loss after iteration 19000: 4.154756678559332\n",
      "three-layer Loss after iteration 20000: 4.193040734909028\n",
      "three-layer Loss after iteration 21000: 4.128973883757314\n",
      "three-layer Loss after iteration 22000: 4.136899209557142\n",
      "three-layer Loss after iteration 23000: 4.037951505380563\n",
      "three-layer Loss after iteration 24000: 3.9951059149971764\n",
      "three-layer Loss after iteration 25000: 3.932091323344997\n",
      "three-layer Loss after iteration 26000: 3.922416269674085\n",
      "three-layer Loss after iteration 27000: 3.8854094649996114\n",
      "three-layer Loss after iteration 28000: 3.8658246398138383\n",
      "three-layer Loss after iteration 29000: 3.8338590303081577\n",
      "three-layer Loss after iteration 30000: 3.81424728251798\n",
      "three-layer Loss after iteration 31000: 3.793788762440668\n",
      "three-layer Loss after iteration 32000: 3.772872189222548\n",
      "three-layer Loss after iteration 33000: 3.7598484789799107\n",
      "three-layer Loss after iteration 34000: 3.74969022651301\n",
      "three-layer Loss after iteration 35000: 3.738946184219195\n",
      "three-layer Loss after iteration 36000: 3.7289612923377233\n",
      "three-layer Loss after iteration 37000: 3.720312926862896\n",
      "three-layer Loss after iteration 38000: 3.711588463354656\n",
      "three-layer Loss after iteration 39000: 3.7024253666193423\n",
      "three-layer Loss after iteration 40000: 3.694520479772312\n",
      "three-layer Loss after iteration 41000: 3.686749986308508\n",
      "three-layer Loss after iteration 42000: 3.679730246641175\n",
      "three-layer Loss after iteration 43000: 3.6732207729366495\n",
      "three-layer Loss after iteration 44000: 3.6667420682226677\n",
      "three-layer Loss after iteration 45000: 3.6607717693842368\n",
      "three-layer Loss after iteration 46000: 3.654891103913954\n",
      "three-layer Loss after iteration 47000: 3.6493629326396197\n",
      "three-layer Loss after iteration 48000: 3.644231224790356\n",
      "three-layer Loss after iteration 49000: 3.6389445298062713\n",
      "three-layer Loss after iteration 50000: 3.634130188163484\n",
      "three-layer Loss after iteration 51000: 3.629416782104691\n",
      "three-layer Loss after iteration 52000: 3.6246963269469417\n",
      "three-layer Loss after iteration 53000: 3.6202201646547434\n",
      "three-layer Loss after iteration 54000: 3.6159013913064335\n",
      "three-layer Loss after iteration 55000: 3.6116641975803656\n",
      "three-layer Loss after iteration 56000: 3.606852110187931\n",
      "three-layer Loss after iteration 57000: 3.603695216663733\n",
      "three-layer Loss after iteration 58000: 3.6000358158540147\n",
      "three-layer Loss after iteration 59000: 3.596121936420647\n",
      "three-layer Loss after iteration 60000: 3.592568803901983\n",
      "three-layer Loss after iteration 61000: 3.5885918253019105\n",
      "three-layer Loss after iteration 62000: 3.585300266985483\n",
      "three-layer Loss after iteration 63000: 3.5818727069847744\n",
      "three-layer Loss after iteration 64000: 3.5785321215859627\n",
      "three-layer Loss after iteration 65000: 3.5754279113308782\n",
      "three-layer Loss after iteration 66000: 3.5724180394034524\n",
      "three-layer Loss after iteration 67000: 3.569335271577353\n",
      "three-layer Loss after iteration 68000: 3.5664642211333466\n",
      "three-layer Loss after iteration 69000: 3.563668344370077\n",
      "three-layer Loss after iteration 70000: 3.5609018515510122\n",
      "three-layer Loss after iteration 71000: 3.5581652399962023\n",
      "three-layer Loss after iteration 72000: 3.5554991132881106\n",
      "three-layer Loss after iteration 73000: 3.552896232211362\n",
      "three-layer Loss after iteration 74000: 3.55032120732354\n",
      "three-layer Loss after iteration 75000: 3.5478321996354634\n",
      "three-layer Loss after iteration 76000: 3.545351079530905\n",
      "three-layer Loss after iteration 77000: 3.5429490227166034\n",
      "three-layer Loss after iteration 78000: 3.540642750398147\n",
      "three-layer Loss after iteration 79000: 3.538325693167458\n",
      "three-layer Loss after iteration 80000: 3.5360819349227093\n",
      "three-layer Loss after iteration 81000: 3.5339020897812876\n",
      "three-layer Loss after iteration 82000: 3.5317815576229483\n",
      "three-layer Loss after iteration 83000: 3.5296566475174087\n",
      "three-layer Loss after iteration 84000: 3.527596978594968\n",
      "three-layer Loss after iteration 85000: 3.525553428867224\n",
      "three-layer Loss after iteration 86000: 3.5235656878586674\n",
      "three-layer Loss after iteration 87000: 3.5216013409354603\n",
      "three-layer Loss after iteration 88000: 3.5197211566009585\n",
      "three-layer Loss after iteration 89000: 3.5178266520105796\n",
      "three-layer Loss after iteration 90000: 3.5159910453960665\n",
      "three-layer Loss after iteration 91000: 3.5141999093384544\n",
      "three-layer Loss after iteration 92000: 3.5124531439267552\n",
      "three-layer Loss after iteration 93000: 3.5107385204907837\n",
      "three-layer Loss after iteration 94000: 3.5090333665759488\n",
      "three-layer Loss after iteration 95000: 3.507403931928674\n",
      "three-layer Loss after iteration 96000: 3.50580709189951\n",
      "three-layer Loss after iteration 97000: 3.5042544210945548\n",
      "three-layer Loss after iteration 98000: 3.502668630547814\n",
      "three-layer Loss after iteration 99000: 3.5012118317153695\n",
      "three-layer Loss after iteration 0: 1635.7767961143243\n",
      "three-layer Loss after iteration 1000: 12.8512720783083\n",
      "three-layer Loss after iteration 2000: 13.688669447643479\n",
      "three-layer Loss after iteration 3000: 14.434699560666393\n",
      "three-layer Loss after iteration 4000: 11.619321561204528\n",
      "three-layer Loss after iteration 5000: 9.670321687284451\n",
      "three-layer Loss after iteration 6000: 9.312662590228008\n",
      "three-layer Loss after iteration 7000: 8.721841422549987\n",
      "three-layer Loss after iteration 8000: 10.302255748547836\n",
      "three-layer Loss after iteration 9000: 8.09623000895314\n",
      "three-layer Loss after iteration 10000: 8.475085719646485\n",
      "three-layer Loss after iteration 11000: 7.751788642883528\n",
      "three-layer Loss after iteration 12000: 8.462693103078426\n",
      "three-layer Loss after iteration 13000: 7.782849437306346\n",
      "three-layer Loss after iteration 14000: 8.756483730697633\n",
      "three-layer Loss after iteration 15000: 11.322406190310435\n",
      "three-layer Loss after iteration 16000: 6.951005062491091\n",
      "three-layer Loss after iteration 17000: 5.801672794795524\n",
      "three-layer Loss after iteration 18000: 5.321508799778828\n",
      "three-layer Loss after iteration 19000: 5.102006886440246\n",
      "three-layer Loss after iteration 20000: 4.93560376214545\n",
      "three-layer Loss after iteration 21000: 4.811533141611878\n",
      "three-layer Loss after iteration 22000: 4.749904503165884\n",
      "three-layer Loss after iteration 23000: 4.670037232973561\n",
      "three-layer Loss after iteration 24000: 4.586493154181439\n",
      "three-layer Loss after iteration 25000: 4.518345249660645\n",
      "three-layer Loss after iteration 26000: 4.433398663812435\n",
      "three-layer Loss after iteration 27000: 4.410301142767702\n",
      "three-layer Loss after iteration 28000: 4.291259892743281\n",
      "three-layer Loss after iteration 29000: 4.322410159808867\n",
      "three-layer Loss after iteration 30000: 4.282090120298037\n",
      "three-layer Loss after iteration 31000: 4.244596402086443\n",
      "three-layer Loss after iteration 32000: 4.220461595368798\n",
      "three-layer Loss after iteration 33000: 4.1990462456581055\n",
      "three-layer Loss after iteration 34000: 4.176034211570784\n",
      "three-layer Loss after iteration 35000: 4.235698915710781\n",
      "three-layer Loss after iteration 36000: 4.4375217227188495\n",
      "three-layer Loss after iteration 37000: 4.347711229995339\n",
      "three-layer Loss after iteration 38000: 4.147108939248443\n",
      "three-layer Loss after iteration 39000: 4.1334942997496285\n",
      "three-layer Loss after iteration 40000: 4.129587623117535\n",
      "three-layer Loss after iteration 41000: 4.0887807243625875\n",
      "three-layer Loss after iteration 42000: 4.082385650198615\n",
      "three-layer Loss after iteration 43000: 4.075987058691418\n",
      "three-layer Loss after iteration 44000: 4.054683422846583\n",
      "three-layer Loss after iteration 45000: 4.0561999054101125\n",
      "three-layer Loss after iteration 46000: 4.0500451762139225\n",
      "three-layer Loss after iteration 47000: 4.171877258084264\n",
      "three-layer Loss after iteration 48000: 4.186209964704192\n",
      "three-layer Loss after iteration 49000: 4.185027498066208\n",
      "three-layer Loss after iteration 50000: 4.120853939420913\n",
      "three-layer Loss after iteration 51000: 4.141584953100702\n",
      "three-layer Loss after iteration 52000: 4.157343466102502\n",
      "three-layer Loss after iteration 53000: 4.134058270583931\n",
      "three-layer Loss after iteration 54000: 4.117976299990913\n",
      "three-layer Loss after iteration 55000: 4.104915138692742\n",
      "three-layer Loss after iteration 56000: 4.08985131993756\n",
      "three-layer Loss after iteration 57000: 4.082255184858609\n",
      "three-layer Loss after iteration 58000: 4.06991103654644\n",
      "three-layer Loss after iteration 59000: 4.038172224028524\n",
      "three-layer Loss after iteration 60000: 4.038384484205147\n",
      "5.256342841441235e-05 4.038172224028524 4.038384484205147\n",
      "three-layer Loss after iteration 0: 1277.8089192499917\n",
      "three-layer Loss after iteration 1000: 9.688632138311752\n",
      "three-layer Loss after iteration 2000: 9.023609691235178\n",
      "three-layer Loss after iteration 3000: 7.34117268402127\n",
      "three-layer Loss after iteration 4000: 6.4269299891094915\n",
      "three-layer Loss after iteration 5000: 5.6058957302799435\n",
      "three-layer Loss after iteration 6000: 5.128734040355818\n",
      "three-layer Loss after iteration 7000: 4.658518065686362\n",
      "three-layer Loss after iteration 8000: 4.1902595577511645\n",
      "three-layer Loss after iteration 9000: 4.2058645206444325\n",
      "three-layer Loss after iteration 10000: 3.993352121249967\n",
      "three-layer Loss after iteration 11000: 3.326295845945511\n",
      "three-layer Loss after iteration 12000: 3.2622898490533\n",
      "three-layer Loss after iteration 13000: 3.2071987019462656\n",
      "three-layer Loss after iteration 14000: 3.1719586066238192\n",
      "three-layer Loss after iteration 15000: 3.2219798619344417\n",
      "three-layer Loss after iteration 16000: 3.158565371324121\n",
      "three-layer Loss after iteration 17000: 3.1320330226639777\n",
      "three-layer Loss after iteration 18000: 3.1121647234184167\n",
      "three-layer Loss after iteration 19000: 3.0980159158988103\n",
      "three-layer Loss after iteration 20000: 3.0944372375724196\n",
      "three-layer Loss after iteration 21000: 3.0848469003450534\n",
      "three-layer Loss after iteration 22000: 3.081715911211294\n",
      "three-layer Loss after iteration 23000: 3.0793634400837515\n",
      "three-layer Loss after iteration 24000: 3.078536642211631\n",
      "three-layer Loss after iteration 25000: 3.077916244322953\n",
      "three-layer Loss after iteration 26000: 3.0776456592501362\n",
      "8.791177255584286e-05 3.077916244322953 3.0776456592501362\n",
      "three-layer Loss after iteration 0: 1544.1567589218175\n",
      "three-layer Loss after iteration 1000: 14.09005663664603\n",
      "three-layer Loss after iteration 2000: 12.834774405760433\n",
      "three-layer Loss after iteration 3000: 12.032734750824428\n",
      "three-layer Loss after iteration 4000: 10.604602594606826\n",
      "three-layer Loss after iteration 5000: 9.979898516036005\n",
      "three-layer Loss after iteration 6000: 10.141569629267991\n",
      "three-layer Loss after iteration 7000: 9.842027747809977\n",
      "three-layer Loss after iteration 8000: 10.86692479931971\n",
      "three-layer Loss after iteration 9000: 9.977706984090304\n",
      "three-layer Loss after iteration 10000: 8.665441589308593\n",
      "three-layer Loss after iteration 11000: 8.28372955273241\n",
      "three-layer Loss after iteration 12000: 7.647953057412687\n",
      "three-layer Loss after iteration 13000: 7.9498868567452865\n",
      "three-layer Loss after iteration 14000: 7.657888439729062\n",
      "three-layer Loss after iteration 15000: 7.849286543571824\n",
      "three-layer Loss after iteration 16000: 8.129930070008228\n",
      "three-layer Loss after iteration 17000: 7.079747730443799\n",
      "three-layer Loss after iteration 18000: 6.8333875964586275\n",
      "three-layer Loss after iteration 19000: 8.211951451657697\n",
      "three-layer Loss after iteration 20000: 7.5327056530199235\n",
      "three-layer Loss after iteration 21000: 8.993939203158844\n",
      "three-layer Loss after iteration 22000: 7.240676355231418\n",
      "three-layer Loss after iteration 23000: 7.550252434333316\n",
      "three-layer Loss after iteration 24000: 6.300536469031489\n",
      "three-layer Loss after iteration 25000: 9.55556779335884\n",
      "three-layer Loss after iteration 26000: 6.663790618293219\n",
      "three-layer Loss after iteration 27000: 6.663582240271976\n",
      "3.127019337469643e-05 6.663790618293219 6.663582240271976\n",
      "three-layer Loss after iteration 0: 1199.1044288356893\n",
      "three-layer Loss after iteration 1000: 30.480406761579943\n",
      "three-layer Loss after iteration 2000: 24.342159331678015\n",
      "three-layer Loss after iteration 3000: 13.913189180451745\n",
      "three-layer Loss after iteration 4000: 16.029969004192456\n",
      "three-layer Loss after iteration 5000: 16.87455281033593\n",
      "three-layer Loss after iteration 6000: 15.365560366752277\n",
      "three-layer Loss after iteration 7000: 12.432418715995473\n",
      "three-layer Loss after iteration 8000: 12.96706506967619\n",
      "three-layer Loss after iteration 9000: 9.551544140158176\n",
      "three-layer Loss after iteration 10000: 8.59568319132891\n",
      "three-layer Loss after iteration 11000: 8.806841824214134\n",
      "three-layer Loss after iteration 12000: 8.704027217326033\n",
      "three-layer Loss after iteration 13000: 8.993683572952937\n",
      "three-layer Loss after iteration 14000: 9.079160028969184\n",
      "three-layer Loss after iteration 15000: 9.255424672552149\n",
      "three-layer Loss after iteration 16000: 10.636994436741366\n",
      "three-layer Loss after iteration 17000: 8.519625594042093\n",
      "three-layer Loss after iteration 18000: 8.560896202781214\n",
      "three-layer Loss after iteration 19000: 8.554521692929614\n",
      "three-layer Loss after iteration 20000: 8.275642638268641\n",
      "three-layer Loss after iteration 21000: 8.001161339517733\n",
      "three-layer Loss after iteration 22000: 8.142412874145714\n",
      "three-layer Loss after iteration 23000: 8.373661720031361\n",
      "three-layer Loss after iteration 24000: 10.246917994589781\n",
      "three-layer Loss after iteration 25000: 11.157111604251869\n",
      "three-layer Loss after iteration 26000: 9.187902508364596\n",
      "three-layer Loss after iteration 27000: 9.343319261639051\n",
      "three-layer Loss after iteration 28000: 9.297449010197921\n",
      "three-layer Loss after iteration 29000: 6.640394842634084\n",
      "three-layer Loss after iteration 30000: 7.838438475348178\n",
      "three-layer Loss after iteration 31000: 7.8571735537998535\n",
      "three-layer Loss after iteration 32000: 6.732908873119898\n",
      "three-layer Loss after iteration 33000: 8.804947797672739\n",
      "three-layer Loss after iteration 34000: 8.905988364915965\n",
      "three-layer Loss after iteration 35000: 6.317318019984127\n",
      "three-layer Loss after iteration 36000: 8.093899930479239\n",
      "three-layer Loss after iteration 37000: 7.427064633877926\n",
      "three-layer Loss after iteration 38000: 7.1124795431454695\n",
      "three-layer Loss after iteration 39000: 6.8462462274447216\n",
      "three-layer Loss after iteration 40000: 7.116030462034513\n",
      "three-layer Loss after iteration 41000: 7.847882667584108\n",
      "three-layer Loss after iteration 42000: 8.038231608057309\n",
      "three-layer Loss after iteration 43000: 7.800934947466044\n",
      "three-layer Loss after iteration 44000: 7.771622323207128\n",
      "three-layer Loss after iteration 45000: 8.484489410441023\n",
      "three-layer Loss after iteration 46000: 7.778521753916429\n",
      "three-layer Loss after iteration 47000: 7.751670228147658\n",
      "three-layer Loss after iteration 48000: 7.685125134203866\n",
      "three-layer Loss after iteration 49000: 7.847282860523222\n",
      "three-layer Loss after iteration 50000: 7.7589441757686455\n",
      "three-layer Loss after iteration 51000: 7.924233909962736\n",
      "three-layer Loss after iteration 52000: 7.955099671729819\n",
      "three-layer Loss after iteration 53000: 7.73312777569492\n",
      "three-layer Loss after iteration 54000: 7.863919564329729\n",
      "three-layer Loss after iteration 55000: 7.699130717572152\n",
      "three-layer Loss after iteration 56000: 7.171983803862893\n",
      "three-layer Loss after iteration 57000: 7.9171759490387865\n",
      "three-layer Loss after iteration 58000: 8.026457661248847\n",
      "three-layer Loss after iteration 59000: 7.804486370911849\n",
      "three-layer Loss after iteration 60000: 7.902322219763362\n",
      "three-layer Loss after iteration 61000: 7.567254968859543\n",
      "three-layer Loss after iteration 62000: 7.403549467986975\n",
      "three-layer Loss after iteration 63000: 7.414001930273341\n",
      "three-layer Loss after iteration 64000: 7.244369224961679\n",
      "three-layer Loss after iteration 65000: 7.172562903960156\n",
      "three-layer Loss after iteration 66000: 7.215198681054365\n",
      "three-layer Loss after iteration 67000: 7.148952132213095\n",
      "three-layer Loss after iteration 68000: 7.131321113425531\n",
      "three-layer Loss after iteration 69000: 7.013187097588867\n",
      "three-layer Loss after iteration 70000: 6.800532699787806\n",
      "three-layer Loss after iteration 71000: 6.9463288299944175\n",
      "three-layer Loss after iteration 72000: 6.738642777904592\n",
      "three-layer Loss after iteration 73000: 6.82334949305545\n",
      "three-layer Loss after iteration 74000: 6.63760179425239\n",
      "three-layer Loss after iteration 75000: 6.783423390816238\n",
      "three-layer Loss after iteration 76000: 6.362181251214582\n",
      "three-layer Loss after iteration 77000: 6.123115030861665\n",
      "three-layer Loss after iteration 78000: 6.4077382479882425\n",
      "three-layer Loss after iteration 79000: 6.105299237102328\n",
      "three-layer Loss after iteration 80000: 6.203478809843834\n",
      "three-layer Loss after iteration 81000: 6.193608397283634\n",
      "three-layer Loss after iteration 82000: 6.204254419458778\n",
      "three-layer Loss after iteration 83000: 5.950982360173145\n",
      "three-layer Loss after iteration 84000: 6.143764162874166\n",
      "three-layer Loss after iteration 85000: 6.070725187856047\n",
      "three-layer Loss after iteration 86000: 5.570421642875814\n",
      "three-layer Loss after iteration 87000: 6.279598956640659\n",
      "three-layer Loss after iteration 88000: 6.026369780584766\n",
      "three-layer Loss after iteration 89000: 5.52982388173609\n",
      "three-layer Loss after iteration 90000: 5.734514509313625\n",
      "three-layer Loss after iteration 91000: 5.788959948110607\n",
      "three-layer Loss after iteration 92000: 5.860796565826388\n",
      "three-layer Loss after iteration 93000: 5.268229747454963\n",
      "three-layer Loss after iteration 94000: 5.705793146328834\n",
      "three-layer Loss after iteration 95000: 5.198089129849711\n",
      "three-layer Loss after iteration 96000: 5.824476217113663\n",
      "three-layer Loss after iteration 97000: 5.181813643202856\n",
      "three-layer Loss after iteration 98000: 5.48769157754683\n",
      "three-layer Loss after iteration 99000: 5.738767284963293\n",
      "three-layer Loss after iteration 0: 1671.4129463509864\n",
      "three-layer Loss after iteration 1000: 22.80592223259329\n",
      "three-layer Loss after iteration 2000: 29.44866396903328\n",
      "three-layer Loss after iteration 3000: 25.674577053269402\n",
      "three-layer Loss after iteration 4000: 25.82796059003073\n",
      "three-layer Loss after iteration 5000: 23.790911521686326\n",
      "three-layer Loss after iteration 6000: 22.312695829456967\n",
      "three-layer Loss after iteration 7000: 21.112463631056947\n",
      "three-layer Loss after iteration 8000: 19.876601882149654\n",
      "three-layer Loss after iteration 9000: 18.495867471449376\n",
      "three-layer Loss after iteration 10000: 17.773777911555\n",
      "three-layer Loss after iteration 11000: 17.137269898944943\n",
      "three-layer Loss after iteration 12000: 16.9810871471321\n",
      "three-layer Loss after iteration 13000: 16.035331818589693\n",
      "three-layer Loss after iteration 14000: 15.582766833954084\n",
      "three-layer Loss after iteration 15000: 15.233271277740466\n",
      "three-layer Loss after iteration 16000: 15.593917932982723\n",
      "three-layer Loss after iteration 17000: 15.051476208710321\n",
      "three-layer Loss after iteration 18000: 14.980654743430193\n",
      "three-layer Loss after iteration 19000: 14.66463729062398\n",
      "three-layer Loss after iteration 20000: 14.538958221526288\n",
      "three-layer Loss after iteration 21000: 15.276700666950111\n",
      "three-layer Loss after iteration 22000: 14.441799906897556\n",
      "three-layer Loss after iteration 23000: 14.280627100172431\n",
      "three-layer Loss after iteration 24000: 13.786408934528149\n",
      "three-layer Loss after iteration 25000: 13.870300643590873\n",
      "three-layer Loss after iteration 26000: 13.310550585052164\n",
      "three-layer Loss after iteration 27000: 13.449309550255988\n",
      "three-layer Loss after iteration 28000: 13.23584594500049\n",
      "three-layer Loss after iteration 29000: 11.902652045526006\n",
      "three-layer Loss after iteration 30000: 12.823152226770256\n",
      "three-layer Loss after iteration 31000: 12.978871368641665\n",
      "three-layer Loss after iteration 32000: 12.667064509756749\n",
      "three-layer Loss after iteration 33000: 12.449613336517112\n",
      "three-layer Loss after iteration 34000: 12.702369857214979\n",
      "three-layer Loss after iteration 35000: 12.495421809833955\n",
      "three-layer Loss after iteration 36000: 12.305647705227981\n",
      "three-layer Loss after iteration 37000: 12.074884693867807\n",
      "three-layer Loss after iteration 38000: 12.040977347282315\n",
      "three-layer Loss after iteration 39000: 11.925103428296483\n",
      "three-layer Loss after iteration 40000: 11.967396193254341\n",
      "three-layer Loss after iteration 41000: 11.7634670201692\n",
      "three-layer Loss after iteration 42000: 11.63419170072527\n",
      "three-layer Loss after iteration 43000: 11.499822165022874\n",
      "three-layer Loss after iteration 44000: 11.504797585438583\n",
      "three-layer Loss after iteration 45000: 11.375963467756103\n",
      "three-layer Loss after iteration 46000: 11.582968650487475\n",
      "three-layer Loss after iteration 47000: 11.299785020670392\n",
      "three-layer Loss after iteration 48000: 11.195628185857677\n",
      "three-layer Loss after iteration 49000: 11.097898086186465\n",
      "three-layer Loss after iteration 50000: 11.014454081827562\n",
      "three-layer Loss after iteration 51000: 10.914606231325052\n",
      "three-layer Loss after iteration 52000: 10.848666477827436\n",
      "three-layer Loss after iteration 53000: 10.84352877285236\n",
      "three-layer Loss after iteration 54000: 10.892486922438112\n",
      "three-layer Loss after iteration 55000: 10.949644521623101\n",
      "three-layer Loss after iteration 56000: 11.100492529860364\n",
      "three-layer Loss after iteration 57000: 11.405696869219287\n",
      "three-layer Loss after iteration 58000: 11.065705714316532\n",
      "three-layer Loss after iteration 59000: 10.726146009734435\n",
      "three-layer Loss after iteration 60000: 10.676612978565222\n",
      "three-layer Loss after iteration 61000: 11.09867594098219\n",
      "three-layer Loss after iteration 62000: 11.545052117294508\n",
      "three-layer Loss after iteration 63000: 11.139403659335068\n",
      "three-layer Loss after iteration 64000: 10.518678107912708\n",
      "three-layer Loss after iteration 65000: 11.185474292134714\n",
      "three-layer Loss after iteration 66000: 10.442145035431443\n",
      "three-layer Loss after iteration 67000: 12.165693466260516\n",
      "three-layer Loss after iteration 68000: 12.303598006453823\n",
      "three-layer Loss after iteration 69000: 12.47794474016291\n",
      "three-layer Loss after iteration 70000: 11.340638288211428\n",
      "three-layer Loss after iteration 71000: 11.703344494421204\n",
      "three-layer Loss after iteration 72000: 12.186045871149444\n",
      "three-layer Loss after iteration 73000: 12.262466717129044\n",
      "three-layer Loss after iteration 74000: 13.501024760826986\n",
      "three-layer Loss after iteration 75000: 12.55043017377608\n",
      "three-layer Loss after iteration 76000: 11.9248942995386\n",
      "three-layer Loss after iteration 77000: 12.261509343536119\n",
      "three-layer Loss after iteration 78000: 10.815101938967645\n",
      "three-layer Loss after iteration 79000: 9.49246785453623\n",
      "three-layer Loss after iteration 80000: 9.597482000287155\n",
      "three-layer Loss after iteration 81000: 10.770788667942496\n",
      "three-layer Loss after iteration 82000: 10.954813681705717\n",
      "three-layer Loss after iteration 83000: 10.650044106806304\n",
      "three-layer Loss after iteration 84000: 10.349220666602626\n",
      "three-layer Loss after iteration 85000: 10.60830308215137\n",
      "three-layer Loss after iteration 86000: 9.929304684731807\n",
      "three-layer Loss after iteration 87000: 9.865245474050935\n",
      "three-layer Loss after iteration 88000: 9.45391919054897\n",
      "three-layer Loss after iteration 89000: 10.59386576520657\n",
      "three-layer Loss after iteration 90000: 7.991464796340171\n",
      "three-layer Loss after iteration 91000: 6.515831258042487\n",
      "three-layer Loss after iteration 92000: 6.499925896569259\n",
      "three-layer Loss after iteration 93000: 6.788960139718133\n",
      "three-layer Loss after iteration 94000: 7.6776424183519785\n",
      "three-layer Loss after iteration 95000: 8.140012060864091\n",
      "three-layer Loss after iteration 96000: 7.799032896851202\n",
      "three-layer Loss after iteration 97000: 6.896461558524531\n",
      "three-layer Loss after iteration 98000: 11.035258964191769\n",
      "three-layer Loss after iteration 99000: 9.781045436898502\n",
      "three-layer Loss after iteration 0: 1681.5355448945793\n",
      "three-layer Loss after iteration 1000: 25.08240605844892\n",
      "three-layer Loss after iteration 2000: 21.597415233643563\n",
      "three-layer Loss after iteration 3000: 14.792830585117061\n",
      "three-layer Loss after iteration 4000: 12.102013664351494\n",
      "three-layer Loss after iteration 5000: 13.08650776819365\n",
      "three-layer Loss after iteration 6000: 12.01976616134447\n",
      "three-layer Loss after iteration 7000: 11.326622927018136\n",
      "three-layer Loss after iteration 8000: 11.04030251518457\n",
      "three-layer Loss after iteration 9000: 10.455707112066499\n",
      "three-layer Loss after iteration 10000: 10.114811101343788\n",
      "three-layer Loss after iteration 11000: 9.715305439506443\n",
      "three-layer Loss after iteration 12000: 8.403944907197562\n",
      "three-layer Loss after iteration 13000: 8.631300773827173\n",
      "three-layer Loss after iteration 14000: 8.107553160868461\n",
      "three-layer Loss after iteration 15000: 8.513292432322737\n",
      "three-layer Loss after iteration 16000: 8.221203467798198\n",
      "three-layer Loss after iteration 17000: 7.891369380785984\n",
      "three-layer Loss after iteration 18000: 8.261426945666742\n",
      "three-layer Loss after iteration 19000: 7.5687029317547925\n",
      "three-layer Loss after iteration 20000: 7.775989887164056\n",
      "three-layer Loss after iteration 21000: 6.691293583138149\n",
      "three-layer Loss after iteration 22000: 7.169276708438625\n",
      "three-layer Loss after iteration 23000: 8.190163446072317\n",
      "three-layer Loss after iteration 24000: 6.603120974058731\n",
      "three-layer Loss after iteration 25000: 7.310559888021564\n",
      "three-layer Loss after iteration 26000: 7.619082134705446\n",
      "three-layer Loss after iteration 27000: 9.199610437751758\n",
      "three-layer Loss after iteration 28000: 4.99752670864228\n",
      "three-layer Loss after iteration 29000: 4.9141870042178475\n",
      "three-layer Loss after iteration 30000: 7.265487533785583\n",
      "three-layer Loss after iteration 31000: 7.141070908561679\n",
      "three-layer Loss after iteration 32000: 7.074565521871292\n",
      "three-layer Loss after iteration 33000: 7.037947609737651\n",
      "three-layer Loss after iteration 34000: 5.246796690149105\n",
      "three-layer Loss after iteration 35000: 6.288213380233903\n",
      "three-layer Loss after iteration 36000: 6.26695024164038\n",
      "three-layer Loss after iteration 37000: 6.263496590914646\n",
      "three-layer Loss after iteration 38000: 6.151767123805112\n",
      "three-layer Loss after iteration 39000: 6.057476547434508\n",
      "three-layer Loss after iteration 40000: 5.990152267380368\n",
      "three-layer Loss after iteration 41000: 6.11924933022788\n",
      "three-layer Loss after iteration 42000: 5.4510695664807844\n",
      "three-layer Loss after iteration 43000: 5.209975165790661\n",
      "three-layer Loss after iteration 44000: 5.110262567668026\n",
      "three-layer Loss after iteration 45000: 5.838387981640479\n",
      "three-layer Loss after iteration 46000: 5.623282037766655\n",
      "three-layer Loss after iteration 47000: 5.397397139258899\n",
      "three-layer Loss after iteration 48000: 5.347990528335728\n",
      "three-layer Loss after iteration 49000: 5.58092189646402\n",
      "three-layer Loss after iteration 50000: 5.418701747756419\n",
      "three-layer Loss after iteration 51000: 5.4667321694630795\n",
      "three-layer Loss after iteration 52000: 5.4551775042373825\n",
      "three-layer Loss after iteration 53000: 5.867581954783342\n",
      "three-layer Loss after iteration 54000: 5.354122867592624\n",
      "three-layer Loss after iteration 55000: 5.3388828675815105\n",
      "three-layer Loss after iteration 56000: 5.499674920376372\n",
      "three-layer Loss after iteration 57000: 5.305782269448769\n",
      "three-layer Loss after iteration 58000: 5.232679881185846\n",
      "three-layer Loss after iteration 59000: 5.322824323976363\n",
      "three-layer Loss after iteration 60000: 5.346325906488547\n",
      "three-layer Loss after iteration 61000: 5.4919106414391194\n",
      "three-layer Loss after iteration 62000: 5.4697626367597305\n",
      "three-layer Loss after iteration 63000: 5.161296188383419\n",
      "three-layer Loss after iteration 64000: 5.1665708585138255\n",
      "three-layer Loss after iteration 65000: 5.162624392376215\n",
      "three-layer Loss after iteration 66000: 5.155422859468506\n",
      "three-layer Loss after iteration 67000: 5.147238828216881\n",
      "three-layer Loss after iteration 68000: 5.121709776980731\n",
      "three-layer Loss after iteration 69000: 5.113163144517645\n",
      "three-layer Loss after iteration 70000: 5.116099889625082\n",
      "three-layer Loss after iteration 71000: 5.120098383573392\n",
      "three-layer Loss after iteration 72000: 5.122221163975076\n",
      "three-layer Loss after iteration 73000: 4.825643439484956\n",
      "three-layer Loss after iteration 74000: 5.115212833868987\n",
      "three-layer Loss after iteration 75000: 5.151348045214334\n",
      "three-layer Loss after iteration 76000: 5.156166829007354\n",
      "three-layer Loss after iteration 77000: 4.803307265618676\n",
      "three-layer Loss after iteration 78000: 4.9640913205577615\n",
      "three-layer Loss after iteration 79000: 4.888421793378072\n",
      "three-layer Loss after iteration 80000: 4.892736520598683\n",
      "three-layer Loss after iteration 81000: 5.049844282355782\n",
      "three-layer Loss after iteration 82000: 4.9522493416402735\n",
      "three-layer Loss after iteration 83000: 4.937492591828057\n",
      "three-layer Loss after iteration 84000: 4.908541962962757\n",
      "three-layer Loss after iteration 85000: 4.75330291637666\n",
      "three-layer Loss after iteration 86000: 4.741451904275117\n",
      "three-layer Loss after iteration 87000: 4.725299927098371\n",
      "three-layer Loss after iteration 88000: 4.761969733546268\n",
      "three-layer Loss after iteration 89000: 4.7550887468163365\n",
      "three-layer Loss after iteration 90000: 4.757376316078337\n",
      "three-layer Loss after iteration 91000: 4.9690525760967486\n",
      "three-layer Loss after iteration 92000: 4.892547614647555\n",
      "three-layer Loss after iteration 93000: 4.953577280505093\n",
      "three-layer Loss after iteration 94000: 4.991096222816761\n",
      "three-layer Loss after iteration 95000: 4.975169088098943\n",
      "three-layer Loss after iteration 96000: 4.97128701650518\n",
      "three-layer Loss after iteration 97000: 4.961248191111532\n",
      "three-layer Loss after iteration 98000: 4.939858298025591\n",
      "three-layer Loss after iteration 99000: 4.639918441400931\n",
      "three-layer Loss after iteration 0: 1610.7296020999734\n",
      "three-layer Loss after iteration 1000: 26.68765013471747\n",
      "three-layer Loss after iteration 2000: 27.410655655333056\n",
      "three-layer Loss after iteration 3000: 16.971570526771607\n",
      "three-layer Loss after iteration 4000: 13.839026079701366\n",
      "three-layer Loss after iteration 5000: 10.89518298104002\n",
      "three-layer Loss after iteration 6000: 9.242264876578314\n",
      "three-layer Loss after iteration 7000: 8.241799716038212\n",
      "three-layer Loss after iteration 8000: 7.64501372461331\n",
      "three-layer Loss after iteration 9000: 7.303681485623892\n",
      "three-layer Loss after iteration 10000: 7.4865622169796975\n",
      "three-layer Loss after iteration 11000: 7.502158144126984\n",
      "three-layer Loss after iteration 12000: 7.189920313525536\n",
      "three-layer Loss after iteration 13000: 6.527443659523557\n",
      "three-layer Loss after iteration 14000: 6.025348475327417\n",
      "three-layer Loss after iteration 15000: 5.152490192286059\n",
      "three-layer Loss after iteration 16000: 5.071903923048449\n",
      "three-layer Loss after iteration 17000: 4.728872892763689\n",
      "three-layer Loss after iteration 18000: 4.522489556114783\n",
      "three-layer Loss after iteration 19000: 4.890318298848568\n",
      "three-layer Loss after iteration 20000: 4.682691768324553\n",
      "three-layer Loss after iteration 21000: 4.2368793752695275\n",
      "three-layer Loss after iteration 22000: 4.025790662722553\n",
      "three-layer Loss after iteration 23000: 3.900511698529889\n",
      "three-layer Loss after iteration 24000: 3.8073771985154123\n",
      "three-layer Loss after iteration 25000: 3.9678436846943343\n",
      "three-layer Loss after iteration 26000: 4.488610925938428\n",
      "three-layer Loss after iteration 27000: 3.6528195703386315\n",
      "three-layer Loss after iteration 28000: 4.135450945606904\n",
      "three-layer Loss after iteration 29000: 3.630260069823756\n",
      "three-layer Loss after iteration 30000: 4.211114843954376\n",
      "three-layer Loss after iteration 31000: 3.8394472516365323\n",
      "three-layer Loss after iteration 32000: 4.473232823099001\n",
      "three-layer Loss after iteration 33000: 4.142905620965998\n",
      "three-layer Loss after iteration 34000: 3.5003893758805726\n",
      "three-layer Loss after iteration 35000: 3.2373489621348503\n",
      "three-layer Loss after iteration 36000: 3.7248287220046996\n",
      "three-layer Loss after iteration 37000: 3.8024955894105346\n",
      "three-layer Loss after iteration 38000: 3.673787279377348\n",
      "three-layer Loss after iteration 39000: 3.433149988686029\n",
      "three-layer Loss after iteration 40000: 3.723457039096951\n",
      "three-layer Loss after iteration 41000: 3.4171900700069693\n",
      "three-layer Loss after iteration 42000: 3.468128520543041\n",
      "three-layer Loss after iteration 43000: 3.1051976887226873\n",
      "three-layer Loss after iteration 44000: 2.8901457128511336\n",
      "three-layer Loss after iteration 45000: 2.8602436423215787\n",
      "three-layer Loss after iteration 46000: 3.826356900689733\n",
      "three-layer Loss after iteration 47000: 3.424407074999008\n",
      "three-layer Loss after iteration 48000: 3.428707020700808\n",
      "three-layer Loss after iteration 49000: 3.3871074432433126\n",
      "three-layer Loss after iteration 50000: 2.9598540209248196\n",
      "three-layer Loss after iteration 51000: 4.381341318338681\n",
      "three-layer Loss after iteration 52000: 3.2765662358302476\n",
      "three-layer Loss after iteration 53000: 3.3843530822413577\n",
      "three-layer Loss after iteration 54000: 3.291942629577936\n",
      "three-layer Loss after iteration 55000: 3.405048472658941\n",
      "three-layer Loss after iteration 56000: 3.614493914386275\n",
      "three-layer Loss after iteration 57000: 3.808988722042708\n",
      "three-layer Loss after iteration 58000: 3.6805491998328437\n",
      "three-layer Loss after iteration 59000: 3.0662374582936582\n",
      "three-layer Loss after iteration 60000: 3.0554786358347794\n",
      "three-layer Loss after iteration 61000: 3.872920541315577\n",
      "three-layer Loss after iteration 62000: 2.872930175083704\n",
      "three-layer Loss after iteration 63000: 3.2299748919466453\n",
      "three-layer Loss after iteration 64000: 3.081567466010565\n",
      "three-layer Loss after iteration 65000: 3.080837843822675\n",
      "three-layer Loss after iteration 66000: 3.3523534574626614\n",
      "three-layer Loss after iteration 67000: 2.9459941242758405\n",
      "three-layer Loss after iteration 68000: 3.226090963175012\n",
      "three-layer Loss after iteration 69000: 3.289914567326924\n",
      "three-layer Loss after iteration 70000: 2.8703660402799103\n",
      "three-layer Loss after iteration 71000: 3.1661512630036728\n",
      "three-layer Loss after iteration 72000: 3.4127411226126405\n",
      "three-layer Loss after iteration 73000: 3.1420478723695155\n",
      "three-layer Loss after iteration 74000: 2.90279049628289\n",
      "three-layer Loss after iteration 75000: 3.867755637448678\n",
      "three-layer Loss after iteration 76000: 2.7982795745587925\n",
      "three-layer Loss after iteration 77000: 3.5074764790561215\n",
      "three-layer Loss after iteration 78000: 2.8171639216370497\n",
      "three-layer Loss after iteration 79000: 3.092869627012293\n",
      "three-layer Loss after iteration 80000: 3.8472995663376452\n",
      "three-layer Loss after iteration 81000: 2.7560357680799545\n",
      "three-layer Loss after iteration 82000: 4.526694068429743\n",
      "three-layer Loss after iteration 83000: 3.3591072223589165\n",
      "three-layer Loss after iteration 84000: 3.3396539591330976\n",
      "three-layer Loss after iteration 85000: 2.8808019742692603\n",
      "three-layer Loss after iteration 86000: 2.9622544604479226\n",
      "three-layer Loss after iteration 87000: 3.1790925708874034\n",
      "three-layer Loss after iteration 88000: 2.8005524111183715\n",
      "three-layer Loss after iteration 89000: 3.8754148049149975\n",
      "three-layer Loss after iteration 90000: 3.0884742462300205\n",
      "three-layer Loss after iteration 91000: 2.9288087161779606\n",
      "three-layer Loss after iteration 92000: 3.499781258850638\n",
      "three-layer Loss after iteration 93000: 2.774691000480331\n",
      "three-layer Loss after iteration 94000: 3.8291761757330125\n",
      "three-layer Loss after iteration 95000: 2.7935197100310356\n",
      "three-layer Loss after iteration 96000: 3.8528420686428206\n",
      "three-layer Loss after iteration 97000: 4.903634267357119\n",
      "three-layer Loss after iteration 98000: 3.6541052844655497\n",
      "three-layer Loss after iteration 99000: 2.886974818811544\n",
      "three-layer Loss after iteration 0: 1705.9568041322468\n",
      "three-layer Loss after iteration 1000: 25.766720095703366\n",
      "three-layer Loss after iteration 2000: 26.684540308458235\n",
      "three-layer Loss after iteration 3000: 24.645522565072316\n",
      "three-layer Loss after iteration 4000: 21.85308828968554\n",
      "three-layer Loss after iteration 5000: 19.503042436477372\n",
      "three-layer Loss after iteration 6000: 15.84636757628368\n",
      "three-layer Loss after iteration 7000: 17.24835563208498\n",
      "three-layer Loss after iteration 8000: 16.471606221608695\n",
      "three-layer Loss after iteration 9000: 15.527729372259982\n",
      "three-layer Loss after iteration 10000: 18.463006796926905\n",
      "three-layer Loss after iteration 11000: 18.243238856638552\n",
      "three-layer Loss after iteration 12000: 17.44593308183317\n",
      "three-layer Loss after iteration 13000: 16.804299962453662\n",
      "three-layer Loss after iteration 14000: 16.437462811197438\n",
      "three-layer Loss after iteration 15000: 15.979430175289195\n",
      "three-layer Loss after iteration 16000: 15.589563788231713\n",
      "three-layer Loss after iteration 17000: 18.126768701049976\n",
      "three-layer Loss after iteration 18000: 14.76236084919166\n",
      "three-layer Loss after iteration 19000: 14.569519016802415\n",
      "three-layer Loss after iteration 20000: 14.045462099544022\n",
      "three-layer Loss after iteration 21000: 13.750777224705452\n",
      "three-layer Loss after iteration 22000: 13.463155145379185\n",
      "three-layer Loss after iteration 23000: 13.26574072510678\n",
      "three-layer Loss after iteration 24000: 12.826080833849465\n",
      "three-layer Loss after iteration 25000: 12.28647809421771\n",
      "three-layer Loss after iteration 26000: 11.899251980897867\n",
      "three-layer Loss after iteration 27000: 11.683020770442567\n",
      "three-layer Loss after iteration 28000: 11.257480947034233\n",
      "three-layer Loss after iteration 29000: 11.030313424000871\n",
      "three-layer Loss after iteration 30000: 11.054453274150331\n",
      "three-layer Loss after iteration 31000: 11.267162405627781\n",
      "three-layer Loss after iteration 32000: 10.65429417466876\n",
      "three-layer Loss after iteration 33000: 10.417474383556687\n",
      "three-layer Loss after iteration 34000: 10.269479106000778\n",
      "three-layer Loss after iteration 35000: 10.14504568181798\n",
      "three-layer Loss after iteration 36000: 9.121811899285657\n",
      "three-layer Loss after iteration 37000: 9.881504418476789\n",
      "three-layer Loss after iteration 38000: 9.673245938116974\n",
      "three-layer Loss after iteration 39000: 9.458018553582608\n",
      "three-layer Loss after iteration 40000: 9.484304168663177\n",
      "three-layer Loss after iteration 41000: 9.817453567171144\n",
      "three-layer Loss after iteration 42000: 9.574257501057057\n",
      "three-layer Loss after iteration 43000: 9.413861310913518\n",
      "three-layer Loss after iteration 44000: 8.581116559550198\n",
      "three-layer Loss after iteration 45000: 9.310710348430161\n",
      "three-layer Loss after iteration 46000: 8.426073220066666\n",
      "three-layer Loss after iteration 47000: 7.900654403485461\n",
      "three-layer Loss after iteration 48000: 7.872974803547387\n",
      "three-layer Loss after iteration 49000: 7.847765591159267\n",
      "three-layer Loss after iteration 50000: 9.544441299460162\n",
      "three-layer Loss after iteration 51000: 8.653541135681337\n",
      "three-layer Loss after iteration 52000: 7.8394986366092905\n",
      "three-layer Loss after iteration 53000: 7.815425266224937\n",
      "three-layer Loss after iteration 54000: 10.029640710238802\n",
      "three-layer Loss after iteration 55000: 8.623866078033405\n",
      "three-layer Loss after iteration 56000: 9.208171731882567\n",
      "three-layer Loss after iteration 57000: 9.005978462946416\n",
      "three-layer Loss after iteration 58000: 12.965414437481405\n",
      "three-layer Loss after iteration 59000: 9.431418714893839\n",
      "three-layer Loss after iteration 60000: 9.380860227780413\n",
      "three-layer Loss after iteration 61000: 9.794430730293358\n",
      "three-layer Loss after iteration 62000: 9.445068010137602\n",
      "three-layer Loss after iteration 63000: 8.266885530908757\n",
      "three-layer Loss after iteration 64000: 9.978871897245794\n",
      "three-layer Loss after iteration 65000: 8.4065061612103\n",
      "three-layer Loss after iteration 66000: 8.451168836088144\n",
      "three-layer Loss after iteration 67000: 8.477597700826623\n",
      "three-layer Loss after iteration 68000: 8.934188872761426\n",
      "three-layer Loss after iteration 69000: 9.657375336635951\n",
      "three-layer Loss after iteration 70000: 9.142221882814294\n",
      "three-layer Loss after iteration 71000: 9.271506738376221\n",
      "three-layer Loss after iteration 72000: 9.451392956881136\n",
      "three-layer Loss after iteration 73000: 9.264858970015897\n",
      "three-layer Loss after iteration 74000: 9.138921452965048\n",
      "three-layer Loss after iteration 75000: 9.06033432693816\n",
      "three-layer Loss after iteration 76000: 8.990622174286822\n",
      "three-layer Loss after iteration 77000: 8.93752091627138\n",
      "three-layer Loss after iteration 78000: 8.892454080959832\n",
      "three-layer Loss after iteration 79000: 8.845354407238075\n",
      "three-layer Loss after iteration 80000: 8.808502893929415\n",
      "three-layer Loss after iteration 81000: 8.777946350744969\n",
      "three-layer Loss after iteration 82000: 8.740158148810387\n",
      "three-layer Loss after iteration 83000: 8.710454422755998\n",
      "three-layer Loss after iteration 84000: 8.685142487630031\n",
      "three-layer Loss after iteration 85000: 8.708471464909827\n",
      "three-layer Loss after iteration 86000: 8.659957055057662\n",
      "three-layer Loss after iteration 87000: 8.618097753595745\n",
      "three-layer Loss after iteration 88000: 8.561099724221028\n",
      "three-layer Loss after iteration 89000: 8.52703303775439\n",
      "three-layer Loss after iteration 90000: 8.493863344740612\n",
      "three-layer Loss after iteration 91000: 8.47279907932365\n",
      "three-layer Loss after iteration 92000: 8.454491970098951\n",
      "three-layer Loss after iteration 93000: 8.435109642126683\n",
      "three-layer Loss after iteration 94000: 8.40058272419885\n",
      "three-layer Loss after iteration 95000: 8.380961375366734\n",
      "three-layer Loss after iteration 96000: 8.35698443350823\n",
      "three-layer Loss after iteration 97000: 8.326697345845616\n",
      "three-layer Loss after iteration 98000: 8.29812061384741\n",
      "three-layer Loss after iteration 99000: 8.273450943365578\n",
      "three-layer Loss after iteration 0: 1606.6731621045515\n",
      "three-layer Loss after iteration 1000: 16.939759806820735\n",
      "three-layer Loss after iteration 2000: 14.301776306300086\n",
      "three-layer Loss after iteration 3000: 11.310661640230235\n",
      "three-layer Loss after iteration 4000: 10.567120557101545\n",
      "three-layer Loss after iteration 5000: 10.228709233152134\n",
      "three-layer Loss after iteration 6000: 9.161749399917364\n",
      "three-layer Loss after iteration 7000: 8.94889162368515\n",
      "three-layer Loss after iteration 8000: 9.15041868861021\n",
      "three-layer Loss after iteration 9000: 8.271181799473275\n",
      "three-layer Loss after iteration 10000: 8.418337680889517\n",
      "three-layer Loss after iteration 11000: 7.772008990147656\n",
      "three-layer Loss after iteration 12000: 6.7314081669201595\n",
      "three-layer Loss after iteration 13000: 6.987386879339855\n",
      "three-layer Loss after iteration 14000: 6.482707185136432\n",
      "three-layer Loss after iteration 15000: 7.622126765660558\n",
      "three-layer Loss after iteration 16000: 7.243504211562152\n",
      "three-layer Loss after iteration 17000: 6.842260698536735\n",
      "three-layer Loss after iteration 18000: 6.979095292057697\n",
      "three-layer Loss after iteration 19000: 6.578898565221892\n",
      "three-layer Loss after iteration 20000: 6.811054903697067\n",
      "three-layer Loss after iteration 21000: 6.347565220744766\n",
      "three-layer Loss after iteration 22000: 6.193315738834718\n",
      "three-layer Loss after iteration 23000: 6.052618944440997\n",
      "three-layer Loss after iteration 24000: 5.945153384557991\n",
      "three-layer Loss after iteration 25000: 5.844370033100291\n",
      "three-layer Loss after iteration 26000: 5.744544206159472\n",
      "three-layer Loss after iteration 27000: 5.821696638684833\n",
      "three-layer Loss after iteration 28000: 5.772506391151896\n",
      "three-layer Loss after iteration 29000: 5.706688977059941\n",
      "three-layer Loss after iteration 30000: 5.651650178288075\n",
      "three-layer Loss after iteration 31000: 5.685296032074609\n",
      "three-layer Loss after iteration 32000: 5.601236298498812\n",
      "three-layer Loss after iteration 33000: 5.443956007731166\n",
      "three-layer Loss after iteration 34000: 5.439332472626088\n",
      "three-layer Loss after iteration 35000: 5.392849641899897\n",
      "three-layer Loss after iteration 36000: 5.354112036787498\n",
      "three-layer Loss after iteration 37000: 5.318970007075473\n",
      "three-layer Loss after iteration 38000: 5.288602520546125\n",
      "three-layer Loss after iteration 39000: 5.26067403792766\n",
      "three-layer Loss after iteration 40000: 5.237610497902501\n",
      "three-layer Loss after iteration 41000: 5.2184574738911325\n",
      "three-layer Loss after iteration 42000: 5.197092392173392\n",
      "three-layer Loss after iteration 43000: 5.180299289617108\n",
      "three-layer Loss after iteration 44000: 5.164306432401029\n",
      "three-layer Loss after iteration 45000: 5.149707008459678\n",
      "three-layer Loss after iteration 46000: 5.135514027562516\n",
      "three-layer Loss after iteration 47000: 5.123312335496617\n",
      "three-layer Loss after iteration 48000: 5.110074843940722\n",
      "three-layer Loss after iteration 49000: 5.12447885539246\n",
      "three-layer Loss after iteration 50000: 5.105718679767827\n",
      "three-layer Loss after iteration 51000: 5.099762699156454\n",
      "three-layer Loss after iteration 52000: 5.091866084939857\n",
      "three-layer Loss after iteration 53000: 5.081207184449046\n",
      "three-layer Loss after iteration 54000: 5.073615737051177\n",
      "three-layer Loss after iteration 55000: 5.071128810054264\n",
      "three-layer Loss after iteration 56000: 5.061621863307155\n",
      "three-layer Loss after iteration 57000: 5.054418808246298\n",
      "three-layer Loss after iteration 58000: 5.037266963474364\n",
      "three-layer Loss after iteration 59000: 5.047657829089142\n",
      "three-layer Loss after iteration 60000: 5.021089382010333\n",
      "three-layer Loss after iteration 61000: 5.006813038989957\n",
      "three-layer Loss after iteration 62000: 4.994919765476371\n",
      "three-layer Loss after iteration 63000: 4.960363370979263\n",
      "three-layer Loss after iteration 64000: 4.949999571561533\n",
      "three-layer Loss after iteration 65000: 4.951819347139713\n",
      "three-layer Loss after iteration 66000: 4.965438787595061\n",
      "three-layer Loss after iteration 67000: 4.96041052973483\n",
      "three-layer Loss after iteration 68000: 4.9586501003283265\n",
      "three-layer Loss after iteration 69000: 4.951675045126153\n",
      "three-layer Loss after iteration 70000: 4.945439434315636\n",
      "three-layer Loss after iteration 71000: 4.942086223032307\n",
      "three-layer Loss after iteration 72000: 4.936019106616246\n",
      "three-layer Loss after iteration 73000: 4.931835098632091\n",
      "three-layer Loss after iteration 74000: 4.92624867036829\n",
      "three-layer Loss after iteration 75000: 4.9202217587442165\n",
      "three-layer Loss after iteration 76000: 4.912644256572565\n",
      "three-layer Loss after iteration 77000: 4.904914536945728\n",
      "three-layer Loss after iteration 78000: 4.897891404061074\n",
      "three-layer Loss after iteration 79000: 4.8888271636518805\n",
      "three-layer Loss after iteration 80000: 4.879507783587769\n",
      "three-layer Loss after iteration 81000: 4.872444744909872\n",
      "three-layer Loss after iteration 82000: 4.854644766847394\n",
      "three-layer Loss after iteration 83000: 4.847723170804286\n",
      "three-layer Loss after iteration 84000: 4.837421046121343\n",
      "three-layer Loss after iteration 85000: 4.820276317724345\n",
      "three-layer Loss after iteration 86000: 4.8217897126198555\n",
      "three-layer Loss after iteration 87000: 4.8102847261643245\n",
      "three-layer Loss after iteration 88000: 4.805550427510273\n",
      "three-layer Loss after iteration 89000: 4.80033119070052\n",
      "three-layer Loss after iteration 90000: 4.792644944354912\n",
      "three-layer Loss after iteration 91000: 4.792177222813165\n",
      "9.759152767986775e-05 4.792644944354912 4.792177222813165\n",
      "three-layer Loss after iteration 0: 1666.5493501723652\n",
      "three-layer Loss after iteration 1000: 26.307053273305858\n",
      "three-layer Loss after iteration 2000: 28.523421872586542\n",
      "three-layer Loss after iteration 3000: 25.472003469204513\n",
      "three-layer Loss after iteration 4000: 25.74013675718641\n",
      "three-layer Loss after iteration 5000: 16.65162421954812\n",
      "three-layer Loss after iteration 6000: 15.792864833400836\n",
      "three-layer Loss after iteration 7000: 14.894843670760965\n",
      "three-layer Loss after iteration 8000: 14.026716600579539\n",
      "three-layer Loss after iteration 9000: 12.707174494024212\n",
      "three-layer Loss after iteration 10000: 14.559477174940403\n",
      "three-layer Loss after iteration 11000: 10.69844381704027\n",
      "three-layer Loss after iteration 12000: 12.847842603050804\n",
      "three-layer Loss after iteration 13000: 11.516101567308239\n",
      "three-layer Loss after iteration 14000: 10.676004103056291\n",
      "three-layer Loss after iteration 15000: 10.039583140126192\n",
      "three-layer Loss after iteration 16000: 9.812911166002658\n",
      "three-layer Loss after iteration 17000: 8.918454812549985\n",
      "three-layer Loss after iteration 18000: 9.068100292432062\n",
      "three-layer Loss after iteration 19000: 8.550289507740542\n",
      "three-layer Loss after iteration 20000: 8.396387104512298\n",
      "three-layer Loss after iteration 21000: 8.260753800897305\n",
      "three-layer Loss after iteration 22000: 7.819466038014119\n",
      "three-layer Loss after iteration 23000: 8.006860005117176\n",
      "three-layer Loss after iteration 24000: 7.991518145890787\n",
      "three-layer Loss after iteration 25000: 7.3531155039652685\n",
      "three-layer Loss after iteration 26000: 7.665036177547461\n",
      "three-layer Loss after iteration 27000: 7.510839907399517\n",
      "three-layer Loss after iteration 28000: 7.579876779996789\n",
      "three-layer Loss after iteration 29000: 7.22508658671367\n",
      "three-layer Loss after iteration 30000: 7.932813748441432\n",
      "three-layer Loss after iteration 31000: 5.9775395502730015\n",
      "three-layer Loss after iteration 32000: 6.059036097722969\n",
      "three-layer Loss after iteration 33000: 7.133126129259694\n",
      "three-layer Loss after iteration 34000: 7.200526884658099\n",
      "three-layer Loss after iteration 35000: 7.3478147877693845\n",
      "three-layer Loss after iteration 36000: 6.347470735370888\n",
      "three-layer Loss after iteration 37000: 6.078028094809692\n",
      "three-layer Loss after iteration 38000: 6.055684924395055\n",
      "three-layer Loss after iteration 39000: 6.177383934324331\n",
      "three-layer Loss after iteration 40000: 6.404920723381505\n",
      "three-layer Loss after iteration 41000: 6.621705159942451\n",
      "three-layer Loss after iteration 42000: 6.246589888057729\n",
      "three-layer Loss after iteration 43000: 6.341176970969743\n",
      "three-layer Loss after iteration 44000: 5.97346919372844\n",
      "three-layer Loss after iteration 45000: 6.207947650943454\n",
      "three-layer Loss after iteration 46000: 6.137682618221552\n",
      "three-layer Loss after iteration 47000: 6.195973473585775\n",
      "three-layer Loss after iteration 48000: 6.165554543433768\n",
      "three-layer Loss after iteration 49000: 6.0483995578281124\n",
      "three-layer Loss after iteration 50000: 5.944491179756032\n",
      "three-layer Loss after iteration 51000: 5.90498217769522\n",
      "three-layer Loss after iteration 52000: 5.853513782398165\n",
      "three-layer Loss after iteration 53000: 5.824203115824783\n",
      "three-layer Loss after iteration 54000: 5.798072734350076\n",
      "three-layer Loss after iteration 55000: 5.740629936102909\n",
      "three-layer Loss after iteration 56000: 5.754998436923848\n",
      "three-layer Loss after iteration 57000: 5.695433774701687\n",
      "three-layer Loss after iteration 58000: 5.6526461222029125\n",
      "three-layer Loss after iteration 59000: 5.601599008429112\n",
      "three-layer Loss after iteration 60000: 5.566420659721783\n",
      "three-layer Loss after iteration 61000: 5.532029946058356\n",
      "three-layer Loss after iteration 62000: 5.489977126771597\n",
      "three-layer Loss after iteration 63000: 5.440024789479535\n",
      "three-layer Loss after iteration 64000: 5.40615292986482\n",
      "three-layer Loss after iteration 65000: 5.385139676270588\n",
      "three-layer Loss after iteration 66000: 5.350861726531873\n",
      "three-layer Loss after iteration 67000: 5.322281987557021\n",
      "three-layer Loss after iteration 68000: 5.284567189071182\n",
      "three-layer Loss after iteration 69000: 5.254072482129587\n",
      "three-layer Loss after iteration 70000: 5.221825708461001\n",
      "three-layer Loss after iteration 71000: 5.203094798460833\n",
      "three-layer Loss after iteration 72000: 5.167465502056317\n",
      "three-layer Loss after iteration 73000: 5.135538156332542\n",
      "three-layer Loss after iteration 74000: 5.113377210337279\n",
      "three-layer Loss after iteration 75000: 5.089393540211808\n",
      "three-layer Loss after iteration 76000: 5.0531624020804635\n",
      "three-layer Loss after iteration 77000: 5.034324624310045\n",
      "three-layer Loss after iteration 78000: 5.009840156955426\n",
      "three-layer Loss after iteration 79000: 4.989932402283512\n",
      "three-layer Loss after iteration 80000: 4.969590057580466\n",
      "three-layer Loss after iteration 81000: 4.954820850026489\n",
      "three-layer Loss after iteration 82000: 4.929637624886012\n",
      "three-layer Loss after iteration 83000: 4.913154313009529\n",
      "three-layer Loss after iteration 84000: 4.901657743575233\n",
      "three-layer Loss after iteration 85000: 4.883408971360725\n",
      "three-layer Loss after iteration 86000: 4.861802968664894\n",
      "three-layer Loss after iteration 87000: 4.8479766988913795\n",
      "three-layer Loss after iteration 88000: 5.118848393004314\n",
      "three-layer Loss after iteration 89000: 5.192655590376194\n",
      "three-layer Loss after iteration 90000: 5.15810930662574\n",
      "three-layer Loss after iteration 91000: 5.106455761283549\n",
      "three-layer Loss after iteration 92000: 5.075302618801565\n",
      "three-layer Loss after iteration 93000: 5.0295479491386175\n",
      "three-layer Loss after iteration 94000: 5.013359282815989\n",
      "three-layer Loss after iteration 95000: 4.9929707253616655\n",
      "three-layer Loss after iteration 96000: 4.958616988872598\n",
      "three-layer Loss after iteration 97000: 4.9340573495963165\n",
      "three-layer Loss after iteration 98000: 4.898871482291262\n",
      "three-layer Loss after iteration 99000: 4.870994692908066\n",
      "three-layer Loss after iteration 0: 1740.7419706101616\n",
      "three-layer Loss after iteration 1000: 31.948515335982375\n",
      "three-layer Loss after iteration 2000: 27.40809060305542\n",
      "three-layer Loss after iteration 3000: 23.509619157979234\n",
      "three-layer Loss after iteration 4000: 18.3867845083233\n",
      "three-layer Loss after iteration 5000: 16.718074015479683\n",
      "three-layer Loss after iteration 6000: 14.829462200715716\n",
      "three-layer Loss after iteration 7000: 13.468769547585337\n",
      "three-layer Loss after iteration 8000: 11.66059902417098\n",
      "three-layer Loss after iteration 9000: 10.495849368148932\n",
      "three-layer Loss after iteration 10000: 10.350832892113118\n",
      "three-layer Loss after iteration 11000: 8.675753289084962\n",
      "three-layer Loss after iteration 12000: 9.555995038122637\n",
      "three-layer Loss after iteration 13000: 9.803485098043643\n",
      "three-layer Loss after iteration 14000: 9.114916910283368\n",
      "three-layer Loss after iteration 15000: 8.823363262825575\n",
      "three-layer Loss after iteration 16000: 8.557047227433284\n",
      "three-layer Loss after iteration 17000: 8.206347606228151\n",
      "three-layer Loss after iteration 18000: 8.176189349065869\n",
      "three-layer Loss after iteration 19000: 8.037147491005575\n",
      "three-layer Loss after iteration 20000: 7.881988275575334\n",
      "three-layer Loss after iteration 21000: 7.8040266898617565\n",
      "three-layer Loss after iteration 22000: 7.729787489588489\n",
      "three-layer Loss after iteration 23000: 7.665244108668337\n",
      "three-layer Loss after iteration 24000: 7.585261621535714\n",
      "three-layer Loss after iteration 25000: 7.524782517817508\n",
      "three-layer Loss after iteration 26000: 7.455247717214125\n",
      "three-layer Loss after iteration 27000: 7.388350630883625\n",
      "three-layer Loss after iteration 28000: 7.3429520699530055\n",
      "three-layer Loss after iteration 29000: 7.279225365369548\n",
      "three-layer Loss after iteration 30000: 7.225850693517774\n",
      "three-layer Loss after iteration 31000: 7.185660650567473\n",
      "three-layer Loss after iteration 32000: 7.145892225185574\n",
      "three-layer Loss after iteration 33000: 7.114421623433427\n",
      "three-layer Loss after iteration 34000: 6.686407505926109\n",
      "three-layer Loss after iteration 35000: 6.651116511054346\n",
      "three-layer Loss after iteration 36000: 8.269397784439313\n",
      "three-layer Loss after iteration 37000: 7.847181789926267\n",
      "three-layer Loss after iteration 38000: 7.62043921864517\n",
      "three-layer Loss after iteration 39000: 7.419208509934171\n",
      "three-layer Loss after iteration 40000: 7.309250188369414\n",
      "three-layer Loss after iteration 41000: 7.222148214946143\n",
      "three-layer Loss after iteration 42000: 7.144184099944006\n",
      "three-layer Loss after iteration 43000: 7.083954284953631\n",
      "three-layer Loss after iteration 44000: 6.548305765111254\n",
      "three-layer Loss after iteration 45000: 6.553825575139631\n",
      "three-layer Loss after iteration 46000: 6.986395565652759\n",
      "three-layer Loss after iteration 47000: 7.199399643437719\n",
      "three-layer Loss after iteration 48000: 6.933671585255623\n",
      "three-layer Loss after iteration 49000: 6.699913521178536\n",
      "three-layer Loss after iteration 50000: 6.50555146603002\n",
      "three-layer Loss after iteration 51000: 6.492396418991028\n",
      "three-layer Loss after iteration 52000: 6.48623096887142\n",
      "three-layer Loss after iteration 53000: 6.48040414554438\n",
      "three-layer Loss after iteration 54000: 6.479987362997522\n",
      "6.431428310601774e-05 6.48040414554438 6.479987362997522\n",
      "three-layer Loss after iteration 0: 1646.1869190494353\n",
      "three-layer Loss after iteration 1000: 16.928134823850623\n",
      "three-layer Loss after iteration 2000: 10.355277691917525\n",
      "three-layer Loss after iteration 3000: 12.587441737044163\n",
      "three-layer Loss after iteration 4000: 10.707216045674095\n",
      "three-layer Loss after iteration 5000: 9.992214916203157\n",
      "three-layer Loss after iteration 6000: 9.582546622583896\n",
      "three-layer Loss after iteration 7000: 9.099890085027264\n",
      "three-layer Loss after iteration 8000: 8.578014867232653\n",
      "three-layer Loss after iteration 9000: 8.148070115660808\n",
      "three-layer Loss after iteration 10000: 7.725836942780122\n",
      "three-layer Loss after iteration 11000: 7.701160268513128\n",
      "three-layer Loss after iteration 12000: 7.592007500366016\n",
      "three-layer Loss after iteration 13000: 7.596113264197939\n",
      "three-layer Loss after iteration 14000: 7.5358879863235915\n",
      "three-layer Loss after iteration 15000: 7.312637593384687\n",
      "three-layer Loss after iteration 16000: 7.531882634858475\n",
      "three-layer Loss after iteration 17000: 7.292234886224847\n",
      "three-layer Loss after iteration 18000: 7.157979007249966\n",
      "three-layer Loss after iteration 19000: 7.052743494461692\n",
      "three-layer Loss after iteration 20000: 6.966058795783327\n",
      "three-layer Loss after iteration 21000: 6.895371048244621\n",
      "three-layer Loss after iteration 22000: 6.836231002461629\n",
      "three-layer Loss after iteration 23000: 6.7894768847454\n",
      "three-layer Loss after iteration 24000: 6.749128719428302\n",
      "three-layer Loss after iteration 25000: 6.71887365553923\n",
      "three-layer Loss after iteration 26000: 6.658145816430408\n",
      "three-layer Loss after iteration 27000: 6.632962135485056\n",
      "three-layer Loss after iteration 28000: 6.628208650115855\n",
      "three-layer Loss after iteration 29000: 6.604324414360292\n",
      "three-layer Loss after iteration 30000: 6.5938966885551595\n",
      "three-layer Loss after iteration 31000: 6.579642819232121\n",
      "three-layer Loss after iteration 32000: 6.556722903800114\n",
      "three-layer Loss after iteration 33000: 6.55160978417955\n",
      "three-layer Loss after iteration 34000: 6.5411793093329775\n",
      "three-layer Loss after iteration 35000: 6.529863899525948\n",
      "three-layer Loss after iteration 36000: 6.519206297639209\n",
      "three-layer Loss after iteration 37000: 6.5121582078668006\n",
      "three-layer Loss after iteration 38000: 6.551705925455428\n",
      "three-layer Loss after iteration 39000: 6.493799720758745\n",
      "three-layer Loss after iteration 40000: 6.4892148902037485\n",
      "three-layer Loss after iteration 41000: 6.484739400283039\n",
      "three-layer Loss after iteration 42000: 6.4815925845661955\n",
      "three-layer Loss after iteration 43000: 6.478915797206921\n",
      "three-layer Loss after iteration 44000: 6.476152578885574\n",
      "three-layer Loss after iteration 45000: 6.472782400624025\n",
      "three-layer Loss after iteration 46000: 6.491925777747753\n",
      "three-layer Loss after iteration 47000: 6.470496302283379\n",
      "three-layer Loss after iteration 48000: 6.464088373491005\n",
      "three-layer Loss after iteration 49000: 6.459485838777659\n",
      "three-layer Loss after iteration 50000: 6.456508169010835\n",
      "three-layer Loss after iteration 51000: 6.451875525527495\n",
      "three-layer Loss after iteration 52000: 6.449152516880545\n",
      "three-layer Loss after iteration 53000: 6.446597723017082\n",
      "three-layer Loss after iteration 54000: 6.443015517899344\n",
      "three-layer Loss after iteration 55000: 6.441060264956046\n",
      "three-layer Loss after iteration 56000: 6.438208276369243\n",
      "three-layer Loss after iteration 57000: 6.436036216584133\n",
      "three-layer Loss after iteration 58000: 6.461958842898126\n",
      "three-layer Loss after iteration 59000: 6.443895367810229\n",
      "three-layer Loss after iteration 60000: 6.43552206382125\n",
      "three-layer Loss after iteration 61000: 6.414980290773306\n",
      "three-layer Loss after iteration 62000: 6.419600311037585\n",
      "three-layer Loss after iteration 63000: 6.419332096769126\n",
      "4.178052455975749e-05 6.419600311037585 6.419332096769126\n",
      "three-layer Loss after iteration 0: 1579.785642576877\n",
      "three-layer Loss after iteration 1000: 11.236761688594136\n",
      "three-layer Loss after iteration 2000: 8.785193075224385\n",
      "three-layer Loss after iteration 3000: 7.954097340868163\n",
      "three-layer Loss after iteration 4000: 6.489517164694332\n",
      "three-layer Loss after iteration 5000: 5.685369562951609\n",
      "three-layer Loss after iteration 6000: 5.241572732243096\n",
      "three-layer Loss after iteration 7000: 5.333943575152276\n",
      "three-layer Loss after iteration 8000: 5.204589511467913\n",
      "three-layer Loss after iteration 9000: 4.786753121294488\n",
      "three-layer Loss after iteration 10000: 4.413555819792492\n",
      "three-layer Loss after iteration 11000: 3.999481939863258\n",
      "three-layer Loss after iteration 12000: 4.045644209761471\n",
      "three-layer Loss after iteration 13000: 3.6786674364858296\n",
      "three-layer Loss after iteration 14000: 3.538728434129865\n",
      "three-layer Loss after iteration 15000: 3.397202961024138\n",
      "three-layer Loss after iteration 16000: 3.3366109783045306\n",
      "three-layer Loss after iteration 17000: 3.2759345213692552\n",
      "three-layer Loss after iteration 18000: 3.4204781828206894\n",
      "three-layer Loss after iteration 19000: 3.273444525153436\n",
      "three-layer Loss after iteration 20000: 3.108148387013198\n",
      "three-layer Loss after iteration 21000: 3.202051259256279\n",
      "three-layer Loss after iteration 22000: 3.0802162431210287\n",
      "three-layer Loss after iteration 23000: 3.0788824879060903\n",
      "three-layer Loss after iteration 24000: 3.0975162591603023\n",
      "three-layer Loss after iteration 25000: 3.117430545908717\n",
      "three-layer Loss after iteration 26000: 3.0941790046136237\n",
      "three-layer Loss after iteration 27000: 3.070088806206236\n",
      "three-layer Loss after iteration 28000: 3.065354117568286\n",
      "three-layer Loss after iteration 29000: 3.0643529877352704\n",
      "three-layer Loss after iteration 30000: 3.0637938619706264\n",
      "three-layer Loss after iteration 31000: 3.0634243344031375\n",
      "three-layer Loss after iteration 32000: 3.063185029474159\n",
      "7.811680748603735e-05 3.0634243344031375 3.063185029474159\n",
      "{'8': {'losses': [3.6174003731743847, 4.0302617318691345, 2.3542526157535666, 2.527842325041258, 2.2425777626677728, 3.2213845196611124, 3.4575327594329943, 3.3096772983501146, 3.725686206897967, 2.4431653297436964, 2.3393728189569836, 3.184104310011226, 3.4690188981178847, 1.8694220917511037, 3.1217868139570055, 2.38247654431638, 3.5096327922166832, 2.470640460743997, 3.033816853544125, 3.6605498237904737], 'iterations': [26001, 52001, 82001, 45001, 57001, 42001, 57001, 48001, 80001, 89001, 100000, 100000, 83001, 59001, 100000, 36001, 74001, 100000, 42001, 38001]}} {'4+4': {'losses': [4.909367626583326, 3.6378994430040743, 7.275345492905362, 3.3269489985532434, 4.653638691089553, 2.6738579400352975, 3.5012118317153695, 4.038384484205147, 3.0776456592501362, 6.663582240271976, 5.738767284963293, 9.781045436898502, 4.639918441400931, 2.886974818811544, 8.273450943365578, 4.792177222813165, 4.870994692908066, 6.479987362997522, 6.419332096769126, 3.063185029474159], 'iterations': [56001, 79001, 74001, 63001, 76001, 100000, 100000, 60001, 26001, 27001, 100000, 100000, 100000, 100000, 100000, 91001, 100000, 54001, 63001, 32001]}}\n",
      "two-layer Loss after iteration 0: 1315.4978104834272\n",
      "two-layer Loss after iteration 1000: 24.057850084090887\n",
      "two-layer Loss after iteration 2000: 9.175701462639575\n",
      "two-layer Loss after iteration 3000: 6.57544365537579\n",
      "two-layer Loss after iteration 4000: 4.606526198539874\n",
      "two-layer Loss after iteration 5000: 3.115978461541764\n",
      "two-layer Loss after iteration 6000: 2.1376499466167878\n",
      "two-layer Loss after iteration 7000: 1.6914137750024152\n",
      "two-layer Loss after iteration 8000: 1.3973814454157552\n",
      "two-layer Loss after iteration 9000: 1.2154369422275588\n",
      "two-layer Loss after iteration 10000: 1.067479638873474\n",
      "two-layer Loss after iteration 11000: 0.9709806369975885\n",
      "two-layer Loss after iteration 12000: 0.8812691262587777\n",
      "two-layer Loss after iteration 13000: 0.785585611114013\n",
      "two-layer Loss after iteration 14000: 0.7466399350205879\n",
      "two-layer Loss after iteration 15000: 0.7197957052626773\n",
      "two-layer Loss after iteration 16000: 0.7035613348293859\n",
      "two-layer Loss after iteration 17000: 0.6907243034345028\n",
      "two-layer Loss after iteration 18000: 0.6808215700699835\n",
      "two-layer Loss after iteration 19000: 0.6727115638316645\n",
      "two-layer Loss after iteration 20000: 0.6657901280811241\n",
      "two-layer Loss after iteration 21000: 0.659440367987292\n",
      "two-layer Loss after iteration 22000: 0.6538097465095637\n",
      "two-layer Loss after iteration 23000: 0.6487371827581604\n",
      "two-layer Loss after iteration 24000: 0.64241012940211\n",
      "two-layer Loss after iteration 25000: 0.6359379290523839\n",
      "two-layer Loss after iteration 26000: 0.629497931069995\n",
      "two-layer Loss after iteration 27000: 0.6247982592006142\n",
      "two-layer Loss after iteration 28000: 0.6205314157556937\n",
      "two-layer Loss after iteration 29000: 0.6162703515484631\n",
      "two-layer Loss after iteration 30000: 0.6124812294013257\n",
      "two-layer Loss after iteration 31000: 0.6090593202518848\n",
      "two-layer Loss after iteration 32000: 0.6048858333270263\n",
      "two-layer Loss after iteration 33000: 0.6014213359039465\n",
      "two-layer Loss after iteration 34000: 0.5982136756201617\n",
      "two-layer Loss after iteration 35000: 0.595058850011335\n",
      "two-layer Loss after iteration 36000: 0.5918492731259953\n",
      "two-layer Loss after iteration 37000: 0.588849860738893\n",
      "two-layer Loss after iteration 38000: 0.5859055499510822\n",
      "two-layer Loss after iteration 39000: 0.5824307254211951\n",
      "two-layer Loss after iteration 40000: 0.5792536855709641\n",
      "two-layer Loss after iteration 41000: 0.5762701954940873\n",
      "two-layer Loss after iteration 42000: 0.5721216223780805\n",
      "two-layer Loss after iteration 43000: 0.5688220994428472\n",
      "two-layer Loss after iteration 44000: 0.5657973623916203\n",
      "two-layer Loss after iteration 45000: 0.5629875002885607\n",
      "two-layer Loss after iteration 46000: 0.5603333673599594\n",
      "two-layer Loss after iteration 47000: 0.5571126889542546\n",
      "two-layer Loss after iteration 48000: 0.5541398982477318\n",
      "two-layer Loss after iteration 49000: 0.5515231565181757\n",
      "two-layer Loss after iteration 50000: 0.5490691785565809\n",
      "two-layer Loss after iteration 51000: 0.5466817122041251\n",
      "two-layer Loss after iteration 52000: 0.5444546632338783\n",
      "two-layer Loss after iteration 53000: 0.5422275152489349\n",
      "two-layer Loss after iteration 54000: 0.5401728825972237\n",
      "two-layer Loss after iteration 55000: 0.5382593398962331\n",
      "two-layer Loss after iteration 56000: 0.5364721034379252\n",
      "two-layer Loss after iteration 57000: 0.5347955057148938\n",
      "two-layer Loss after iteration 58000: 0.533213099539255\n",
      "two-layer Loss after iteration 59000: 0.531652497003212\n",
      "two-layer Loss after iteration 60000: 0.5300711948744495\n",
      "two-layer Loss after iteration 61000: 0.5285247991473044\n",
      "two-layer Loss after iteration 62000: 0.5271204495640586\n",
      "two-layer Loss after iteration 63000: 0.525407497620064\n",
      "two-layer Loss after iteration 64000: 0.5239975043914084\n",
      "two-layer Loss after iteration 65000: 0.520837129417151\n",
      "two-layer Loss after iteration 66000: 0.517337688453848\n",
      "two-layer Loss after iteration 67000: 0.5150389004640088\n",
      "two-layer Loss after iteration 68000: 0.5130137345181746\n",
      "two-layer Loss after iteration 69000: 0.5111758631750122\n",
      "two-layer Loss after iteration 70000: 0.5094710760710385\n",
      "two-layer Loss after iteration 71000: 0.5052637525760199\n",
      "two-layer Loss after iteration 72000: 0.501100275030232\n",
      "two-layer Loss after iteration 73000: 0.49790728057865696\n",
      "two-layer Loss after iteration 74000: 0.4952781802050997\n",
      "two-layer Loss after iteration 75000: 0.4927764641749991\n",
      "two-layer Loss after iteration 76000: 0.4904949183827253\n",
      "two-layer Loss after iteration 77000: 0.4885584584408014\n",
      "two-layer Loss after iteration 78000: 0.48684207176484406\n",
      "two-layer Loss after iteration 79000: 0.48526320609489826\n",
      "two-layer Loss after iteration 80000: 0.47286075466547484\n",
      "two-layer Loss after iteration 81000: 0.468665302381371\n",
      "two-layer Loss after iteration 82000: 0.46509919331818267\n",
      "two-layer Loss after iteration 83000: 0.4620890792472057\n",
      "two-layer Loss after iteration 84000: 0.4560215489875059\n",
      "two-layer Loss after iteration 85000: 0.4510435733625905\n",
      "two-layer Loss after iteration 86000: 0.4471958118813386\n",
      "two-layer Loss after iteration 87000: 0.44391076303696875\n",
      "two-layer Loss after iteration 88000: 0.44010623497730333\n",
      "two-layer Loss after iteration 89000: 0.4365887948163554\n",
      "two-layer Loss after iteration 90000: 0.43328466438712326\n",
      "two-layer Loss after iteration 91000: 0.42962202805470584\n",
      "two-layer Loss after iteration 92000: 0.42688275859691405\n",
      "two-layer Loss after iteration 93000: 0.42452475591951805\n",
      "two-layer Loss after iteration 94000: 0.42244409380634124\n",
      "two-layer Loss after iteration 95000: 0.42036522228384676\n",
      "two-layer Loss after iteration 96000: 0.41819092838741206\n",
      "two-layer Loss after iteration 97000: 0.41619369864638595\n",
      "two-layer Loss after iteration 98000: 0.41358572710964153\n",
      "two-layer Loss after iteration 99000: 0.41130095817391416\n",
      "two-layer Loss after iteration 0: 1500.072519400362\n",
      "two-layer Loss after iteration 1000: 24.66703085688383\n",
      "two-layer Loss after iteration 2000: 9.376712569639963\n",
      "two-layer Loss after iteration 3000: 6.215710921685482\n",
      "two-layer Loss after iteration 4000: 4.787984362591753\n",
      "two-layer Loss after iteration 5000: 3.789781672550421\n",
      "two-layer Loss after iteration 6000: 3.0806363128294283\n",
      "two-layer Loss after iteration 7000: 2.63932123457493\n",
      "two-layer Loss after iteration 8000: 2.3401516939759888\n",
      "two-layer Loss after iteration 9000: 2.1165000127869313\n",
      "two-layer Loss after iteration 10000: 1.918825027415921\n",
      "two-layer Loss after iteration 11000: 1.59061847804666\n",
      "two-layer Loss after iteration 12000: 1.1649273568817369\n",
      "two-layer Loss after iteration 13000: 1.0293537022041026\n",
      "two-layer Loss after iteration 14000: 0.9463911355427255\n",
      "two-layer Loss after iteration 15000: 0.8836166465255381\n",
      "two-layer Loss after iteration 16000: 0.8255652817475768\n",
      "two-layer Loss after iteration 17000: 0.7554545563803333\n",
      "two-layer Loss after iteration 18000: 0.7275806524332924\n",
      "two-layer Loss after iteration 19000: 0.7042199003063019\n",
      "two-layer Loss after iteration 20000: 0.6861963413379935\n",
      "two-layer Loss after iteration 21000: 0.6690048175275454\n",
      "two-layer Loss after iteration 22000: 0.6563093997205205\n",
      "two-layer Loss after iteration 23000: 0.6458411117015173\n",
      "two-layer Loss after iteration 24000: 0.6364948994799394\n",
      "two-layer Loss after iteration 25000: 0.6276892165261251\n",
      "two-layer Loss after iteration 26000: 0.6178475254611688\n",
      "two-layer Loss after iteration 27000: 0.6093825788440532\n",
      "two-layer Loss after iteration 28000: 0.6009192479101019\n",
      "two-layer Loss after iteration 29000: 0.5938064869186662\n",
      "two-layer Loss after iteration 30000: 0.5873146617282795\n",
      "two-layer Loss after iteration 31000: 0.5813307542847522\n",
      "two-layer Loss after iteration 32000: 0.5757159461457377\n",
      "two-layer Loss after iteration 33000: 0.5704145477081837\n",
      "two-layer Loss after iteration 34000: 0.5653532779802914\n",
      "two-layer Loss after iteration 35000: 0.5605049057429902\n",
      "two-layer Loss after iteration 36000: 0.5558516986755931\n",
      "two-layer Loss after iteration 37000: 0.5512376019261804\n",
      "two-layer Loss after iteration 38000: 0.5465982892708153\n",
      "two-layer Loss after iteration 39000: 0.5423156832816077\n",
      "two-layer Loss after iteration 40000: 0.5381709836949081\n",
      "two-layer Loss after iteration 41000: 0.5341552891395213\n",
      "two-layer Loss after iteration 42000: 0.5294231238540014\n",
      "two-layer Loss after iteration 43000: 0.5253219937074219\n",
      "two-layer Loss after iteration 44000: 0.5206874816781635\n",
      "two-layer Loss after iteration 45000: 0.5160775932278164\n",
      "two-layer Loss after iteration 46000: 0.511965781239284\n",
      "two-layer Loss after iteration 47000: 0.508010612265522\n",
      "two-layer Loss after iteration 48000: 0.5041803997809627\n",
      "two-layer Loss after iteration 49000: 0.5004648742342095\n",
      "two-layer Loss after iteration 50000: 0.4968433480078593\n",
      "two-layer Loss after iteration 51000: 0.49314119423248715\n",
      "two-layer Loss after iteration 52000: 0.489532081145805\n",
      "two-layer Loss after iteration 53000: 0.4858165350856277\n",
      "two-layer Loss after iteration 54000: 0.48234843140680483\n",
      "two-layer Loss after iteration 55000: 0.47897768873323826\n",
      "two-layer Loss after iteration 56000: 0.4757135651727183\n",
      "two-layer Loss after iteration 57000: 0.472590385724674\n",
      "two-layer Loss after iteration 58000: 0.46955873132962117\n",
      "two-layer Loss after iteration 59000: 0.4665909173907364\n",
      "two-layer Loss after iteration 60000: 0.46359259736388747\n",
      "two-layer Loss after iteration 61000: 0.46024584568433385\n",
      "two-layer Loss after iteration 62000: 0.4571951400193266\n",
      "two-layer Loss after iteration 63000: 0.4542958301555939\n",
      "two-layer Loss after iteration 64000: 0.4515069799102274\n",
      "two-layer Loss after iteration 65000: 0.4488169386629263\n",
      "two-layer Loss after iteration 66000: 0.4461890064939836\n",
      "two-layer Loss after iteration 67000: 0.44327106739224365\n",
      "two-layer Loss after iteration 68000: 0.439325576819881\n",
      "two-layer Loss after iteration 69000: 0.4365979325270911\n",
      "two-layer Loss after iteration 70000: 0.433510916267509\n",
      "two-layer Loss after iteration 71000: 0.43056244670733135\n",
      "two-layer Loss after iteration 72000: 0.4260399889174842\n",
      "two-layer Loss after iteration 73000: 0.42268183707869905\n",
      "two-layer Loss after iteration 74000: 0.4201961490189251\n",
      "two-layer Loss after iteration 75000: 0.4179016235485959\n",
      "two-layer Loss after iteration 76000: 0.415680244259111\n",
      "two-layer Loss after iteration 77000: 0.4135653549864788\n",
      "two-layer Loss after iteration 78000: 0.4115525812078086\n",
      "two-layer Loss after iteration 79000: 0.4096521819733914\n",
      "two-layer Loss after iteration 80000: 0.40763646617828775\n",
      "two-layer Loss after iteration 81000: 0.40575343743364417\n",
      "two-layer Loss after iteration 82000: 0.4039681317851358\n",
      "two-layer Loss after iteration 83000: 0.4022387382637239\n",
      "two-layer Loss after iteration 84000: 0.4005893148212811\n",
      "two-layer Loss after iteration 85000: 0.39901907050205404\n",
      "two-layer Loss after iteration 86000: 0.3975208769907082\n",
      "two-layer Loss after iteration 87000: 0.3961001785266708\n",
      "two-layer Loss after iteration 88000: 0.3947903016461377\n",
      "two-layer Loss after iteration 89000: 0.3935414224300638\n",
      "two-layer Loss after iteration 90000: 0.39234600395603747\n",
      "two-layer Loss after iteration 91000: 0.3912009044330462\n",
      "two-layer Loss after iteration 92000: 0.390103782986143\n",
      "two-layer Loss after iteration 93000: 0.3890521564615556\n",
      "two-layer Loss after iteration 94000: 0.3880436624332461\n",
      "two-layer Loss after iteration 95000: 0.38705148328563105\n",
      "two-layer Loss after iteration 96000: 0.3860658769096533\n",
      "two-layer Loss after iteration 97000: 0.385081226160009\n",
      "two-layer Loss after iteration 98000: 0.3838415948839385\n",
      "two-layer Loss after iteration 99000: 0.38291627664080036\n",
      "two-layer Loss after iteration 0: 1344.2170058199167\n",
      "two-layer Loss after iteration 1000: 23.998172330136157\n",
      "two-layer Loss after iteration 2000: 9.296373039097404\n",
      "two-layer Loss after iteration 3000: 6.659534399686488\n",
      "two-layer Loss after iteration 4000: 5.005098605939258\n",
      "two-layer Loss after iteration 5000: 4.142055672807259\n",
      "two-layer Loss after iteration 6000: 3.591605380683285\n",
      "two-layer Loss after iteration 7000: 2.785508500377574\n",
      "two-layer Loss after iteration 8000: 2.3024262096260273\n",
      "two-layer Loss after iteration 9000: 1.950884392597287\n",
      "two-layer Loss after iteration 10000: 1.7125224847172744\n",
      "two-layer Loss after iteration 11000: 1.5375191758908557\n",
      "two-layer Loss after iteration 12000: 1.2826009894241457\n",
      "two-layer Loss after iteration 13000: 0.9783798108463045\n",
      "two-layer Loss after iteration 14000: 0.8434517290674073\n",
      "two-layer Loss after iteration 15000: 0.7736342148399543\n",
      "two-layer Loss after iteration 16000: 0.7284901378835581\n",
      "two-layer Loss after iteration 17000: 0.696367949750946\n",
      "two-layer Loss after iteration 18000: 0.6742391247428338\n",
      "two-layer Loss after iteration 19000: 0.655913073254917\n",
      "two-layer Loss after iteration 20000: 0.6404582098657732\n",
      "two-layer Loss after iteration 21000: 0.6280934479725584\n",
      "two-layer Loss after iteration 22000: 0.6180477515274112\n",
      "two-layer Loss after iteration 23000: 0.6102434062394633\n",
      "two-layer Loss after iteration 24000: 0.6011169111112519\n",
      "two-layer Loss after iteration 25000: 0.5938850881775845\n",
      "two-layer Loss after iteration 26000: 0.5880552899507943\n",
      "two-layer Loss after iteration 27000: 0.58341402373713\n",
      "two-layer Loss after iteration 28000: 0.5783229637130174\n",
      "two-layer Loss after iteration 29000: 0.572065136815168\n",
      "two-layer Loss after iteration 30000: 0.5630459462502423\n",
      "two-layer Loss after iteration 31000: 0.5576854297420482\n",
      "two-layer Loss after iteration 32000: 0.5527691676724087\n",
      "two-layer Loss after iteration 33000: 0.5472928131200301\n",
      "two-layer Loss after iteration 34000: 0.5426856893885661\n",
      "two-layer Loss after iteration 35000: 0.5377534622206781\n",
      "two-layer Loss after iteration 36000: 0.5330596039790335\n",
      "two-layer Loss after iteration 37000: 0.5290725394355451\n",
      "two-layer Loss after iteration 38000: 0.5255375271077818\n",
      "two-layer Loss after iteration 39000: 0.5223208373297198\n",
      "two-layer Loss after iteration 40000: 0.5178757395532809\n",
      "two-layer Loss after iteration 41000: 0.5140061427050268\n",
      "two-layer Loss after iteration 42000: 0.5105540746837606\n",
      "two-layer Loss after iteration 43000: 0.507454729796702\n",
      "two-layer Loss after iteration 44000: 0.504660295302359\n",
      "two-layer Loss after iteration 45000: 0.5021315166875552\n",
      "two-layer Loss after iteration 46000: 0.499835620309846\n",
      "two-layer Loss after iteration 47000: 0.49672629493551623\n",
      "two-layer Loss after iteration 48000: 0.4937635159583572\n",
      "two-layer Loss after iteration 49000: 0.4905038132708589\n",
      "two-layer Loss after iteration 50000: 0.48752922941050525\n",
      "two-layer Loss after iteration 51000: 0.484774321134877\n",
      "two-layer Loss after iteration 52000: 0.4823715211597822\n",
      "two-layer Loss after iteration 53000: 0.48025651742841563\n",
      "two-layer Loss after iteration 54000: 0.4783851506367338\n",
      "two-layer Loss after iteration 55000: 0.4767250279175802\n",
      "two-layer Loss after iteration 56000: 0.47524683302684706\n",
      "two-layer Loss after iteration 57000: 0.4730612519159605\n",
      "two-layer Loss after iteration 58000: 0.47131917176836957\n",
      "two-layer Loss after iteration 59000: 0.4698230781992114\n",
      "two-layer Loss after iteration 60000: 0.46851056183959316\n",
      "two-layer Loss after iteration 61000: 0.4669311732824623\n",
      "two-layer Loss after iteration 62000: 0.46505062635272665\n",
      "two-layer Loss after iteration 63000: 0.46343676658560856\n",
      "two-layer Loss after iteration 64000: 0.46203630849534233\n",
      "two-layer Loss after iteration 65000: 0.4608143459005309\n",
      "two-layer Loss after iteration 66000: 0.4597433324242428\n",
      "two-layer Loss after iteration 67000: 0.458623064633379\n",
      "two-layer Loss after iteration 68000: 0.45727449616874044\n",
      "two-layer Loss after iteration 69000: 0.4561746479686365\n",
      "two-layer Loss after iteration 70000: 0.4552233618180922\n",
      "two-layer Loss after iteration 71000: 0.4543962206284472\n",
      "two-layer Loss after iteration 72000: 0.45366739299296893\n",
      "two-layer Loss after iteration 73000: 0.453028592996433\n",
      "two-layer Loss after iteration 74000: 0.45246667515563005\n",
      "two-layer Loss after iteration 75000: 0.45197051690295703\n",
      "two-layer Loss after iteration 76000: 0.4512833601554918\n",
      "two-layer Loss after iteration 77000: 0.45054542738141073\n",
      "two-layer Loss after iteration 78000: 0.44992253566537305\n",
      "two-layer Loss after iteration 79000: 0.44938197581287365\n",
      "two-layer Loss after iteration 80000: 0.44891003804106966\n",
      "two-layer Loss after iteration 81000: 0.44716440199767066\n",
      "two-layer Loss after iteration 82000: 0.4462433852034969\n",
      "two-layer Loss after iteration 83000: 0.4455833448195363\n",
      "two-layer Loss after iteration 84000: 0.4450372553579253\n",
      "two-layer Loss after iteration 85000: 0.44457523491539297\n",
      "two-layer Loss after iteration 86000: 0.4441789818945504\n",
      "two-layer Loss after iteration 87000: 0.44383591433101616\n",
      "two-layer Loss after iteration 88000: 0.44353584303776716\n",
      "two-layer Loss after iteration 89000: 0.4432721280176258\n",
      "two-layer Loss after iteration 90000: 0.4427131823498327\n",
      "two-layer Loss after iteration 91000: 0.4423856408220127\n",
      "two-layer Loss after iteration 92000: 0.44207479508263875\n",
      "two-layer Loss after iteration 93000: 0.44180466477847036\n",
      "two-layer Loss after iteration 94000: 0.44157276598462153\n",
      "two-layer Loss after iteration 95000: 0.44137233593953873\n",
      "two-layer Loss after iteration 96000: 0.44119628568150315\n",
      "two-layer Loss after iteration 97000: 0.4410178591600758\n",
      "two-layer Loss after iteration 98000: 0.4408165128249699\n",
      "two-layer Loss after iteration 99000: 0.44065475858222536\n",
      "two-layer Loss after iteration 0: 1566.3085650147311\n",
      "two-layer Loss after iteration 1000: 24.455520237294518\n",
      "two-layer Loss after iteration 2000: 9.186968231002966\n",
      "two-layer Loss after iteration 3000: 6.321183737067634\n",
      "two-layer Loss after iteration 4000: 4.922707636032341\n",
      "two-layer Loss after iteration 5000: 3.7783476998766794\n",
      "two-layer Loss after iteration 6000: 3.2722197148884042\n",
      "two-layer Loss after iteration 7000: 2.9412475370326217\n",
      "two-layer Loss after iteration 8000: 2.617446498855054\n",
      "two-layer Loss after iteration 9000: 2.072616001268778\n",
      "two-layer Loss after iteration 10000: 1.7408117803103906\n",
      "two-layer Loss after iteration 11000: 1.5256258102270759\n",
      "two-layer Loss after iteration 12000: 1.3471142067162762\n",
      "two-layer Loss after iteration 13000: 1.2193338378428245\n",
      "two-layer Loss after iteration 14000: 0.910882746210531\n",
      "two-layer Loss after iteration 15000: 0.7304278858615413\n",
      "two-layer Loss after iteration 16000: 0.6569213163536528\n",
      "two-layer Loss after iteration 17000: 0.5978180434163336\n",
      "two-layer Loss after iteration 18000: 0.5348201963797515\n",
      "two-layer Loss after iteration 19000: 0.5093488315545546\n",
      "two-layer Loss after iteration 20000: 0.49778049270338404\n",
      "two-layer Loss after iteration 21000: 0.487364920581521\n",
      "two-layer Loss after iteration 22000: 0.4799279504389406\n",
      "two-layer Loss after iteration 23000: 0.47231016554050537\n",
      "two-layer Loss after iteration 24000: 0.4649075619581726\n",
      "two-layer Loss after iteration 25000: 0.4578373638057787\n",
      "two-layer Loss after iteration 26000: 0.451264685865624\n",
      "two-layer Loss after iteration 27000: 0.44741717491933986\n",
      "two-layer Loss after iteration 28000: 0.44299665266111954\n",
      "two-layer Loss after iteration 29000: 0.438034985094593\n",
      "two-layer Loss after iteration 30000: 0.4332596691704142\n",
      "two-layer Loss after iteration 31000: 0.42743230837581286\n",
      "two-layer Loss after iteration 32000: 0.42294637924823825\n",
      "two-layer Loss after iteration 33000: 0.4198956186763917\n",
      "two-layer Loss after iteration 34000: 0.4181834468703318\n",
      "two-layer Loss after iteration 35000: 0.4167610842416643\n",
      "two-layer Loss after iteration 36000: 0.4154357968919007\n",
      "two-layer Loss after iteration 37000: 0.4125442178316063\n",
      "two-layer Loss after iteration 38000: 0.41092848782832875\n",
      "two-layer Loss after iteration 39000: 0.40909415107339886\n",
      "two-layer Loss after iteration 40000: 0.40745199594651166\n",
      "two-layer Loss after iteration 41000: 0.4057110978911635\n",
      "two-layer Loss after iteration 42000: 0.40419752625735844\n",
      "two-layer Loss after iteration 43000: 0.4027795048567009\n",
      "two-layer Loss after iteration 44000: 0.4014419762275519\n",
      "two-layer Loss after iteration 45000: 0.39959330065167037\n",
      "two-layer Loss after iteration 46000: 0.39824336450543524\n",
      "two-layer Loss after iteration 47000: 0.39696073597763604\n",
      "two-layer Loss after iteration 48000: 0.39573846492445464\n",
      "two-layer Loss after iteration 49000: 0.3943818862894009\n",
      "two-layer Loss after iteration 50000: 0.39317121239577607\n",
      "two-layer Loss after iteration 51000: 0.39201548827932065\n",
      "two-layer Loss after iteration 52000: 0.3909107651029261\n",
      "two-layer Loss after iteration 53000: 0.38985362193761475\n",
      "two-layer Loss after iteration 54000: 0.38883948359042314\n",
      "two-layer Loss after iteration 55000: 0.38786668445722966\n",
      "two-layer Loss after iteration 56000: 0.38693442304976855\n",
      "two-layer Loss after iteration 57000: 0.3860403539864866\n",
      "two-layer Loss after iteration 58000: 0.3838340477450893\n",
      "two-layer Loss after iteration 59000: 0.3826977020716601\n",
      "two-layer Loss after iteration 60000: 0.38116291714623785\n",
      "two-layer Loss after iteration 61000: 0.3797639594665529\n",
      "two-layer Loss after iteration 62000: 0.3776103569204967\n",
      "two-layer Loss after iteration 63000: 0.37637743192713496\n",
      "two-layer Loss after iteration 64000: 0.3752402974841598\n",
      "two-layer Loss after iteration 65000: 0.37417820743311836\n",
      "two-layer Loss after iteration 66000: 0.3731820965291523\n",
      "two-layer Loss after iteration 67000: 0.37222555894177844\n",
      "two-layer Loss after iteration 68000: 0.37132294816695294\n",
      "two-layer Loss after iteration 69000: 0.3704741243803258\n",
      "two-layer Loss after iteration 70000: 0.36967640641652316\n",
      "two-layer Loss after iteration 71000: 0.3689347165966399\n",
      "two-layer Loss after iteration 72000: 0.36823436758371547\n",
      "two-layer Loss after iteration 73000: 0.36730793179264354\n",
      "two-layer Loss after iteration 74000: 0.36649276617475435\n",
      "two-layer Loss after iteration 75000: 0.36580701386507486\n",
      "two-layer Loss after iteration 76000: 0.36517196908816574\n",
      "two-layer Loss after iteration 77000: 0.36457090624118893\n",
      "two-layer Loss after iteration 78000: 0.3640045998308491\n",
      "two-layer Loss after iteration 79000: 0.36346795675764293\n",
      "two-layer Loss after iteration 80000: 0.36296448094835\n",
      "two-layer Loss after iteration 81000: 0.3624912194089432\n",
      "two-layer Loss after iteration 82000: 0.3620449473455084\n",
      "two-layer Loss after iteration 83000: 0.36162697970898483\n",
      "two-layer Loss after iteration 84000: 0.36122635631637967\n",
      "two-layer Loss after iteration 85000: 0.36085892772131045\n",
      "two-layer Loss after iteration 86000: 0.3605064774773176\n",
      "two-layer Loss after iteration 87000: 0.36017629221072855\n",
      "two-layer Loss after iteration 88000: 0.35933501073637936\n",
      "two-layer Loss after iteration 89000: 0.35889253813563693\n",
      "two-layer Loss after iteration 90000: 0.3585010000983487\n",
      "two-layer Loss after iteration 91000: 0.35814633334378204\n",
      "two-layer Loss after iteration 92000: 0.3578226494221455\n",
      "two-layer Loss after iteration 93000: 0.3574516988188136\n",
      "two-layer Loss after iteration 94000: 0.3571162571705418\n",
      "two-layer Loss after iteration 95000: 0.35681543348549666\n",
      "two-layer Loss after iteration 96000: 0.3565389291109507\n",
      "two-layer Loss after iteration 97000: 0.35628476064664144\n",
      "two-layer Loss after iteration 98000: 0.35605444581154666\n",
      "two-layer Loss after iteration 99000: 0.3558453688266477\n",
      "two-layer Loss after iteration 0: 1660.6277278650448\n",
      "two-layer Loss after iteration 1000: 23.690416502654152\n",
      "two-layer Loss after iteration 2000: 8.838650153115033\n",
      "two-layer Loss after iteration 3000: 5.977308416820481\n",
      "two-layer Loss after iteration 4000: 4.46550136862868\n",
      "two-layer Loss after iteration 5000: 3.2481830793262705\n",
      "two-layer Loss after iteration 6000: 2.4690755604527252\n",
      "two-layer Loss after iteration 7000: 2.159229383543718\n",
      "two-layer Loss after iteration 8000: 1.954741948656383\n",
      "two-layer Loss after iteration 9000: 1.8106947034873417\n",
      "two-layer Loss after iteration 10000: 1.4593411370920524\n",
      "two-layer Loss after iteration 11000: 1.0185821491626483\n",
      "two-layer Loss after iteration 12000: 0.8998091468753356\n",
      "two-layer Loss after iteration 13000: 0.8434473979715224\n",
      "two-layer Loss after iteration 14000: 0.8106422245702117\n",
      "two-layer Loss after iteration 15000: 0.7858990771805178\n",
      "two-layer Loss after iteration 16000: 0.7678873755527675\n",
      "two-layer Loss after iteration 17000: 0.7545118856667917\n",
      "two-layer Loss after iteration 18000: 0.7412780544428247\n",
      "two-layer Loss after iteration 19000: 0.7273123779605514\n",
      "two-layer Loss after iteration 20000: 0.7172878895108843\n",
      "two-layer Loss after iteration 21000: 0.7094552512525761\n",
      "two-layer Loss after iteration 22000: 0.7027234356482134\n",
      "two-layer Loss after iteration 23000: 0.6949748005017151\n",
      "two-layer Loss after iteration 24000: 0.6850142812903713\n",
      "two-layer Loss after iteration 25000: 0.6776713985960585\n",
      "two-layer Loss after iteration 26000: 0.672812793390358\n",
      "two-layer Loss after iteration 27000: 0.668712180415401\n",
      "two-layer Loss after iteration 28000: 0.665111533649175\n",
      "two-layer Loss after iteration 29000: 0.6618607336998585\n",
      "two-layer Loss after iteration 30000: 0.6588979310218028\n",
      "two-layer Loss after iteration 31000: 0.6563934830247475\n",
      "two-layer Loss after iteration 32000: 0.6541822675926884\n",
      "two-layer Loss after iteration 33000: 0.6519783429216954\n",
      "two-layer Loss after iteration 34000: 0.6497166608179683\n",
      "two-layer Loss after iteration 35000: 0.6469164121309124\n",
      "two-layer Loss after iteration 36000: 0.6445611208779771\n",
      "two-layer Loss after iteration 37000: 0.6425437412858039\n",
      "two-layer Loss after iteration 38000: 0.6407028454282591\n",
      "two-layer Loss after iteration 39000: 0.6389818018671006\n",
      "two-layer Loss after iteration 40000: 0.6373591798842159\n",
      "two-layer Loss after iteration 41000: 0.6358182868337808\n",
      "two-layer Loss after iteration 42000: 0.6343424146918111\n",
      "two-layer Loss after iteration 43000: 0.6329176451740449\n",
      "two-layer Loss after iteration 44000: 0.6315641477499547\n",
      "two-layer Loss after iteration 45000: 0.6302874729274444\n",
      "two-layer Loss after iteration 46000: 0.6256706282166717\n",
      "two-layer Loss after iteration 47000: 0.6214063758567782\n",
      "two-layer Loss after iteration 48000: 0.6193817416718783\n",
      "two-layer Loss after iteration 49000: 0.6141284625868004\n",
      "two-layer Loss after iteration 50000: 0.6118674108202737\n",
      "two-layer Loss after iteration 51000: 0.6094420447581891\n",
      "two-layer Loss after iteration 52000: 0.6075371635308725\n",
      "two-layer Loss after iteration 53000: 0.6058561346787388\n",
      "two-layer Loss after iteration 54000: 0.6043097727233608\n",
      "two-layer Loss after iteration 55000: 0.6028636768791444\n",
      "two-layer Loss after iteration 56000: 0.6015088738830239\n",
      "two-layer Loss after iteration 57000: 0.6002229452854876\n",
      "two-layer Loss after iteration 58000: 0.5989944479307053\n",
      "two-layer Loss after iteration 59000: 0.5978328639243957\n",
      "two-layer Loss after iteration 60000: 0.5967318959117606\n",
      "two-layer Loss after iteration 61000: 0.5956896545728997\n",
      "two-layer Loss after iteration 62000: 0.5947047027463481\n",
      "two-layer Loss after iteration 63000: 0.5937637548648594\n",
      "two-layer Loss after iteration 64000: 0.5928781811262719\n",
      "two-layer Loss after iteration 65000: 0.5920285000324144\n",
      "two-layer Loss after iteration 66000: 0.5912248010999808\n",
      "two-layer Loss after iteration 67000: 0.5904671713891647\n",
      "two-layer Loss after iteration 68000: 0.5897366131129578\n",
      "two-layer Loss after iteration 69000: 0.5890517665268971\n",
      "two-layer Loss after iteration 70000: 0.5883962316052934\n",
      "two-layer Loss after iteration 71000: 0.5877721647364842\n",
      "two-layer Loss after iteration 72000: 0.5871787035422904\n",
      "two-layer Loss after iteration 73000: 0.5866125842630378\n",
      "two-layer Loss after iteration 74000: 0.586076483916913\n",
      "two-layer Loss after iteration 75000: 0.5855641781951171\n",
      "two-layer Loss after iteration 76000: 0.5850786910572161\n",
      "two-layer Loss after iteration 77000: 0.5846190443681616\n",
      "two-layer Loss after iteration 78000: 0.5841809878190625\n",
      "two-layer Loss after iteration 79000: 0.5837572380563562\n",
      "two-layer Loss after iteration 80000: 0.5833583816251922\n",
      "two-layer Loss after iteration 81000: 0.5829771872884809\n",
      "two-layer Loss after iteration 82000: 0.5826104180731579\n",
      "two-layer Loss after iteration 83000: 0.5822594035223921\n",
      "two-layer Loss after iteration 84000: 0.5819220355855205\n",
      "two-layer Loss after iteration 85000: 0.5816032933216027\n",
      "two-layer Loss after iteration 86000: 0.5812938230340443\n",
      "two-layer Loss after iteration 87000: 0.5809918642747605\n",
      "two-layer Loss after iteration 88000: 0.5807099943635524\n",
      "two-layer Loss after iteration 89000: 0.5804334598235662\n",
      "two-layer Loss after iteration 90000: 0.5801667953041338\n",
      "two-layer Loss after iteration 91000: 0.5799124242109008\n",
      "two-layer Loss after iteration 92000: 0.5796706013777526\n",
      "two-layer Loss after iteration 93000: 0.5794326403488882\n",
      "two-layer Loss after iteration 94000: 0.5792024497908812\n",
      "two-layer Loss after iteration 95000: 0.578979640108523\n",
      "two-layer Loss after iteration 96000: 0.5787624852640316\n",
      "two-layer Loss after iteration 97000: 0.5785552340291876\n",
      "two-layer Loss after iteration 98000: 0.5783536438894153\n",
      "two-layer Loss after iteration 99000: 0.5781605137284591\n",
      "two-layer Loss after iteration 0: 1499.877555644865\n",
      "two-layer Loss after iteration 1000: 23.993755494081366\n",
      "two-layer Loss after iteration 2000: 8.910216118324508\n",
      "two-layer Loss after iteration 3000: 5.899611198896707\n",
      "two-layer Loss after iteration 4000: 4.406572503925635\n",
      "two-layer Loss after iteration 5000: 3.3554653213653673\n",
      "two-layer Loss after iteration 6000: 2.7043407487923306\n",
      "two-layer Loss after iteration 7000: 1.9655715572556383\n",
      "two-layer Loss after iteration 8000: 1.4158904113275128\n",
      "two-layer Loss after iteration 9000: 1.1879731081198328\n",
      "two-layer Loss after iteration 10000: 1.0336775154665945\n",
      "two-layer Loss after iteration 11000: 0.9491115584909529\n",
      "two-layer Loss after iteration 12000: 0.8951163409843975\n",
      "two-layer Loss after iteration 13000: 0.8619562659539161\n",
      "two-layer Loss after iteration 14000: 0.8404515363930808\n",
      "two-layer Loss after iteration 15000: 0.8240146104441942\n",
      "two-layer Loss after iteration 16000: 0.8097653763421871\n",
      "two-layer Loss after iteration 17000: 0.7995529004836731\n",
      "two-layer Loss after iteration 18000: 0.7908027824179712\n",
      "two-layer Loss after iteration 19000: 0.7817080754732284\n",
      "two-layer Loss after iteration 20000: 0.773565068601806\n",
      "two-layer Loss after iteration 21000: 0.7658843004575874\n",
      "two-layer Loss after iteration 22000: 0.7593133214882382\n",
      "two-layer Loss after iteration 23000: 0.753439820885591\n",
      "two-layer Loss after iteration 24000: 0.7481955080720718\n",
      "two-layer Loss after iteration 25000: 0.7434721017695458\n",
      "two-layer Loss after iteration 26000: 0.7392297520374604\n",
      "two-layer Loss after iteration 27000: 0.7354299276128096\n",
      "two-layer Loss after iteration 28000: 0.7319736648927687\n",
      "two-layer Loss after iteration 29000: 0.7287964603917908\n",
      "two-layer Loss after iteration 30000: 0.7220694920242462\n",
      "two-layer Loss after iteration 31000: 0.7178125105035047\n",
      "two-layer Loss after iteration 32000: 0.7145717572527956\n",
      "two-layer Loss after iteration 33000: 0.7115005165680193\n",
      "two-layer Loss after iteration 34000: 0.7083784261382109\n",
      "two-layer Loss after iteration 35000: 0.7043723732132788\n",
      "two-layer Loss after iteration 36000: 0.6997998680653859\n",
      "two-layer Loss after iteration 37000: 0.6960298900143221\n",
      "two-layer Loss after iteration 38000: 0.6919840454829295\n",
      "two-layer Loss after iteration 39000: 0.6889468135370842\n",
      "two-layer Loss after iteration 40000: 0.686305321956563\n",
      "two-layer Loss after iteration 41000: 0.6839652996469023\n",
      "two-layer Loss after iteration 42000: 0.6821333092554119\n",
      "two-layer Loss after iteration 43000: 0.6801274410842041\n",
      "two-layer Loss after iteration 44000: 0.6783199190383573\n",
      "two-layer Loss after iteration 45000: 0.6766685944364434\n",
      "two-layer Loss after iteration 46000: 0.6751445316657283\n",
      "two-layer Loss after iteration 47000: 0.6736592983912695\n",
      "two-layer Loss after iteration 48000: 0.6720973821491782\n",
      "two-layer Loss after iteration 49000: 0.6708192494429175\n",
      "two-layer Loss after iteration 50000: 0.6695085322022275\n",
      "two-layer Loss after iteration 51000: 0.6680719600869518\n",
      "two-layer Loss after iteration 52000: 0.6666495771075767\n",
      "two-layer Loss after iteration 53000: 0.6654076409388548\n",
      "two-layer Loss after iteration 54000: 0.6641459665907836\n",
      "two-layer Loss after iteration 55000: 0.6630274094906364\n",
      "two-layer Loss after iteration 56000: 0.6620826139246703\n",
      "two-layer Loss after iteration 57000: 0.6611798541671823\n",
      "two-layer Loss after iteration 58000: 0.6603063628234284\n",
      "two-layer Loss after iteration 59000: 0.6594655009930701\n",
      "two-layer Loss after iteration 60000: 0.6585669357207325\n",
      "two-layer Loss after iteration 61000: 0.6571643157877624\n",
      "two-layer Loss after iteration 62000: 0.6560997771508398\n",
      "two-layer Loss after iteration 63000: 0.6551434406912459\n",
      "two-layer Loss after iteration 64000: 0.6542450808131748\n",
      "two-layer Loss after iteration 65000: 0.6533782409911149\n",
      "two-layer Loss after iteration 66000: 0.6510961993301998\n",
      "two-layer Loss after iteration 67000: 0.6487724324893785\n",
      "two-layer Loss after iteration 68000: 0.6473770626547493\n",
      "two-layer Loss after iteration 69000: 0.6460934748310118\n",
      "two-layer Loss after iteration 70000: 0.6448790320777932\n",
      "two-layer Loss after iteration 71000: 0.6437130308112382\n",
      "two-layer Loss after iteration 72000: 0.6425915494459061\n",
      "two-layer Loss after iteration 73000: 0.6415107602883692\n",
      "two-layer Loss after iteration 74000: 0.640463853776293\n",
      "two-layer Loss after iteration 75000: 0.6394532848846052\n",
      "two-layer Loss after iteration 76000: 0.63847456559449\n",
      "two-layer Loss after iteration 77000: 0.6375278719436587\n",
      "two-layer Loss after iteration 78000: 0.6366114441605444\n",
      "two-layer Loss after iteration 79000: 0.6357246996572463\n",
      "two-layer Loss after iteration 80000: 0.6348683318591452\n",
      "two-layer Loss after iteration 81000: 0.6340402555351442\n",
      "two-layer Loss after iteration 82000: 0.6332395650556886\n",
      "two-layer Loss after iteration 83000: 0.6324675840300096\n",
      "two-layer Loss after iteration 84000: 0.6317202704823544\n",
      "two-layer Loss after iteration 85000: 0.6310000953648055\n",
      "two-layer Loss after iteration 86000: 0.6303061074484306\n",
      "two-layer Loss after iteration 87000: 0.6296377287005488\n",
      "two-layer Loss after iteration 88000: 0.6289937645888037\n",
      "two-layer Loss after iteration 89000: 0.6283728737004826\n",
      "two-layer Loss after iteration 90000: 0.6277747527447659\n",
      "two-layer Loss after iteration 91000: 0.6269422900281746\n",
      "two-layer Loss after iteration 92000: 0.6261383586836062\n",
      "two-layer Loss after iteration 93000: 0.6254729804495016\n",
      "two-layer Loss after iteration 94000: 0.6248685671293088\n",
      "two-layer Loss after iteration 95000: 0.6242952572037238\n",
      "two-layer Loss after iteration 96000: 0.6237517202962487\n",
      "two-layer Loss after iteration 97000: 0.6230401940708991\n",
      "two-layer Loss after iteration 98000: 0.6224994093518519\n",
      "two-layer Loss after iteration 99000: 0.6219842677585644\n",
      "two-layer Loss after iteration 0: 1807.8124019203644\n",
      "two-layer Loss after iteration 1000: 23.782497215136075\n",
      "two-layer Loss after iteration 2000: 8.871355742341304\n",
      "two-layer Loss after iteration 3000: 6.120368489590803\n",
      "two-layer Loss after iteration 4000: 4.64144685726511\n",
      "two-layer Loss after iteration 5000: 3.182856870460588\n",
      "two-layer Loss after iteration 6000: 2.651865455481979\n",
      "two-layer Loss after iteration 7000: 1.9706928517566489\n",
      "two-layer Loss after iteration 8000: 1.5665936317655984\n",
      "two-layer Loss after iteration 9000: 1.3211137755985005\n",
      "two-layer Loss after iteration 10000: 1.1873932511222929\n",
      "two-layer Loss after iteration 11000: 1.1067748384592813\n",
      "two-layer Loss after iteration 12000: 1.0428728405529513\n",
      "two-layer Loss after iteration 13000: 0.9899160267713946\n",
      "two-layer Loss after iteration 14000: 0.9550252286658186\n",
      "two-layer Loss after iteration 15000: 0.9280382784140461\n",
      "two-layer Loss after iteration 16000: 0.9023333278167509\n",
      "two-layer Loss after iteration 17000: 0.8846811742197211\n",
      "two-layer Loss after iteration 18000: 0.8707859516022332\n",
      "two-layer Loss after iteration 19000: 0.8592975121338462\n",
      "two-layer Loss after iteration 20000: 0.8493113904886402\n",
      "two-layer Loss after iteration 21000: 0.8402701294775219\n",
      "two-layer Loss after iteration 22000: 0.8321321631020621\n",
      "two-layer Loss after iteration 23000: 0.824659741741639\n",
      "two-layer Loss after iteration 24000: 0.8177093297763923\n",
      "two-layer Loss after iteration 25000: 0.8111989430568719\n",
      "two-layer Loss after iteration 26000: 0.8051062405281896\n",
      "two-layer Loss after iteration 27000: 0.7993534626061574\n",
      "two-layer Loss after iteration 28000: 0.7939073350882371\n",
      "two-layer Loss after iteration 29000: 0.788736379344145\n",
      "two-layer Loss after iteration 30000: 0.7838208099533642\n",
      "two-layer Loss after iteration 31000: 0.7791514039256309\n",
      "two-layer Loss after iteration 32000: 0.7746054039544983\n",
      "two-layer Loss after iteration 33000: 0.7700779180180912\n",
      "two-layer Loss after iteration 34000: 0.7657662540242145\n",
      "two-layer Loss after iteration 35000: 0.7616528683594854\n",
      "two-layer Loss after iteration 36000: 0.7577311301002798\n",
      "two-layer Loss after iteration 37000: 0.7541721265825312\n",
      "two-layer Loss after iteration 38000: 0.7508263869344766\n",
      "two-layer Loss after iteration 39000: 0.7476668056487359\n",
      "two-layer Loss after iteration 40000: 0.7446828507125317\n",
      "two-layer Loss after iteration 41000: 0.7418610718961625\n",
      "two-layer Loss after iteration 42000: 0.739224550086072\n",
      "two-layer Loss after iteration 43000: 0.7368214072335888\n",
      "two-layer Loss after iteration 44000: 0.734583177121175\n",
      "two-layer Loss after iteration 45000: 0.7324885059575391\n",
      "two-layer Loss after iteration 46000: 0.7305203662603009\n",
      "two-layer Loss after iteration 47000: 0.7286087596735075\n",
      "two-layer Loss after iteration 48000: 0.7268105793035874\n",
      "two-layer Loss after iteration 49000: 0.7251311376753651\n",
      "two-layer Loss after iteration 50000: 0.723558798228421\n",
      "two-layer Loss after iteration 51000: 0.7220966991560588\n",
      "two-layer Loss after iteration 52000: 0.7207368811251451\n",
      "two-layer Loss after iteration 53000: 0.7194745264343111\n",
      "two-layer Loss after iteration 54000: 0.7182066168939094\n",
      "two-layer Loss after iteration 55000: 0.7166304600630575\n",
      "two-layer Loss after iteration 56000: 0.7153228322746688\n",
      "two-layer Loss after iteration 57000: 0.7141764405845876\n",
      "two-layer Loss after iteration 58000: 0.7131409356245855\n",
      "two-layer Loss after iteration 59000: 0.7121856736186731\n",
      "two-layer Loss after iteration 60000: 0.7113011158423167\n",
      "two-layer Loss after iteration 61000: 0.7104921888323602\n",
      "two-layer Loss after iteration 62000: 0.7096956585308165\n",
      "two-layer Loss after iteration 63000: 0.7090126735765635\n",
      "two-layer Loss after iteration 64000: 0.708381178430774\n",
      "two-layer Loss after iteration 65000: 0.7077906758542133\n",
      "two-layer Loss after iteration 66000: 0.7072362330594655\n",
      "two-layer Loss after iteration 67000: 0.7067142198969545\n",
      "two-layer Loss after iteration 68000: 0.706221557183415\n",
      "two-layer Loss after iteration 69000: 0.7057559130192023\n",
      "two-layer Loss after iteration 70000: 0.7053150419112867\n",
      "two-layer Loss after iteration 71000: 0.7048978560971293\n",
      "two-layer Loss after iteration 72000: 0.7045023219354979\n",
      "two-layer Loss after iteration 73000: 0.704127504104417\n",
      "two-layer Loss after iteration 74000: 0.7033828448649859\n",
      "two-layer Loss after iteration 75000: 0.7028931565828637\n",
      "two-layer Loss after iteration 76000: 0.7024477662956675\n",
      "two-layer Loss after iteration 77000: 0.7020339736733993\n",
      "two-layer Loss after iteration 78000: 0.7016481175773149\n",
      "two-layer Loss after iteration 79000: 0.701284742367192\n",
      "two-layer Loss after iteration 80000: 0.7009439304319275\n",
      "two-layer Loss after iteration 81000: 0.7006217876901618\n",
      "two-layer Loss after iteration 82000: 0.700319684339666\n",
      "two-layer Loss after iteration 83000: 0.700015747404038\n",
      "two-layer Loss after iteration 84000: 0.6996814830267527\n",
      "two-layer Loss after iteration 85000: 0.6993747754947611\n",
      "two-layer Loss after iteration 86000: 0.6990879054867472\n",
      "two-layer Loss after iteration 87000: 0.6988173054228192\n",
      "two-layer Loss after iteration 88000: 0.6985620409366171\n",
      "two-layer Loss after iteration 89000: 0.6983204390880776\n",
      "two-layer Loss after iteration 90000: 0.698090448814118\n",
      "two-layer Loss after iteration 91000: 0.6978734659904345\n",
      "two-layer Loss after iteration 92000: 0.6976659758562608\n",
      "two-layer Loss after iteration 93000: 0.6974689061569334\n",
      "two-layer Loss after iteration 94000: 0.6972824273426247\n",
      "two-layer Loss after iteration 95000: 0.6971040277208701\n",
      "two-layer Loss after iteration 96000: 0.6969330564938216\n",
      "two-layer Loss after iteration 97000: 0.6967713392234324\n",
      "two-layer Loss after iteration 98000: 0.6966159656758645\n",
      "two-layer Loss after iteration 99000: 0.6964661643887197\n",
      "two-layer Loss after iteration 0: 1422.9244900780866\n",
      "two-layer Loss after iteration 1000: 23.72368977148971\n",
      "two-layer Loss after iteration 2000: 9.053758855053522\n",
      "two-layer Loss after iteration 3000: 6.2515402913752425\n",
      "two-layer Loss after iteration 4000: 4.9684433300209\n",
      "two-layer Loss after iteration 5000: 3.5373571020782353\n",
      "two-layer Loss after iteration 6000: 2.7234182494955355\n",
      "two-layer Loss after iteration 7000: 2.3236807496541036\n",
      "two-layer Loss after iteration 8000: 2.051602805786923\n",
      "two-layer Loss after iteration 9000: 1.8057471244139318\n",
      "two-layer Loss after iteration 10000: 1.4364603710976491\n",
      "two-layer Loss after iteration 11000: 1.0620778684149965\n",
      "two-layer Loss after iteration 12000: 0.8993470786110405\n",
      "two-layer Loss after iteration 13000: 0.7356495558723755\n",
      "two-layer Loss after iteration 14000: 0.6462405553616558\n",
      "two-layer Loss after iteration 15000: 0.5691936506641065\n",
      "two-layer Loss after iteration 16000: 0.5353537522494756\n",
      "two-layer Loss after iteration 17000: 0.5088468057333382\n",
      "two-layer Loss after iteration 18000: 0.4859718008096825\n",
      "two-layer Loss after iteration 19000: 0.4649509655710519\n",
      "two-layer Loss after iteration 20000: 0.44639369170469256\n",
      "two-layer Loss after iteration 21000: 0.42835551462110577\n",
      "two-layer Loss after iteration 22000: 0.4126026586649252\n",
      "two-layer Loss after iteration 23000: 0.3993478944594179\n",
      "two-layer Loss after iteration 24000: 0.3847186161821652\n",
      "two-layer Loss after iteration 25000: 0.3730619387902157\n",
      "two-layer Loss after iteration 26000: 0.3608579583855863\n",
      "two-layer Loss after iteration 27000: 0.3485030928713852\n",
      "two-layer Loss after iteration 28000: 0.34058305511961856\n",
      "two-layer Loss after iteration 29000: 0.3320978507204773\n",
      "two-layer Loss after iteration 30000: 0.3256355173569024\n",
      "two-layer Loss after iteration 31000: 0.3196649449736467\n",
      "two-layer Loss after iteration 32000: 0.3129897406289901\n",
      "two-layer Loss after iteration 33000: 0.3077212335761075\n",
      "two-layer Loss after iteration 34000: 0.30308033213451496\n",
      "two-layer Loss after iteration 35000: 0.2988687809439974\n",
      "two-layer Loss after iteration 36000: 0.2934298900967494\n",
      "two-layer Loss after iteration 37000: 0.28918514766046804\n",
      "two-layer Loss after iteration 38000: 0.2850321599944223\n",
      "two-layer Loss after iteration 39000: 0.28146483505135006\n",
      "two-layer Loss after iteration 40000: 0.27818376973127973\n",
      "two-layer Loss after iteration 41000: 0.27513539467766457\n",
      "two-layer Loss after iteration 42000: 0.27227211336410545\n",
      "two-layer Loss after iteration 43000: 0.269568000212413\n",
      "two-layer Loss after iteration 44000: 0.2670010003496752\n",
      "two-layer Loss after iteration 45000: 0.26455274041681415\n",
      "two-layer Loss after iteration 46000: 0.26220919574286844\n",
      "two-layer Loss after iteration 47000: 0.25995721608776023\n",
      "two-layer Loss after iteration 48000: 0.25778445353208135\n",
      "two-layer Loss after iteration 49000: 0.25562029873565634\n",
      "two-layer Loss after iteration 50000: 0.25337398401850453\n",
      "two-layer Loss after iteration 51000: 0.25122717955193163\n",
      "two-layer Loss after iteration 52000: 0.24915405630413343\n",
      "two-layer Loss after iteration 53000: 0.24714529007625283\n",
      "two-layer Loss after iteration 54000: 0.2452037800358674\n",
      "two-layer Loss after iteration 55000: 0.24244608297610748\n",
      "two-layer Loss after iteration 56000: 0.24044403430456127\n",
      "two-layer Loss after iteration 57000: 0.23856982363075502\n",
      "two-layer Loss after iteration 58000: 0.23677804169332245\n",
      "two-layer Loss after iteration 59000: 0.23499828301677578\n",
      "two-layer Loss after iteration 60000: 0.23329393148925184\n",
      "two-layer Loss after iteration 61000: 0.23157089476014925\n",
      "two-layer Loss after iteration 62000: 0.22973594372454026\n",
      "two-layer Loss after iteration 63000: 0.22799047618750462\n",
      "two-layer Loss after iteration 64000: 0.2263185846864852\n",
      "two-layer Loss after iteration 65000: 0.224718279360033\n",
      "two-layer Loss after iteration 66000: 0.22317883574522304\n",
      "two-layer Loss after iteration 67000: 0.22169903740929411\n",
      "two-layer Loss after iteration 68000: 0.2202677802680618\n",
      "two-layer Loss after iteration 69000: 0.21887950953843416\n",
      "two-layer Loss after iteration 70000: 0.21748788201253028\n",
      "two-layer Loss after iteration 71000: 0.21618240120567647\n",
      "two-layer Loss after iteration 72000: 0.2149167192574461\n",
      "two-layer Loss after iteration 73000: 0.2136914169379582\n",
      "two-layer Loss after iteration 74000: 0.21250424938674767\n",
      "two-layer Loss after iteration 75000: 0.21135084889575556\n",
      "two-layer Loss after iteration 76000: 0.2102326142153597\n",
      "two-layer Loss after iteration 77000: 0.2091464596369619\n",
      "two-layer Loss after iteration 78000: 0.20809277897145598\n",
      "two-layer Loss after iteration 79000: 0.20707047630757783\n",
      "two-layer Loss after iteration 80000: 0.2060781416052099\n",
      "two-layer Loss after iteration 81000: 0.20511698975352197\n",
      "two-layer Loss after iteration 82000: 0.20418500601427397\n",
      "two-layer Loss after iteration 83000: 0.20328042287447018\n",
      "two-layer Loss after iteration 84000: 0.20240363691194477\n",
      "two-layer Loss after iteration 85000: 0.20155744918467386\n",
      "two-layer Loss after iteration 86000: 0.2007317692462592\n",
      "two-layer Loss after iteration 87000: 0.1997894856986136\n",
      "two-layer Loss after iteration 88000: 0.19891634809455203\n",
      "two-layer Loss after iteration 89000: 0.19808202787613174\n",
      "two-layer Loss after iteration 90000: 0.19727979549646135\n",
      "two-layer Loss after iteration 91000: 0.1965067332001017\n",
      "two-layer Loss after iteration 92000: 0.195764245424559\n",
      "two-layer Loss after iteration 93000: 0.19505040813458643\n",
      "two-layer Loss after iteration 94000: 0.19436304688894368\n",
      "two-layer Loss after iteration 95000: 0.19369781350669146\n",
      "two-layer Loss after iteration 96000: 0.19306000106756946\n",
      "two-layer Loss after iteration 97000: 0.19244626680230112\n",
      "two-layer Loss after iteration 98000: 0.1918225421146958\n",
      "two-layer Loss after iteration 99000: 0.19106949562662356\n",
      "two-layer Loss after iteration 0: 1286.252300647669\n",
      "two-layer Loss after iteration 1000: 23.90947331664994\n",
      "two-layer Loss after iteration 2000: 9.191831285168053\n",
      "two-layer Loss after iteration 3000: 6.670132320886819\n",
      "two-layer Loss after iteration 4000: 4.88253853702992\n",
      "two-layer Loss after iteration 5000: 3.77831543670449\n",
      "two-layer Loss after iteration 6000: 2.9565317484354616\n",
      "two-layer Loss after iteration 7000: 2.5555166450750058\n",
      "two-layer Loss after iteration 8000: 2.126074689288715\n",
      "two-layer Loss after iteration 9000: 1.49810713283499\n",
      "two-layer Loss after iteration 10000: 1.1742378033578686\n",
      "two-layer Loss after iteration 11000: 0.9598080673576866\n",
      "two-layer Loss after iteration 12000: 0.8039469253820383\n",
      "two-layer Loss after iteration 13000: 0.6763772483494258\n",
      "two-layer Loss after iteration 14000: 0.5742749493069064\n",
      "two-layer Loss after iteration 15000: 0.5181008597045789\n",
      "two-layer Loss after iteration 16000: 0.4796266768926215\n",
      "two-layer Loss after iteration 17000: 0.44999964493262473\n",
      "two-layer Loss after iteration 18000: 0.425104109227461\n",
      "two-layer Loss after iteration 19000: 0.4000014895926362\n",
      "two-layer Loss after iteration 20000: 0.3731359670847693\n",
      "two-layer Loss after iteration 21000: 0.3564577379703501\n",
      "two-layer Loss after iteration 22000: 0.34436512858978086\n",
      "two-layer Loss after iteration 23000: 0.33203707937041577\n",
      "two-layer Loss after iteration 24000: 0.3220595018508541\n",
      "two-layer Loss after iteration 25000: 0.31364174993811955\n",
      "two-layer Loss after iteration 26000: 0.3073596491396837\n",
      "two-layer Loss after iteration 27000: 0.30283218797334055\n",
      "two-layer Loss after iteration 28000: 0.29942710718455995\n",
      "two-layer Loss after iteration 29000: 0.2969139915353544\n",
      "two-layer Loss after iteration 30000: 0.2949605289514756\n",
      "two-layer Loss after iteration 31000: 0.29339359585090236\n",
      "two-layer Loss after iteration 32000: 0.2921544464062955\n",
      "two-layer Loss after iteration 33000: 0.29063828628697164\n",
      "two-layer Loss after iteration 34000: 0.2895972488285934\n",
      "two-layer Loss after iteration 35000: 0.2884783134164444\n",
      "two-layer Loss after iteration 36000: 0.2876025990067696\n",
      "two-layer Loss after iteration 37000: 0.2826296974775504\n",
      "two-layer Loss after iteration 38000: 0.28026806200504417\n",
      "two-layer Loss after iteration 39000: 0.2783446380990142\n",
      "two-layer Loss after iteration 40000: 0.27663671938709067\n",
      "two-layer Loss after iteration 41000: 0.2746885524938671\n",
      "two-layer Loss after iteration 42000: 0.2730833411967081\n",
      "two-layer Loss after iteration 43000: 0.2717458279365415\n",
      "two-layer Loss after iteration 44000: 0.27035039959118196\n",
      "two-layer Loss after iteration 45000: 0.26916265999263533\n",
      "two-layer Loss after iteration 46000: 0.267785082363713\n",
      "two-layer Loss after iteration 47000: 0.26649257169037316\n",
      "two-layer Loss after iteration 48000: 0.26552291174542036\n",
      "two-layer Loss after iteration 49000: 0.2646690991894584\n",
      "two-layer Loss after iteration 50000: 0.2638896397393364\n",
      "two-layer Loss after iteration 51000: 0.26309215658409707\n",
      "two-layer Loss after iteration 52000: 0.26236927447344466\n",
      "two-layer Loss after iteration 53000: 0.26173615181847515\n",
      "two-layer Loss after iteration 54000: 0.26115132748978503\n",
      "two-layer Loss after iteration 55000: 0.26060666333997035\n",
      "two-layer Loss after iteration 56000: 0.26010570218531676\n",
      "two-layer Loss after iteration 57000: 0.259644291664053\n",
      "two-layer Loss after iteration 58000: 0.259213811164087\n",
      "two-layer Loss after iteration 59000: 0.25882156275320495\n",
      "two-layer Loss after iteration 60000: 0.25845653739016294\n",
      "two-layer Loss after iteration 61000: 0.2578300272857407\n",
      "two-layer Loss after iteration 62000: 0.2574316892598646\n",
      "two-layer Loss after iteration 63000: 0.25706457949654626\n",
      "two-layer Loss after iteration 64000: 0.25673063731150764\n",
      "two-layer Loss after iteration 65000: 0.25641794267120094\n",
      "two-layer Loss after iteration 66000: 0.2561300512881906\n",
      "two-layer Loss after iteration 67000: 0.25581499267360464\n",
      "two-layer Loss after iteration 68000: 0.25554206998384676\n",
      "two-layer Loss after iteration 69000: 0.2552596697826465\n",
      "two-layer Loss after iteration 70000: 0.25499886522239107\n",
      "two-layer Loss after iteration 71000: 0.2547595991193325\n",
      "two-layer Loss after iteration 72000: 0.25455648567380906\n",
      "two-layer Loss after iteration 73000: 0.25436421850546925\n",
      "two-layer Loss after iteration 74000: 0.25418852040277284\n",
      "two-layer Loss after iteration 75000: 0.254019299921132\n",
      "two-layer Loss after iteration 76000: 0.2538649646097939\n",
      "two-layer Loss after iteration 77000: 0.25371614512296936\n",
      "two-layer Loss after iteration 78000: 0.2535768243821041\n",
      "two-layer Loss after iteration 79000: 0.25344638693781807\n",
      "two-layer Loss after iteration 80000: 0.2533211236504444\n",
      "two-layer Loss after iteration 81000: 0.25320418059663824\n",
      "two-layer Loss after iteration 82000: 0.25309083342738653\n",
      "two-layer Loss after iteration 83000: 0.2529820934844483\n",
      "two-layer Loss after iteration 84000: 0.25288391640207086\n",
      "two-layer Loss after iteration 85000: 0.25278368621228586\n",
      "two-layer Loss after iteration 86000: 0.2526935541684728\n",
      "two-layer Loss after iteration 87000: 0.2526048814334346\n",
      "two-layer Loss after iteration 88000: 0.2525222449886666\n",
      "two-layer Loss after iteration 89000: 0.25243914515556887\n",
      "two-layer Loss after iteration 90000: 0.2523609801600902\n",
      "two-layer Loss after iteration 91000: 0.2522878851513978\n",
      "two-layer Loss after iteration 92000: 0.25221843096060176\n",
      "two-layer Loss after iteration 93000: 0.252149767140096\n",
      "two-layer Loss after iteration 94000: 0.25163222854148326\n",
      "two-layer Loss after iteration 95000: 0.2513214184768364\n",
      "two-layer Loss after iteration 96000: 0.2511659596917823\n",
      "two-layer Loss after iteration 97000: 0.25105738347786505\n",
      "two-layer Loss after iteration 98000: 0.25095356837910715\n",
      "two-layer Loss after iteration 99000: 0.2508523416262352\n",
      "two-layer Loss after iteration 0: 1541.3451043304083\n",
      "two-layer Loss after iteration 1000: 22.5810689425106\n",
      "two-layer Loss after iteration 2000: 8.863156410384274\n",
      "two-layer Loss after iteration 3000: 6.222123436158777\n",
      "two-layer Loss after iteration 4000: 4.6502863472935525\n",
      "two-layer Loss after iteration 5000: 3.2136563045215656\n",
      "two-layer Loss after iteration 6000: 2.489542343205007\n",
      "two-layer Loss after iteration 7000: 2.0268213803638178\n",
      "two-layer Loss after iteration 8000: 1.7571393604639263\n",
      "two-layer Loss after iteration 9000: 1.522442289652661\n",
      "two-layer Loss after iteration 10000: 1.3624584503823625\n",
      "two-layer Loss after iteration 11000: 1.2541848651289387\n",
      "two-layer Loss after iteration 12000: 1.1428547598327978\n",
      "two-layer Loss after iteration 13000: 0.679390395202678\n",
      "two-layer Loss after iteration 14000: 0.548957464249616\n",
      "two-layer Loss after iteration 15000: 0.49040508420306406\n",
      "two-layer Loss after iteration 16000: 0.4617801968349368\n",
      "two-layer Loss after iteration 17000: 0.44690990062681735\n",
      "two-layer Loss after iteration 18000: 0.4362314895857454\n",
      "two-layer Loss after iteration 19000: 0.4270501972293767\n",
      "two-layer Loss after iteration 20000: 0.4178379971788488\n",
      "two-layer Loss after iteration 21000: 0.4096018657384814\n",
      "two-layer Loss after iteration 22000: 0.3988256596963295\n",
      "two-layer Loss after iteration 23000: 0.391745153242384\n",
      "two-layer Loss after iteration 24000: 0.38516625627249856\n",
      "two-layer Loss after iteration 25000: 0.3792620666478252\n",
      "two-layer Loss after iteration 26000: 0.372143969429273\n",
      "two-layer Loss after iteration 27000: 0.3667183581062633\n",
      "two-layer Loss after iteration 28000: 0.3619522681471413\n",
      "two-layer Loss after iteration 29000: 0.35766550909038575\n",
      "two-layer Loss after iteration 30000: 0.353753127988525\n",
      "two-layer Loss after iteration 31000: 0.3501280774740257\n",
      "two-layer Loss after iteration 32000: 0.3467399277873946\n",
      "two-layer Loss after iteration 33000: 0.34262495985485025\n",
      "two-layer Loss after iteration 34000: 0.3392141647801058\n",
      "two-layer Loss after iteration 35000: 0.3360639998243382\n",
      "two-layer Loss after iteration 36000: 0.33314143091515286\n",
      "two-layer Loss after iteration 37000: 0.32967385857682197\n",
      "two-layer Loss after iteration 38000: 0.3264607188291664\n",
      "two-layer Loss after iteration 39000: 0.32373447135742084\n",
      "two-layer Loss after iteration 40000: 0.3211856457781651\n",
      "two-layer Loss after iteration 41000: 0.3185064508079141\n",
      "two-layer Loss after iteration 42000: 0.3159335332609861\n",
      "two-layer Loss after iteration 43000: 0.31353154800238753\n",
      "two-layer Loss after iteration 44000: 0.3112756247738707\n",
      "two-layer Loss after iteration 45000: 0.30914982150339393\n",
      "two-layer Loss after iteration 46000: 0.3071414063209105\n",
      "two-layer Loss after iteration 47000: 0.3052393624501081\n",
      "two-layer Loss after iteration 48000: 0.3033157140376218\n",
      "two-layer Loss after iteration 49000: 0.3012809360337994\n",
      "two-layer Loss after iteration 50000: 0.2994457259118821\n",
      "two-layer Loss after iteration 51000: 0.29773854082287887\n",
      "two-layer Loss after iteration 52000: 0.29612295560141483\n",
      "two-layer Loss after iteration 53000: 0.2945877115799325\n",
      "two-layer Loss after iteration 54000: 0.2931257110279187\n",
      "two-layer Loss after iteration 55000: 0.2917312599816623\n",
      "two-layer Loss after iteration 56000: 0.29039952257049256\n",
      "two-layer Loss after iteration 57000: 0.2890021509234771\n",
      "two-layer Loss after iteration 58000: 0.2875136860460581\n",
      "two-layer Loss after iteration 59000: 0.2861886064710332\n",
      "two-layer Loss after iteration 60000: 0.28493636744681017\n",
      "two-layer Loss after iteration 61000: 0.2837422200774981\n",
      "two-layer Loss after iteration 62000: 0.28260042511796285\n",
      "two-layer Loss after iteration 63000: 0.2815073157407893\n",
      "two-layer Loss after iteration 64000: 0.28043726828217796\n",
      "two-layer Loss after iteration 65000: 0.2792539184707217\n",
      "two-layer Loss after iteration 66000: 0.2781732978418413\n",
      "two-layer Loss after iteration 67000: 0.27714594905871354\n",
      "two-layer Loss after iteration 68000: 0.2761653319021406\n",
      "two-layer Loss after iteration 69000: 0.2752314969167158\n",
      "two-layer Loss after iteration 70000: 0.2743414059910785\n",
      "two-layer Loss after iteration 71000: 0.2727126110364958\n",
      "two-layer Loss after iteration 72000: 0.27148491220892934\n",
      "two-layer Loss after iteration 73000: 0.2697897414119634\n",
      "two-layer Loss after iteration 74000: 0.2684241737685008\n",
      "two-layer Loss after iteration 75000: 0.26716496129388967\n",
      "two-layer Loss after iteration 76000: 0.26595697902862\n",
      "two-layer Loss after iteration 77000: 0.2648431238089896\n",
      "two-layer Loss after iteration 78000: 0.2620359122617413\n",
      "two-layer Loss after iteration 79000: 0.2604800586184456\n",
      "two-layer Loss after iteration 80000: 0.2591480733366819\n",
      "two-layer Loss after iteration 81000: 0.2580111218350541\n",
      "two-layer Loss after iteration 82000: 0.2560586906088942\n",
      "two-layer Loss after iteration 83000: 0.25491705018618427\n",
      "two-layer Loss after iteration 84000: 0.25361987837692473\n",
      "two-layer Loss after iteration 85000: 0.2525025826992312\n",
      "two-layer Loss after iteration 86000: 0.2500588039354648\n",
      "two-layer Loss after iteration 87000: 0.24880591470212626\n",
      "two-layer Loss after iteration 88000: 0.24778986722140076\n",
      "two-layer Loss after iteration 89000: 0.24612832775066162\n",
      "two-layer Loss after iteration 90000: 0.24477725181955948\n",
      "two-layer Loss after iteration 91000: 0.24326537133619178\n",
      "two-layer Loss after iteration 92000: 0.2419153969859295\n",
      "two-layer Loss after iteration 93000: 0.2406190713898085\n",
      "two-layer Loss after iteration 94000: 0.23956891558887522\n",
      "two-layer Loss after iteration 95000: 0.2386886540145478\n",
      "two-layer Loss after iteration 96000: 0.237834182410952\n",
      "two-layer Loss after iteration 97000: 0.23706986386052056\n",
      "two-layer Loss after iteration 98000: 0.2363986064036078\n",
      "two-layer Loss after iteration 99000: 0.23581087772034895\n",
      "two-layer Loss after iteration 0: 1471.0524659631137\n",
      "two-layer Loss after iteration 1000: 23.959400186503814\n",
      "two-layer Loss after iteration 2000: 8.85114355734089\n",
      "two-layer Loss after iteration 3000: 5.529923263744391\n",
      "two-layer Loss after iteration 4000: 3.9036434118026646\n",
      "two-layer Loss after iteration 5000: 2.661402632847788\n",
      "two-layer Loss after iteration 6000: 2.1649663134950665\n",
      "two-layer Loss after iteration 7000: 1.854246831873943\n",
      "two-layer Loss after iteration 8000: 1.5636692076166876\n",
      "two-layer Loss after iteration 9000: 1.3375386375755154\n",
      "two-layer Loss after iteration 10000: 1.1500288965819843\n",
      "two-layer Loss after iteration 11000: 0.9953928498344211\n",
      "two-layer Loss after iteration 12000: 0.8950330195353394\n",
      "two-layer Loss after iteration 13000: 0.8131267490807684\n",
      "two-layer Loss after iteration 14000: 0.7494562899838793\n",
      "two-layer Loss after iteration 15000: 0.7023772739118592\n",
      "two-layer Loss after iteration 16000: 0.665712203655917\n",
      "two-layer Loss after iteration 17000: 0.6349563269430516\n",
      "two-layer Loss after iteration 18000: 0.6099550916869617\n",
      "two-layer Loss after iteration 19000: 0.5892343434642966\n",
      "two-layer Loss after iteration 20000: 0.5734848679313963\n",
      "two-layer Loss after iteration 21000: 0.5581627076458132\n",
      "two-layer Loss after iteration 22000: 0.5475544570963842\n",
      "two-layer Loss after iteration 23000: 0.5389358068483848\n",
      "two-layer Loss after iteration 24000: 0.5319215279865004\n",
      "two-layer Loss after iteration 25000: 0.5261735067665448\n",
      "two-layer Loss after iteration 26000: 0.5189205968008815\n",
      "two-layer Loss after iteration 27000: 0.5133854747493176\n",
      "two-layer Loss after iteration 28000: 0.5085414547064437\n",
      "two-layer Loss after iteration 29000: 0.5049382014182244\n",
      "two-layer Loss after iteration 30000: 0.501754949841735\n",
      "two-layer Loss after iteration 31000: 0.49907743222475776\n",
      "two-layer Loss after iteration 32000: 0.4965885391822878\n",
      "two-layer Loss after iteration 33000: 0.4944028643778374\n",
      "two-layer Loss after iteration 34000: 0.4918783854758413\n",
      "two-layer Loss after iteration 35000: 0.48842197809371396\n",
      "two-layer Loss after iteration 36000: 0.48229233751399647\n",
      "two-layer Loss after iteration 37000: 0.4749637732702521\n",
      "two-layer Loss after iteration 38000: 0.47180474460478866\n",
      "two-layer Loss after iteration 39000: 0.4694063292528968\n",
      "two-layer Loss after iteration 40000: 0.46640305364278184\n",
      "two-layer Loss after iteration 41000: 0.46404515187080325\n",
      "two-layer Loss after iteration 42000: 0.4619614234043517\n",
      "two-layer Loss after iteration 43000: 0.45982057611369437\n",
      "two-layer Loss after iteration 44000: 0.4581781559228371\n",
      "two-layer Loss after iteration 45000: 0.45654179844792603\n",
      "two-layer Loss after iteration 46000: 0.45517669896964386\n",
      "two-layer Loss after iteration 47000: 0.4538254709940791\n",
      "two-layer Loss after iteration 48000: 0.452638852339742\n",
      "two-layer Loss after iteration 49000: 0.4515470553322814\n",
      "two-layer Loss after iteration 50000: 0.44932914061282153\n",
      "two-layer Loss after iteration 51000: 0.44762471884046645\n",
      "two-layer Loss after iteration 52000: 0.4455778086817427\n",
      "two-layer Loss after iteration 53000: 0.4432311952228396\n",
      "two-layer Loss after iteration 54000: 0.44142874558442713\n",
      "two-layer Loss after iteration 55000: 0.4399038436942898\n",
      "two-layer Loss after iteration 56000: 0.4385728168907384\n",
      "two-layer Loss after iteration 57000: 0.43742908565824423\n",
      "two-layer Loss after iteration 58000: 0.4364048374922677\n",
      "two-layer Loss after iteration 59000: 0.4356253808531489\n",
      "two-layer Loss after iteration 60000: 0.4350207400762895\n",
      "two-layer Loss after iteration 61000: 0.4344650112165867\n",
      "two-layer Loss after iteration 62000: 0.4339575627467936\n",
      "two-layer Loss after iteration 63000: 0.43348657930910867\n",
      "two-layer Loss after iteration 64000: 0.4330540167418696\n",
      "two-layer Loss after iteration 65000: 0.43263628269616256\n",
      "two-layer Loss after iteration 66000: 0.4322467552404469\n",
      "two-layer Loss after iteration 67000: 0.4318775996679445\n",
      "two-layer Loss after iteration 68000: 0.43152042990426104\n",
      "two-layer Loss after iteration 69000: 0.43117641494370545\n",
      "two-layer Loss after iteration 70000: 0.43084828961751137\n",
      "two-layer Loss after iteration 71000: 0.4305248783916529\n",
      "two-layer Loss after iteration 72000: 0.43012034091400814\n",
      "two-layer Loss after iteration 73000: 0.42976463545516963\n",
      "two-layer Loss after iteration 74000: 0.4294310778397272\n",
      "two-layer Loss after iteration 75000: 0.429119025875019\n",
      "two-layer Loss after iteration 76000: 0.4288208577173725\n",
      "two-layer Loss after iteration 77000: 0.4285312833598382\n",
      "two-layer Loss after iteration 78000: 0.42824881084439625\n",
      "two-layer Loss after iteration 79000: 0.4279796065693996\n",
      "two-layer Loss after iteration 80000: 0.4277113802685981\n",
      "two-layer Loss after iteration 81000: 0.42744698195155717\n",
      "two-layer Loss after iteration 82000: 0.42718536735811996\n",
      "two-layer Loss after iteration 83000: 0.42693180437713385\n",
      "two-layer Loss after iteration 84000: 0.4266883027618309\n",
      "two-layer Loss after iteration 85000: 0.4264380391848275\n",
      "two-layer Loss after iteration 86000: 0.42619529202490464\n",
      "two-layer Loss after iteration 87000: 0.42595412432191215\n",
      "two-layer Loss after iteration 88000: 0.4257177952237891\n",
      "two-layer Loss after iteration 89000: 0.42548735883612226\n",
      "two-layer Loss after iteration 90000: 0.42525615114787935\n",
      "two-layer Loss after iteration 91000: 0.4250324329277819\n",
      "two-layer Loss after iteration 92000: 0.42480175304611867\n",
      "two-layer Loss after iteration 93000: 0.42457982332969946\n",
      "two-layer Loss after iteration 94000: 0.42436131407254213\n",
      "two-layer Loss after iteration 95000: 0.4241503646581215\n",
      "two-layer Loss after iteration 96000: 0.4239441686498491\n",
      "two-layer Loss after iteration 97000: 0.4237425303299723\n",
      "two-layer Loss after iteration 98000: 0.42353850100093926\n",
      "two-layer Loss after iteration 99000: 0.42334327257315063\n",
      "two-layer Loss after iteration 0: 1568.5575972368274\n",
      "two-layer Loss after iteration 1000: 23.23225548705033\n",
      "two-layer Loss after iteration 2000: 8.81807480051179\n",
      "two-layer Loss after iteration 3000: 5.678579619005284\n",
      "two-layer Loss after iteration 4000: 4.418464063468607\n",
      "two-layer Loss after iteration 5000: 3.5110144310667515\n",
      "two-layer Loss after iteration 6000: 2.9986547588651136\n",
      "two-layer Loss after iteration 7000: 2.6324080735640916\n",
      "two-layer Loss after iteration 8000: 2.1308146750243386\n",
      "two-layer Loss after iteration 9000: 1.7938634454208826\n",
      "two-layer Loss after iteration 10000: 1.5547183513512217\n",
      "two-layer Loss after iteration 11000: 1.3598623878338258\n",
      "two-layer Loss after iteration 12000: 1.1599697405873697\n",
      "two-layer Loss after iteration 13000: 1.0327684539348791\n",
      "two-layer Loss after iteration 14000: 0.9606507314020113\n",
      "two-layer Loss after iteration 15000: 0.920592539600306\n",
      "two-layer Loss after iteration 16000: 0.8852077633039486\n",
      "two-layer Loss after iteration 17000: 0.8652606343014314\n",
      "two-layer Loss after iteration 18000: 0.8462649057396251\n",
      "two-layer Loss after iteration 19000: 0.8317003719824537\n",
      "two-layer Loss after iteration 20000: 0.8056375306483581\n",
      "two-layer Loss after iteration 21000: 0.7835194889466169\n",
      "two-layer Loss after iteration 22000: 0.7684639211540936\n",
      "two-layer Loss after iteration 23000: 0.7575478332206045\n",
      "two-layer Loss after iteration 24000: 0.745481425463688\n",
      "two-layer Loss after iteration 25000: 0.7356973942309236\n",
      "two-layer Loss after iteration 26000: 0.728630760938838\n",
      "two-layer Loss after iteration 27000: 0.7225686267326804\n",
      "two-layer Loss after iteration 28000: 0.7173491068810055\n",
      "two-layer Loss after iteration 29000: 0.7099553546873947\n",
      "two-layer Loss after iteration 30000: 0.7050819867000552\n",
      "two-layer Loss after iteration 31000: 0.7006597208893661\n",
      "two-layer Loss after iteration 32000: 0.6965444078571061\n",
      "two-layer Loss after iteration 33000: 0.6924623362999053\n",
      "two-layer Loss after iteration 34000: 0.6888077596966294\n",
      "two-layer Loss after iteration 35000: 0.6852781175415837\n",
      "two-layer Loss after iteration 36000: 0.6818627781065462\n",
      "two-layer Loss after iteration 37000: 0.6785537836004469\n",
      "two-layer Loss after iteration 38000: 0.6753462089517762\n",
      "two-layer Loss after iteration 39000: 0.6723955398294094\n",
      "two-layer Loss after iteration 40000: 0.6698579714650807\n",
      "two-layer Loss after iteration 41000: 0.6674376112448973\n",
      "two-layer Loss after iteration 42000: 0.665123516087235\n",
      "two-layer Loss after iteration 43000: 0.6628920556465311\n",
      "two-layer Loss after iteration 44000: 0.660739475820142\n",
      "two-layer Loss after iteration 45000: 0.6586451143930502\n",
      "two-layer Loss after iteration 46000: 0.6566190993351431\n",
      "two-layer Loss after iteration 47000: 0.6546611523783064\n",
      "two-layer Loss after iteration 48000: 0.6527304825331908\n",
      "two-layer Loss after iteration 49000: 0.6508752531591525\n",
      "two-layer Loss after iteration 50000: 0.649044747608497\n",
      "two-layer Loss after iteration 51000: 0.6472682112139285\n",
      "two-layer Loss after iteration 52000: 0.6455371808635624\n",
      "two-layer Loss after iteration 53000: 0.6438593198039454\n",
      "two-layer Loss after iteration 54000: 0.6422258163389961\n",
      "two-layer Loss after iteration 55000: 0.6406320364024676\n",
      "two-layer Loss after iteration 56000: 0.6390719566460216\n",
      "two-layer Loss after iteration 57000: 0.6375382903143468\n",
      "two-layer Loss after iteration 58000: 0.636045864720155\n",
      "two-layer Loss after iteration 59000: 0.6345831238511829\n",
      "two-layer Loss after iteration 60000: 0.6331569737953032\n",
      "two-layer Loss after iteration 61000: 0.631746609691045\n",
      "two-layer Loss after iteration 62000: 0.6303752264167871\n",
      "two-layer Loss after iteration 63000: 0.6290284536771807\n",
      "two-layer Loss after iteration 64000: 0.6277093706219544\n",
      "two-layer Loss after iteration 65000: 0.6264219050768666\n",
      "two-layer Loss after iteration 66000: 0.6251602808458677\n",
      "two-layer Loss after iteration 67000: 0.6239264694433473\n",
      "two-layer Loss after iteration 68000: 0.6227253260796978\n",
      "two-layer Loss after iteration 69000: 0.6215401280647646\n",
      "two-layer Loss after iteration 70000: 0.6203862245261383\n",
      "two-layer Loss after iteration 71000: 0.6192570407849924\n",
      "two-layer Loss after iteration 72000: 0.6181482082490258\n",
      "two-layer Loss after iteration 73000: 0.6170658068197789\n",
      "two-layer Loss after iteration 74000: 0.6160014104552722\n",
      "two-layer Loss after iteration 75000: 0.6149685899201821\n",
      "two-layer Loss after iteration 76000: 0.6139491794775009\n",
      "two-layer Loss after iteration 77000: 0.612956807978749\n",
      "two-layer Loss after iteration 78000: 0.6119920762461224\n",
      "two-layer Loss after iteration 79000: 0.611038947559207\n",
      "two-layer Loss after iteration 80000: 0.6101083655121263\n",
      "two-layer Loss after iteration 81000: 0.6092062279873951\n",
      "two-layer Loss after iteration 82000: 0.6083207042899185\n",
      "two-layer Loss after iteration 83000: 0.6074520109480497\n",
      "two-layer Loss after iteration 84000: 0.6066039380528969\n",
      "two-layer Loss after iteration 85000: 0.6057886288948852\n",
      "two-layer Loss after iteration 86000: 0.6049725695999019\n",
      "two-layer Loss after iteration 87000: 0.6011515667363045\n",
      "two-layer Loss after iteration 88000: 0.5999270671708273\n",
      "two-layer Loss after iteration 89000: 0.5987976849113456\n",
      "two-layer Loss after iteration 90000: 0.5977219779898191\n",
      "two-layer Loss after iteration 91000: 0.5966941144500671\n",
      "two-layer Loss after iteration 92000: 0.5956992221436095\n",
      "two-layer Loss after iteration 93000: 0.5947460107986386\n",
      "two-layer Loss after iteration 94000: 0.5938224203623809\n",
      "two-layer Loss after iteration 95000: 0.5929270223405868\n",
      "two-layer Loss after iteration 96000: 0.5920533541378189\n",
      "two-layer Loss after iteration 97000: 0.5912084819442188\n",
      "two-layer Loss after iteration 98000: 0.5903817065088675\n",
      "two-layer Loss after iteration 99000: 0.5895815341028131\n",
      "two-layer Loss after iteration 0: 1562.5993028870307\n",
      "two-layer Loss after iteration 1000: 24.939937019726663\n",
      "two-layer Loss after iteration 2000: 9.478245893104914\n",
      "two-layer Loss after iteration 3000: 6.81642760057159\n",
      "two-layer Loss after iteration 4000: 5.034784913268983\n",
      "two-layer Loss after iteration 5000: 3.7336231991706277\n",
      "two-layer Loss after iteration 6000: 2.8969661974104706\n",
      "two-layer Loss after iteration 7000: 2.3224943623741034\n",
      "two-layer Loss after iteration 8000: 2.0018689292424723\n",
      "two-layer Loss after iteration 9000: 1.612425405035551\n",
      "two-layer Loss after iteration 10000: 1.3940133247176452\n",
      "two-layer Loss after iteration 11000: 1.2337437313857607\n",
      "two-layer Loss after iteration 12000: 1.1005452308370525\n",
      "two-layer Loss after iteration 13000: 0.9938046123267127\n",
      "two-layer Loss after iteration 14000: 0.911779597815727\n",
      "two-layer Loss after iteration 15000: 0.8454998666982783\n",
      "two-layer Loss after iteration 16000: 0.7862180447114598\n",
      "two-layer Loss after iteration 17000: 0.7312268122085858\n",
      "two-layer Loss after iteration 18000: 0.6876355834100648\n",
      "two-layer Loss after iteration 19000: 0.6510705411113877\n",
      "two-layer Loss after iteration 20000: 0.6149787658316218\n",
      "two-layer Loss after iteration 21000: 0.582706436292017\n",
      "two-layer Loss after iteration 22000: 0.5516714265682878\n",
      "two-layer Loss after iteration 23000: 0.5202587511300857\n",
      "two-layer Loss after iteration 24000: 0.49435057179918934\n",
      "two-layer Loss after iteration 25000: 0.4674428342856344\n",
      "two-layer Loss after iteration 26000: 0.4483018109036795\n",
      "two-layer Loss after iteration 27000: 0.430601232979118\n",
      "two-layer Loss after iteration 28000: 0.4008549697967006\n",
      "two-layer Loss after iteration 29000: 0.38394500200038834\n",
      "two-layer Loss after iteration 30000: 0.37121417705570475\n",
      "two-layer Loss after iteration 31000: 0.3588482809084422\n",
      "two-layer Loss after iteration 32000: 0.34916613079473185\n",
      "two-layer Loss after iteration 33000: 0.3408612666794099\n",
      "two-layer Loss after iteration 34000: 0.33363866982351975\n",
      "two-layer Loss after iteration 35000: 0.3274704086185251\n",
      "two-layer Loss after iteration 36000: 0.32165580482954814\n",
      "two-layer Loss after iteration 37000: 0.3160164230288708\n",
      "two-layer Loss after iteration 38000: 0.3116100314004401\n",
      "two-layer Loss after iteration 39000: 0.3078623235834971\n",
      "two-layer Loss after iteration 40000: 0.30462047208187704\n",
      "two-layer Loss after iteration 41000: 0.2994062214611692\n",
      "two-layer Loss after iteration 42000: 0.294461190165616\n",
      "two-layer Loss after iteration 43000: 0.29014262923968664\n",
      "two-layer Loss after iteration 44000: 0.28364906062593076\n",
      "two-layer Loss after iteration 45000: 0.2790565124429002\n",
      "two-layer Loss after iteration 46000: 0.2761827364163429\n",
      "two-layer Loss after iteration 47000: 0.2739622640486595\n",
      "two-layer Loss after iteration 48000: 0.27182095031315934\n",
      "two-layer Loss after iteration 49000: 0.2697064734196577\n",
      "two-layer Loss after iteration 50000: 0.266013895286487\n",
      "two-layer Loss after iteration 51000: 0.2643351365757538\n",
      "two-layer Loss after iteration 52000: 0.2629580035077929\n",
      "two-layer Loss after iteration 53000: 0.2617994824114733\n",
      "two-layer Loss after iteration 54000: 0.2608127436573919\n",
      "two-layer Loss after iteration 55000: 0.25996505599363295\n",
      "two-layer Loss after iteration 56000: 0.2592320160753196\n",
      "two-layer Loss after iteration 57000: 0.2585945206729188\n",
      "two-layer Loss after iteration 58000: 0.2580376546301404\n",
      "two-layer Loss after iteration 59000: 0.257388138347383\n",
      "two-layer Loss after iteration 60000: 0.2567624946962228\n",
      "two-layer Loss after iteration 61000: 0.25628508351689194\n",
      "two-layer Loss after iteration 62000: 0.25588533880797587\n",
      "two-layer Loss after iteration 63000: 0.25553953653604305\n",
      "two-layer Loss after iteration 64000: 0.2552360575900188\n",
      "two-layer Loss after iteration 65000: 0.25496747091464306\n",
      "two-layer Loss after iteration 66000: 0.2547282933702315\n",
      "two-layer Loss after iteration 67000: 0.2545145436366605\n",
      "two-layer Loss after iteration 68000: 0.2543015728561559\n",
      "two-layer Loss after iteration 69000: 0.2540586338150711\n",
      "two-layer Loss after iteration 70000: 0.2538582729067183\n",
      "two-layer Loss after iteration 71000: 0.25367937191128653\n",
      "two-layer Loss after iteration 72000: 0.25349464902970076\n",
      "two-layer Loss after iteration 73000: 0.2533370323841121\n",
      "two-layer Loss after iteration 74000: 0.25301140800406596\n",
      "two-layer Loss after iteration 75000: 0.25288218912015176\n",
      "two-layer Loss after iteration 76000: 0.2527688683971784\n",
      "two-layer Loss after iteration 77000: 0.2526653929176141\n",
      "two-layer Loss after iteration 78000: 0.25256929976397546\n",
      "two-layer Loss after iteration 79000: 0.2524790531107108\n",
      "two-layer Loss after iteration 80000: 0.2523938438275093\n",
      "two-layer Loss after iteration 81000: 0.25231313279603146\n",
      "two-layer Loss after iteration 82000: 0.25223640385266777\n",
      "two-layer Loss after iteration 83000: 0.2521633242595042\n",
      "two-layer Loss after iteration 84000: 0.25209363848484995\n",
      "two-layer Loss after iteration 85000: 0.25202711353673163\n",
      "two-layer Loss after iteration 86000: 0.25196351726258226\n",
      "two-layer Loss after iteration 87000: 0.25190263748544156\n",
      "two-layer Loss after iteration 88000: 0.2518443618153083\n",
      "two-layer Loss after iteration 89000: 0.25171273184616355\n",
      "two-layer Loss after iteration 90000: 0.2513089825367156\n",
      "two-layer Loss after iteration 91000: 0.2511544461693882\n",
      "two-layer Loss after iteration 92000: 0.2510330258901341\n",
      "two-layer Loss after iteration 93000: 0.2509279103247958\n",
      "two-layer Loss after iteration 94000: 0.250832596155175\n",
      "two-layer Loss after iteration 95000: 0.2507444550175739\n",
      "two-layer Loss after iteration 96000: 0.25066208821712244\n",
      "two-layer Loss after iteration 97000: 0.25058469913866677\n",
      "two-layer Loss after iteration 98000: 0.2505118035359542\n",
      "two-layer Loss after iteration 99000: 0.25044299122072866\n",
      "two-layer Loss after iteration 0: 1276.1262871339318\n",
      "two-layer Loss after iteration 1000: 25.567499267666694\n",
      "two-layer Loss after iteration 2000: 9.753146103335332\n",
      "two-layer Loss after iteration 3000: 6.461296037276207\n",
      "two-layer Loss after iteration 4000: 4.966379153546494\n",
      "two-layer Loss after iteration 5000: 3.920560120654808\n",
      "two-layer Loss after iteration 6000: 3.3699423635608197\n",
      "two-layer Loss after iteration 7000: 2.9494576135772754\n",
      "two-layer Loss after iteration 8000: 2.3355524901701754\n",
      "two-layer Loss after iteration 9000: 1.9657861209847973\n",
      "two-layer Loss after iteration 10000: 1.749745362534761\n",
      "two-layer Loss after iteration 11000: 1.4609592602584587\n",
      "two-layer Loss after iteration 12000: 1.3099884519946154\n",
      "two-layer Loss after iteration 13000: 1.1869905836469687\n",
      "two-layer Loss after iteration 14000: 1.1242793584052504\n",
      "two-layer Loss after iteration 15000: 1.0829596984777734\n",
      "two-layer Loss after iteration 16000: 1.0534571229772924\n",
      "two-layer Loss after iteration 17000: 1.0315329800459063\n",
      "two-layer Loss after iteration 18000: 1.014561121187021\n",
      "two-layer Loss after iteration 19000: 1.0005048425793963\n",
      "two-layer Loss after iteration 20000: 0.9866792084942848\n",
      "two-layer Loss after iteration 21000: 0.9748271295704972\n",
      "two-layer Loss after iteration 22000: 0.9625518626698188\n",
      "two-layer Loss after iteration 23000: 0.9504546551215192\n",
      "two-layer Loss after iteration 24000: 0.9309922984468302\n",
      "two-layer Loss after iteration 25000: 0.9192507505319003\n",
      "two-layer Loss after iteration 26000: 0.9074978141145923\n",
      "two-layer Loss after iteration 27000: 0.8920197785027668\n",
      "two-layer Loss after iteration 28000: 0.883247765299797\n",
      "two-layer Loss after iteration 29000: 0.8765268304168409\n",
      "two-layer Loss after iteration 30000: 0.8720212853508322\n",
      "two-layer Loss after iteration 31000: 0.8684791377843941\n",
      "two-layer Loss after iteration 32000: 0.8651105235073572\n",
      "two-layer Loss after iteration 33000: 0.8626613731744371\n",
      "two-layer Loss after iteration 34000: 0.860688187135507\n",
      "two-layer Loss after iteration 35000: 0.8550579107206917\n",
      "two-layer Loss after iteration 36000: 0.8503740248766476\n",
      "two-layer Loss after iteration 37000: 0.8483316986008632\n",
      "two-layer Loss after iteration 38000: 0.8466387357008749\n",
      "two-layer Loss after iteration 39000: 0.8452688032178005\n",
      "two-layer Loss after iteration 40000: 0.8440241562005169\n",
      "two-layer Loss after iteration 41000: 0.8430874731625058\n",
      "two-layer Loss after iteration 42000: 0.8422843165714279\n",
      "two-layer Loss after iteration 43000: 0.8415427225781881\n",
      "two-layer Loss after iteration 44000: 0.8408476157595277\n",
      "two-layer Loss after iteration 45000: 0.840190077625397\n",
      "two-layer Loss after iteration 46000: 0.8395639989767085\n",
      "two-layer Loss after iteration 47000: 0.8389649045047237\n",
      "two-layer Loss after iteration 48000: 0.8383893995535145\n",
      "two-layer Loss after iteration 49000: 0.8377950176507042\n",
      "two-layer Loss after iteration 50000: 0.8372168461341877\n",
      "two-layer Loss after iteration 51000: 0.8366657924612884\n",
      "two-layer Loss after iteration 52000: 0.8361345818398435\n",
      "two-layer Loss after iteration 53000: 0.8356205961685237\n",
      "two-layer Loss after iteration 54000: 0.8351226239169425\n",
      "two-layer Loss after iteration 55000: 0.8346394735824894\n",
      "two-layer Loss after iteration 56000: 0.8341736414181835\n",
      "two-layer Loss after iteration 57000: 0.8337222563133327\n",
      "two-layer Loss after iteration 58000: 0.8332845744957958\n",
      "two-layer Loss after iteration 59000: 0.8328595538760891\n",
      "two-layer Loss after iteration 60000: 0.8309470118190562\n",
      "two-layer Loss after iteration 61000: 0.8301871801379846\n",
      "two-layer Loss after iteration 62000: 0.8296049464968622\n",
      "two-layer Loss after iteration 63000: 0.8291085314162028\n",
      "two-layer Loss after iteration 64000: 0.828665149088686\n",
      "two-layer Loss after iteration 65000: 0.828255048849196\n",
      "two-layer Loss after iteration 66000: 0.8278458769599991\n",
      "two-layer Loss after iteration 67000: 0.826879437933031\n",
      "two-layer Loss after iteration 68000: 0.8260236920938385\n",
      "two-layer Loss after iteration 69000: 0.8252271105784795\n",
      "two-layer Loss after iteration 70000: 0.8244787625536234\n",
      "two-layer Loss after iteration 71000: 0.8237732987492679\n",
      "two-layer Loss after iteration 72000: 0.8231060522152729\n",
      "two-layer Loss after iteration 73000: 0.8224720728862722\n",
      "two-layer Loss after iteration 74000: 0.8218689698885513\n",
      "two-layer Loss after iteration 75000: 0.8212944106378071\n",
      "two-layer Loss after iteration 76000: 0.8203832225696657\n",
      "two-layer Loss after iteration 77000: 0.8193960785152237\n",
      "two-layer Loss after iteration 78000: 0.8181708868260125\n",
      "two-layer Loss after iteration 79000: 0.8131033747512761\n",
      "two-layer Loss after iteration 80000: 0.8100731675383112\n",
      "two-layer Loss after iteration 81000: 0.8081335338783378\n",
      "two-layer Loss after iteration 82000: 0.8064711688271835\n",
      "two-layer Loss after iteration 83000: 0.8050170976347191\n",
      "two-layer Loss after iteration 84000: 0.8037293444544845\n",
      "two-layer Loss after iteration 85000: 0.8025733195773885\n",
      "two-layer Loss after iteration 86000: 0.8014440893314829\n",
      "two-layer Loss after iteration 87000: 0.8003532666023292\n",
      "two-layer Loss after iteration 88000: 0.7993815133615256\n",
      "two-layer Loss after iteration 89000: 0.7977228074731465\n",
      "two-layer Loss after iteration 90000: 0.7968748850349407\n",
      "two-layer Loss after iteration 91000: 0.7958537719550731\n",
      "two-layer Loss after iteration 92000: 0.7891138806559659\n",
      "two-layer Loss after iteration 93000: 0.7867797599676348\n",
      "two-layer Loss after iteration 94000: 0.7851114746692471\n",
      "two-layer Loss after iteration 95000: 0.7832689953714786\n",
      "two-layer Loss after iteration 96000: 0.7815490647638096\n",
      "two-layer Loss after iteration 97000: 0.7803528802544368\n",
      "two-layer Loss after iteration 98000: 0.7796200522677104\n",
      "two-layer Loss after iteration 99000: 0.7789920998127091\n",
      "two-layer Loss after iteration 0: 1500.6393133874537\n",
      "two-layer Loss after iteration 1000: 23.549558751289272\n",
      "two-layer Loss after iteration 2000: 8.93078731596547\n",
      "two-layer Loss after iteration 3000: 6.192199054131193\n",
      "two-layer Loss after iteration 4000: 4.724267908081549\n",
      "two-layer Loss after iteration 5000: 3.3304415664752933\n",
      "two-layer Loss after iteration 6000: 2.7457606036208144\n",
      "two-layer Loss after iteration 7000: 2.363954323364616\n",
      "two-layer Loss after iteration 8000: 2.079554889771226\n",
      "two-layer Loss after iteration 9000: 1.8330454986567197\n",
      "two-layer Loss after iteration 10000: 1.624568656371241\n",
      "two-layer Loss after iteration 11000: 1.478250367433396\n",
      "two-layer Loss after iteration 12000: 1.3668806298843188\n",
      "two-layer Loss after iteration 13000: 1.2692088152462757\n",
      "two-layer Loss after iteration 14000: 1.193100583929419\n",
      "two-layer Loss after iteration 15000: 1.1235937716825135\n",
      "two-layer Loss after iteration 16000: 1.060841483038594\n",
      "two-layer Loss after iteration 17000: 1.0032428820988954\n",
      "two-layer Loss after iteration 18000: 0.7767592095045961\n",
      "two-layer Loss after iteration 19000: 0.6588032439714467\n",
      "two-layer Loss after iteration 20000: 0.5222855870714834\n",
      "two-layer Loss after iteration 21000: 0.48819682111516394\n",
      "two-layer Loss after iteration 22000: 0.4686014706174688\n",
      "two-layer Loss after iteration 23000: 0.45549388809357577\n",
      "two-layer Loss after iteration 24000: 0.4453367479029172\n",
      "two-layer Loss after iteration 25000: 0.43726978363112357\n",
      "two-layer Loss after iteration 26000: 0.4288827986590299\n",
      "two-layer Loss after iteration 27000: 0.4216749428568671\n",
      "two-layer Loss after iteration 28000: 0.4154139102868505\n",
      "two-layer Loss after iteration 29000: 0.4096154795920985\n",
      "two-layer Loss after iteration 30000: 0.4043539099795976\n",
      "two-layer Loss after iteration 31000: 0.3996017529034009\n",
      "two-layer Loss after iteration 32000: 0.39532460058955554\n",
      "two-layer Loss after iteration 33000: 0.3914238839793327\n",
      "two-layer Loss after iteration 34000: 0.3878628287011585\n",
      "two-layer Loss after iteration 35000: 0.38458888047274525\n",
      "two-layer Loss after iteration 36000: 0.38126420848185677\n",
      "two-layer Loss after iteration 37000: 0.37806444234542447\n",
      "two-layer Loss after iteration 38000: 0.3750556895792253\n",
      "two-layer Loss after iteration 39000: 0.3722592989885309\n",
      "two-layer Loss after iteration 40000: 0.36936517603066665\n",
      "two-layer Loss after iteration 41000: 0.3668996159878159\n",
      "two-layer Loss after iteration 42000: 0.36454174352626967\n",
      "two-layer Loss after iteration 43000: 0.3623989144433915\n",
      "two-layer Loss after iteration 44000: 0.3603564190119141\n",
      "two-layer Loss after iteration 45000: 0.35808697007201734\n",
      "two-layer Loss after iteration 46000: 0.35587217553104816\n",
      "two-layer Loss after iteration 47000: 0.3538872766861194\n",
      "two-layer Loss after iteration 48000: 0.35213703570429705\n",
      "two-layer Loss after iteration 49000: 0.3505627865916943\n",
      "two-layer Loss after iteration 50000: 0.34885212618366596\n",
      "two-layer Loss after iteration 51000: 0.34746716743410094\n",
      "two-layer Loss after iteration 52000: 0.3461540383339274\n",
      "two-layer Loss after iteration 53000: 0.34501138879628457\n",
      "two-layer Loss after iteration 54000: 0.3440040086069168\n",
      "two-layer Loss after iteration 55000: 0.34304308753082585\n",
      "two-layer Loss after iteration 56000: 0.34219012854314507\n",
      "two-layer Loss after iteration 57000: 0.34138801543668795\n",
      "two-layer Loss after iteration 58000: 0.34062653413366756\n",
      "two-layer Loss after iteration 59000: 0.33990232235921775\n",
      "two-layer Loss after iteration 60000: 0.3392126234164353\n",
      "two-layer Loss after iteration 61000: 0.3385557767682389\n",
      "two-layer Loss after iteration 62000: 0.3379295632431078\n",
      "two-layer Loss after iteration 63000: 0.3373316849873685\n",
      "two-layer Loss after iteration 64000: 0.33676101508361916\n",
      "two-layer Loss after iteration 65000: 0.33621596675110416\n",
      "two-layer Loss after iteration 66000: 0.33568355380493686\n",
      "two-layer Loss after iteration 67000: 0.33513498580512063\n",
      "two-layer Loss after iteration 68000: 0.33462009488617245\n",
      "two-layer Loss after iteration 69000: 0.33413049895434715\n",
      "two-layer Loss after iteration 70000: 0.3336641078638052\n",
      "two-layer Loss after iteration 71000: 0.33321962499282426\n",
      "two-layer Loss after iteration 72000: 0.3327955402195888\n",
      "two-layer Loss after iteration 73000: 0.3323906660707428\n",
      "two-layer Loss after iteration 74000: 0.332003968575281\n",
      "two-layer Loss after iteration 75000: 0.33163459169200776\n",
      "two-layer Loss after iteration 76000: 0.33128096084997666\n",
      "two-layer Loss after iteration 77000: 0.33094276194415595\n",
      "two-layer Loss after iteration 78000: 0.3306191979308176\n",
      "two-layer Loss after iteration 79000: 0.3302226092732776\n",
      "two-layer Loss after iteration 80000: 0.32988044922457616\n",
      "two-layer Loss after iteration 81000: 0.3295538124064163\n",
      "two-layer Loss after iteration 82000: 0.3292417197992441\n",
      "two-layer Loss after iteration 83000: 0.3289431656390677\n",
      "two-layer Loss after iteration 84000: 0.32865722552654625\n",
      "two-layer Loss after iteration 85000: 0.32838313903132\n",
      "two-layer Loss after iteration 86000: 0.3281203612433709\n",
      "two-layer Loss after iteration 87000: 0.32786797444416255\n",
      "two-layer Loss after iteration 88000: 0.32762561327434697\n",
      "two-layer Loss after iteration 89000: 0.3273927487806725\n",
      "two-layer Loss after iteration 90000: 0.3271688467594004\n",
      "two-layer Loss after iteration 91000: 0.32695341206016504\n",
      "two-layer Loss after iteration 92000: 0.3267460245136598\n",
      "two-layer Loss after iteration 93000: 0.32654640161521625\n",
      "two-layer Loss after iteration 94000: 0.32635403561319704\n",
      "two-layer Loss after iteration 95000: 0.32616865305037834\n",
      "two-layer Loss after iteration 96000: 0.3259897751653384\n",
      "two-layer Loss after iteration 97000: 0.325817414653494\n",
      "two-layer Loss after iteration 98000: 0.32564000873833676\n",
      "two-layer Loss after iteration 99000: 0.3254134617324659\n",
      "two-layer Loss after iteration 0: 1472.8738609354532\n",
      "two-layer Loss after iteration 1000: 23.76905360598797\n",
      "two-layer Loss after iteration 2000: 9.244645158807298\n",
      "two-layer Loss after iteration 3000: 6.7200243342821055\n",
      "two-layer Loss after iteration 4000: 5.354270707883033\n",
      "two-layer Loss after iteration 5000: 4.511268802913259\n",
      "two-layer Loss after iteration 6000: 3.5483981073512165\n",
      "two-layer Loss after iteration 7000: 2.4266774500661055\n",
      "two-layer Loss after iteration 8000: 1.970362537567671\n",
      "two-layer Loss after iteration 9000: 1.7829375977223347\n",
      "two-layer Loss after iteration 10000: 1.6924269550152289\n",
      "two-layer Loss after iteration 11000: 1.64676632974311\n",
      "two-layer Loss after iteration 12000: 1.6124355320112524\n",
      "two-layer Loss after iteration 13000: 1.5804605670913163\n",
      "two-layer Loss after iteration 14000: 1.5522373218377135\n",
      "two-layer Loss after iteration 15000: 1.531778250255851\n",
      "two-layer Loss after iteration 16000: 1.5128383284681859\n",
      "two-layer Loss after iteration 17000: 1.4973095483730972\n",
      "two-layer Loss after iteration 18000: 1.4847086937615617\n",
      "two-layer Loss after iteration 19000: 1.4737466921421845\n",
      "two-layer Loss after iteration 20000: 1.4640780233983544\n",
      "two-layer Loss after iteration 21000: 1.4554408498655114\n",
      "two-layer Loss after iteration 22000: 1.4475261447456684\n",
      "two-layer Loss after iteration 23000: 1.440107389194001\n",
      "two-layer Loss after iteration 24000: 1.4329326122582025\n",
      "two-layer Loss after iteration 25000: 1.426613954419457\n",
      "two-layer Loss after iteration 26000: 1.4180916907568446\n",
      "two-layer Loss after iteration 27000: 1.411675347560731\n",
      "two-layer Loss after iteration 28000: 1.4061406021869982\n",
      "two-layer Loss after iteration 29000: 1.4012586008661194\n",
      "two-layer Loss after iteration 30000: 1.3964361814141568\n",
      "two-layer Loss after iteration 31000: 1.3914952102153322\n",
      "two-layer Loss after iteration 32000: 1.3872593030368692\n",
      "two-layer Loss after iteration 33000: 1.3835382668830507\n",
      "two-layer Loss after iteration 34000: 1.3800281332625168\n",
      "two-layer Loss after iteration 35000: 1.376263746615752\n",
      "two-layer Loss after iteration 36000: 1.3711560447986721\n",
      "two-layer Loss after iteration 37000: 1.3669905244699323\n",
      "two-layer Loss after iteration 38000: 1.3637026471305096\n",
      "two-layer Loss after iteration 39000: 1.3607198491667103\n",
      "two-layer Loss after iteration 40000: 1.358189069852571\n",
      "two-layer Loss after iteration 41000: 1.35594989877724\n",
      "two-layer Loss after iteration 42000: 1.3539843915725946\n",
      "two-layer Loss after iteration 43000: 1.3522233097681728\n",
      "two-layer Loss after iteration 44000: 1.35035898988418\n",
      "two-layer Loss after iteration 45000: 1.3484180742774248\n",
      "two-layer Loss after iteration 46000: 1.3466350743426938\n",
      "two-layer Loss after iteration 47000: 1.34498381302994\n",
      "two-layer Loss after iteration 48000: 1.343468617074208\n",
      "two-layer Loss after iteration 49000: 1.342112108884509\n",
      "two-layer Loss after iteration 50000: 1.340837145620192\n",
      "two-layer Loss after iteration 51000: 1.3396643559762649\n",
      "two-layer Loss after iteration 52000: 1.3385702514070759\n",
      "two-layer Loss after iteration 53000: 1.3375836554521412\n",
      "two-layer Loss after iteration 54000: 1.3366756589107303\n",
      "two-layer Loss after iteration 55000: 1.3358239515545802\n",
      "two-layer Loss after iteration 56000: 1.3350124578854032\n",
      "two-layer Loss after iteration 57000: 1.334248380037202\n",
      "two-layer Loss after iteration 58000: 1.3335189037796025\n",
      "two-layer Loss after iteration 59000: 1.3328174992953614\n",
      "two-layer Loss after iteration 60000: 1.3320544807243322\n",
      "two-layer Loss after iteration 61000: 1.3310218105786307\n",
      "two-layer Loss after iteration 62000: 1.3300744492377659\n",
      "two-layer Loss after iteration 63000: 1.3292028688776474\n",
      "two-layer Loss after iteration 64000: 1.3283898955839637\n",
      "two-layer Loss after iteration 65000: 1.327627658490076\n",
      "two-layer Loss after iteration 66000: 1.3269097315733045\n",
      "two-layer Loss after iteration 67000: 1.3262245597849642\n",
      "two-layer Loss after iteration 68000: 1.3255737860187158\n",
      "two-layer Loss after iteration 69000: 1.3249571318973827\n",
      "two-layer Loss after iteration 70000: 1.3243680997203076\n",
      "two-layer Loss after iteration 71000: 1.3237977496005706\n",
      "two-layer Loss after iteration 72000: 1.3232579589662792\n",
      "two-layer Loss after iteration 73000: 1.3227310681172366\n",
      "two-layer Loss after iteration 74000: 1.3222283698528008\n",
      "two-layer Loss after iteration 75000: 1.3217456842702213\n",
      "two-layer Loss after iteration 76000: 1.3212779644357329\n",
      "two-layer Loss after iteration 77000: 1.3208299607506362\n",
      "two-layer Loss after iteration 78000: 1.3203918409282536\n",
      "two-layer Loss after iteration 79000: 1.319973375172418\n",
      "two-layer Loss after iteration 80000: 1.3194143393416278\n",
      "two-layer Loss after iteration 81000: 1.31878033775269\n",
      "two-layer Loss after iteration 82000: 1.3181714386715542\n",
      "two-layer Loss after iteration 83000: 1.317579944345033\n",
      "two-layer Loss after iteration 84000: 1.317015464964962\n",
      "two-layer Loss after iteration 85000: 1.3164655571877213\n",
      "two-layer Loss after iteration 86000: 1.3159342992667133\n",
      "two-layer Loss after iteration 87000: 1.3154183836074322\n",
      "two-layer Loss after iteration 88000: 1.3149230155454483\n",
      "two-layer Loss after iteration 89000: 1.31445536322772\n",
      "two-layer Loss after iteration 90000: 1.3140451641597848\n",
      "two-layer Loss after iteration 91000: 1.3136470542353411\n",
      "two-layer Loss after iteration 92000: 1.3132621554997363\n",
      "two-layer Loss after iteration 93000: 1.3128839135347283\n",
      "two-layer Loss after iteration 94000: 1.3125117995287312\n",
      "two-layer Loss after iteration 95000: 1.312153164718428\n",
      "two-layer Loss after iteration 96000: 1.3118028470983816\n",
      "two-layer Loss after iteration 97000: 1.3114598675882982\n",
      "two-layer Loss after iteration 98000: 1.3111280930731846\n",
      "two-layer Loss after iteration 99000: 1.3108029681837894\n",
      "two-layer Loss after iteration 0: 1499.8524965038985\n",
      "two-layer Loss after iteration 1000: 24.674132032216715\n",
      "two-layer Loss after iteration 2000: 9.280419195295593\n",
      "two-layer Loss after iteration 3000: 6.33777523940039\n",
      "two-layer Loss after iteration 4000: 4.781066315448609\n",
      "two-layer Loss after iteration 5000: 3.8496726318424686\n",
      "two-layer Loss after iteration 6000: 3.2625430640473465\n",
      "two-layer Loss after iteration 7000: 2.773933018539094\n",
      "two-layer Loss after iteration 8000: 1.9697085717046987\n",
      "two-layer Loss after iteration 9000: 1.6194970133041975\n",
      "two-layer Loss after iteration 10000: 1.351144280514378\n",
      "two-layer Loss after iteration 11000: 1.1107546403044255\n",
      "two-layer Loss after iteration 12000: 0.8869777062604771\n",
      "two-layer Loss after iteration 13000: 0.7556524595465787\n",
      "two-layer Loss after iteration 14000: 0.6748667163741532\n",
      "two-layer Loss after iteration 15000: 0.6071453794311337\n",
      "two-layer Loss after iteration 16000: 0.5487280490910172\n",
      "two-layer Loss after iteration 17000: 0.49667161940077903\n",
      "two-layer Loss after iteration 18000: 0.4396027668382587\n",
      "two-layer Loss after iteration 19000: 0.402838371549175\n",
      "two-layer Loss after iteration 20000: 0.37247958119595503\n",
      "two-layer Loss after iteration 21000: 0.3363571354719214\n",
      "two-layer Loss after iteration 22000: 0.32924811353653305\n",
      "two-layer Loss after iteration 23000: 0.3237272735621229\n",
      "two-layer Loss after iteration 24000: 0.317894768763232\n",
      "two-layer Loss after iteration 25000: 0.30970718911641937\n",
      "two-layer Loss after iteration 26000: 0.3032938479569162\n",
      "two-layer Loss after iteration 27000: 0.29970523332351817\n",
      "two-layer Loss after iteration 28000: 0.2928397275255024\n",
      "two-layer Loss after iteration 29000: 0.25377040469306944\n",
      "two-layer Loss after iteration 30000: 0.24583318339819712\n",
      "two-layer Loss after iteration 31000: 0.24299394625159046\n",
      "two-layer Loss after iteration 32000: 0.24144778931298175\n",
      "two-layer Loss after iteration 33000: 0.24036840932503736\n",
      "two-layer Loss after iteration 34000: 0.23962516996495442\n",
      "two-layer Loss after iteration 35000: 0.23906401517722398\n",
      "two-layer Loss after iteration 36000: 0.2386200845456344\n",
      "two-layer Loss after iteration 37000: 0.2382518522961702\n",
      "two-layer Loss after iteration 38000: 0.237935494185755\n",
      "two-layer Loss after iteration 39000: 0.23765640387512482\n",
      "two-layer Loss after iteration 40000: 0.23740154954719014\n",
      "two-layer Loss after iteration 41000: 0.23716414433593294\n",
      "two-layer Loss after iteration 42000: 0.23694107317067126\n",
      "two-layer Loss after iteration 43000: 0.23673267315154806\n",
      "two-layer Loss after iteration 44000: 0.23653190425104448\n",
      "two-layer Loss after iteration 45000: 0.23634042492886573\n",
      "two-layer Loss after iteration 46000: 0.23615231962105832\n",
      "two-layer Loss after iteration 47000: 0.2359694751032366\n",
      "two-layer Loss after iteration 48000: 0.23579744202140723\n",
      "two-layer Loss after iteration 49000: 0.2356326843933026\n",
      "two-layer Loss after iteration 50000: 0.2354722061230414\n",
      "two-layer Loss after iteration 51000: 0.23531663896185528\n",
      "two-layer Loss after iteration 52000: 0.23516097390495014\n",
      "two-layer Loss after iteration 53000: 0.23501539622622317\n",
      "two-layer Loss after iteration 54000: 0.23486984012615375\n",
      "two-layer Loss after iteration 55000: 0.23473089115431536\n",
      "two-layer Loss after iteration 56000: 0.23459748158931817\n",
      "two-layer Loss after iteration 57000: 0.23446368293047334\n",
      "two-layer Loss after iteration 58000: 0.2343368322496737\n",
      "two-layer Loss after iteration 59000: 0.23421027279081763\n",
      "two-layer Loss after iteration 60000: 0.23409048583315148\n",
      "two-layer Loss after iteration 61000: 0.23397128941908002\n",
      "two-layer Loss after iteration 62000: 0.2338538402292667\n",
      "two-layer Loss after iteration 63000: 0.23374349047721618\n",
      "two-layer Loss after iteration 64000: 0.23363420004804217\n",
      "two-layer Loss after iteration 65000: 0.23352801799898187\n",
      "two-layer Loss after iteration 66000: 0.23342252805235464\n",
      "two-layer Loss after iteration 67000: 0.23332256457798778\n",
      "two-layer Loss after iteration 68000: 0.2332227820589409\n",
      "two-layer Loss after iteration 69000: 0.23312729935504792\n",
      "two-layer Loss after iteration 70000: 0.2330315950006209\n",
      "two-layer Loss after iteration 71000: 0.23293959312916987\n",
      "two-layer Loss after iteration 72000: 0.23285128152910753\n",
      "two-layer Loss after iteration 73000: 0.23276252547094708\n",
      "two-layer Loss after iteration 74000: 0.23267491052141068\n",
      "two-layer Loss after iteration 75000: 0.23259058403302374\n",
      "two-layer Loss after iteration 76000: 0.23250881057366263\n",
      "two-layer Loss after iteration 77000: 0.23242907412371602\n",
      "two-layer Loss after iteration 78000: 0.23234784231683245\n",
      "two-layer Loss after iteration 79000: 0.23227274811306067\n",
      "two-layer Loss after iteration 80000: 0.23219880063854\n",
      "two-layer Loss after iteration 81000: 0.23212474604249583\n",
      "two-layer Loss after iteration 82000: 0.23204980119228077\n",
      "two-layer Loss after iteration 83000: 0.23198174997151802\n",
      "two-layer Loss after iteration 84000: 0.23191426164059548\n",
      "two-layer Loss after iteration 85000: 0.2318491455510138\n",
      "two-layer Loss after iteration 86000: 0.23178594046035111\n",
      "two-layer Loss after iteration 87000: 0.23172308783252357\n",
      "two-layer Loss after iteration 88000: 0.23166388924783565\n",
      "two-layer Loss after iteration 89000: 0.2316081095173637\n",
      "two-layer Loss after iteration 90000: 0.23155241406067673\n",
      "two-layer Loss after iteration 91000: 0.23149761391223087\n",
      "two-layer Loss after iteration 92000: 0.23144410450653943\n",
      "two-layer Loss after iteration 93000: 0.2313920230403368\n",
      "two-layer Loss after iteration 94000: 0.23134200333651203\n",
      "two-layer Loss after iteration 95000: 0.23129598585852396\n",
      "two-layer Loss after iteration 96000: 0.2312467397949957\n",
      "two-layer Loss after iteration 97000: 0.2312022290565964\n",
      "two-layer Loss after iteration 98000: 0.23115553134906613\n",
      "two-layer Loss after iteration 99000: 0.2311094775286161\n",
      "two-layer Loss after iteration 0: 1465.5321938529569\n",
      "two-layer Loss after iteration 1000: 23.82259288268978\n",
      "two-layer Loss after iteration 2000: 9.112697639631778\n",
      "two-layer Loss after iteration 3000: 6.362123276956065\n",
      "two-layer Loss after iteration 4000: 4.835959095475557\n",
      "two-layer Loss after iteration 5000: 3.8777750899752106\n",
      "two-layer Loss after iteration 6000: 3.4418860641824924\n",
      "two-layer Loss after iteration 7000: 3.110555891589763\n",
      "two-layer Loss after iteration 8000: 2.7727837278206353\n",
      "two-layer Loss after iteration 9000: 2.4874740200682055\n",
      "two-layer Loss after iteration 10000: 2.189443847924244\n",
      "two-layer Loss after iteration 11000: 1.9995591840050004\n",
      "two-layer Loss after iteration 12000: 1.8865975826126724\n",
      "two-layer Loss after iteration 13000: 1.7865752340602206\n",
      "two-layer Loss after iteration 14000: 1.721100863807576\n",
      "two-layer Loss after iteration 15000: 1.6746804297388451\n",
      "two-layer Loss after iteration 16000: 1.628037860124912\n",
      "two-layer Loss after iteration 17000: 1.597963179835912\n",
      "two-layer Loss after iteration 18000: 1.5695100653802627\n",
      "two-layer Loss after iteration 19000: 1.5407724099311317\n",
      "two-layer Loss after iteration 20000: 1.5226601392436043\n",
      "two-layer Loss after iteration 21000: 1.5098140683630643\n",
      "two-layer Loss after iteration 22000: 1.4995475914021559\n",
      "two-layer Loss after iteration 23000: 1.4905869052726701\n",
      "two-layer Loss after iteration 24000: 1.4827169233274298\n",
      "two-layer Loss after iteration 25000: 1.4738997775698712\n",
      "two-layer Loss after iteration 26000: 1.466410030092952\n",
      "two-layer Loss after iteration 27000: 1.4600308197880556\n",
      "two-layer Loss after iteration 28000: 1.4544638131157093\n",
      "two-layer Loss after iteration 29000: 1.4495458560134638\n",
      "two-layer Loss after iteration 30000: 1.442539911834674\n",
      "two-layer Loss after iteration 31000: 1.4354563348093947\n",
      "two-layer Loss after iteration 32000: 1.4278448773685948\n",
      "two-layer Loss after iteration 33000: 1.4098950359878681\n",
      "two-layer Loss after iteration 34000: 1.4012631701246938\n",
      "two-layer Loss after iteration 35000: 1.3949367230401966\n",
      "two-layer Loss after iteration 36000: 1.3896036869902497\n",
      "two-layer Loss after iteration 37000: 1.3848700599555777\n",
      "two-layer Loss after iteration 38000: 1.3790162878583032\n",
      "two-layer Loss after iteration 39000: 1.373665686959441\n",
      "two-layer Loss after iteration 40000: 1.3689061824256288\n",
      "two-layer Loss after iteration 41000: 1.364627981224309\n",
      "two-layer Loss after iteration 42000: 1.360815992183866\n",
      "two-layer Loss after iteration 43000: 1.3575307511305224\n",
      "two-layer Loss after iteration 44000: 1.3546573977535488\n",
      "two-layer Loss after iteration 45000: 1.3521207198163954\n",
      "two-layer Loss after iteration 46000: 1.349694797948034\n",
      "two-layer Loss after iteration 47000: 1.346973879035491\n",
      "two-layer Loss after iteration 48000: 1.3434893683533948\n",
      "two-layer Loss after iteration 49000: 1.3406229118682191\n",
      "two-layer Loss after iteration 50000: 1.3381603423500879\n",
      "two-layer Loss after iteration 51000: 1.3360043062087956\n",
      "two-layer Loss after iteration 52000: 1.3339626896814893\n",
      "two-layer Loss after iteration 53000: 1.332064823047631\n",
      "two-layer Loss after iteration 54000: 1.330400406182908\n",
      "two-layer Loss after iteration 55000: 1.3288780572407826\n",
      "two-layer Loss after iteration 56000: 1.3274720088738352\n",
      "two-layer Loss after iteration 57000: 1.3261737751362974\n",
      "two-layer Loss after iteration 58000: 1.3249055676933879\n",
      "two-layer Loss after iteration 59000: 1.3232183483620363\n",
      "two-layer Loss after iteration 60000: 1.321554637650433\n",
      "two-layer Loss after iteration 61000: 1.3204123658499098\n",
      "two-layer Loss after iteration 62000: 1.319360464260769\n",
      "two-layer Loss after iteration 63000: 1.3182551572052716\n",
      "two-layer Loss after iteration 64000: 1.317102974026582\n",
      "two-layer Loss after iteration 65000: 1.2786012946703298\n",
      "two-layer Loss after iteration 66000: 1.2651221599256939\n",
      "two-layer Loss after iteration 67000: 1.2582144748042845\n",
      "two-layer Loss after iteration 68000: 1.2531643093144773\n",
      "two-layer Loss after iteration 69000: 1.249142162555833\n",
      "two-layer Loss after iteration 70000: 1.245797672886565\n",
      "two-layer Loss after iteration 71000: 1.242919088702349\n",
      "two-layer Loss after iteration 72000: 1.2404016203118169\n",
      "two-layer Loss after iteration 73000: 1.238243768101207\n",
      "two-layer Loss after iteration 74000: 1.236359609812382\n",
      "two-layer Loss after iteration 75000: 1.2346606109761504\n",
      "two-layer Loss after iteration 76000: 1.2331168188498358\n",
      "two-layer Loss after iteration 77000: 1.2317127914989927\n",
      "two-layer Loss after iteration 78000: 1.2304289514134072\n",
      "two-layer Loss after iteration 79000: 1.2289614958339412\n",
      "two-layer Loss after iteration 80000: 1.2263705432979661\n",
      "two-layer Loss after iteration 81000: 1.2240982549394521\n",
      "two-layer Loss after iteration 82000: 1.2217429323733147\n",
      "two-layer Loss after iteration 83000: 1.2190577000891158\n",
      "two-layer Loss after iteration 84000: 1.2095544054659666\n",
      "two-layer Loss after iteration 85000: 1.206620937565711\n",
      "two-layer Loss after iteration 86000: 1.2042827694912699\n",
      "two-layer Loss after iteration 87000: 1.202142769644005\n",
      "two-layer Loss after iteration 88000: 1.1999883255047767\n",
      "two-layer Loss after iteration 89000: 1.1981451790800868\n",
      "two-layer Loss after iteration 90000: 1.1951691001841855\n",
      "two-layer Loss after iteration 91000: 1.19356656444037\n",
      "two-layer Loss after iteration 92000: 1.1921544145967513\n",
      "two-layer Loss after iteration 93000: 1.1908964963792374\n",
      "two-layer Loss after iteration 94000: 1.1894909887124478\n",
      "two-layer Loss after iteration 95000: 1.1875003789222296\n",
      "two-layer Loss after iteration 96000: 1.1858415711866341\n",
      "two-layer Loss after iteration 97000: 1.1844213578602127\n",
      "two-layer Loss after iteration 98000: 1.1831856896523376\n",
      "two-layer Loss after iteration 99000: 1.1821093820552602\n",
      "two-layer Loss after iteration 0: 1575.9527715164777\n",
      "two-layer Loss after iteration 1000: 23.687920657907444\n",
      "two-layer Loss after iteration 2000: 9.026644702558917\n",
      "two-layer Loss after iteration 3000: 6.041549376129471\n",
      "two-layer Loss after iteration 4000: 4.285079814042321\n",
      "two-layer Loss after iteration 5000: 3.340169036846443\n",
      "two-layer Loss after iteration 6000: 2.4278259040434236\n",
      "two-layer Loss after iteration 7000: 1.8764378335388932\n",
      "two-layer Loss after iteration 8000: 1.5043225303163383\n",
      "two-layer Loss after iteration 9000: 1.237068823958207\n",
      "two-layer Loss after iteration 10000: 1.1079918162616944\n",
      "two-layer Loss after iteration 11000: 1.0345514785978143\n",
      "two-layer Loss after iteration 12000: 0.9772916753836765\n",
      "two-layer Loss after iteration 13000: 0.9376211398886204\n",
      "two-layer Loss after iteration 14000: 0.9080311763980163\n",
      "two-layer Loss after iteration 15000: 0.8809858496479678\n",
      "two-layer Loss after iteration 16000: 0.8626113992462473\n",
      "two-layer Loss after iteration 17000: 0.8391981508474071\n",
      "two-layer Loss after iteration 18000: 0.821911015000093\n",
      "two-layer Loss after iteration 19000: 0.8058026501505271\n",
      "two-layer Loss after iteration 20000: 0.7917885963424841\n",
      "two-layer Loss after iteration 21000: 0.7831765280597741\n",
      "two-layer Loss after iteration 22000: 0.7770367527792715\n",
      "two-layer Loss after iteration 23000: 0.7721810840625499\n",
      "two-layer Loss after iteration 24000: 0.7678342705128353\n",
      "two-layer Loss after iteration 25000: 0.7640680855766201\n",
      "two-layer Loss after iteration 26000: 0.760538142031239\n",
      "two-layer Loss after iteration 27000: 0.7563819299168537\n",
      "two-layer Loss after iteration 28000: 0.752033465693196\n",
      "two-layer Loss after iteration 29000: 0.7486739741626546\n",
      "two-layer Loss after iteration 30000: 0.7458367074726988\n",
      "two-layer Loss after iteration 31000: 0.7423303956492898\n",
      "two-layer Loss after iteration 32000: 0.7386544767290139\n",
      "two-layer Loss after iteration 33000: 0.7357589170353968\n",
      "two-layer Loss after iteration 34000: 0.7303590681120019\n",
      "two-layer Loss after iteration 35000: 0.7265849434084238\n",
      "two-layer Loss after iteration 36000: 0.7234179869851748\n",
      "two-layer Loss after iteration 37000: 0.7207570685010498\n",
      "two-layer Loss after iteration 38000: 0.7184210296091008\n",
      "two-layer Loss after iteration 39000: 0.7161394914921281\n",
      "two-layer Loss after iteration 40000: 0.7139396729545979\n",
      "two-layer Loss after iteration 41000: 0.712007026552327\n",
      "two-layer Loss after iteration 42000: 0.7102425391632065\n",
      "two-layer Loss after iteration 43000: 0.7086587223968779\n",
      "two-layer Loss after iteration 44000: 0.707222437503606\n",
      "two-layer Loss after iteration 45000: 0.7057113218918913\n",
      "two-layer Loss after iteration 46000: 0.7042589317799315\n",
      "two-layer Loss after iteration 47000: 0.702967352804357\n",
      "two-layer Loss after iteration 48000: 0.7017863389511123\n",
      "two-layer Loss after iteration 49000: 0.7006978504396989\n",
      "two-layer Loss after iteration 50000: 0.6996936023109033\n",
      "two-layer Loss after iteration 51000: 0.6987012066443062\n",
      "two-layer Loss after iteration 52000: 0.6977190370064005\n",
      "two-layer Loss after iteration 53000: 0.6968232538700481\n",
      "two-layer Loss after iteration 54000: 0.6959900161829028\n",
      "two-layer Loss after iteration 55000: 0.6924463323684376\n",
      "two-layer Loss after iteration 56000: 0.6899987367766726\n",
      "two-layer Loss after iteration 57000: 0.6884570839801732\n",
      "two-layer Loss after iteration 58000: 0.6871965816425294\n",
      "two-layer Loss after iteration 59000: 0.6859660784201599\n",
      "two-layer Loss after iteration 60000: 0.6846964803256752\n",
      "two-layer Loss after iteration 61000: 0.6836062861731839\n",
      "two-layer Loss after iteration 62000: 0.6826419886690495\n",
      "two-layer Loss after iteration 63000: 0.6817800551625196\n",
      "two-layer Loss after iteration 64000: 0.6794744460056078\n",
      "two-layer Loss after iteration 65000: 0.6780030479335283\n",
      "two-layer Loss after iteration 66000: 0.6768655616455655\n",
      "two-layer Loss after iteration 67000: 0.6758943528396968\n",
      "two-layer Loss after iteration 68000: 0.6750930277506996\n",
      "two-layer Loss after iteration 69000: 0.6743686510899299\n",
      "two-layer Loss after iteration 70000: 0.6734600223185238\n",
      "two-layer Loss after iteration 71000: 0.6725912823606405\n",
      "two-layer Loss after iteration 72000: 0.671673753278788\n",
      "two-layer Loss after iteration 73000: 0.6709055959353625\n",
      "two-layer Loss after iteration 74000: 0.6702392321383068\n",
      "two-layer Loss after iteration 75000: 0.6696552220416476\n",
      "two-layer Loss after iteration 76000: 0.6691434067987924\n",
      "two-layer Loss after iteration 77000: 0.6686878347318662\n",
      "two-layer Loss after iteration 78000: 0.6682883733759488\n",
      "two-layer Loss after iteration 79000: 0.6673406468261373\n",
      "two-layer Loss after iteration 80000: 0.6657832154236151\n",
      "two-layer Loss after iteration 81000: 0.6589750817033175\n",
      "two-layer Loss after iteration 82000: 0.6565525775627084\n",
      "two-layer Loss after iteration 83000: 0.6555193061287494\n",
      "two-layer Loss after iteration 84000: 0.6549230121047342\n",
      "two-layer Loss after iteration 85000: 0.6540593780258672\n",
      "two-layer Loss after iteration 86000: 0.6525494414738038\n",
      "two-layer Loss after iteration 87000: 0.6517517879610631\n",
      "two-layer Loss after iteration 88000: 0.65123520566832\n",
      "two-layer Loss after iteration 89000: 0.6508423434166122\n",
      "two-layer Loss after iteration 90000: 0.650252411947811\n",
      "two-layer Loss after iteration 91000: 0.6497074446495034\n",
      "two-layer Loss after iteration 92000: 0.6493192854900375\n",
      "two-layer Loss after iteration 93000: 0.6490169106687884\n",
      "two-layer Loss after iteration 94000: 0.6487697857251404\n",
      "two-layer Loss after iteration 95000: 0.6485581653039135\n",
      "two-layer Loss after iteration 96000: 0.6483852637384027\n",
      "two-layer Loss after iteration 97000: 0.6482465630583975\n",
      "two-layer Loss after iteration 98000: 0.648133144896062\n",
      "two-layer Loss after iteration 99000: 0.6458799438523286\n",
      "two-layer Loss after iteration 0: 1319.0887882813024\n",
      "two-layer Loss after iteration 1000: 26.836113435907745\n",
      "two-layer Loss after iteration 2000: 9.891764852515436\n",
      "two-layer Loss after iteration 3000: 6.655481190151058\n",
      "two-layer Loss after iteration 4000: 4.934810785931105\n",
      "two-layer Loss after iteration 5000: 3.7500762578705196\n",
      "two-layer Loss after iteration 6000: 2.508037613488722\n",
      "two-layer Loss after iteration 7000: 2.0230150793703676\n",
      "two-layer Loss after iteration 8000: 1.6544643505152776\n",
      "two-layer Loss after iteration 9000: 1.3911706531386603\n",
      "two-layer Loss after iteration 10000: 1.131231454280257\n",
      "two-layer Loss after iteration 11000: 0.879446246832583\n",
      "two-layer Loss after iteration 12000: 0.805960176732562\n",
      "two-layer Loss after iteration 13000: 0.7611340168038373\n",
      "two-layer Loss after iteration 14000: 0.728740908112769\n",
      "two-layer Loss after iteration 15000: 0.703668097560817\n",
      "two-layer Loss after iteration 16000: 0.6831586147984151\n",
      "two-layer Loss after iteration 17000: 0.6613808286020162\n",
      "two-layer Loss after iteration 18000: 0.6452746518936422\n",
      "two-layer Loss after iteration 19000: 0.6276081056473038\n",
      "two-layer Loss after iteration 20000: 0.6146141712115775\n",
      "two-layer Loss after iteration 21000: 0.6034944013535816\n",
      "two-layer Loss after iteration 22000: 0.5934622214761094\n",
      "two-layer Loss after iteration 23000: 0.5848044820933864\n",
      "two-layer Loss after iteration 24000: 0.5769977622700059\n",
      "two-layer Loss after iteration 25000: 0.5669209975067369\n",
      "two-layer Loss after iteration 26000: 0.5589045747310626\n",
      "two-layer Loss after iteration 27000: 0.5513457566632051\n",
      "two-layer Loss after iteration 28000: 0.5434611913648024\n",
      "two-layer Loss after iteration 29000: 0.5356514584598682\n",
      "two-layer Loss after iteration 30000: 0.5292798013807388\n",
      "two-layer Loss after iteration 31000: 0.5235207730455086\n",
      "two-layer Loss after iteration 32000: 0.5182518007106591\n",
      "two-layer Loss after iteration 33000: 0.51331700412952\n",
      "two-layer Loss after iteration 34000: 0.5086508947806885\n",
      "two-layer Loss after iteration 35000: 0.5042928211156752\n",
      "two-layer Loss after iteration 36000: 0.5001873028209796\n",
      "two-layer Loss after iteration 37000: 0.4962874596603232\n",
      "two-layer Loss after iteration 38000: 0.4921528837907763\n",
      "two-layer Loss after iteration 39000: 0.4885328375989429\n",
      "two-layer Loss after iteration 40000: 0.48495847435932976\n",
      "two-layer Loss after iteration 41000: 0.4814831143086048\n",
      "two-layer Loss after iteration 42000: 0.47813159415905665\n",
      "two-layer Loss after iteration 43000: 0.47487209905139255\n",
      "two-layer Loss after iteration 44000: 0.47151392190916525\n",
      "two-layer Loss after iteration 45000: 0.4683083970878201\n",
      "two-layer Loss after iteration 46000: 0.4652038100398366\n",
      "two-layer Loss after iteration 47000: 0.46172001606097585\n",
      "two-layer Loss after iteration 48000: 0.4585923209948153\n",
      "two-layer Loss after iteration 49000: 0.45499464530697226\n",
      "two-layer Loss after iteration 50000: 0.4503171548323855\n",
      "two-layer Loss after iteration 51000: 0.44653956012077717\n",
      "two-layer Loss after iteration 52000: 0.4428975915185425\n",
      "two-layer Loss after iteration 53000: 0.4384990292972918\n",
      "two-layer Loss after iteration 54000: 0.4349049375100569\n",
      "two-layer Loss after iteration 55000: 0.43156862424683534\n",
      "two-layer Loss after iteration 56000: 0.4282379048944751\n",
      "two-layer Loss after iteration 57000: 0.4189210737423051\n",
      "two-layer Loss after iteration 58000: 0.4151131176609162\n",
      "two-layer Loss after iteration 59000: 0.41143828907902674\n",
      "two-layer Loss after iteration 60000: 0.4080379587940982\n",
      "two-layer Loss after iteration 61000: 0.4048091336099367\n",
      "two-layer Loss after iteration 62000: 0.4017316167169037\n",
      "two-layer Loss after iteration 63000: 0.39878608428169027\n",
      "two-layer Loss after iteration 64000: 0.39593705496373255\n",
      "two-layer Loss after iteration 65000: 0.3931804233172569\n",
      "two-layer Loss after iteration 66000: 0.3905035237606933\n",
      "two-layer Loss after iteration 67000: 0.3877586251948959\n",
      "two-layer Loss after iteration 68000: 0.3849176318641913\n",
      "two-layer Loss after iteration 69000: 0.38218627087530677\n",
      "two-layer Loss after iteration 70000: 0.37955476466031995\n",
      "two-layer Loss after iteration 71000: 0.3770186127376253\n",
      "two-layer Loss after iteration 72000: 0.37429539447268134\n",
      "two-layer Loss after iteration 73000: 0.37170949780038454\n",
      "two-layer Loss after iteration 74000: 0.3692040565715284\n",
      "two-layer Loss after iteration 75000: 0.3667427309207032\n",
      "two-layer Loss after iteration 76000: 0.36434659495827504\n",
      "two-layer Loss after iteration 77000: 0.36177749609127885\n",
      "two-layer Loss after iteration 78000: 0.358063047977487\n",
      "two-layer Loss after iteration 79000: 0.3537890319595739\n",
      "two-layer Loss after iteration 80000: 0.3506098613409089\n",
      "two-layer Loss after iteration 81000: 0.3477544933141023\n",
      "two-layer Loss after iteration 82000: 0.3450778477577422\n",
      "two-layer Loss after iteration 83000: 0.34252060636304893\n",
      "two-layer Loss after iteration 84000: 0.3400843161690936\n",
      "two-layer Loss after iteration 85000: 0.33774895863774\n",
      "two-layer Loss after iteration 86000: 0.3355217030691473\n",
      "two-layer Loss after iteration 87000: 0.3334774808322612\n",
      "two-layer Loss after iteration 88000: 0.3316541264157689\n",
      "two-layer Loss after iteration 89000: 0.32993940367207897\n",
      "two-layer Loss after iteration 90000: 0.32830756197533256\n",
      "two-layer Loss after iteration 91000: 0.32674745947793365\n",
      "two-layer Loss after iteration 92000: 0.32525533386365535\n",
      "two-layer Loss after iteration 93000: 0.3236589857470432\n",
      "two-layer Loss after iteration 94000: 0.32222499737985333\n",
      "two-layer Loss after iteration 95000: 0.3208421537331364\n",
      "two-layer Loss after iteration 96000: 0.31951348349618514\n",
      "two-layer Loss after iteration 97000: 0.3182305379497939\n",
      "two-layer Loss after iteration 98000: 0.3169966580393989\n",
      "two-layer Loss after iteration 99000: 0.3157959641200527\n",
      "three-layer Loss after iteration 0: 1669.0446556277302\n",
      "three-layer Loss after iteration 1000: 9.835342984147132\n",
      "three-layer Loss after iteration 2000: 7.325411949648272\n",
      "three-layer Loss after iteration 3000: 5.531225719325648\n",
      "three-layer Loss after iteration 4000: 6.718582099289134\n",
      "three-layer Loss after iteration 5000: 5.855324023952428\n",
      "three-layer Loss after iteration 6000: 4.392759590375265\n",
      "three-layer Loss after iteration 7000: 4.2563697078096006\n",
      "three-layer Loss after iteration 8000: 3.4552529780765817\n",
      "three-layer Loss after iteration 9000: 2.978184695878583\n",
      "three-layer Loss after iteration 10000: 2.7095978606025053\n",
      "three-layer Loss after iteration 11000: 3.567161845176478\n",
      "three-layer Loss after iteration 12000: 2.200831774354362\n",
      "three-layer Loss after iteration 13000: 1.7609180096954538\n",
      "three-layer Loss after iteration 14000: 1.7291572829182753\n",
      "three-layer Loss after iteration 15000: 1.2066659913110203\n",
      "three-layer Loss after iteration 16000: 1.3176178294910565\n",
      "three-layer Loss after iteration 17000: 1.36870896486674\n",
      "three-layer Loss after iteration 18000: 1.1790779262504976\n",
      "three-layer Loss after iteration 19000: 1.0770340127614413\n",
      "three-layer Loss after iteration 20000: 1.162765245471964\n",
      "three-layer Loss after iteration 21000: 1.1058233319521877\n",
      "three-layer Loss after iteration 22000: 1.1038603379800518\n",
      "three-layer Loss after iteration 23000: 1.0269911982192028\n",
      "three-layer Loss after iteration 24000: 1.0815831609383888\n",
      "three-layer Loss after iteration 25000: 1.111182155254249\n",
      "three-layer Loss after iteration 26000: 1.0485643030622762\n",
      "three-layer Loss after iteration 27000: 1.0226477254826056\n",
      "three-layer Loss after iteration 28000: 1.0133130251429825\n",
      "three-layer Loss after iteration 29000: 0.9932708532910296\n",
      "three-layer Loss after iteration 30000: 0.9793325598585042\n",
      "three-layer Loss after iteration 31000: 0.9593222703476938\n",
      "three-layer Loss after iteration 32000: 0.949577563561328\n",
      "three-layer Loss after iteration 33000: 0.9421543383914135\n",
      "three-layer Loss after iteration 34000: 0.9362113427480333\n",
      "three-layer Loss after iteration 35000: 0.9303273626522748\n",
      "three-layer Loss after iteration 36000: 0.9234357511450425\n",
      "three-layer Loss after iteration 37000: 0.9181311905421399\n",
      "three-layer Loss after iteration 38000: 0.9130917142666707\n",
      "three-layer Loss after iteration 39000: 0.9099238235476858\n",
      "three-layer Loss after iteration 40000: 0.9057585813016865\n",
      "three-layer Loss after iteration 41000: 0.9023105746336811\n",
      "three-layer Loss after iteration 42000: 0.8690217436113554\n",
      "three-layer Loss after iteration 43000: 0.887007699729989\n",
      "three-layer Loss after iteration 44000: 0.8876880241221667\n",
      "three-layer Loss after iteration 45000: 0.88503856942831\n",
      "three-layer Loss after iteration 46000: 0.8789056572270165\n",
      "three-layer Loss after iteration 47000: 0.8772725573984802\n",
      "three-layer Loss after iteration 48000: 0.872570864066272\n",
      "three-layer Loss after iteration 49000: 0.8708456038398507\n",
      "three-layer Loss after iteration 50000: 0.8682789106341278\n",
      "three-layer Loss after iteration 51000: 0.8655573783083557\n",
      "three-layer Loss after iteration 52000: 0.8648445571618486\n",
      "three-layer Loss after iteration 53000: 0.8716138437898737\n",
      "three-layer Loss after iteration 54000: 0.8503611226730474\n",
      "three-layer Loss after iteration 55000: 0.8558395742648482\n",
      "three-layer Loss after iteration 56000: 0.8568133737093357\n",
      "three-layer Loss after iteration 57000: 0.8415910000168012\n",
      "three-layer Loss after iteration 58000: 0.8532377003712511\n",
      "three-layer Loss after iteration 59000: 0.8368543571391313\n",
      "three-layer Loss after iteration 60000: 0.8325197954458975\n",
      "three-layer Loss after iteration 61000: 0.8210297452527228\n",
      "three-layer Loss after iteration 62000: 0.8675205437698911\n",
      "three-layer Loss after iteration 63000: 0.8591265621539375\n",
      "three-layer Loss after iteration 64000: 0.8528584893244804\n",
      "three-layer Loss after iteration 65000: 0.8610984742638887\n",
      "three-layer Loss after iteration 66000: 0.8624019174719021\n",
      "three-layer Loss after iteration 67000: 0.8642717738662984\n",
      "three-layer Loss after iteration 68000: 0.8633755150172301\n",
      "three-layer Loss after iteration 69000: 0.8611992508150176\n",
      "three-layer Loss after iteration 70000: 0.8583563511218061\n",
      "three-layer Loss after iteration 71000: 0.8530197543277461\n",
      "three-layer Loss after iteration 72000: 0.8213855992021443\n",
      "three-layer Loss after iteration 73000: 0.8463347820530718\n",
      "three-layer Loss after iteration 74000: 0.8448087032825414\n",
      "three-layer Loss after iteration 75000: 0.8424043567869882\n",
      "three-layer Loss after iteration 76000: 0.8416937987124187\n",
      "three-layer Loss after iteration 77000: 0.8402327057578979\n",
      "three-layer Loss after iteration 78000: 0.8074493315150061\n",
      "three-layer Loss after iteration 79000: 0.8208201241899342\n",
      "three-layer Loss after iteration 80000: 0.7819094383406965\n",
      "three-layer Loss after iteration 81000: 0.7631672720583115\n",
      "three-layer Loss after iteration 82000: 0.7565798153727415\n",
      "three-layer Loss after iteration 83000: 0.8981890015836774\n",
      "three-layer Loss after iteration 84000: 1.026833932912199\n",
      "three-layer Loss after iteration 85000: 0.9515684691732773\n",
      "three-layer Loss after iteration 86000: 1.0092469014787229\n",
      "three-layer Loss after iteration 87000: 0.8842181711131709\n",
      "three-layer Loss after iteration 88000: 0.781752227248693\n",
      "three-layer Loss after iteration 89000: 1.1850418199400024\n",
      "three-layer Loss after iteration 90000: 0.9627846446317463\n",
      "three-layer Loss after iteration 91000: 0.984816544903249\n",
      "three-layer Loss after iteration 92000: 1.1607668252315742\n",
      "three-layer Loss after iteration 93000: 0.8693949292381373\n",
      "three-layer Loss after iteration 94000: 1.383351426941527\n",
      "three-layer Loss after iteration 95000: 0.6779125656325345\n",
      "three-layer Loss after iteration 96000: 0.5861989161527368\n",
      "three-layer Loss after iteration 97000: 0.6421909439115795\n",
      "three-layer Loss after iteration 98000: 0.8873330394378949\n",
      "three-layer Loss after iteration 99000: 1.1150995426789545\n",
      "three-layer Loss after iteration 0: 1558.4014942265649\n",
      "three-layer Loss after iteration 1000: 10.589700011884913\n",
      "three-layer Loss after iteration 2000: 6.864510082942022\n",
      "three-layer Loss after iteration 3000: 6.2085431486399045\n",
      "three-layer Loss after iteration 4000: 4.68646041265766\n",
      "three-layer Loss after iteration 5000: 4.0851060317254015\n",
      "three-layer Loss after iteration 6000: 2.6115388251569653\n",
      "three-layer Loss after iteration 7000: 2.437641841391884\n",
      "three-layer Loss after iteration 8000: 2.279985282532163\n",
      "three-layer Loss after iteration 9000: 1.366748724130831\n",
      "three-layer Loss after iteration 10000: 2.0362047324868984\n",
      "three-layer Loss after iteration 11000: 1.7441947794071733\n",
      "three-layer Loss after iteration 12000: 1.6383969549105906\n",
      "three-layer Loss after iteration 13000: 1.2239318179133913\n",
      "three-layer Loss after iteration 14000: 1.138929374727486\n",
      "three-layer Loss after iteration 15000: 0.9804341028687761\n",
      "three-layer Loss after iteration 16000: 0.8288510321210556\n",
      "three-layer Loss after iteration 17000: 0.8322635390437797\n",
      "three-layer Loss after iteration 18000: 0.6963215167962706\n",
      "three-layer Loss after iteration 19000: 0.6039191660168409\n",
      "three-layer Loss after iteration 20000: 0.9446069584042185\n",
      "three-layer Loss after iteration 21000: 0.9944551646051756\n",
      "three-layer Loss after iteration 22000: 0.7195354472371274\n",
      "three-layer Loss after iteration 23000: 0.622548467923008\n",
      "three-layer Loss after iteration 24000: 0.6062015155515921\n",
      "three-layer Loss after iteration 25000: 0.6911770880026877\n",
      "three-layer Loss after iteration 26000: 0.7187090831117365\n",
      "three-layer Loss after iteration 27000: 0.5278726317272366\n",
      "three-layer Loss after iteration 28000: 0.5058319550304268\n",
      "three-layer Loss after iteration 29000: 0.5098827822726189\n",
      "three-layer Loss after iteration 30000: 0.5462108402273191\n",
      "three-layer Loss after iteration 31000: 0.5628431944262906\n",
      "three-layer Loss after iteration 32000: 0.4421280700281965\n",
      "three-layer Loss after iteration 33000: 0.4111348560717215\n",
      "three-layer Loss after iteration 34000: 0.40180366674354656\n",
      "three-layer Loss after iteration 35000: 0.3840225783555642\n",
      "three-layer Loss after iteration 36000: 0.3621432749993389\n",
      "three-layer Loss after iteration 37000: 0.3986660361057181\n",
      "three-layer Loss after iteration 38000: 0.4578838951658588\n",
      "three-layer Loss after iteration 39000: 0.48755739757298916\n",
      "three-layer Loss after iteration 40000: 0.48589385423691484\n",
      "three-layer Loss after iteration 41000: 0.478157983287706\n",
      "three-layer Loss after iteration 42000: 0.4635385802344782\n",
      "three-layer Loss after iteration 43000: 0.43613003694626273\n",
      "three-layer Loss after iteration 44000: 0.41827467880088104\n",
      "three-layer Loss after iteration 45000: 0.4124802827782664\n",
      "three-layer Loss after iteration 46000: 0.7321847626186269\n",
      "three-layer Loss after iteration 47000: 0.899258725785717\n",
      "three-layer Loss after iteration 48000: 0.3422295182270751\n",
      "three-layer Loss after iteration 49000: 0.3339646150869611\n",
      "three-layer Loss after iteration 50000: 0.39367490725075954\n",
      "three-layer Loss after iteration 51000: 0.45737271641900595\n",
      "three-layer Loss after iteration 52000: 0.4706872512631687\n",
      "three-layer Loss after iteration 53000: 0.36197554557392586\n",
      "three-layer Loss after iteration 54000: 0.3267532567160516\n",
      "three-layer Loss after iteration 55000: 0.4027028926632907\n",
      "three-layer Loss after iteration 56000: 0.3416997132183538\n",
      "three-layer Loss after iteration 57000: 0.32536188484573775\n",
      "three-layer Loss after iteration 58000: 0.3211688338438205\n",
      "three-layer Loss after iteration 59000: 0.31396627384541437\n",
      "three-layer Loss after iteration 60000: 0.31308068166028685\n",
      "three-layer Loss after iteration 61000: 0.3109059114768599\n",
      "three-layer Loss after iteration 62000: 0.32381382328348046\n",
      "three-layer Loss after iteration 63000: 0.3446985494791833\n",
      "three-layer Loss after iteration 64000: 0.36472780463549853\n",
      "three-layer Loss after iteration 65000: 0.3825457663900021\n",
      "three-layer Loss after iteration 66000: 0.38712088460587213\n",
      "three-layer Loss after iteration 67000: 0.3615576718755339\n",
      "three-layer Loss after iteration 68000: 0.34507527621945416\n",
      "three-layer Loss after iteration 69000: 0.3274153370353008\n",
      "three-layer Loss after iteration 70000: 0.34812629991089505\n",
      "three-layer Loss after iteration 71000: 0.32100313188468566\n",
      "three-layer Loss after iteration 72000: 0.3544862426496844\n",
      "three-layer Loss after iteration 73000: 0.3812256624212056\n",
      "three-layer Loss after iteration 74000: 0.3911858353906429\n",
      "three-layer Loss after iteration 75000: 0.3942038872903662\n",
      "three-layer Loss after iteration 76000: 0.37817972126283783\n",
      "three-layer Loss after iteration 77000: 0.37789318947099304\n",
      "three-layer Loss after iteration 78000: 0.3789367521926606\n",
      "three-layer Loss after iteration 79000: 0.3570963285359111\n",
      "three-layer Loss after iteration 80000: 0.3628135128621213\n",
      "three-layer Loss after iteration 81000: 0.37560846981860985\n",
      "three-layer Loss after iteration 82000: 0.3730555710947236\n",
      "three-layer Loss after iteration 83000: 0.3704279227172758\n",
      "three-layer Loss after iteration 84000: 0.36715613958110715\n",
      "three-layer Loss after iteration 85000: 0.36461978750077156\n",
      "three-layer Loss after iteration 86000: 0.3890243005141905\n",
      "three-layer Loss after iteration 87000: 0.3652640381060402\n",
      "three-layer Loss after iteration 88000: 0.3780208337097046\n",
      "three-layer Loss after iteration 89000: 0.3707896638447031\n",
      "three-layer Loss after iteration 90000: 0.3558934124426685\n",
      "three-layer Loss after iteration 91000: 0.3503656194743582\n",
      "three-layer Loss after iteration 92000: 0.3488509607486488\n",
      "three-layer Loss after iteration 93000: 0.34560960771730503\n",
      "three-layer Loss after iteration 94000: 0.3425841906519746\n",
      "three-layer Loss after iteration 95000: 0.34021973671949973\n",
      "three-layer Loss after iteration 96000: 0.3297945959459599\n",
      "three-layer Loss after iteration 97000: 0.33377364368658363\n",
      "three-layer Loss after iteration 98000: 0.33715790684547475\n",
      "three-layer Loss after iteration 99000: 0.33987655365228253\n",
      "three-layer Loss after iteration 0: 1650.515869466491\n",
      "three-layer Loss after iteration 1000: 9.655413623011235\n",
      "three-layer Loss after iteration 2000: 8.158951036147753\n",
      "three-layer Loss after iteration 3000: 6.674789430236817\n",
      "three-layer Loss after iteration 4000: 6.172546535497929\n",
      "three-layer Loss after iteration 5000: 5.096566654150162\n",
      "three-layer Loss after iteration 6000: 4.450720425006815\n",
      "three-layer Loss after iteration 7000: 4.28764349649182\n",
      "three-layer Loss after iteration 8000: 3.3915115495368484\n",
      "three-layer Loss after iteration 9000: 2.6948280517350356\n",
      "three-layer Loss after iteration 10000: 2.1298206821137886\n",
      "three-layer Loss after iteration 11000: 1.8785258377390428\n",
      "three-layer Loss after iteration 12000: 1.870652080394483\n",
      "three-layer Loss after iteration 13000: 1.8061180614580583\n",
      "three-layer Loss after iteration 14000: 1.7001362235969004\n",
      "three-layer Loss after iteration 15000: 1.6583738047691192\n",
      "three-layer Loss after iteration 16000: 1.541372452988394\n",
      "three-layer Loss after iteration 17000: 1.3777547837796742\n",
      "three-layer Loss after iteration 18000: 1.5949941770195615\n",
      "three-layer Loss after iteration 19000: 1.4793775011163814\n",
      "three-layer Loss after iteration 20000: 1.1985096287232015\n",
      "three-layer Loss after iteration 21000: 1.4397818518274463\n",
      "three-layer Loss after iteration 22000: 1.3087075235224332\n",
      "three-layer Loss after iteration 23000: 1.0277995926798824\n",
      "three-layer Loss after iteration 24000: 1.2256361283937118\n",
      "three-layer Loss after iteration 25000: 1.1631250988138198\n",
      "three-layer Loss after iteration 26000: 1.1427843199708019\n",
      "three-layer Loss after iteration 27000: 1.4827924327933668\n",
      "three-layer Loss after iteration 28000: 0.9563347252826234\n",
      "three-layer Loss after iteration 29000: 1.355894379934181\n",
      "three-layer Loss after iteration 30000: 1.3040820579592654\n",
      "three-layer Loss after iteration 31000: 1.1510980618677276\n",
      "three-layer Loss after iteration 32000: 1.338641308826558\n",
      "three-layer Loss after iteration 33000: 1.0838953965217135\n",
      "three-layer Loss after iteration 34000: 0.8120287220110476\n",
      "three-layer Loss after iteration 35000: 0.8699935768663775\n",
      "three-layer Loss after iteration 36000: 2.7068602680487825\n",
      "three-layer Loss after iteration 37000: 0.5868926362587267\n",
      "three-layer Loss after iteration 38000: 0.6924182296135905\n",
      "three-layer Loss after iteration 39000: 0.6999591963184159\n",
      "three-layer Loss after iteration 40000: 0.6325135796314334\n",
      "three-layer Loss after iteration 41000: 0.7411438930330838\n",
      "three-layer Loss after iteration 42000: 1.5636111716079766\n",
      "three-layer Loss after iteration 43000: 0.7119157442407494\n",
      "three-layer Loss after iteration 44000: 1.0149435685803736\n",
      "three-layer Loss after iteration 45000: 0.8636550006914957\n",
      "three-layer Loss after iteration 46000: 1.1465490970908538\n",
      "three-layer Loss after iteration 47000: 1.2682892823181549\n",
      "three-layer Loss after iteration 48000: 0.9526294412469447\n",
      "three-layer Loss after iteration 49000: 0.9221591221222216\n",
      "three-layer Loss after iteration 50000: 0.7090934499279412\n",
      "three-layer Loss after iteration 51000: 0.6558007303320752\n",
      "three-layer Loss after iteration 52000: 1.4501566709757714\n",
      "three-layer Loss after iteration 53000: 0.682087015110379\n",
      "three-layer Loss after iteration 54000: 0.4742239750214287\n",
      "three-layer Loss after iteration 55000: 0.49247265117949357\n",
      "three-layer Loss after iteration 56000: 0.4305761074332412\n",
      "three-layer Loss after iteration 57000: 0.4192077202884348\n",
      "three-layer Loss after iteration 58000: 0.6701127203824436\n",
      "three-layer Loss after iteration 59000: 0.4997488959169419\n",
      "three-layer Loss after iteration 60000: 0.8938625033247275\n",
      "three-layer Loss after iteration 61000: 0.4081460262382578\n",
      "three-layer Loss after iteration 62000: 0.4020483819795989\n",
      "three-layer Loss after iteration 63000: 0.43442514001910404\n",
      "three-layer Loss after iteration 64000: 0.4438461750080282\n",
      "three-layer Loss after iteration 65000: 0.6042681620329927\n",
      "three-layer Loss after iteration 66000: 0.5348382272028357\n",
      "three-layer Loss after iteration 67000: 0.7101434446588507\n",
      "three-layer Loss after iteration 68000: 0.7084263234032014\n",
      "three-layer Loss after iteration 69000: 0.6739849069409201\n",
      "three-layer Loss after iteration 70000: 0.6549764033661543\n",
      "three-layer Loss after iteration 71000: 0.7373189387705553\n",
      "three-layer Loss after iteration 72000: 0.4988959029395336\n",
      "three-layer Loss after iteration 73000: 0.40563306517661446\n",
      "three-layer Loss after iteration 74000: 0.384057599815077\n",
      "three-layer Loss after iteration 75000: 0.4456500870675973\n",
      "three-layer Loss after iteration 76000: 0.3535248874576785\n",
      "three-layer Loss after iteration 77000: 0.38143070000011436\n",
      "three-layer Loss after iteration 78000: 0.3700497054228289\n",
      "three-layer Loss after iteration 79000: 0.38617358419811176\n",
      "three-layer Loss after iteration 80000: 0.37035256827007224\n",
      "three-layer Loss after iteration 81000: 0.4492900971204195\n",
      "three-layer Loss after iteration 82000: 0.6549788143233016\n",
      "three-layer Loss after iteration 83000: 0.6542052020588272\n",
      "three-layer Loss after iteration 84000: 0.4816484276067148\n",
      "three-layer Loss after iteration 85000: 0.5327788536535121\n",
      "three-layer Loss after iteration 86000: 0.5641387276795369\n",
      "three-layer Loss after iteration 87000: 0.47747488314812736\n",
      "three-layer Loss after iteration 88000: 0.54492179086984\n",
      "three-layer Loss after iteration 89000: 0.5502334513838285\n",
      "three-layer Loss after iteration 90000: 0.540803745629302\n",
      "three-layer Loss after iteration 91000: 0.4862731582454577\n",
      "three-layer Loss after iteration 92000: 0.46450466256766854\n",
      "three-layer Loss after iteration 93000: 0.5886986319583213\n",
      "three-layer Loss after iteration 94000: 0.5757249053856156\n",
      "three-layer Loss after iteration 95000: 0.5416401824295902\n",
      "three-layer Loss after iteration 96000: 0.5305816004580172\n",
      "three-layer Loss after iteration 97000: 0.5260848101419529\n",
      "three-layer Loss after iteration 98000: 0.4947242043605299\n",
      "three-layer Loss after iteration 99000: 0.5161141752104437\n",
      "three-layer Loss after iteration 0: 1575.5261303259124\n",
      "three-layer Loss after iteration 1000: 11.740815622147647\n",
      "three-layer Loss after iteration 2000: 8.772314062101698\n",
      "three-layer Loss after iteration 3000: 5.32701951247471\n",
      "three-layer Loss after iteration 4000: 4.089345363445984\n",
      "three-layer Loss after iteration 5000: 3.7625312487862277\n",
      "three-layer Loss after iteration 6000: 3.500294273630729\n",
      "three-layer Loss after iteration 7000: 4.348754916705306\n",
      "three-layer Loss after iteration 8000: 2.17394524515201\n",
      "three-layer Loss after iteration 9000: 2.338729727268349\n",
      "three-layer Loss after iteration 10000: 1.6550901522866994\n",
      "three-layer Loss after iteration 11000: 2.2658418433410006\n",
      "three-layer Loss after iteration 12000: 2.020590561522347\n",
      "three-layer Loss after iteration 13000: 1.776820839107368\n",
      "three-layer Loss after iteration 14000: 1.6850654273709151\n",
      "three-layer Loss after iteration 15000: 1.4378827773262706\n",
      "three-layer Loss after iteration 16000: 1.468770591316792\n",
      "three-layer Loss after iteration 17000: 1.8490096660239472\n",
      "three-layer Loss after iteration 18000: 1.5226481451030591\n",
      "three-layer Loss after iteration 19000: 1.4591628579989784\n",
      "three-layer Loss after iteration 20000: 1.2882798669909445\n",
      "three-layer Loss after iteration 21000: 1.2418051380710167\n",
      "three-layer Loss after iteration 22000: 1.215330077772334\n",
      "three-layer Loss after iteration 23000: 1.4544056647188497\n",
      "three-layer Loss after iteration 24000: 1.599399216288956\n",
      "three-layer Loss after iteration 25000: 1.6598333207453029\n",
      "three-layer Loss after iteration 26000: 1.4771620180109766\n",
      "three-layer Loss after iteration 27000: 1.3403000768124205\n",
      "three-layer Loss after iteration 28000: 1.2123114936095407\n",
      "three-layer Loss after iteration 29000: 1.1576757442038115\n",
      "three-layer Loss after iteration 30000: 1.1861014955594091\n",
      "three-layer Loss after iteration 31000: 1.14662755447969\n",
      "three-layer Loss after iteration 32000: 0.9739063187910435\n",
      "three-layer Loss after iteration 33000: 1.1263211256306345\n",
      "three-layer Loss after iteration 34000: 1.1027635528151736\n",
      "three-layer Loss after iteration 35000: 1.0516232119748614\n",
      "three-layer Loss after iteration 36000: 1.018724641362011\n",
      "three-layer Loss after iteration 37000: 0.964260761515632\n",
      "three-layer Loss after iteration 38000: 0.9618062969086876\n",
      "three-layer Loss after iteration 39000: 0.8553914481483735\n",
      "three-layer Loss after iteration 40000: 0.867591217600025\n",
      "three-layer Loss after iteration 41000: 0.8517704923951055\n",
      "three-layer Loss after iteration 42000: 0.8047797105910983\n",
      "three-layer Loss after iteration 43000: 0.8221823411476331\n",
      "three-layer Loss after iteration 44000: 0.8615635253486362\n",
      "three-layer Loss after iteration 45000: 0.8464628458387028\n",
      "three-layer Loss after iteration 46000: 0.8245857341816258\n",
      "three-layer Loss after iteration 47000: 0.819964591035048\n",
      "three-layer Loss after iteration 48000: 0.8163668741197704\n",
      "three-layer Loss after iteration 49000: 0.8140301040735811\n",
      "three-layer Loss after iteration 50000: 0.8091092380000825\n",
      "three-layer Loss after iteration 51000: 0.8015899848253091\n",
      "three-layer Loss after iteration 52000: 0.7995409479955872\n",
      "three-layer Loss after iteration 53000: 0.8019624135404332\n",
      "three-layer Loss after iteration 54000: 0.7988241039348617\n",
      "three-layer Loss after iteration 55000: 0.7961521017884998\n",
      "three-layer Loss after iteration 56000: 0.7887955102164658\n",
      "three-layer Loss after iteration 57000: 0.7933740959800276\n",
      "three-layer Loss after iteration 58000: 0.7861292397477516\n",
      "three-layer Loss after iteration 59000: 0.7724539186966802\n",
      "three-layer Loss after iteration 60000: 0.7843114078497678\n",
      "three-layer Loss after iteration 61000: 0.7896833823538423\n",
      "three-layer Loss after iteration 62000: 0.7865153607288565\n",
      "three-layer Loss after iteration 63000: 0.7832044060140515\n",
      "three-layer Loss after iteration 64000: 0.7780598643305277\n",
      "three-layer Loss after iteration 65000: 0.7724315803663497\n",
      "three-layer Loss after iteration 66000: 0.7651955762265563\n",
      "three-layer Loss after iteration 67000: 0.7534183621120734\n",
      "three-layer Loss after iteration 68000: 0.6789381983576481\n",
      "three-layer Loss after iteration 69000: 0.7536203060651414\n",
      "three-layer Loss after iteration 70000: 0.7473693819173371\n",
      "three-layer Loss after iteration 71000: 0.7317781143890215\n",
      "three-layer Loss after iteration 72000: 0.7267289869397783\n",
      "three-layer Loss after iteration 73000: 0.7116058292877658\n",
      "three-layer Loss after iteration 74000: 0.6798126928613417\n",
      "three-layer Loss after iteration 75000: 0.6836506323720557\n",
      "three-layer Loss after iteration 76000: 0.6781975606981003\n",
      "three-layer Loss after iteration 77000: 0.676263107382274\n",
      "three-layer Loss after iteration 78000: 0.6377765442531566\n",
      "three-layer Loss after iteration 79000: 0.6794023081693766\n",
      "three-layer Loss after iteration 80000: 0.6531827442919015\n",
      "three-layer Loss after iteration 81000: 0.6470501576143068\n",
      "three-layer Loss after iteration 82000: 0.6500229373464939\n",
      "three-layer Loss after iteration 83000: 0.6508340563289471\n",
      "three-layer Loss after iteration 84000: 0.6514891540543765\n",
      "three-layer Loss after iteration 85000: 0.6564418828762657\n",
      "three-layer Loss after iteration 86000: 0.6641178643149476\n",
      "three-layer Loss after iteration 87000: 0.6497313234222836\n",
      "three-layer Loss after iteration 88000: 0.6488399086199051\n",
      "three-layer Loss after iteration 89000: 0.6511232777063699\n",
      "three-layer Loss after iteration 90000: 0.6599476517147226\n",
      "three-layer Loss after iteration 91000: 0.6628198651956192\n",
      "three-layer Loss after iteration 92000: 0.6632657318751571\n",
      "three-layer Loss after iteration 93000: 0.6578592612629726\n",
      "three-layer Loss after iteration 94000: 0.653585611794065\n",
      "three-layer Loss after iteration 95000: 0.651115981971058\n",
      "three-layer Loss after iteration 96000: 0.6758084680233529\n",
      "three-layer Loss after iteration 97000: 0.6377548020651855\n",
      "three-layer Loss after iteration 98000: 0.6559269088570507\n",
      "three-layer Loss after iteration 99000: 0.645472388118697\n",
      "three-layer Loss after iteration 0: 1646.083526554416\n",
      "three-layer Loss after iteration 1000: 11.944108342521686\n",
      "three-layer Loss after iteration 2000: 7.395987425922402\n",
      "three-layer Loss after iteration 3000: 5.587746184625046\n",
      "three-layer Loss after iteration 4000: 4.14162352008498\n",
      "three-layer Loss after iteration 5000: 3.46182565388198\n",
      "three-layer Loss after iteration 6000: 2.0123779031817475\n",
      "three-layer Loss after iteration 7000: 1.7555514307266358\n",
      "three-layer Loss after iteration 8000: 1.7045040070432502\n",
      "three-layer Loss after iteration 9000: 1.597119152121233\n",
      "three-layer Loss after iteration 10000: 1.2874792848996048\n",
      "three-layer Loss after iteration 11000: 1.2819490319562339\n",
      "three-layer Loss after iteration 12000: 1.1768436221022747\n",
      "three-layer Loss after iteration 13000: 1.0515026939976184\n",
      "three-layer Loss after iteration 14000: 1.0359087113950647\n",
      "three-layer Loss after iteration 15000: 1.0080073704418138\n",
      "three-layer Loss after iteration 16000: 0.9628663368306186\n",
      "three-layer Loss after iteration 17000: 0.9596220874081516\n",
      "three-layer Loss after iteration 18000: 0.9388022009518805\n",
      "three-layer Loss after iteration 19000: 0.901719450964888\n",
      "three-layer Loss after iteration 20000: 0.9513488407401395\n",
      "three-layer Loss after iteration 21000: 0.917393523418084\n",
      "three-layer Loss after iteration 22000: 0.8916129451876286\n",
      "three-layer Loss after iteration 23000: 0.8705468903070045\n",
      "three-layer Loss after iteration 24000: 0.8543795693135614\n",
      "three-layer Loss after iteration 25000: 0.8430218994046024\n",
      "three-layer Loss after iteration 26000: 0.830268326131918\n",
      "three-layer Loss after iteration 27000: 0.8072471342297212\n",
      "three-layer Loss after iteration 28000: 0.7959699192503459\n",
      "three-layer Loss after iteration 29000: 0.7858406763359803\n",
      "three-layer Loss after iteration 30000: 0.777936532432658\n",
      "three-layer Loss after iteration 31000: 0.790410151858183\n",
      "three-layer Loss after iteration 32000: 0.8152092573511788\n",
      "three-layer Loss after iteration 33000: 0.8098575590859289\n",
      "three-layer Loss after iteration 34000: 0.8066504064723474\n",
      "three-layer Loss after iteration 35000: 0.8250948208367723\n",
      "three-layer Loss after iteration 36000: 1.0399864105277976\n",
      "three-layer Loss after iteration 37000: 0.950281642581242\n",
      "three-layer Loss after iteration 38000: 0.8150611951023718\n",
      "three-layer Loss after iteration 39000: 0.9961645207592571\n",
      "three-layer Loss after iteration 40000: 0.8477579029886261\n",
      "three-layer Loss after iteration 41000: 0.8400342073115545\n",
      "three-layer Loss after iteration 42000: 0.7177631464191618\n",
      "three-layer Loss after iteration 43000: 0.7082442768869069\n",
      "three-layer Loss after iteration 44000: 0.7396948026899407\n",
      "three-layer Loss after iteration 45000: 0.7146037584230549\n",
      "three-layer Loss after iteration 46000: 0.7833199261082127\n",
      "three-layer Loss after iteration 47000: 0.7719932005614996\n",
      "three-layer Loss after iteration 48000: 0.7416794498855341\n",
      "three-layer Loss after iteration 49000: 0.7336900132606238\n",
      "three-layer Loss after iteration 50000: 0.7151479463507981\n",
      "three-layer Loss after iteration 51000: 0.7169487928952697\n",
      "three-layer Loss after iteration 52000: 0.723824562940054\n",
      "three-layer Loss after iteration 53000: 0.7164929462013684\n",
      "three-layer Loss after iteration 54000: 0.6714150937281422\n",
      "three-layer Loss after iteration 55000: 0.6900262255077151\n",
      "three-layer Loss after iteration 56000: 0.7131017068583453\n",
      "three-layer Loss after iteration 57000: 0.6986478185137563\n",
      "three-layer Loss after iteration 58000: 0.6298790161559643\n",
      "three-layer Loss after iteration 59000: 0.6619295439856178\n",
      "three-layer Loss after iteration 60000: 0.6694734404460849\n",
      "three-layer Loss after iteration 61000: 0.700756239952742\n",
      "three-layer Loss after iteration 62000: 0.7002802617565282\n",
      "three-layer Loss after iteration 63000: 0.6972138719522413\n",
      "three-layer Loss after iteration 64000: 0.6931104608838918\n",
      "three-layer Loss after iteration 65000: 0.6856731081510691\n",
      "three-layer Loss after iteration 66000: 0.6792167770541565\n",
      "three-layer Loss after iteration 67000: 0.6661108392758429\n",
      "three-layer Loss after iteration 68000: 0.6622001971588293\n",
      "three-layer Loss after iteration 69000: 0.6579408098325022\n",
      "three-layer Loss after iteration 70000: 0.6502584571811404\n",
      "three-layer Loss after iteration 71000: 0.648690454584045\n",
      "three-layer Loss after iteration 72000: 0.6472752174052863\n",
      "three-layer Loss after iteration 73000: 0.6452502397310287\n",
      "three-layer Loss after iteration 74000: 0.6458867773743332\n",
      "three-layer Loss after iteration 75000: 0.6438708473620551\n",
      "three-layer Loss after iteration 76000: 0.6414997133531634\n",
      "three-layer Loss after iteration 77000: 0.6408178658207218\n",
      "three-layer Loss after iteration 78000: 0.6396021830539621\n",
      "three-layer Loss after iteration 79000: 0.6375350401015264\n",
      "three-layer Loss after iteration 80000: 0.6255868961250787\n",
      "three-layer Loss after iteration 81000: 0.5796681896988722\n",
      "three-layer Loss after iteration 82000: 0.630719453571816\n",
      "three-layer Loss after iteration 83000: 0.584877698032109\n",
      "three-layer Loss after iteration 84000: 0.563920850594445\n",
      "three-layer Loss after iteration 85000: 0.5757170948015136\n",
      "three-layer Loss after iteration 86000: 0.5938616420024735\n",
      "three-layer Loss after iteration 87000: 0.564130782565062\n",
      "three-layer Loss after iteration 88000: 0.6257387156258084\n",
      "three-layer Loss after iteration 89000: 0.6013983831429124\n",
      "three-layer Loss after iteration 90000: 0.5109714392647337\n",
      "three-layer Loss after iteration 91000: 0.6479085002629937\n",
      "three-layer Loss after iteration 92000: 0.6543711555979409\n",
      "three-layer Loss after iteration 93000: 0.6506229304783057\n",
      "three-layer Loss after iteration 94000: 0.6382984486044648\n",
      "three-layer Loss after iteration 95000: 0.6324774537024226\n",
      "three-layer Loss after iteration 96000: 0.629432911232868\n",
      "three-layer Loss after iteration 97000: 0.6256894605969867\n",
      "three-layer Loss after iteration 98000: 0.639961074310406\n",
      "three-layer Loss after iteration 99000: 0.6315748532334666\n",
      "three-layer Loss after iteration 0: 1724.4765057613488\n",
      "three-layer Loss after iteration 1000: 9.76676405585546\n",
      "three-layer Loss after iteration 2000: 7.091187607086304\n",
      "three-layer Loss after iteration 3000: 5.6785157972482905\n",
      "three-layer Loss after iteration 4000: 4.145805500940631\n",
      "three-layer Loss after iteration 5000: 4.135036204271831\n",
      "three-layer Loss after iteration 6000: 2.345960737311405\n",
      "three-layer Loss after iteration 7000: 2.4786271930366337\n",
      "three-layer Loss after iteration 8000: 2.6531653195580738\n",
      "three-layer Loss after iteration 9000: 2.1108960619231705\n",
      "three-layer Loss after iteration 10000: 2.1243014030500205\n",
      "three-layer Loss after iteration 11000: 2.231594649945381\n",
      "three-layer Loss after iteration 12000: 2.225458631780772\n",
      "three-layer Loss after iteration 13000: 2.022036765520596\n",
      "three-layer Loss after iteration 14000: 1.4278275274944834\n",
      "three-layer Loss after iteration 15000: 1.2363790307807854\n",
      "three-layer Loss after iteration 16000: 1.5775214624969975\n",
      "three-layer Loss after iteration 17000: 1.6483839547659598\n",
      "three-layer Loss after iteration 18000: 1.9777113450430306\n",
      "three-layer Loss after iteration 19000: 1.7230032293635769\n",
      "three-layer Loss after iteration 20000: 1.5959230106576308\n",
      "three-layer Loss after iteration 21000: 2.296363917487364\n",
      "three-layer Loss after iteration 22000: 1.0992028070446245\n",
      "three-layer Loss after iteration 23000: 3.1125587452948\n",
      "three-layer Loss after iteration 24000: 1.0407461110173586\n",
      "three-layer Loss after iteration 25000: 2.148641299949496\n",
      "three-layer Loss after iteration 26000: 1.176144916835664\n",
      "three-layer Loss after iteration 27000: 1.4524732999536667\n",
      "three-layer Loss after iteration 28000: 1.3570712408537673\n",
      "three-layer Loss after iteration 29000: 1.3547751355990154\n",
      "three-layer Loss after iteration 30000: 1.3536739769878456\n",
      "three-layer Loss after iteration 31000: 1.347458034749452\n",
      "three-layer Loss after iteration 32000: 1.3530198087181793\n",
      "three-layer Loss after iteration 33000: 1.3595048513636978\n",
      "three-layer Loss after iteration 34000: 1.2932413619735101\n",
      "three-layer Loss after iteration 35000: 1.3477965335883277\n",
      "three-layer Loss after iteration 36000: 1.0553721533711133\n",
      "three-layer Loss after iteration 37000: 1.1111964471205613\n",
      "three-layer Loss after iteration 38000: 1.1844156017017078\n",
      "three-layer Loss after iteration 39000: 1.3636219301363715\n",
      "three-layer Loss after iteration 40000: 1.0659139953824586\n",
      "three-layer Loss after iteration 41000: 0.8185900614514815\n",
      "three-layer Loss after iteration 42000: 1.0507405305111046\n",
      "three-layer Loss after iteration 43000: 0.8308536678255198\n",
      "three-layer Loss after iteration 44000: 1.3781185845953752\n",
      "three-layer Loss after iteration 45000: 0.7234879569026101\n",
      "three-layer Loss after iteration 46000: 0.7235156483907781\n",
      "3.827498150280124e-05 0.7234879569026101 0.7235156483907781\n",
      "three-layer Loss after iteration 0: 1759.704103845564\n",
      "three-layer Loss after iteration 1000: 12.620815370007598\n",
      "three-layer Loss after iteration 2000: 8.983588382601289\n",
      "three-layer Loss after iteration 3000: 5.920533474333744\n",
      "three-layer Loss after iteration 4000: 5.317767890959512\n",
      "three-layer Loss after iteration 5000: 4.240159865071146\n",
      "three-layer Loss after iteration 6000: 3.645993710681012\n",
      "three-layer Loss after iteration 7000: 3.303714575624865\n",
      "three-layer Loss after iteration 8000: 3.0634213230196887\n",
      "three-layer Loss after iteration 9000: 3.4218740000896224\n",
      "three-layer Loss after iteration 10000: 2.8554181254250155\n",
      "three-layer Loss after iteration 11000: 1.968640054670771\n",
      "three-layer Loss after iteration 12000: 1.819632539622076\n",
      "three-layer Loss after iteration 13000: 2.7608391641996355\n",
      "three-layer Loss after iteration 14000: 1.4662202698024747\n",
      "three-layer Loss after iteration 15000: 1.7112233791316966\n",
      "three-layer Loss after iteration 16000: 1.5724513922366599\n",
      "three-layer Loss after iteration 17000: 1.5564997284018953\n",
      "three-layer Loss after iteration 18000: 1.8317650654287412\n",
      "three-layer Loss after iteration 19000: 1.4025515201618712\n",
      "three-layer Loss after iteration 20000: 1.509930827989515\n",
      "three-layer Loss after iteration 21000: 1.5205525094123418\n",
      "three-layer Loss after iteration 22000: 1.5651703191256592\n",
      "three-layer Loss after iteration 23000: 1.4377668660893062\n",
      "three-layer Loss after iteration 24000: 1.4495760371133768\n",
      "three-layer Loss after iteration 25000: 1.4334909557948647\n",
      "three-layer Loss after iteration 26000: 1.3714349187579744\n",
      "three-layer Loss after iteration 27000: 1.154011985228992\n",
      "three-layer Loss after iteration 28000: 1.3734477135634557\n",
      "three-layer Loss after iteration 29000: 1.5794398634988713\n",
      "three-layer Loss after iteration 30000: 1.3896841768765391\n",
      "three-layer Loss after iteration 31000: 1.288892589160408\n",
      "three-layer Loss after iteration 32000: 1.2929582246268976\n",
      "three-layer Loss after iteration 33000: 1.5918751074436013\n",
      "three-layer Loss after iteration 34000: 1.3568262385449867\n",
      "three-layer Loss after iteration 35000: 1.28847549991708\n",
      "three-layer Loss after iteration 36000: 1.247551652934177\n",
      "three-layer Loss after iteration 37000: 1.1141933472302106\n",
      "three-layer Loss after iteration 38000: 1.116281531689058\n",
      "three-layer Loss after iteration 39000: 1.0594264763197938\n",
      "three-layer Loss after iteration 40000: 1.0084818181258792\n",
      "three-layer Loss after iteration 41000: 1.600488798662849\n",
      "three-layer Loss after iteration 42000: 0.8486358753606194\n",
      "three-layer Loss after iteration 43000: 0.7915710596667279\n",
      "three-layer Loss after iteration 44000: 0.9399239369760375\n",
      "three-layer Loss after iteration 45000: 1.285399996256536\n",
      "three-layer Loss after iteration 46000: 1.189072074965571\n",
      "three-layer Loss after iteration 47000: 0.840229242889134\n",
      "three-layer Loss after iteration 48000: 0.9262016018957666\n",
      "three-layer Loss after iteration 49000: 1.002084729288257\n",
      "three-layer Loss after iteration 50000: 0.9109648272052222\n",
      "three-layer Loss after iteration 51000: 0.9949873063590405\n",
      "three-layer Loss after iteration 52000: 1.2264970391333105\n",
      "three-layer Loss after iteration 53000: 0.9376494100309873\n",
      "three-layer Loss after iteration 54000: 1.0164649538548647\n",
      "three-layer Loss after iteration 55000: 0.804565852462226\n",
      "three-layer Loss after iteration 56000: 1.0850366969782348\n",
      "three-layer Loss after iteration 57000: 0.8223315621041916\n",
      "three-layer Loss after iteration 58000: 0.9368847668304117\n",
      "three-layer Loss after iteration 59000: 0.8803772935193436\n",
      "three-layer Loss after iteration 60000: 1.0119634353509228\n",
      "three-layer Loss after iteration 61000: 0.7383965330245492\n",
      "three-layer Loss after iteration 62000: 0.7965529029286046\n",
      "three-layer Loss after iteration 63000: 0.9144193906820558\n",
      "three-layer Loss after iteration 64000: 0.700384083173577\n",
      "three-layer Loss after iteration 65000: 0.9254309264727192\n",
      "three-layer Loss after iteration 66000: 0.7779049793241251\n",
      "three-layer Loss after iteration 67000: 0.8337361241676671\n",
      "three-layer Loss after iteration 68000: 0.7321779733468652\n",
      "three-layer Loss after iteration 69000: 0.6981642496951365\n",
      "three-layer Loss after iteration 70000: 0.8213861798739331\n",
      "three-layer Loss after iteration 71000: 0.8004134540096379\n",
      "three-layer Loss after iteration 72000: 0.830357800336215\n",
      "three-layer Loss after iteration 73000: 0.7588807283377368\n",
      "three-layer Loss after iteration 74000: 0.7895832535260807\n",
      "three-layer Loss after iteration 75000: 0.77186353116047\n",
      "three-layer Loss after iteration 76000: 0.7610930794798163\n",
      "three-layer Loss after iteration 77000: 0.7520442175551534\n",
      "three-layer Loss after iteration 78000: 0.7486891503574925\n",
      "three-layer Loss after iteration 79000: 0.6947148679598253\n",
      "three-layer Loss after iteration 80000: 0.6962943288112683\n",
      "three-layer Loss after iteration 81000: 0.7123872219728344\n",
      "three-layer Loss after iteration 82000: 0.7065094438383805\n",
      "three-layer Loss after iteration 83000: 0.6780531979678659\n",
      "three-layer Loss after iteration 84000: 0.708198790066103\n",
      "three-layer Loss after iteration 85000: 0.6715825600987096\n",
      "three-layer Loss after iteration 86000: 0.6549960108826877\n",
      "three-layer Loss after iteration 87000: 0.6530892608985204\n",
      "three-layer Loss after iteration 88000: 0.6558340580670986\n",
      "three-layer Loss after iteration 89000: 0.6527051154347545\n",
      "three-layer Loss after iteration 90000: 0.6459456371815763\n",
      "three-layer Loss after iteration 91000: 0.6434608445043776\n",
      "three-layer Loss after iteration 92000: 0.6437534282734656\n",
      "three-layer Loss after iteration 93000: 0.6539195003647764\n",
      "three-layer Loss after iteration 94000: 0.6480940343993561\n",
      "three-layer Loss after iteration 95000: 0.6449072229868554\n",
      "three-layer Loss after iteration 96000: 0.6411315398705384\n",
      "three-layer Loss after iteration 97000: 0.7010269317180235\n",
      "three-layer Loss after iteration 98000: 0.555209274055814\n",
      "three-layer Loss after iteration 99000: 0.6003349624621227\n",
      "three-layer Loss after iteration 0: 1718.5737793785588\n",
      "three-layer Loss after iteration 1000: 10.775264874093814\n",
      "three-layer Loss after iteration 2000: 7.3994292497459435\n",
      "three-layer Loss after iteration 3000: 7.576187364190585\n",
      "three-layer Loss after iteration 4000: 5.916073947816108\n",
      "three-layer Loss after iteration 5000: 3.4887910909677258\n",
      "three-layer Loss after iteration 6000: 3.3029746867292022\n",
      "three-layer Loss after iteration 7000: 2.343517382933841\n",
      "three-layer Loss after iteration 8000: 2.4002905480784724\n",
      "three-layer Loss after iteration 9000: 2.027859832626442\n",
      "three-layer Loss after iteration 10000: 2.018223515197385\n",
      "three-layer Loss after iteration 11000: 1.953527969371803\n",
      "three-layer Loss after iteration 12000: 1.6455553687605238\n",
      "three-layer Loss after iteration 13000: 2.080787266627475\n",
      "three-layer Loss after iteration 14000: 1.681436827146951\n",
      "three-layer Loss after iteration 15000: 0.9806361285684595\n",
      "three-layer Loss after iteration 16000: 1.500348725526417\n",
      "three-layer Loss after iteration 17000: 1.0535062497564676\n",
      "three-layer Loss after iteration 18000: 0.9389180917457136\n",
      "three-layer Loss after iteration 19000: 1.5690263886523992\n",
      "three-layer Loss after iteration 20000: 1.1960928916929985\n",
      "three-layer Loss after iteration 21000: 1.4584805426024903\n",
      "three-layer Loss after iteration 22000: 1.0929846550780795\n",
      "three-layer Loss after iteration 23000: 1.1208458209251144\n",
      "three-layer Loss after iteration 24000: 0.9927786448368662\n",
      "three-layer Loss after iteration 25000: 1.053714133487046\n",
      "three-layer Loss after iteration 26000: 0.9950670787113668\n",
      "three-layer Loss after iteration 27000: 0.922064058008347\n",
      "three-layer Loss after iteration 28000: 0.92406597284888\n",
      "three-layer Loss after iteration 29000: 0.9109571143574249\n",
      "three-layer Loss after iteration 30000: 0.8885847118913711\n",
      "three-layer Loss after iteration 31000: 0.8484597206027201\n",
      "three-layer Loss after iteration 32000: 1.304961055469562\n",
      "three-layer Loss after iteration 33000: 0.8095696082747275\n",
      "three-layer Loss after iteration 34000: 0.7057321333602512\n",
      "three-layer Loss after iteration 35000: 1.0319471909485054\n",
      "three-layer Loss after iteration 36000: 0.750409123967435\n",
      "three-layer Loss after iteration 37000: 0.7448010281503692\n",
      "three-layer Loss after iteration 38000: 0.8787469520210186\n",
      "three-layer Loss after iteration 39000: 0.8088311312773581\n",
      "three-layer Loss after iteration 40000: 0.7800749101715913\n",
      "three-layer Loss after iteration 41000: 0.7731833656688855\n",
      "three-layer Loss after iteration 42000: 0.7596930155269703\n",
      "three-layer Loss after iteration 43000: 0.7576460358867929\n",
      "three-layer Loss after iteration 44000: 0.8416944679119585\n",
      "three-layer Loss after iteration 45000: 0.7686598170591655\n",
      "three-layer Loss after iteration 46000: 0.7551086765680288\n",
      "three-layer Loss after iteration 47000: 0.7604450963196334\n",
      "three-layer Loss after iteration 48000: 0.7550492680383238\n",
      "three-layer Loss after iteration 49000: 0.7899671925297115\n",
      "three-layer Loss after iteration 50000: 0.7437939052487776\n",
      "three-layer Loss after iteration 51000: 0.7291769499512011\n",
      "three-layer Loss after iteration 52000: 0.71685864479306\n",
      "three-layer Loss after iteration 53000: 0.7111907521795532\n",
      "three-layer Loss after iteration 54000: 0.7245294588841726\n",
      "three-layer Loss after iteration 55000: 0.7081435265023295\n",
      "three-layer Loss after iteration 56000: 0.7004237667254419\n",
      "three-layer Loss after iteration 57000: 0.6961355961084209\n",
      "three-layer Loss after iteration 58000: 0.6967352689033579\n",
      "three-layer Loss after iteration 59000: 0.7096102129441146\n",
      "three-layer Loss after iteration 60000: 0.6971067569671007\n",
      "three-layer Loss after iteration 61000: 0.6927624153967556\n",
      "three-layer Loss after iteration 62000: 0.6959097978112593\n",
      "three-layer Loss after iteration 63000: 0.6858580202620341\n",
      "three-layer Loss after iteration 64000: 0.683082773933453\n",
      "three-layer Loss after iteration 65000: 0.6820152489630906\n",
      "three-layer Loss after iteration 66000: 0.6814635026426511\n",
      "three-layer Loss after iteration 67000: 0.6907870262870612\n",
      "three-layer Loss after iteration 68000: 0.697837323726087\n",
      "three-layer Loss after iteration 69000: 0.700718885020726\n",
      "three-layer Loss after iteration 70000: 0.6975029342165315\n",
      "three-layer Loss after iteration 71000: 0.6736769128199083\n",
      "three-layer Loss after iteration 72000: 0.6524549489412135\n",
      "three-layer Loss after iteration 73000: 0.6538540469525593\n",
      "three-layer Loss after iteration 74000: 0.7745216902547847\n",
      "three-layer Loss after iteration 75000: 0.7489025010368228\n",
      "three-layer Loss after iteration 76000: 0.7665549659669386\n",
      "three-layer Loss after iteration 77000: 0.7666802419797237\n",
      "three-layer Loss after iteration 78000: 0.7441281645178184\n",
      "three-layer Loss after iteration 79000: 0.8159920901550252\n",
      "three-layer Loss after iteration 80000: 0.7989827740955566\n",
      "three-layer Loss after iteration 81000: 0.7497253419751033\n",
      "three-layer Loss after iteration 82000: 0.718064561375037\n",
      "three-layer Loss after iteration 83000: 0.718168796489115\n",
      "three-layer Loss after iteration 84000: 0.7289216188595673\n",
      "three-layer Loss after iteration 85000: 0.7043217771112774\n",
      "three-layer Loss after iteration 86000: 0.6902443101169881\n",
      "three-layer Loss after iteration 87000: 0.9690966193246936\n",
      "three-layer Loss after iteration 88000: 0.630708954296251\n",
      "three-layer Loss after iteration 89000: 0.6356845152895916\n",
      "three-layer Loss after iteration 90000: 1.1155556879943431\n",
      "three-layer Loss after iteration 91000: 0.6359384619792097\n",
      "three-layer Loss after iteration 92000: 0.652129502022877\n",
      "three-layer Loss after iteration 93000: 0.6903157700059793\n",
      "three-layer Loss after iteration 94000: 0.6289540230628515\n",
      "three-layer Loss after iteration 95000: 0.6553276900039108\n",
      "three-layer Loss after iteration 96000: 0.7840406902098472\n",
      "three-layer Loss after iteration 97000: 0.6523552347328678\n",
      "three-layer Loss after iteration 98000: 0.8455893101695418\n",
      "three-layer Loss after iteration 99000: 0.6425382793048617\n",
      "three-layer Loss after iteration 0: 1734.8722279268975\n",
      "three-layer Loss after iteration 1000: 9.380213485387408\n",
      "three-layer Loss after iteration 2000: 7.3543632454969305\n",
      "three-layer Loss after iteration 3000: 6.2936152096653615\n",
      "three-layer Loss after iteration 4000: 6.0566798137823445\n",
      "three-layer Loss after iteration 5000: 5.033538350334206\n",
      "three-layer Loss after iteration 6000: 3.34556802501627\n",
      "three-layer Loss after iteration 7000: 3.574550059157112\n",
      "three-layer Loss after iteration 8000: 2.6902186881297054\n",
      "three-layer Loss after iteration 9000: 2.59547105367569\n",
      "three-layer Loss after iteration 10000: 2.4205202808760857\n",
      "three-layer Loss after iteration 11000: 2.094040993231797\n",
      "three-layer Loss after iteration 12000: 1.9064382671320073\n",
      "three-layer Loss after iteration 13000: 1.6498792530093669\n",
      "three-layer Loss after iteration 14000: 1.3581558398366913\n",
      "three-layer Loss after iteration 15000: 1.375385240469447\n",
      "three-layer Loss after iteration 16000: 1.341364171205907\n",
      "three-layer Loss after iteration 17000: 1.4094304097114148\n",
      "three-layer Loss after iteration 18000: 1.3739548603098386\n",
      "three-layer Loss after iteration 19000: 1.24853562799352\n",
      "three-layer Loss after iteration 20000: 1.1470797190852775\n",
      "three-layer Loss after iteration 21000: 1.2100343027642064\n",
      "three-layer Loss after iteration 22000: 1.1732493770469323\n",
      "three-layer Loss after iteration 23000: 1.1550054310602405\n",
      "three-layer Loss after iteration 24000: 1.0797036808713512\n",
      "three-layer Loss after iteration 25000: 1.0816259835315432\n",
      "three-layer Loss after iteration 26000: 1.0171769659269212\n",
      "three-layer Loss after iteration 27000: 0.9979191273106074\n",
      "three-layer Loss after iteration 28000: 1.006816606832362\n",
      "three-layer Loss after iteration 29000: 1.0264849366987483\n",
      "three-layer Loss after iteration 30000: 1.0127253291019394\n",
      "three-layer Loss after iteration 31000: 0.9919723027565277\n",
      "three-layer Loss after iteration 32000: 0.9180354092836895\n",
      "three-layer Loss after iteration 33000: 0.9708638326561301\n",
      "three-layer Loss after iteration 34000: 0.9533017810793657\n",
      "three-layer Loss after iteration 35000: 0.9475737049465062\n",
      "three-layer Loss after iteration 36000: 1.1400092511880884\n",
      "three-layer Loss after iteration 37000: 0.9874222494397792\n",
      "three-layer Loss after iteration 38000: 0.9518860825272216\n",
      "three-layer Loss after iteration 39000: 0.9200348551604374\n",
      "three-layer Loss after iteration 40000: 0.9049054058946947\n",
      "three-layer Loss after iteration 41000: 0.8833525201656173\n",
      "three-layer Loss after iteration 42000: 0.8842468203152013\n",
      "three-layer Loss after iteration 43000: 0.8728280456381632\n",
      "three-layer Loss after iteration 44000: 0.8611709330195214\n",
      "three-layer Loss after iteration 45000: 0.8702744928141634\n",
      "three-layer Loss after iteration 46000: 0.823145883094257\n",
      "three-layer Loss after iteration 47000: 0.827708183333316\n",
      "three-layer Loss after iteration 48000: 0.8265865103935011\n",
      "three-layer Loss after iteration 49000: 0.8227971207506791\n",
      "three-layer Loss after iteration 50000: 0.8178119628018445\n",
      "three-layer Loss after iteration 51000: 0.8123936142472777\n",
      "three-layer Loss after iteration 52000: 0.8069922644078825\n",
      "three-layer Loss after iteration 53000: 0.8019091542369834\n",
      "three-layer Loss after iteration 54000: 0.822842393059017\n",
      "three-layer Loss after iteration 55000: 0.7953643557537831\n",
      "three-layer Loss after iteration 56000: 0.7306317885808704\n",
      "three-layer Loss after iteration 57000: 0.7988709172512368\n",
      "three-layer Loss after iteration 58000: 0.771792272235004\n",
      "three-layer Loss after iteration 59000: 0.799435775232112\n",
      "three-layer Loss after iteration 60000: 0.7746766913322095\n",
      "three-layer Loss after iteration 61000: 0.7172123552748908\n",
      "three-layer Loss after iteration 62000: 0.7723326665259996\n",
      "three-layer Loss after iteration 63000: 0.7144077906435888\n",
      "three-layer Loss after iteration 64000: 0.7122473874161679\n",
      "three-layer Loss after iteration 65000: 0.725944747402353\n",
      "three-layer Loss after iteration 66000: 0.7210358020000253\n",
      "three-layer Loss after iteration 67000: 0.7208803960384301\n",
      "three-layer Loss after iteration 68000: 0.7206946081765426\n",
      "three-layer Loss after iteration 69000: 0.7169891497202949\n",
      "three-layer Loss after iteration 70000: 0.7040849311048647\n",
      "three-layer Loss after iteration 71000: 0.7126344911329784\n",
      "three-layer Loss after iteration 72000: 0.8097726552578929\n",
      "three-layer Loss after iteration 73000: 0.7065593644929582\n",
      "three-layer Loss after iteration 74000: 0.723780955900095\n",
      "three-layer Loss after iteration 75000: 0.7118193226879485\n",
      "three-layer Loss after iteration 76000: 0.7025273028805792\n",
      "three-layer Loss after iteration 77000: 0.7557759582259969\n",
      "three-layer Loss after iteration 78000: 0.7095643565255891\n",
      "three-layer Loss after iteration 79000: 0.7299495585436702\n",
      "three-layer Loss after iteration 80000: 0.7320527115365155\n",
      "three-layer Loss after iteration 81000: 0.7158512213944249\n",
      "three-layer Loss after iteration 82000: 0.721303669389328\n",
      "three-layer Loss after iteration 83000: 0.7197507393185925\n",
      "three-layer Loss after iteration 84000: 0.7191143386509227\n",
      "three-layer Loss after iteration 85000: 0.7204362301899176\n",
      "three-layer Loss after iteration 86000: 0.7191069110082154\n",
      "three-layer Loss after iteration 87000: 0.7175732791217083\n",
      "three-layer Loss after iteration 88000: 0.716504714908602\n",
      "three-layer Loss after iteration 89000: 0.714265989158087\n",
      "three-layer Loss after iteration 90000: 0.71249367003704\n",
      "three-layer Loss after iteration 91000: 0.7105694926369922\n",
      "three-layer Loss after iteration 92000: 0.7097222284850907\n",
      "three-layer Loss after iteration 93000: 0.710392069993395\n",
      "three-layer Loss after iteration 94000: 0.7094677208954014\n",
      "three-layer Loss after iteration 95000: 0.7078451743876978\n",
      "three-layer Loss after iteration 96000: 0.7070961869480227\n",
      "three-layer Loss after iteration 97000: 0.7062149404962154\n",
      "three-layer Loss after iteration 98000: 0.7054001116998172\n",
      "three-layer Loss after iteration 99000: 0.7046102902216136\n",
      "three-layer Loss after iteration 0: 1538.9374763625508\n",
      "three-layer Loss after iteration 1000: 9.80286319628617\n",
      "three-layer Loss after iteration 2000: 8.54847990370589\n",
      "three-layer Loss after iteration 3000: 6.995246118780695\n",
      "three-layer Loss after iteration 4000: 5.005982719422319\n",
      "three-layer Loss after iteration 5000: 4.608340632600888\n",
      "three-layer Loss after iteration 6000: 3.3700156849079606\n",
      "three-layer Loss after iteration 7000: 2.898355169159823\n",
      "three-layer Loss after iteration 8000: 3.5221934389845435\n",
      "three-layer Loss after iteration 9000: 2.0526540095934904\n",
      "three-layer Loss after iteration 10000: 1.8815878730175624\n",
      "three-layer Loss after iteration 11000: 1.9058217671574411\n",
      "three-layer Loss after iteration 12000: 1.9010448567453746\n",
      "three-layer Loss after iteration 13000: 1.8119308790854205\n",
      "three-layer Loss after iteration 14000: 1.3938048400576302\n",
      "three-layer Loss after iteration 15000: 1.5462389963015637\n",
      "three-layer Loss after iteration 16000: 1.4923637811392498\n",
      "three-layer Loss after iteration 17000: 1.3542036114132106\n",
      "three-layer Loss after iteration 18000: 1.2942474991695503\n",
      "three-layer Loss after iteration 19000: 1.2056267671033578\n",
      "three-layer Loss after iteration 20000: 1.005043428975037\n",
      "three-layer Loss after iteration 21000: 1.0975415825397763\n",
      "three-layer Loss after iteration 22000: 0.7464317721922212\n",
      "three-layer Loss after iteration 23000: 1.2156190896615706\n",
      "three-layer Loss after iteration 24000: 1.1320301771802375\n",
      "three-layer Loss after iteration 25000: 1.093281372033701\n",
      "three-layer Loss after iteration 26000: 1.0448204024286927\n",
      "three-layer Loss after iteration 27000: 0.7586497922452936\n",
      "three-layer Loss after iteration 28000: 1.1299770599113517\n",
      "three-layer Loss after iteration 29000: 1.0163220325778715\n",
      "three-layer Loss after iteration 30000: 0.8107398939528933\n",
      "three-layer Loss after iteration 31000: 1.01261382916562\n",
      "three-layer Loss after iteration 32000: 1.068642399437765\n",
      "three-layer Loss after iteration 33000: 0.9514778056521164\n",
      "three-layer Loss after iteration 34000: 0.9401171383606718\n",
      "three-layer Loss after iteration 35000: 0.9316424363306719\n",
      "three-layer Loss after iteration 36000: 0.9158287054059391\n",
      "three-layer Loss after iteration 37000: 0.9011033695305403\n",
      "three-layer Loss after iteration 38000: 0.8918919644119446\n",
      "three-layer Loss after iteration 39000: 0.8561958730023781\n",
      "three-layer Loss after iteration 40000: 0.8609496678974804\n",
      "three-layer Loss after iteration 41000: 0.8582078587469618\n",
      "three-layer Loss after iteration 42000: 0.8503276027070069\n",
      "three-layer Loss after iteration 43000: 0.8426789070618682\n",
      "three-layer Loss after iteration 44000: 0.8335594291590711\n",
      "three-layer Loss after iteration 45000: 0.7600296577474196\n",
      "three-layer Loss after iteration 46000: 0.8499536752215512\n",
      "three-layer Loss after iteration 47000: 0.8463403799427837\n",
      "three-layer Loss after iteration 48000: 0.8374286071538305\n",
      "three-layer Loss after iteration 49000: 0.8298128177039552\n",
      "three-layer Loss after iteration 50000: 0.8232719717312773\n",
      "three-layer Loss after iteration 51000: 0.8174862541805077\n",
      "three-layer Loss after iteration 52000: 0.8123741333116583\n",
      "three-layer Loss after iteration 53000: 0.8050191450169527\n",
      "three-layer Loss after iteration 54000: 0.802034906274656\n",
      "three-layer Loss after iteration 55000: 0.7974405628196591\n",
      "three-layer Loss after iteration 56000: 0.7941966700630276\n",
      "three-layer Loss after iteration 57000: 0.7886245012785379\n",
      "three-layer Loss after iteration 58000: 0.7863671284895764\n",
      "three-layer Loss after iteration 59000: 0.783363363036243\n",
      "three-layer Loss after iteration 60000: 0.780358342847618\n",
      "three-layer Loss after iteration 61000: 0.7778329715938856\n",
      "three-layer Loss after iteration 62000: 0.7789805046081335\n",
      "three-layer Loss after iteration 63000: 0.7902472454769165\n",
      "three-layer Loss after iteration 64000: 0.866159442964865\n",
      "three-layer Loss after iteration 65000: 0.8291132903902828\n",
      "three-layer Loss after iteration 66000: 0.8231363799987381\n",
      "three-layer Loss after iteration 67000: 0.8185340578329882\n",
      "three-layer Loss after iteration 68000: 0.8143239181116091\n",
      "three-layer Loss after iteration 69000: 0.8099375237730102\n",
      "three-layer Loss after iteration 70000: 0.8052323369873244\n",
      "three-layer Loss after iteration 71000: 0.7957165259201169\n",
      "three-layer Loss after iteration 72000: 0.792260749911049\n",
      "three-layer Loss after iteration 73000: 0.7886060544945136\n",
      "three-layer Loss after iteration 74000: 0.775028705604461\n",
      "three-layer Loss after iteration 75000: 0.7996363260247035\n",
      "three-layer Loss after iteration 76000: 0.7716263466922212\n",
      "three-layer Loss after iteration 77000: 0.7657107915138669\n",
      "three-layer Loss after iteration 78000: 0.7632912446675871\n",
      "three-layer Loss after iteration 79000: 0.7569017605650237\n",
      "three-layer Loss after iteration 80000: 0.7451415964070613\n",
      "three-layer Loss after iteration 81000: 0.7451816315539627\n",
      "5.372824050403378e-05 0.7451415964070613 0.7451816315539627\n",
      "three-layer Loss after iteration 0: 1684.4692639219347\n",
      "three-layer Loss after iteration 1000: 11.42170552415395\n",
      "three-layer Loss after iteration 2000: 7.2921664243592\n",
      "three-layer Loss after iteration 3000: 5.9936897037047085\n",
      "three-layer Loss after iteration 4000: 4.5779463416429405\n",
      "three-layer Loss after iteration 5000: 2.6925962945667807\n",
      "three-layer Loss after iteration 6000: 2.3284220983240784\n",
      "three-layer Loss after iteration 7000: 1.9917668336480063\n",
      "three-layer Loss after iteration 8000: 1.7277611796360806\n",
      "three-layer Loss after iteration 9000: 1.77082837915881\n",
      "three-layer Loss after iteration 10000: 0.6832098874579638\n",
      "three-layer Loss after iteration 11000: 0.6756587322828023\n",
      "three-layer Loss after iteration 12000: 1.2819489850246837\n",
      "three-layer Loss after iteration 13000: 1.529052302172752\n",
      "three-layer Loss after iteration 14000: 1.0706681225455534\n",
      "three-layer Loss after iteration 15000: 0.6629942971154787\n",
      "three-layer Loss after iteration 16000: 0.43180554171695473\n",
      "three-layer Loss after iteration 17000: 1.845293531961862\n",
      "three-layer Loss after iteration 18000: 0.3768692001902577\n",
      "three-layer Loss after iteration 19000: 0.6229806815286444\n",
      "three-layer Loss after iteration 20000: 1.1507345326813372\n",
      "three-layer Loss after iteration 21000: 0.436239387909789\n",
      "three-layer Loss after iteration 22000: 0.3879029646947595\n",
      "three-layer Loss after iteration 23000: 0.7552859643479612\n",
      "three-layer Loss after iteration 24000: 0.41426674426113186\n",
      "three-layer Loss after iteration 25000: 0.45697497188338326\n",
      "three-layer Loss after iteration 26000: 0.801544778429722\n",
      "three-layer Loss after iteration 27000: 0.4840070481599622\n",
      "three-layer Loss after iteration 28000: 0.4610944332004217\n",
      "three-layer Loss after iteration 29000: 0.4618810481663701\n",
      "three-layer Loss after iteration 30000: 0.46062437141911916\n",
      "three-layer Loss after iteration 31000: 0.43427226336636765\n",
      "three-layer Loss after iteration 32000: 0.37714520471270324\n",
      "three-layer Loss after iteration 33000: 0.4056124482644665\n",
      "three-layer Loss after iteration 34000: 0.3813110721032226\n",
      "three-layer Loss after iteration 35000: 0.36362150387295805\n",
      "three-layer Loss after iteration 36000: 0.3457527462573919\n",
      "three-layer Loss after iteration 37000: 0.35700901798200657\n",
      "three-layer Loss after iteration 38000: 0.32248795747084097\n",
      "three-layer Loss after iteration 39000: 0.25106898684643053\n",
      "three-layer Loss after iteration 40000: 0.2479706408363668\n",
      "three-layer Loss after iteration 41000: 0.25766912058063773\n",
      "three-layer Loss after iteration 42000: 0.3718226220320293\n",
      "three-layer Loss after iteration 43000: 0.2876658406971325\n",
      "three-layer Loss after iteration 44000: 0.2826742971082547\n",
      "three-layer Loss after iteration 45000: 0.30544333141872987\n",
      "three-layer Loss after iteration 46000: 0.31892161720319145\n",
      "three-layer Loss after iteration 47000: 0.3090392464693697\n",
      "three-layer Loss after iteration 48000: 0.3169889880162508\n",
      "three-layer Loss after iteration 49000: 0.3272214505065185\n",
      "three-layer Loss after iteration 50000: 0.30221239339325306\n",
      "three-layer Loss after iteration 51000: 0.2960883342550883\n",
      "three-layer Loss after iteration 52000: 0.2934982866492532\n",
      "three-layer Loss after iteration 53000: 0.2601895876505383\n",
      "three-layer Loss after iteration 54000: 0.2814745739019154\n",
      "three-layer Loss after iteration 55000: 0.2792448646551281\n",
      "three-layer Loss after iteration 56000: 0.2797259037667664\n",
      "three-layer Loss after iteration 57000: 0.2739150888976307\n",
      "three-layer Loss after iteration 58000: 0.26993297012633155\n",
      "three-layer Loss after iteration 59000: 0.2665375046786623\n",
      "three-layer Loss after iteration 60000: 0.2631271390004608\n",
      "three-layer Loss after iteration 61000: 0.26474352649052796\n",
      "three-layer Loss after iteration 62000: 0.2629070137377821\n",
      "three-layer Loss after iteration 63000: 0.25355279204894354\n",
      "three-layer Loss after iteration 64000: 0.24690302524033428\n",
      "three-layer Loss after iteration 65000: 0.24279574567200368\n",
      "three-layer Loss after iteration 66000: 0.23893843890356964\n",
      "three-layer Loss after iteration 67000: 0.22171077838589898\n",
      "three-layer Loss after iteration 68000: 0.22838252412507518\n",
      "three-layer Loss after iteration 69000: 0.22637179496181078\n",
      "three-layer Loss after iteration 70000: 0.22418534842152832\n",
      "three-layer Loss after iteration 71000: 0.2232880395601597\n",
      "three-layer Loss after iteration 72000: 0.22142081738489475\n",
      "three-layer Loss after iteration 73000: 0.21993835293524822\n",
      "three-layer Loss after iteration 74000: 0.2183723918186989\n",
      "three-layer Loss after iteration 75000: 0.21699436822540166\n",
      "three-layer Loss after iteration 76000: 0.21403609144196428\n",
      "three-layer Loss after iteration 77000: 0.21278051503308223\n",
      "three-layer Loss after iteration 78000: 0.21155229751158497\n",
      "three-layer Loss after iteration 79000: 0.2106696619648791\n",
      "three-layer Loss after iteration 80000: 0.20965383868189535\n",
      "three-layer Loss after iteration 81000: 0.20690839475964207\n",
      "three-layer Loss after iteration 82000: 0.2076300500738328\n",
      "three-layer Loss after iteration 83000: 0.20725730576111062\n",
      "three-layer Loss after iteration 84000: 0.20591819469545644\n",
      "three-layer Loss after iteration 85000: 0.20523374547169188\n",
      "three-layer Loss after iteration 86000: 0.20462950212472897\n",
      "three-layer Loss after iteration 87000: 0.20423217497184745\n",
      "three-layer Loss after iteration 88000: 0.20435099307299345\n",
      "three-layer Loss after iteration 89000: 0.20443767918723207\n",
      "three-layer Loss after iteration 90000: 0.20433759723963069\n",
      "three-layer Loss after iteration 91000: 0.2035851216287766\n",
      "three-layer Loss after iteration 92000: 0.20274272514940941\n",
      "three-layer Loss after iteration 93000: 0.2019214637343352\n",
      "three-layer Loss after iteration 94000: 0.20135868718520675\n",
      "three-layer Loss after iteration 95000: 0.20088981603548306\n",
      "three-layer Loss after iteration 96000: 0.2006865586576281\n",
      "three-layer Loss after iteration 97000: 0.20052546027333942\n",
      "three-layer Loss after iteration 98000: 0.20039622057753087\n",
      "three-layer Loss after iteration 99000: 0.20038146234976553\n",
      "7.36452400290064e-05 0.20039622057753087 0.20038146234976553\n",
      "three-layer Loss after iteration 0: 1517.116036011875\n",
      "three-layer Loss after iteration 1000: 12.520665824195063\n",
      "three-layer Loss after iteration 2000: 7.562808006124954\n",
      "three-layer Loss after iteration 3000: 5.433664090033041\n",
      "three-layer Loss after iteration 4000: 4.362453005878764\n",
      "three-layer Loss after iteration 5000: 3.230834916601595\n",
      "three-layer Loss after iteration 6000: 3.3965909469383617\n",
      "three-layer Loss after iteration 7000: 3.125694540745269\n",
      "three-layer Loss after iteration 8000: 2.766800564964581\n",
      "three-layer Loss after iteration 9000: 2.6774300014711683\n",
      "three-layer Loss after iteration 10000: 2.5330651152301398\n",
      "three-layer Loss after iteration 11000: 2.5444690391544085\n",
      "three-layer Loss after iteration 12000: 2.3428022063381695\n",
      "three-layer Loss after iteration 13000: 2.311055235857463\n",
      "three-layer Loss after iteration 14000: 1.8572578397261623\n",
      "three-layer Loss after iteration 15000: 2.6193349550266434\n",
      "three-layer Loss after iteration 16000: 2.3906604763606754\n",
      "three-layer Loss after iteration 17000: 1.298144460669795\n",
      "three-layer Loss after iteration 18000: 1.151506584964633\n",
      "three-layer Loss after iteration 19000: 1.8191226419607558\n",
      "three-layer Loss after iteration 20000: 3.004833653698409\n",
      "three-layer Loss after iteration 21000: 2.038467395497901\n",
      "three-layer Loss after iteration 22000: 1.5961623442872117\n",
      "three-layer Loss after iteration 23000: 1.6279928625588274\n",
      "three-layer Loss after iteration 24000: 1.5219485371933106\n",
      "three-layer Loss after iteration 25000: 1.4652792874505243\n",
      "three-layer Loss after iteration 26000: 1.4401582556064743\n",
      "three-layer Loss after iteration 27000: 1.4321336527454822\n",
      "three-layer Loss after iteration 28000: 1.4263082085162997\n",
      "three-layer Loss after iteration 29000: 1.3848756965842959\n",
      "three-layer Loss after iteration 30000: 1.2862527055243966\n",
      "three-layer Loss after iteration 31000: 1.3794494729612956\n",
      "three-layer Loss after iteration 32000: 1.3739278770394505\n",
      "three-layer Loss after iteration 33000: 1.5513963415510064\n",
      "three-layer Loss after iteration 34000: 1.3842936419496936\n",
      "three-layer Loss after iteration 35000: 1.3319783313998743\n",
      "three-layer Loss after iteration 36000: 0.9515605754860383\n",
      "three-layer Loss after iteration 37000: 0.9780094049797794\n",
      "three-layer Loss after iteration 38000: 1.0755085633704284\n",
      "three-layer Loss after iteration 39000: 1.3612526605139996\n",
      "three-layer Loss after iteration 40000: 1.1236449669895905\n",
      "three-layer Loss after iteration 41000: 1.1206184772519037\n",
      "three-layer Loss after iteration 42000: 1.1912135142413176\n",
      "three-layer Loss after iteration 43000: 1.1521301115536622\n",
      "three-layer Loss after iteration 44000: 1.1356680442210212\n",
      "three-layer Loss after iteration 45000: 1.118955202564536\n",
      "three-layer Loss after iteration 46000: 1.1026959809715557\n",
      "three-layer Loss after iteration 47000: 1.088591144908844\n",
      "three-layer Loss after iteration 48000: 1.063347671394122\n",
      "three-layer Loss after iteration 49000: 1.0209435055358644\n",
      "three-layer Loss after iteration 50000: 1.0277719549394548\n",
      "three-layer Loss after iteration 51000: 1.040787837716215\n",
      "three-layer Loss after iteration 52000: 1.0712643236854107\n",
      "three-layer Loss after iteration 53000: 1.0436227394985838\n",
      "three-layer Loss after iteration 54000: 1.3109377101797342\n",
      "three-layer Loss after iteration 55000: 1.1090339161810987\n",
      "three-layer Loss after iteration 56000: 1.030432941547014\n",
      "three-layer Loss after iteration 57000: 0.5766785945115983\n",
      "three-layer Loss after iteration 58000: 1.108029775705227\n",
      "three-layer Loss after iteration 59000: 0.5243701878911494\n",
      "three-layer Loss after iteration 60000: 0.9581575811910943\n",
      "three-layer Loss after iteration 61000: 0.9358052118963066\n",
      "three-layer Loss after iteration 62000: 0.7443389883163847\n",
      "three-layer Loss after iteration 63000: 0.8116590570535013\n",
      "three-layer Loss after iteration 64000: 0.9151222513810876\n",
      "three-layer Loss after iteration 65000: 0.7095544536228889\n",
      "three-layer Loss after iteration 66000: 0.7748623844419578\n",
      "three-layer Loss after iteration 67000: 0.7617020001995994\n",
      "three-layer Loss after iteration 68000: 0.7486531287853186\n",
      "three-layer Loss after iteration 69000: 0.7042977712539323\n",
      "three-layer Loss after iteration 70000: 0.7104244614634777\n",
      "three-layer Loss after iteration 71000: 0.6614761809465123\n",
      "three-layer Loss after iteration 72000: 0.7229996559653133\n",
      "three-layer Loss after iteration 73000: 0.7029918065091402\n",
      "three-layer Loss after iteration 74000: 0.6761418917112929\n",
      "three-layer Loss after iteration 75000: 0.667916979141969\n",
      "three-layer Loss after iteration 76000: 0.6634092117916566\n",
      "three-layer Loss after iteration 77000: 0.6573253651366893\n",
      "three-layer Loss after iteration 78000: 0.6521274869683134\n",
      "three-layer Loss after iteration 79000: 0.6451076418520664\n",
      "three-layer Loss after iteration 80000: 0.6408289660420216\n",
      "three-layer Loss after iteration 81000: 0.6345555972635617\n",
      "three-layer Loss after iteration 82000: 0.627533547196584\n",
      "three-layer Loss after iteration 83000: 0.5706030309415122\n",
      "three-layer Loss after iteration 84000: 0.5928746999773047\n",
      "three-layer Loss after iteration 85000: 0.6005668786599065\n",
      "three-layer Loss after iteration 86000: 0.6008039341462748\n",
      "three-layer Loss after iteration 87000: 0.5957603438258217\n",
      "three-layer Loss after iteration 88000: 0.5903230100680359\n",
      "three-layer Loss after iteration 89000: 0.5864297411484769\n",
      "three-layer Loss after iteration 90000: 0.5893205241652862\n",
      "three-layer Loss after iteration 91000: 0.580960129247877\n",
      "three-layer Loss after iteration 92000: 0.5753917567181199\n",
      "three-layer Loss after iteration 93000: 0.5704445074463477\n",
      "three-layer Loss after iteration 94000: 0.5666691531587685\n",
      "three-layer Loss after iteration 95000: 0.5725548082580352\n",
      "three-layer Loss after iteration 96000: 0.5532361837657307\n",
      "three-layer Loss after iteration 97000: 0.5566272014841164\n",
      "three-layer Loss after iteration 98000: 0.5556083218379764\n",
      "three-layer Loss after iteration 99000: 0.5531722891543709\n",
      "three-layer Loss after iteration 0: 1707.3474928649564\n",
      "three-layer Loss after iteration 1000: 12.625774381307599\n",
      "three-layer Loss after iteration 2000: 8.71090935716163\n",
      "three-layer Loss after iteration 3000: 12.031942400434456\n",
      "three-layer Loss after iteration 4000: 7.951827122917869\n",
      "three-layer Loss after iteration 5000: 5.522137262123666\n",
      "three-layer Loss after iteration 6000: 4.317708157280539\n",
      "three-layer Loss after iteration 7000: 2.992182063226254\n",
      "three-layer Loss after iteration 8000: 2.8752213807384943\n",
      "three-layer Loss after iteration 9000: 2.901452599830439\n",
      "three-layer Loss after iteration 10000: 3.306915404926938\n",
      "three-layer Loss after iteration 11000: 2.196426430578717\n",
      "three-layer Loss after iteration 12000: 1.388029720988161\n",
      "three-layer Loss after iteration 13000: 2.8023608719014166\n",
      "three-layer Loss after iteration 14000: 2.0189879451941577\n",
      "three-layer Loss after iteration 15000: 2.4621963250676346\n",
      "three-layer Loss after iteration 16000: 1.93237455610936\n",
      "three-layer Loss after iteration 17000: 1.587890083360304\n",
      "three-layer Loss after iteration 18000: 1.5683048724571036\n",
      "three-layer Loss after iteration 19000: 1.594249277523636\n",
      "three-layer Loss after iteration 20000: 1.5834108567253629\n",
      "three-layer Loss after iteration 21000: 1.5166841039903551\n",
      "three-layer Loss after iteration 22000: 1.4497357021422117\n",
      "three-layer Loss after iteration 23000: 1.4516815500259994\n",
      "three-layer Loss after iteration 24000: 1.3447766925812799\n",
      "three-layer Loss after iteration 25000: 1.3189577613715355\n",
      "three-layer Loss after iteration 26000: 1.2878438754286248\n",
      "three-layer Loss after iteration 27000: 1.2253834875489162\n",
      "three-layer Loss after iteration 28000: 1.1987511766413677\n",
      "three-layer Loss after iteration 29000: 1.1341609202627367\n",
      "three-layer Loss after iteration 30000: 1.1252675411388553\n",
      "three-layer Loss after iteration 31000: 1.0800594579941056\n",
      "three-layer Loss after iteration 32000: 1.0211602976761247\n",
      "three-layer Loss after iteration 33000: 0.9982595929330005\n",
      "three-layer Loss after iteration 34000: 0.9797819281500122\n",
      "three-layer Loss after iteration 35000: 0.9615278298560577\n",
      "three-layer Loss after iteration 36000: 0.9425205313372572\n",
      "three-layer Loss after iteration 37000: 0.9223386680676066\n",
      "three-layer Loss after iteration 38000: 0.9063066085017829\n",
      "three-layer Loss after iteration 39000: 0.8913139219776294\n",
      "three-layer Loss after iteration 40000: 0.8776339316170574\n",
      "three-layer Loss after iteration 41000: 0.8626585583996504\n",
      "three-layer Loss after iteration 42000: 0.8504828154394555\n",
      "three-layer Loss after iteration 43000: 0.8401403230135253\n",
      "three-layer Loss after iteration 44000: 0.8295705632969721\n",
      "three-layer Loss after iteration 45000: 0.8210085968325044\n",
      "three-layer Loss after iteration 46000: 0.8149951926381\n",
      "three-layer Loss after iteration 47000: 0.806978715167392\n",
      "three-layer Loss after iteration 48000: 0.8009830328524867\n",
      "three-layer Loss after iteration 49000: 0.7957507701182668\n",
      "three-layer Loss after iteration 50000: 0.7883972461916091\n",
      "three-layer Loss after iteration 51000: 0.7840735790206026\n",
      "three-layer Loss after iteration 52000: 0.7726617147242375\n",
      "three-layer Loss after iteration 53000: 0.7652274199997311\n",
      "three-layer Loss after iteration 54000: 0.7644770697008034\n",
      "three-layer Loss after iteration 55000: 0.7766935451534144\n",
      "three-layer Loss after iteration 56000: 0.7676749783470278\n",
      "three-layer Loss after iteration 57000: 0.7634719976203711\n",
      "three-layer Loss after iteration 58000: 0.7599243453434582\n",
      "three-layer Loss after iteration 59000: 0.7558142101754742\n",
      "three-layer Loss after iteration 60000: 0.7528461438557429\n",
      "three-layer Loss after iteration 61000: 0.7517634635914295\n",
      "three-layer Loss after iteration 62000: 0.7004648268960176\n",
      "three-layer Loss after iteration 63000: 0.8191841201059813\n",
      "three-layer Loss after iteration 64000: 0.8421890255529199\n",
      "three-layer Loss after iteration 65000: 0.781264791441395\n",
      "three-layer Loss after iteration 66000: 0.7684653008246033\n",
      "three-layer Loss after iteration 67000: 0.7628088469596737\n",
      "three-layer Loss after iteration 68000: 0.7560115962850245\n",
      "three-layer Loss after iteration 69000: 0.7498269637214314\n",
      "three-layer Loss after iteration 70000: 0.7442708350408003\n",
      "three-layer Loss after iteration 71000: 0.774555582930084\n",
      "three-layer Loss after iteration 72000: 0.76939485992313\n",
      "three-layer Loss after iteration 73000: 0.7577949246571449\n",
      "three-layer Loss after iteration 74000: 0.7502366263390335\n",
      "three-layer Loss after iteration 75000: 0.7461341954997357\n",
      "three-layer Loss after iteration 76000: 0.7413843482036826\n",
      "three-layer Loss after iteration 77000: 0.7376902407662498\n",
      "three-layer Loss after iteration 78000: 0.7293898493856833\n",
      "three-layer Loss after iteration 79000: 0.7302652362523968\n",
      "three-layer Loss after iteration 80000: 0.7277638636331059\n",
      "three-layer Loss after iteration 81000: 0.7343172337806735\n",
      "three-layer Loss after iteration 82000: 0.7348270158764509\n",
      "three-layer Loss after iteration 83000: 0.724330325626208\n",
      "three-layer Loss after iteration 84000: 0.7283696104791201\n",
      "three-layer Loss after iteration 85000: 0.7153147940690412\n",
      "three-layer Loss after iteration 86000: 0.7026631473221209\n",
      "three-layer Loss after iteration 87000: 0.705027059053417\n",
      "three-layer Loss after iteration 88000: 0.6467343746187973\n",
      "three-layer Loss after iteration 89000: 0.7147415454981721\n",
      "three-layer Loss after iteration 90000: 0.714942151940224\n",
      "three-layer Loss after iteration 91000: 0.7129672355619215\n",
      "three-layer Loss after iteration 92000: 0.7090932031213816\n",
      "three-layer Loss after iteration 93000: 0.6717006914892371\n",
      "three-layer Loss after iteration 94000: 0.6916922930905094\n",
      "three-layer Loss after iteration 95000: 0.6980305749513728\n",
      "three-layer Loss after iteration 96000: 0.6975820465654832\n",
      "three-layer Loss after iteration 97000: 0.6978230570613907\n",
      "three-layer Loss after iteration 98000: 0.6957893371665094\n",
      "three-layer Loss after iteration 99000: 0.6677893068030019\n",
      "three-layer Loss after iteration 0: 1657.4136967749914\n",
      "three-layer Loss after iteration 1000: 8.168724644872675\n",
      "three-layer Loss after iteration 2000: 9.623614634659178\n",
      "three-layer Loss after iteration 3000: 8.372394587762713\n",
      "three-layer Loss after iteration 4000: 5.222323793160709\n",
      "three-layer Loss after iteration 5000: 4.252774923198862\n",
      "three-layer Loss after iteration 6000: 4.851521411399228\n",
      "three-layer Loss after iteration 7000: 4.336652576257403\n",
      "three-layer Loss after iteration 8000: 3.481551368442788\n",
      "three-layer Loss after iteration 9000: 2.8977286647781937\n",
      "three-layer Loss after iteration 10000: 2.6929979767219083\n",
      "three-layer Loss after iteration 11000: 2.7249862445831767\n",
      "three-layer Loss after iteration 12000: 2.3093426766048597\n",
      "three-layer Loss after iteration 13000: 2.278688922080635\n",
      "three-layer Loss after iteration 14000: 2.222960349816715\n",
      "three-layer Loss after iteration 15000: 2.2106594287042505\n",
      "three-layer Loss after iteration 16000: 1.9520121608099206\n",
      "three-layer Loss after iteration 17000: 1.9280462279970365\n",
      "three-layer Loss after iteration 18000: 1.9759284911055022\n",
      "three-layer Loss after iteration 19000: 1.7979140426787938\n",
      "three-layer Loss after iteration 20000: 1.6396357890199962\n",
      "three-layer Loss after iteration 21000: 1.6431187861504066\n",
      "three-layer Loss after iteration 22000: 1.8138674886238721\n",
      "three-layer Loss after iteration 23000: 1.5187013636762634\n",
      "three-layer Loss after iteration 24000: 1.4966476976480483\n",
      "three-layer Loss after iteration 25000: 1.4407138173260126\n",
      "three-layer Loss after iteration 26000: 1.4581652356965853\n",
      "three-layer Loss after iteration 27000: 1.3247001951401072\n",
      "three-layer Loss after iteration 28000: 1.450279806858646\n",
      "three-layer Loss after iteration 29000: 1.425028580408782\n",
      "three-layer Loss after iteration 30000: 1.3720246702986236\n",
      "three-layer Loss after iteration 31000: 1.4106071681435697\n",
      "three-layer Loss after iteration 32000: 1.367491290526644\n",
      "three-layer Loss after iteration 33000: 1.2409401789517838\n",
      "three-layer Loss after iteration 34000: 1.3065768503592488\n",
      "three-layer Loss after iteration 35000: 1.2836994119784986\n",
      "three-layer Loss after iteration 36000: 1.128881549885707\n",
      "three-layer Loss after iteration 37000: 1.115445826176322\n",
      "three-layer Loss after iteration 38000: 1.609526987758337\n",
      "three-layer Loss after iteration 39000: 1.3244531782895768\n",
      "three-layer Loss after iteration 40000: 1.0318528809015988\n",
      "three-layer Loss after iteration 41000: 0.9633189175324967\n",
      "three-layer Loss after iteration 42000: 1.0509356505540848\n",
      "three-layer Loss after iteration 43000: 1.2417123197987439\n",
      "three-layer Loss after iteration 44000: 1.0859974337945026\n",
      "three-layer Loss after iteration 45000: 1.166165408933021\n",
      "three-layer Loss after iteration 46000: 1.1868261710643515\n",
      "three-layer Loss after iteration 47000: 1.2354811188778365\n",
      "three-layer Loss after iteration 48000: 1.213690915328301\n",
      "three-layer Loss after iteration 49000: 1.3745087007597832\n",
      "three-layer Loss after iteration 50000: 1.1878768809108058\n",
      "three-layer Loss after iteration 51000: 1.1733625602182696\n",
      "three-layer Loss after iteration 52000: 0.9622683212684077\n",
      "three-layer Loss after iteration 53000: 0.9447073711939217\n",
      "three-layer Loss after iteration 54000: 0.9479327818528545\n",
      "three-layer Loss after iteration 55000: 1.000618041536348\n",
      "three-layer Loss after iteration 56000: 0.8370748441518231\n",
      "three-layer Loss after iteration 57000: 0.927622541484961\n",
      "three-layer Loss after iteration 58000: 0.8855373223178885\n",
      "three-layer Loss after iteration 59000: 0.8690318603990297\n",
      "three-layer Loss after iteration 60000: 0.853790679574251\n",
      "three-layer Loss after iteration 61000: 0.8417094858579504\n",
      "three-layer Loss after iteration 62000: 0.8106884507509025\n",
      "three-layer Loss after iteration 63000: 0.8094113918521654\n",
      "three-layer Loss after iteration 64000: 0.7838376930845291\n",
      "three-layer Loss after iteration 65000: 0.8357545132291951\n",
      "three-layer Loss after iteration 66000: 0.8078539927573153\n",
      "three-layer Loss after iteration 67000: 0.7930995573561298\n",
      "three-layer Loss after iteration 68000: 0.7845682043568274\n",
      "three-layer Loss after iteration 69000: 0.7782054365276903\n",
      "three-layer Loss after iteration 70000: 0.7705522543341315\n",
      "three-layer Loss after iteration 71000: 0.7625969656543546\n",
      "three-layer Loss after iteration 72000: 0.7544332435728339\n",
      "three-layer Loss after iteration 73000: 0.7415243568347338\n",
      "three-layer Loss after iteration 74000: 0.7339065742226244\n",
      "three-layer Loss after iteration 75000: 0.7253122856136488\n",
      "three-layer Loss after iteration 76000: 0.7186532934634935\n",
      "three-layer Loss after iteration 77000: 0.7118266604225455\n",
      "three-layer Loss after iteration 78000: 0.7051335139683733\n",
      "three-layer Loss after iteration 79000: 0.716399634267823\n",
      "three-layer Loss after iteration 80000: 0.6982546370297\n",
      "three-layer Loss after iteration 81000: 0.6914601441429004\n",
      "three-layer Loss after iteration 82000: 0.630715270336219\n",
      "three-layer Loss after iteration 83000: 0.6286633803299416\n",
      "three-layer Loss after iteration 84000: 0.6272340451440513\n",
      "three-layer Loss after iteration 85000: 0.627454705800625\n",
      "three-layer Loss after iteration 86000: 0.6075237488561918\n",
      "three-layer Loss after iteration 87000: 0.751019295987902\n",
      "three-layer Loss after iteration 88000: 0.7360364193457631\n",
      "three-layer Loss after iteration 89000: 0.6876105602335433\n",
      "three-layer Loss after iteration 90000: 0.6816695665605171\n",
      "three-layer Loss after iteration 91000: 0.6716461981375349\n",
      "three-layer Loss after iteration 92000: 0.6572774558024644\n",
      "three-layer Loss after iteration 93000: 0.6478741441344834\n",
      "three-layer Loss after iteration 94000: 0.6408784734456074\n",
      "three-layer Loss after iteration 95000: 0.6209218022933737\n",
      "three-layer Loss after iteration 96000: 0.5953548904901822\n",
      "three-layer Loss after iteration 97000: 0.5802282846337304\n",
      "three-layer Loss after iteration 98000: 0.5678712787692962\n",
      "three-layer Loss after iteration 99000: 0.5799111910056228\n",
      "three-layer Loss after iteration 0: 1668.465236854181\n",
      "three-layer Loss after iteration 1000: 9.59684077639965\n",
      "three-layer Loss after iteration 2000: 6.902395792277327\n",
      "three-layer Loss after iteration 3000: 5.749120779362705\n",
      "three-layer Loss after iteration 4000: 5.127316510188721\n",
      "three-layer Loss after iteration 5000: 4.178147093929537\n",
      "three-layer Loss after iteration 6000: 3.5502474157846122\n",
      "three-layer Loss after iteration 7000: 3.3637785232600383\n",
      "three-layer Loss after iteration 8000: 1.6084382042899286\n",
      "three-layer Loss after iteration 9000: 2.148928518178871\n",
      "three-layer Loss after iteration 10000: 1.449438452039245\n",
      "three-layer Loss after iteration 11000: 1.8433641107789034\n",
      "three-layer Loss after iteration 12000: 1.296989299007499\n",
      "three-layer Loss after iteration 13000: 1.3304797250101672\n",
      "three-layer Loss after iteration 14000: 1.2795991498814157\n",
      "three-layer Loss after iteration 15000: 1.5483325068182916\n",
      "three-layer Loss after iteration 16000: 1.3253809672579138\n",
      "three-layer Loss after iteration 17000: 1.2020344145359332\n",
      "three-layer Loss after iteration 18000: 0.8750041712645847\n",
      "three-layer Loss after iteration 19000: 0.9977612830946536\n",
      "three-layer Loss after iteration 20000: 0.9164623208094937\n",
      "three-layer Loss after iteration 21000: 1.0196749708205124\n",
      "three-layer Loss after iteration 22000: 0.8416082663644033\n",
      "three-layer Loss after iteration 23000: 0.6631491588417885\n",
      "three-layer Loss after iteration 24000: 1.0507856852742077\n",
      "three-layer Loss after iteration 25000: 2.336988522339784\n",
      "three-layer Loss after iteration 26000: 0.5459158084157361\n",
      "three-layer Loss after iteration 27000: 0.5348829143546215\n",
      "three-layer Loss after iteration 28000: 0.5885419282816242\n",
      "three-layer Loss after iteration 29000: 0.8538256284773011\n",
      "three-layer Loss after iteration 30000: 0.7786136215567984\n",
      "three-layer Loss after iteration 31000: 0.7540122147717241\n",
      "three-layer Loss after iteration 32000: 0.7467326964584365\n",
      "three-layer Loss after iteration 33000: 0.7366527270669191\n",
      "three-layer Loss after iteration 34000: 0.7150503724783087\n",
      "three-layer Loss after iteration 35000: 0.7115449159182683\n",
      "three-layer Loss after iteration 36000: 0.716594863835715\n",
      "three-layer Loss after iteration 37000: 0.6881922239019936\n",
      "three-layer Loss after iteration 38000: 0.6961749638085195\n",
      "three-layer Loss after iteration 39000: 0.6980460807773103\n",
      "three-layer Loss after iteration 40000: 0.6998968594801985\n",
      "three-layer Loss after iteration 41000: 0.6891823226146029\n",
      "three-layer Loss after iteration 42000: 0.6794421425968191\n",
      "three-layer Loss after iteration 43000: 0.6714566922042984\n",
      "three-layer Loss after iteration 44000: 0.6641735009494338\n",
      "three-layer Loss after iteration 45000: 0.6577512526348017\n",
      "three-layer Loss after iteration 46000: 0.6504521199717186\n",
      "three-layer Loss after iteration 47000: 0.5889184482238428\n",
      "three-layer Loss after iteration 48000: 0.6137310218671176\n",
      "three-layer Loss after iteration 49000: 0.6137592210417256\n",
      "4.5947122767671225e-05 0.6137310218671176 0.6137592210417256\n",
      "three-layer Loss after iteration 0: 1332.4867118317459\n",
      "three-layer Loss after iteration 1000: 10.897483161309792\n",
      "three-layer Loss after iteration 2000: 13.47260234096129\n",
      "three-layer Loss after iteration 3000: 5.653154958818027\n",
      "three-layer Loss after iteration 4000: 4.053123988516646\n",
      "three-layer Loss after iteration 5000: 3.780129412141016\n",
      "three-layer Loss after iteration 6000: 5.030180320701725\n",
      "three-layer Loss after iteration 7000: 2.9890630157289566\n",
      "three-layer Loss after iteration 8000: 2.4336382370946983\n",
      "three-layer Loss after iteration 9000: 2.2495701007729836\n",
      "three-layer Loss after iteration 10000: 2.147835943721056\n",
      "three-layer Loss after iteration 11000: 1.9965816720493816\n",
      "three-layer Loss after iteration 12000: 1.9939259232837159\n",
      "three-layer Loss after iteration 13000: 2.0342458667939165\n",
      "three-layer Loss after iteration 14000: 1.5079977888178764\n",
      "three-layer Loss after iteration 15000: 2.830959341491758\n",
      "three-layer Loss after iteration 16000: 1.5811529406255835\n",
      "three-layer Loss after iteration 17000: 1.5271307150635862\n",
      "three-layer Loss after iteration 18000: 2.193352733205275\n",
      "three-layer Loss after iteration 19000: 1.9168430793446691\n",
      "three-layer Loss after iteration 20000: 1.920612377971111\n",
      "three-layer Loss after iteration 21000: 1.657926205583089\n",
      "three-layer Loss after iteration 22000: 1.3748680363567427\n",
      "three-layer Loss after iteration 23000: 1.5056647622909607\n",
      "three-layer Loss after iteration 24000: 1.510012225088092\n",
      "three-layer Loss after iteration 25000: 1.1721840166910151\n",
      "three-layer Loss after iteration 26000: 1.2282941397946148\n",
      "three-layer Loss after iteration 27000: 1.1053844291978752\n",
      "three-layer Loss after iteration 28000: 1.1941916221645832\n",
      "three-layer Loss after iteration 29000: 1.1181046923334341\n",
      "three-layer Loss after iteration 30000: 1.2107965231517002\n",
      "three-layer Loss after iteration 31000: 1.428705115937459\n",
      "three-layer Loss after iteration 32000: 1.7140423072024793\n",
      "three-layer Loss after iteration 33000: 1.269699081670537\n",
      "three-layer Loss after iteration 34000: 1.357399457011464\n",
      "three-layer Loss after iteration 35000: 1.3546635041006927\n",
      "three-layer Loss after iteration 36000: 1.078649669720646\n",
      "three-layer Loss after iteration 37000: 1.0654564192764424\n",
      "three-layer Loss after iteration 38000: 1.2106379494309791\n",
      "three-layer Loss after iteration 39000: 1.1923957624291839\n",
      "three-layer Loss after iteration 40000: 1.1467032044986394\n",
      "three-layer Loss after iteration 41000: 1.0843838286244372\n",
      "three-layer Loss after iteration 42000: 1.0854094446764366\n",
      "three-layer Loss after iteration 43000: 1.0444057018003858\n",
      "three-layer Loss after iteration 44000: 0.9503116400139322\n",
      "three-layer Loss after iteration 45000: 1.2249065956061556\n",
      "three-layer Loss after iteration 46000: 0.7913514520937227\n",
      "three-layer Loss after iteration 47000: 0.782150211527601\n",
      "three-layer Loss after iteration 48000: 1.1268793764689802\n",
      "three-layer Loss after iteration 49000: 0.7519403535764466\n",
      "three-layer Loss after iteration 50000: 0.8338607003660023\n",
      "three-layer Loss after iteration 51000: 0.868977454123217\n",
      "three-layer Loss after iteration 52000: 0.8624666193591416\n",
      "three-layer Loss after iteration 53000: 0.8381415181347704\n",
      "three-layer Loss after iteration 54000: 0.8182117784170773\n",
      "three-layer Loss after iteration 55000: 0.8033255427921581\n",
      "three-layer Loss after iteration 56000: 0.7821703600857125\n",
      "three-layer Loss after iteration 57000: 0.7684390863557539\n",
      "three-layer Loss after iteration 58000: 0.7875990725844922\n",
      "three-layer Loss after iteration 59000: 0.7675345617877721\n",
      "three-layer Loss after iteration 60000: 0.7493319069320081\n",
      "three-layer Loss after iteration 61000: 0.7164239012899241\n",
      "three-layer Loss after iteration 62000: 0.7260383112866294\n",
      "three-layer Loss after iteration 63000: 0.719516196033124\n",
      "three-layer Loss after iteration 64000: 0.7260856885334968\n",
      "three-layer Loss after iteration 65000: 0.7245568791181957\n",
      "three-layer Loss after iteration 66000: 0.7212357375481842\n",
      "three-layer Loss after iteration 67000: 0.7168949504358069\n",
      "three-layer Loss after iteration 68000: 0.7120523025387137\n",
      "three-layer Loss after iteration 69000: 0.706767387424146\n",
      "three-layer Loss after iteration 70000: 0.7018606398467457\n",
      "three-layer Loss after iteration 71000: 0.6973616115187006\n",
      "three-layer Loss after iteration 72000: 0.6952283806152056\n",
      "three-layer Loss after iteration 73000: 0.6633786260502724\n",
      "three-layer Loss after iteration 74000: 0.680887009151208\n",
      "three-layer Loss after iteration 75000: 0.6793913011518116\n",
      "three-layer Loss after iteration 76000: 0.6756627065790638\n",
      "three-layer Loss after iteration 77000: 0.4527156066130685\n",
      "three-layer Loss after iteration 78000: 0.39915130956971323\n",
      "three-layer Loss after iteration 79000: 0.3860604206402171\n",
      "three-layer Loss after iteration 80000: 0.3790952763767352\n",
      "three-layer Loss after iteration 81000: 0.37962980085040343\n",
      "three-layer Loss after iteration 82000: 0.4042877929516801\n",
      "three-layer Loss after iteration 83000: 0.4696747423197226\n",
      "three-layer Loss after iteration 84000: 0.45264381999715053\n",
      "three-layer Loss after iteration 85000: 0.45414807476456415\n",
      "three-layer Loss after iteration 86000: 0.39227862143746406\n",
      "three-layer Loss after iteration 87000: 0.5473782348815398\n",
      "three-layer Loss after iteration 88000: 0.4561658388107883\n",
      "three-layer Loss after iteration 89000: 0.40202429196172057\n",
      "three-layer Loss after iteration 90000: 0.4617962347379372\n",
      "three-layer Loss after iteration 91000: 0.44589296087726304\n",
      "three-layer Loss after iteration 92000: 0.5176014585872951\n",
      "three-layer Loss after iteration 93000: 0.39341624844950995\n",
      "three-layer Loss after iteration 94000: 0.4474390070330268\n",
      "three-layer Loss after iteration 95000: 0.4500956205037692\n",
      "three-layer Loss after iteration 96000: 0.4399512878683466\n",
      "three-layer Loss after iteration 97000: 0.4451301628989992\n",
      "three-layer Loss after iteration 98000: 0.4456878381770943\n",
      "three-layer Loss after iteration 99000: 0.43913622258011586\n",
      "three-layer Loss after iteration 0: 1703.9342999297505\n",
      "three-layer Loss after iteration 1000: 8.276314307497403\n",
      "three-layer Loss after iteration 2000: 6.327895271630345\n",
      "three-layer Loss after iteration 3000: 2.772051308024446\n",
      "three-layer Loss after iteration 4000: 2.374542764805314\n",
      "three-layer Loss after iteration 5000: 1.7605466281643571\n",
      "three-layer Loss after iteration 6000: 1.3131193424866388\n",
      "three-layer Loss after iteration 7000: 1.5126282896057854\n",
      "three-layer Loss after iteration 8000: 1.5568282145165844\n",
      "three-layer Loss after iteration 9000: 1.568397496578232\n",
      "three-layer Loss after iteration 10000: 1.4260209624016973\n",
      "three-layer Loss after iteration 11000: 1.2196090208745667\n",
      "three-layer Loss after iteration 12000: 1.109271478250845\n",
      "three-layer Loss after iteration 13000: 0.8764828363603182\n",
      "three-layer Loss after iteration 14000: 1.1077808750664935\n",
      "three-layer Loss after iteration 15000: 0.9438825499057459\n",
      "three-layer Loss after iteration 16000: 0.6961619377715723\n",
      "three-layer Loss after iteration 17000: 1.104880020250942\n",
      "three-layer Loss after iteration 18000: 0.8354910550652865\n",
      "three-layer Loss after iteration 19000: 0.6823346836227235\n",
      "three-layer Loss after iteration 20000: 0.7854352725954176\n",
      "three-layer Loss after iteration 21000: 0.8843613921481405\n",
      "three-layer Loss after iteration 22000: 0.7095879730041671\n",
      "three-layer Loss after iteration 23000: 0.6705535999708088\n",
      "three-layer Loss after iteration 24000: 0.7396424153246832\n",
      "three-layer Loss after iteration 25000: 0.7498135175774711\n",
      "three-layer Loss after iteration 26000: 0.72883920951076\n",
      "three-layer Loss after iteration 27000: 0.7276860133836146\n",
      "three-layer Loss after iteration 28000: 0.6017612663263286\n",
      "three-layer Loss after iteration 29000: 0.633136875144341\n",
      "three-layer Loss after iteration 30000: 0.5128149453069414\n",
      "three-layer Loss after iteration 31000: 0.679912444955029\n",
      "three-layer Loss after iteration 32000: 0.5583196571439317\n",
      "three-layer Loss after iteration 33000: 0.6691604513309164\n",
      "three-layer Loss after iteration 34000: 0.5907859423484983\n",
      "three-layer Loss after iteration 35000: 0.6635528890524026\n",
      "three-layer Loss after iteration 36000: 0.6422344408016706\n",
      "three-layer Loss after iteration 37000: 0.5697676458883119\n",
      "three-layer Loss after iteration 38000: 0.5616698994814348\n",
      "three-layer Loss after iteration 39000: 0.5318102187217931\n",
      "three-layer Loss after iteration 40000: 0.5325315350564002\n",
      "three-layer Loss after iteration 41000: 0.42525580084311887\n",
      "three-layer Loss after iteration 42000: 0.4589089496152277\n",
      "three-layer Loss after iteration 43000: 0.4392561911008037\n",
      "three-layer Loss after iteration 44000: 0.47617379505512997\n",
      "three-layer Loss after iteration 45000: 0.4913040649448533\n",
      "three-layer Loss after iteration 46000: 0.466795185781179\n",
      "three-layer Loss after iteration 47000: 0.46014270508156047\n",
      "three-layer Loss after iteration 48000: 0.40459107873661765\n",
      "three-layer Loss after iteration 49000: 0.4605899279316889\n",
      "three-layer Loss after iteration 50000: 0.4389751641062119\n",
      "three-layer Loss after iteration 51000: 0.3955485204634805\n",
      "three-layer Loss after iteration 52000: 0.39911691965078555\n",
      "three-layer Loss after iteration 53000: 0.4119555582723802\n",
      "three-layer Loss after iteration 54000: 0.5787050316757665\n",
      "three-layer Loss after iteration 55000: 0.48744763168797756\n",
      "three-layer Loss after iteration 56000: 0.38980772544084946\n",
      "three-layer Loss after iteration 57000: 0.4209488159208931\n",
      "three-layer Loss after iteration 58000: 0.4231621747019605\n",
      "three-layer Loss after iteration 59000: 0.42297003356386453\n",
      "three-layer Loss after iteration 60000: 0.4210863313805879\n",
      "three-layer Loss after iteration 61000: 0.4239368127301077\n",
      "three-layer Loss after iteration 62000: 0.5602516437073061\n",
      "three-layer Loss after iteration 63000: 0.42654777886427586\n",
      "three-layer Loss after iteration 64000: 0.42687347041002605\n",
      "three-layer Loss after iteration 65000: 0.42060098843612825\n",
      "three-layer Loss after iteration 66000: 0.4283639352164042\n",
      "three-layer Loss after iteration 67000: 0.4028844199028157\n",
      "three-layer Loss after iteration 68000: 0.42248357816655874\n",
      "three-layer Loss after iteration 69000: 0.40225352903957373\n",
      "three-layer Loss after iteration 70000: 0.37785814949296626\n",
      "three-layer Loss after iteration 71000: 0.3747059985959176\n",
      "three-layer Loss after iteration 72000: 0.3800758808698198\n",
      "three-layer Loss after iteration 73000: 0.40374542033524413\n",
      "three-layer Loss after iteration 74000: 0.3609750672286715\n",
      "three-layer Loss after iteration 75000: 0.37187988896669294\n",
      "three-layer Loss after iteration 76000: 0.4073119466505252\n",
      "three-layer Loss after iteration 77000: 0.37520811898758794\n",
      "three-layer Loss after iteration 78000: 0.3876390007249093\n",
      "three-layer Loss after iteration 79000: 0.36644556372986564\n",
      "three-layer Loss after iteration 80000: 0.3678989550770905\n",
      "three-layer Loss after iteration 81000: 0.33731761187029435\n",
      "three-layer Loss after iteration 82000: 0.3354654153586356\n",
      "three-layer Loss after iteration 83000: 0.3345663866210714\n",
      "three-layer Loss after iteration 84000: 0.3549889537510656\n",
      "three-layer Loss after iteration 85000: 0.3333560250369836\n",
      "three-layer Loss after iteration 86000: 0.33270823156355595\n",
      "three-layer Loss after iteration 87000: 0.3313334763903706\n",
      "three-layer Loss after iteration 88000: 0.33089064685632225\n",
      "three-layer Loss after iteration 89000: 0.33035851879233774\n",
      "three-layer Loss after iteration 90000: 0.3303653355847774\n",
      "2.0634529009737826e-05 0.33035851879233774 0.3303653355847774\n",
      "three-layer Loss after iteration 0: 1562.0725633713766\n",
      "three-layer Loss after iteration 1000: 11.969995715901225\n",
      "three-layer Loss after iteration 2000: 8.868387552244108\n",
      "three-layer Loss after iteration 3000: 6.037716414535475\n",
      "three-layer Loss after iteration 4000: 7.088632612800763\n",
      "three-layer Loss after iteration 5000: 4.501784992174934\n",
      "three-layer Loss after iteration 6000: 3.201110212428714\n",
      "three-layer Loss after iteration 7000: 2.948614176345452\n",
      "three-layer Loss after iteration 8000: 2.813150747351155\n",
      "three-layer Loss after iteration 9000: 2.0497526518106413\n",
      "three-layer Loss after iteration 10000: 2.179839855371171\n",
      "three-layer Loss after iteration 11000: 2.8397719836796664\n",
      "three-layer Loss after iteration 12000: 1.900600843124729\n",
      "three-layer Loss after iteration 13000: 1.8815980323157162\n",
      "three-layer Loss after iteration 14000: 1.6613226945006443\n",
      "three-layer Loss after iteration 15000: 1.5687057695331483\n",
      "three-layer Loss after iteration 16000: 1.5712566021944394\n",
      "three-layer Loss after iteration 17000: 1.6396142795198416\n",
      "three-layer Loss after iteration 18000: 1.7654117876668043\n",
      "three-layer Loss after iteration 19000: 1.5522489753094462\n",
      "three-layer Loss after iteration 20000: 1.7596273117384282\n",
      "three-layer Loss after iteration 21000: 1.4465052085622163\n",
      "three-layer Loss after iteration 22000: 1.6039141153271956\n",
      "three-layer Loss after iteration 23000: 1.4199058655378076\n",
      "three-layer Loss after iteration 24000: 1.2948447997969268\n",
      "three-layer Loss after iteration 25000: 1.2797607672574156\n",
      "three-layer Loss after iteration 26000: 1.3621472735729887\n",
      "three-layer Loss after iteration 27000: 1.1769625706158118\n",
      "three-layer Loss after iteration 28000: 1.191456166099811\n",
      "three-layer Loss after iteration 29000: 1.1690477806567725\n",
      "three-layer Loss after iteration 30000: 1.1707944400777746\n",
      "three-layer Loss after iteration 31000: 1.061190199627461\n",
      "three-layer Loss after iteration 32000: 1.0274785192776494\n",
      "three-layer Loss after iteration 33000: 1.1169394514787974\n",
      "three-layer Loss after iteration 34000: 1.1023157554066705\n",
      "three-layer Loss after iteration 35000: 1.044362450522876\n",
      "three-layer Loss after iteration 36000: 1.0233450399437607\n",
      "three-layer Loss after iteration 37000: 0.9507203645470725\n",
      "three-layer Loss after iteration 38000: 0.9229060077356155\n",
      "three-layer Loss after iteration 39000: 1.3097038543538977\n",
      "three-layer Loss after iteration 40000: 1.0009012443397554\n",
      "three-layer Loss after iteration 41000: 0.966148508938555\n",
      "three-layer Loss after iteration 42000: 0.9759565879131675\n",
      "three-layer Loss after iteration 43000: 0.9914426759477142\n",
      "three-layer Loss after iteration 44000: 0.9710305610751091\n",
      "three-layer Loss after iteration 45000: 0.9604139343288365\n",
      "three-layer Loss after iteration 46000: 0.9501318877018691\n",
      "three-layer Loss after iteration 47000: 0.9490438981007858\n",
      "three-layer Loss after iteration 48000: 0.9245145110733126\n",
      "three-layer Loss after iteration 49000: 0.9471728696635788\n",
      "three-layer Loss after iteration 50000: 0.9068700323703216\n",
      "three-layer Loss after iteration 51000: 0.9368118919284284\n",
      "three-layer Loss after iteration 52000: 0.9106113609874766\n",
      "three-layer Loss after iteration 53000: 0.9143037544435095\n",
      "three-layer Loss after iteration 54000: 0.9140181098258203\n",
      "three-layer Loss after iteration 55000: 0.9142697829894559\n",
      "three-layer Loss after iteration 56000: 0.9093857053440143\n",
      "three-layer Loss after iteration 57000: 0.9051249429843193\n",
      "three-layer Loss after iteration 58000: 0.8896947750864782\n",
      "three-layer Loss after iteration 59000: 0.89524844737001\n",
      "three-layer Loss after iteration 60000: 0.905289306381015\n",
      "three-layer Loss after iteration 61000: 0.905318859554252\n",
      "3.264500423083762e-05 0.905289306381015 0.905318859554252\n",
      "three-layer Loss after iteration 0: 1655.4294181536118\n",
      "three-layer Loss after iteration 1000: 13.91513449281605\n",
      "three-layer Loss after iteration 2000: 10.62556567477866\n",
      "three-layer Loss after iteration 3000: 6.69600210055174\n",
      "three-layer Loss after iteration 4000: 3.172560175148133\n",
      "three-layer Loss after iteration 5000: 4.026746802955941\n",
      "three-layer Loss after iteration 6000: 2.3653581120243263\n",
      "three-layer Loss after iteration 7000: 3.20264985709862\n",
      "three-layer Loss after iteration 8000: 2.7075704121398783\n",
      "three-layer Loss after iteration 9000: 2.3163347588543264\n",
      "three-layer Loss after iteration 10000: 3.245382793595167\n",
      "three-layer Loss after iteration 11000: 1.5135555715815527\n",
      "three-layer Loss after iteration 12000: 1.6699261425258374\n",
      "three-layer Loss after iteration 13000: 1.6145405046291323\n",
      "three-layer Loss after iteration 14000: 1.6070078990540797\n",
      "three-layer Loss after iteration 15000: 1.238453489647487\n",
      "three-layer Loss after iteration 16000: 1.2828977261797887\n",
      "three-layer Loss after iteration 17000: 1.1712900324279483\n",
      "three-layer Loss after iteration 18000: 1.1696340543422836\n",
      "three-layer Loss after iteration 19000: 1.3002461797938092\n",
      "three-layer Loss after iteration 20000: 1.1492070518967492\n",
      "three-layer Loss after iteration 21000: 1.0802176427741208\n",
      "three-layer Loss after iteration 22000: 1.2255658133288787\n",
      "three-layer Loss after iteration 23000: 0.9002539398790218\n",
      "three-layer Loss after iteration 24000: 0.8813655395241224\n",
      "three-layer Loss after iteration 25000: 0.8523927869483621\n",
      "three-layer Loss after iteration 26000: 0.7109690886543752\n",
      "three-layer Loss after iteration 27000: 0.7994596048190609\n",
      "three-layer Loss after iteration 28000: 0.8362827834448578\n",
      "three-layer Loss after iteration 29000: 0.6823413284670518\n",
      "three-layer Loss after iteration 30000: 0.6824835695385898\n",
      "three-layer Loss after iteration 31000: 0.9473789905747309\n",
      "three-layer Loss after iteration 32000: 0.7023142278705402\n",
      "three-layer Loss after iteration 33000: 0.6709075673656152\n",
      "three-layer Loss after iteration 34000: 0.8118806608291487\n",
      "three-layer Loss after iteration 35000: 1.252922104814449\n",
      "three-layer Loss after iteration 36000: 0.7532503039661451\n",
      "three-layer Loss after iteration 37000: 0.735304309664847\n",
      "three-layer Loss after iteration 38000: 1.032019177304846\n",
      "three-layer Loss after iteration 39000: 0.706065106505484\n",
      "three-layer Loss after iteration 40000: 0.7307622078754884\n",
      "three-layer Loss after iteration 41000: 1.184697727054701\n",
      "three-layer Loss after iteration 42000: 1.0649723487440823\n",
      "three-layer Loss after iteration 43000: 1.326080412269725\n",
      "three-layer Loss after iteration 44000: 0.7732489976774882\n",
      "three-layer Loss after iteration 45000: 0.7170689045243351\n",
      "three-layer Loss after iteration 46000: 0.8673079479447352\n",
      "three-layer Loss after iteration 47000: 1.0351399548033633\n",
      "three-layer Loss after iteration 48000: 0.7229741633713633\n",
      "three-layer Loss after iteration 49000: 0.6868889790100702\n",
      "three-layer Loss after iteration 50000: 0.6414372257380454\n",
      "three-layer Loss after iteration 51000: 0.8254898711262305\n",
      "three-layer Loss after iteration 52000: 0.5298083938882497\n",
      "three-layer Loss after iteration 53000: 0.8935399823237734\n",
      "three-layer Loss after iteration 54000: 0.5196857417676058\n",
      "three-layer Loss after iteration 55000: 0.4836182330579156\n",
      "three-layer Loss after iteration 56000: 0.7761098650791993\n",
      "three-layer Loss after iteration 57000: 0.8154420660049022\n",
      "three-layer Loss after iteration 58000: 0.6952876527407446\n",
      "three-layer Loss after iteration 59000: 0.6434454322636202\n",
      "three-layer Loss after iteration 60000: 0.8488659620979545\n",
      "three-layer Loss after iteration 61000: 0.6150250806365365\n",
      "three-layer Loss after iteration 62000: 0.46606747716413705\n",
      "three-layer Loss after iteration 63000: 0.6781853808302978\n",
      "three-layer Loss after iteration 64000: 0.6253815317102389\n",
      "three-layer Loss after iteration 65000: 0.7001799536814358\n",
      "three-layer Loss after iteration 66000: 0.49014003766129854\n",
      "three-layer Loss after iteration 67000: 0.6464001382301796\n",
      "three-layer Loss after iteration 68000: 0.45115185599803537\n",
      "three-layer Loss after iteration 69000: 0.9402368861744089\n",
      "three-layer Loss after iteration 70000: 0.43496964404176836\n",
      "three-layer Loss after iteration 71000: 0.5451378348467727\n",
      "three-layer Loss after iteration 72000: 0.4635181960414123\n",
      "three-layer Loss after iteration 73000: 0.42574708078027806\n",
      "three-layer Loss after iteration 74000: 0.6836160525280457\n",
      "three-layer Loss after iteration 75000: 0.42093247839532777\n",
      "three-layer Loss after iteration 76000: 0.5789147141174252\n",
      "three-layer Loss after iteration 77000: 0.4698142187194458\n",
      "three-layer Loss after iteration 78000: 0.47297489859658737\n",
      "three-layer Loss after iteration 79000: 0.704571524369478\n",
      "three-layer Loss after iteration 80000: 0.5494913328297701\n",
      "three-layer Loss after iteration 81000: 0.5491076442320295\n",
      "three-layer Loss after iteration 82000: 0.6790530310249756\n",
      "three-layer Loss after iteration 83000: 0.4086316283977424\n",
      "three-layer Loss after iteration 84000: 0.41395140363441757\n",
      "three-layer Loss after iteration 85000: 0.9094030570000152\n",
      "three-layer Loss after iteration 86000: 0.46895247475062085\n",
      "three-layer Loss after iteration 87000: 0.41205077567692433\n",
      "three-layer Loss after iteration 88000: 0.9796768608548091\n",
      "three-layer Loss after iteration 89000: 0.43752946887621674\n",
      "three-layer Loss after iteration 90000: 0.4045132951853489\n",
      "three-layer Loss after iteration 91000: 0.454055996027924\n",
      "three-layer Loss after iteration 92000: 0.8778822305089312\n",
      "three-layer Loss after iteration 93000: 0.42864275513510103\n",
      "three-layer Loss after iteration 94000: 0.4036241561092956\n",
      "three-layer Loss after iteration 95000: 0.6611335916932624\n",
      "three-layer Loss after iteration 96000: 0.6791670430873369\n",
      "three-layer Loss after iteration 97000: 0.776793437343894\n",
      "three-layer Loss after iteration 98000: 0.8903871046292585\n",
      "three-layer Loss after iteration 99000: 1.0546874188902047\n",
      "three-layer Loss after iteration 0: 1706.0679333929547\n",
      "three-layer Loss after iteration 1000: 8.783082859395272\n",
      "three-layer Loss after iteration 2000: 6.527449475218972\n",
      "three-layer Loss after iteration 3000: 6.229090949046129\n",
      "three-layer Loss after iteration 4000: 4.430402555263031\n",
      "three-layer Loss after iteration 5000: 5.03898172853771\n",
      "three-layer Loss after iteration 6000: 2.619338878184676\n",
      "three-layer Loss after iteration 7000: 1.5872940950135965\n",
      "three-layer Loss after iteration 8000: 1.591085421158904\n",
      "three-layer Loss after iteration 9000: 1.7673106021307012\n",
      "three-layer Loss after iteration 10000: 1.8577076763495735\n",
      "three-layer Loss after iteration 11000: 1.4187533602412203\n",
      "three-layer Loss after iteration 12000: 1.5947975992663388\n",
      "three-layer Loss after iteration 13000: 1.4507375597723573\n",
      "three-layer Loss after iteration 14000: 1.4207214732471307\n",
      "three-layer Loss after iteration 15000: 1.044598949584882\n",
      "three-layer Loss after iteration 16000: 1.0467113909017365\n",
      "three-layer Loss after iteration 17000: 0.9146336732656507\n",
      "three-layer Loss after iteration 18000: 0.8867135096355108\n",
      "three-layer Loss after iteration 19000: 0.8858372202452507\n",
      "three-layer Loss after iteration 20000: 0.8279791297253506\n",
      "three-layer Loss after iteration 21000: 0.7779982621546975\n",
      "three-layer Loss after iteration 22000: 0.8980403699636436\n",
      "three-layer Loss after iteration 23000: 0.8909073588395927\n",
      "three-layer Loss after iteration 24000: 0.7521656168759652\n",
      "three-layer Loss after iteration 25000: 0.7916770151268525\n",
      "three-layer Loss after iteration 26000: 0.7374241342591501\n",
      "three-layer Loss after iteration 27000: 0.7094618310108546\n",
      "three-layer Loss after iteration 28000: 0.7560745650658403\n",
      "three-layer Loss after iteration 29000: 0.7127054545561335\n",
      "three-layer Loss after iteration 30000: 0.7181434734935217\n",
      "three-layer Loss after iteration 31000: 0.7610991849293666\n",
      "three-layer Loss after iteration 32000: 0.7476515736377969\n",
      "three-layer Loss after iteration 33000: 0.8200774846812046\n",
      "three-layer Loss after iteration 34000: 0.7159920124199197\n",
      "three-layer Loss after iteration 35000: 0.6362267551463628\n",
      "three-layer Loss after iteration 36000: 0.776739333781156\n",
      "three-layer Loss after iteration 37000: 0.6744527241207899\n",
      "three-layer Loss after iteration 38000: 0.6270114634101486\n",
      "three-layer Loss after iteration 39000: 0.6183223363075228\n",
      "three-layer Loss after iteration 40000: 0.6201432438658001\n",
      "three-layer Loss after iteration 41000: 0.6474950490208756\n",
      "three-layer Loss after iteration 42000: 0.5950020336840516\n",
      "three-layer Loss after iteration 43000: 0.7260438024254139\n",
      "three-layer Loss after iteration 44000: 0.5814445755904891\n",
      "three-layer Loss after iteration 45000: 0.85235688847222\n",
      "three-layer Loss after iteration 46000: 0.573607043571839\n",
      "three-layer Loss after iteration 47000: 0.7847214275311274\n",
      "three-layer Loss after iteration 48000: 0.5728002135771192\n",
      "three-layer Loss after iteration 49000: 0.8698904363165911\n",
      "three-layer Loss after iteration 50000: 0.6092327567013132\n",
      "three-layer Loss after iteration 51000: 0.5603227172291161\n",
      "three-layer Loss after iteration 52000: 0.5601106577919268\n",
      "three-layer Loss after iteration 53000: 0.5693776777522139\n",
      "three-layer Loss after iteration 54000: 0.8558274867066346\n",
      "three-layer Loss after iteration 55000: 0.5463753107830143\n",
      "three-layer Loss after iteration 56000: 0.8048363862150321\n",
      "three-layer Loss after iteration 57000: 0.5367023545705955\n",
      "three-layer Loss after iteration 58000: 0.6193589201328354\n",
      "three-layer Loss after iteration 59000: 0.6399322350733225\n",
      "three-layer Loss after iteration 60000: 0.5698353789311781\n",
      "three-layer Loss after iteration 61000: 0.6142979551068839\n",
      "three-layer Loss after iteration 62000: 0.6030164732025802\n",
      "three-layer Loss after iteration 63000: 0.5798458368545951\n",
      "three-layer Loss after iteration 64000: 0.5522246813663544\n",
      "three-layer Loss after iteration 65000: 0.5475038659770636\n",
      "three-layer Loss after iteration 66000: 0.6087895193709169\n",
      "three-layer Loss after iteration 67000: 0.5988218404908648\n",
      "three-layer Loss after iteration 68000: 0.5297019126103018\n",
      "three-layer Loss after iteration 69000: 0.5837867974262082\n",
      "three-layer Loss after iteration 70000: 0.5584197673443948\n",
      "three-layer Loss after iteration 71000: 0.5676970136016773\n",
      "three-layer Loss after iteration 72000: 0.5547940036490445\n",
      "three-layer Loss after iteration 73000: 0.6218950414187533\n",
      "three-layer Loss after iteration 74000: 0.5431673660031056\n",
      "three-layer Loss after iteration 75000: 0.5940681926223331\n",
      "three-layer Loss after iteration 76000: 0.5421459288883003\n",
      "three-layer Loss after iteration 77000: 0.7079473320860286\n",
      "three-layer Loss after iteration 78000: 0.5371515548127628\n",
      "three-layer Loss after iteration 79000: 0.4970588237069909\n",
      "three-layer Loss after iteration 80000: 0.8720398137709827\n",
      "three-layer Loss after iteration 81000: 0.7898632956893471\n",
      "three-layer Loss after iteration 82000: 0.643786923898478\n",
      "three-layer Loss after iteration 83000: 0.5845529447362368\n",
      "three-layer Loss after iteration 84000: 0.5786958692664742\n",
      "three-layer Loss after iteration 85000: 0.5730744797214993\n",
      "three-layer Loss after iteration 86000: 0.5716113897309436\n",
      "three-layer Loss after iteration 87000: 0.5699720609433601\n",
      "three-layer Loss after iteration 88000: 0.5686445940101151\n",
      "three-layer Loss after iteration 89000: 0.4927776695335084\n",
      "three-layer Loss after iteration 90000: 0.5643205488349317\n",
      "three-layer Loss after iteration 91000: 0.4350936398976229\n",
      "three-layer Loss after iteration 92000: 0.5311307704525335\n",
      "three-layer Loss after iteration 93000: 0.4417995467271506\n",
      "three-layer Loss after iteration 94000: 0.4261007326874762\n",
      "three-layer Loss after iteration 95000: 0.7484195370602332\n",
      "three-layer Loss after iteration 96000: 0.4292486116459389\n",
      "three-layer Loss after iteration 97000: 0.59737315485709\n",
      "three-layer Loss after iteration 98000: 0.5931864269841804\n",
      "three-layer Loss after iteration 99000: 0.6747251440272726\n",
      "{'8': {'losses': [3.6174003731743847, 4.0302617318691345, 2.3542526157535666, 2.527842325041258, 2.2425777626677728, 3.2213845196611124, 3.4575327594329943, 3.3096772983501146, 3.725686206897967, 2.4431653297436964, 2.3393728189569836, 3.184104310011226, 3.4690188981178847, 1.8694220917511037, 3.1217868139570055, 2.38247654431638, 3.5096327922166832, 2.470640460743997, 3.033816853544125, 3.6605498237904737], 'iterations': [26001, 52001, 82001, 45001, 57001, 42001, 57001, 48001, 80001, 89001, 100000, 100000, 83001, 59001, 100000, 36001, 74001, 100000, 42001, 38001]}, '16': {'losses': [0.41130095817391416, 0.38291627664080036, 0.44065475858222536, 0.3558453688266477, 0.5781605137284591, 0.6219842677585644, 0.6964661643887197, 0.19106949562662356, 0.2508523416262352, 0.23581087772034895, 0.42334327257315063, 0.5895815341028131, 0.25044299122072866, 0.7789920998127091, 0.3254134617324659, 1.3108029681837894, 0.2311094775286161, 1.1821093820552602, 0.6458799438523286, 0.3157959641200527], 'iterations': [100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000]}} {'4+4': {'losses': [4.909367626583326, 3.6378994430040743, 7.275345492905362, 3.3269489985532434, 4.653638691089553, 2.6738579400352975, 3.5012118317153695, 4.038384484205147, 3.0776456592501362, 6.663582240271976, 5.738767284963293, 9.781045436898502, 4.639918441400931, 2.886974818811544, 8.273450943365578, 4.792177222813165, 4.870994692908066, 6.479987362997522, 6.419332096769126, 3.063185029474159], 'iterations': [56001, 79001, 74001, 63001, 76001, 100000, 100000, 60001, 26001, 27001, 100000, 100000, 100000, 100000, 100000, 91001, 100000, 54001, 63001, 32001]}, '8+8': {'losses': [1.1150995426789545, 0.33987655365228253, 0.5161141752104437, 0.645472388118697, 0.6315748532334666, 0.7235156483907781, 0.6003349624621227, 0.6425382793048617, 0.7046102902216136, 0.7451816315539627, 0.20038146234976553, 0.5531722891543709, 0.6677893068030019, 0.5799111910056228, 0.6137592210417256, 0.43913622258011586, 0.3303653355847774, 0.905318859554252, 1.0546874188902047, 0.6747251440272726], 'iterations': [100000, 100000, 100000, 100000, 100000, 46001, 100000, 100000, 100000, 81001, 99001, 100000, 100000, 100000, 49001, 100000, 90001, 61001, 100000, 100000]}}\n",
      "--- 947.2199969291687 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "numDataOne = 15\n",
    "numData = numDataOne*numDataOne\n",
    "# create data for regression\n",
    "xs=np.linspace(-8,8,numDataOne)\n",
    "ys=np.linspace(-8,8,numDataOne)\n",
    "counter=0\n",
    "\n",
    "X = np.empty((0, 2))\n",
    "y = np.empty((0, 1))\n",
    "\n",
    "for r in np.arange(0,numDataOne):\n",
    "    for c in np.arange(0,numDataOne):\n",
    "        X = np.vstack((X, [xs[r], ys[c]]))\n",
    "        y = np.vstack((y, xs[r]**2 + ys[c]**2 + 1))\n",
    "\n",
    "# training set size\n",
    "num_examples = len(X)\n",
    "# input layer dimensionality\n",
    "nn_input_dim = 2\n",
    "# output layer dimensionality\n",
    "nn_output_dim = 1\n",
    "# learning rate for gradient descent\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Parameters\n",
    "num_runs = 20\n",
    "tolerance = 0.0001\n",
    "max_iterations = 100000\n",
    "\n",
    "# Data structures to store results\n",
    "results_old = {\"8\": {\"losses\": [], \"iterations\": []}}\n",
    "results_new = {\"4+4\": {\"losses\": [], \"iterations\": []}}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# Test the old (2-layer) network with 8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 8, 1)\n",
    "    model_old, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"8\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"8\"][\"iterations\"].append(iterations)\n",
    "\n",
    "# Test the new (3-layer) network with 4+4 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [4,4], 1)\n",
    "    model_new, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"4+4\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"4+4\"][\"iterations\"].append(iterations)\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"16\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"8+8\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 16, 1)\n",
    "    model_old, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"16\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 8+8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [8, 8], 1)\n",
    "    model_new, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"8+8\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"8+8\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "'''\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"32\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"16+16\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 32 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 32, 1)\n",
    "    _, losses, iterations = old_train(model_old, X, y, \n",
    "                                      num_passes=max_iterations, \n",
    "                                      learning_rate=learning_rate, \n",
    "                                      tolerance=tolerance)\n",
    "    results_old[\"32\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"32\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 16+16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [16, 16], 1)\n",
    "    _, losses, iterations = train(model_new, X, y, \n",
    "                                  num_passes=max_iterations, \n",
    "                                  learning_rate=learning_rate, \n",
    "                                  tolerance=tolerance)\n",
    "    results_new[\"16+16\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"16+16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "'''\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:16:24.837840400Z",
     "start_time": "2023-10-26T04:00:37.577525900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8': 2.998530116499894, '16': 0.5109266059127227} {'4+4': 5.035185786900769, '8+8': 0.6341782387909146}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8': 65500.8, '16': 100000.0} {'4+4': 75050.6, '8+8': 91300.3}\n"
     ]
    }
   ],
   "source": [
    "# Calculate average losses for old and new models for different configurations\n",
    "avg_losses_old = {config: np.mean(data['losses']) for config, data in results_old.items()}\n",
    "avg_losses_new = {config: np.mean(data['losses']) for config, data in results_new.items()}\n",
    "\n",
    "# Bar chart for average loss values\n",
    "labels = list(avg_losses_old.keys()) + list(avg_losses_new.keys())\n",
    "old_vals = list(avg_losses_old.values())\n",
    "new_vals = list(avg_losses_new.values())\n",
    "bar_width = 0.35\n",
    "\n",
    "\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(r1[:len(old_vals)], old_vals, color='b', width=bar_width, edgecolor='grey', label='Old Model')\n",
    "plt.bar(r2[:len(new_vals)], new_vals, color='r', width=bar_width, edgecolor='grey', label='New Model')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Average Losses for Old vs New Models', fontweight='bold')\n",
    "plt.xlabel('Model Configuration', fontweight='bold')\n",
    "plt.ylabel('Average Loss', fontweight='bold')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xticks([r + 0.5*bar_width for r in range(len(labels))], labels)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(avg_losses_old, avg_losses_new)\n",
    "\n",
    "# Calculate average iterations for old and new models for different configurations\n",
    "avg_iterations_old = {config: np.mean(data[\"iterations\"]) for config, data in results_old.items()}\n",
    "avg_iterations_new = {config: np.mean(data[\"iterations\"]) for config, data in results_new.items()}\n",
    "\n",
    "# Bar chart for average iteration values\n",
    "old_it_vals = list(avg_iterations_old.values())\n",
    "new_it_vals = list(avg_iterations_new.values())\n",
    "\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(r1[:len(old_it_vals)], old_it_vals, color='b', width=bar_width, edgecolor='grey', label='Old Model')\n",
    "plt.bar(r2[:len(new_it_vals)], new_it_vals, color='r', width=bar_width, edgecolor='grey', label='New Model')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Average Iterations for Convergence for Old vs New Models', fontweight='bold')\n",
    "plt.xlabel('Model Configuration', fontweight='bold')\n",
    "plt.ylabel('Average Iterations', fontweight='bold')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xticks([r + 0.5*bar_width for r in range(len(labels))], labels)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(avg_iterations_old, avg_iterations_new)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:16:25.428147500Z",
     "start_time": "2023-10-26T04:16:24.818842700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion\n",
    "\n",
    "### Average final Loss:\n",
    "<p>\n",
    "The two-layer configurations (8 and 16 neurons) generally exhibit lower average loss values compared to the three-layer configurations (4+4 and 8+8).\n",
    "\n",
    "In particular, the configuration with 16 neurons in a single layer has the lowest average loss among all the tested configurations, indicating better performance in approximating the target function.\n",
    "\n",
    "The three-layer configurations (4+4 and 8+8) shows the relatively high average loss, suggesting it may not be the optimal configuration for this specific problem (x<sup>2</sup> + y<sup>2</sup> =1).\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Average Iterations to Convergence :\n",
    "<p>\n",
    "The three-layer configurations tend to converge faster (in fewer iterations) than the two-layer configurations.\n",
    "(Two-layer: [8: 4384.7, 16: 11314.9], Three-layer: [4+4: 1132.65, 8+8: 1116.05])\n",
    "\n",
    "Among all configurations, the 8+8 configuration for the three-layer network takes the shortest time to converge.\n",
    "The two-layer network with 8 neurons also demonstrates relatively fast convergence.\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "### General Trends:\n",
    "<p>\n",
    "Increasing the number of neurons in the two-layer configurations appears to improve performance in terms of loss. \n",
    "\n",
    "However, it leads to slower convergence, taking more iterations.\n",
    "\n",
    "For the three-layer configurations, simply increasing the number of neurons does not guarantee better performance. \n",
    "Notably, when additional experiments were conducted with 16+16 neurons, the loss diverged significantly, and the number of iterations required was greater than that of the 8+8 configuration.\n",
    "\n",
    "In conclusion, for this specific problem and dataset, a two-layer configuration with an appropriate number of neurons appears to be more efficient and performant than the tested three-layer configurations when considering the trade-off between performance and convergence speed.\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Additional experiments with activation functions:**\n",
    "In our case, when using a simple linear regression model, we observed that ReLU performed better (around zero) than tanh and the Logistic function (over 1370). Based on the results, we can conclude as follows:\n",
    "\n",
    "<i> ReLU is a piecewise linear activation function, and it only introduces non-linearity for positive inputs. This means that ReLU can approximate linear functions well and doesn't introduce strong non-linearities that might lead to convergence issues in a linear regression scenario. It's computationally efficient and less likely to cause gradient vanishing/exploding problems. Tanh is a scaled and shifted version of the sigmoid function. It introduces stronger non-linearity than ReLU across its entire range. When you apply tanh to the outputs of a linear regression model, it can introduce oscillations and non-linearities that might make convergence more challenging. The logistic function is also highly non-linear across its entire range. Applying this function to the outputs of a linear regression model can introduce even stronger non-linearities and may lead to convergence issues. It can suffer from the vanishing gradient problem for extreme input values, which can hinder convergence. </i>\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "> Additionally, tanh and logistic activations are prone to the vanishing gradient problem, especially for inputs far from zero. When gradients become too small during backpropagation, it can cause slow convergence or loss divergence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus: arbitrary number of layers (20 points):\n",
    "Change the functions such that they can accept an arbitrary number of layers, but\n",
    "keep the overall call-logic and training loops the same - do NOT use classes! For this,\n",
    "you will need to play around with the dictionaries in create_model, forward,\n",
    "backprop.\n"
   ],
   "metadata": {
    "id": "t8NWou_MVkxi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a multi-layer neural network\n",
    "def create_multilayer_model(X, layer_sizes):\n",
    "    model = {}\n",
    "    # using ReLU as the default activation function\n",
    "    model['activation_function'] = 'relu'  \n",
    "\n",
    "    # Create weights and biases for each layer based on layer_sizes\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        model[f'W{i+1}'] = np.random.randn(layer_sizes[i], layer_sizes[i+1]) / np.sqrt(layer_sizes[i])\n",
    "        model[f'b{i+1}'] = np.zeros((1, layer_sizes[i+1]))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define the forward pass given a model and data\n",
    "def feed_forward_multilayer(model, x):\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "\n",
    "    z = {}\n",
    "    a = {}\n",
    "    a[0] = x  # the input layer\n",
    "\n",
    "    # Compute activations and outputs for each layer\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "    for i in range(1, num_layers+1):\n",
    "        z[i] = a[i-1].dot(model[f'W{i}']) + model[f'b{i}']\n",
    "        a[i] = act_func(z[i])\n",
    "\n",
    "    return z, a\n",
    "\n",
    "# define the regression loss\n",
    "def calculate_loss_multilayer(model, X, y):\n",
    "    z, a = feed_forward_multilayer(model, X)\n",
    "    out = a[len(a) - 1]\n",
    "    \n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "    # data_loss = np.mean((y - output) ** 2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# back-propagation for the multi-layer network\n",
    "def backprop_multilayer(X, y, model, z, a):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "\n",
    "    # Initialize the gradients\n",
    "    dW = {}\n",
    "    db = {}\n",
    "    delta = {}\n",
    "\n",
    "    # Compute the error for the last layer\n",
    "    delta[num_layers] = a[num_layers] - y\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    for i in reversed(range(1, num_layers+1)):\n",
    "        dW[i] = a[i-1].T.dot(delta[i]) / m\n",
    "        db[i] = np.sum(delta[i], axis=0, keepdims=True) / m\n",
    "        \n",
    "        if i > 1:  # Skip delta computation for the input layer\n",
    "            delta[i-1] = delta[i].dot(model[f'W{i}'].T) * act_func_derivative(a[i-1])\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "# training loop\n",
    "def train_multilayer(model, X, y, num_passes=100000, learning_rate=0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z, a = feed_forward_multilayer(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW, db = backprop_multilayer(X, y, model, z, a)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for key in dW:\n",
    "            model[f'W{key}'] -= learning_rate * dW[key]\n",
    "            model[f'b{key}'] -= learning_rate * db[key]\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss_multilayer(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(f\"Loss after iteration {i}: {loss}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "id": "bk4EXE1lVkg1",
    "ExecuteTime": {
     "end_time": "2023-10-26T04:16:25.429146200Z",
     "start_time": "2023-10-26T04:16:25.418960100Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 161878.16526403546\n",
      "Loss after iteration 1000: 25225.3181899537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_32504\\4132343458.py:101: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 2000: 12812.564357425817\n",
      "Loss after iteration 3000: 9571.494707458136\n",
      "Loss after iteration 4000: 7678.65838356349\n",
      "Loss after iteration 5000: 5427.950330161897\n",
      "Loss after iteration 6000: 5098.072223162356\n",
      "Loss after iteration 7000: 4837.83483400984\n",
      "Loss after iteration 8000: 4503.821718215166\n",
      "Loss after iteration 9000: 4298.74671395945\n",
      "Loss after iteration 0: 543990.4012339398\n",
      "Loss after iteration 1000: 105564.54864051248\n",
      "Loss after iteration 2000: 105564.54810495627\n",
      "Loss after iteration 0: 71201.12511443693\n",
      "Loss after iteration 1000: 23253.88900126591\n",
      "Loss after iteration 2000: 11044.910317009657\n",
      "Loss after iteration 3000: 7288.0713171758325\n",
      "Loss after iteration 4000: 5459.477698357303\n",
      "Loss after iteration 5000: 4842.083346459411\n",
      "Loss after iteration 6000: 4363.864217038148\n",
      "Loss after iteration 7000: 4083.900656809986\n",
      "Loss after iteration 8000: 3958.775513006384\n",
      "Loss after iteration 9000: 3763.1066418084365\n",
      "Loss after iteration 0: 250633.5641460008\n",
      "Loss after iteration 1000: 105564.5486536922\n",
      "Loss after iteration 2000: 105564.54810495628\n",
      "--- 3.7379956245422363 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# architectures in lists of layer sizes\n",
    "architectures = [[2, 8, 1], [2, 4, 4, 1], [2, 16, 1], [2, 8, 8, 1]]\n",
    "\n",
    "# re-run the model training process with the provided architectures\n",
    "results = {}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# create and train multiple models with different layer sizes\n",
    "for arch in architectures:\n",
    "    model = create_multilayer_model(X, arch)\n",
    "    trained_model, losses, iterations = train_multilayer(model, X, y, num_passes=10000, learning_rate=0.01, tolerance=0.00001)\n",
    "    results[str(arch)] = losses\n",
    "    \n",
    "    #iterations\n",
    "    results[str(arch)+'iter'] = iterations\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:16:29.172439200Z",
     "start_time": "2023-10-26T04:16:25.426146900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['[2, 8, 1]', '[2, 8, 1]iter', '[2, 4, 4, 1]', '[2, 4, 4, 1]iter', '[2, 16, 1]', '[2, 16, 1]iter', '[2, 8, 8, 1]', '[2, 8, 8, 1]iter'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting data for plotting\n",
    "print(results.keys())\n",
    "\n",
    "configs_two_layer = ['[2, 8, 1]', '[2, 16, 1]']\n",
    "configs_three_layer = ['[2, 4, 4, 1]', '[2, 8, 8, 1]']\n",
    "\n",
    "# Compute Average losses\n",
    "average_losses_two_layer = [np.mean(results[config]) for config in configs_two_layer]\n",
    "average_losses_three_layer = [np.mean(results[config]) for config in configs_three_layer]\n",
    "\n",
    "iterations_of_convergence_two_layer = [np.mean(results[config+'iter']) for config in configs_two_layer]\n",
    "iterations_of_convergence_three_layer = [np.mean(results[config+'iter']) for config in configs_three_layer]\n",
    "\n",
    "# Create the combined bar + line graph\n",
    "fig, ax1 = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Combine the results\n",
    "configs_combined = configs_two_layer + configs_three_layer\n",
    "median_losses_combined = average_losses_two_layer + average_losses_three_layer\n",
    "iterations_of_convergence_combined = iterations_of_convergence_two_layer + iterations_of_convergence_three_layer\n",
    "\n",
    "# Bar graph for Average Loss\n",
    "bars = ax1.bar(configs_combined, median_losses_combined,\n",
    "               color=['blue', 'blue', 'green', 'green'],\n",
    "               width=0.4, align='center')\n",
    "ax1.set_title('Average Loss and Average # of Iterations to Convergence for Each Configuration')\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Average Loss', color='black')\n",
    "ax1.tick_params('y', colors='black')\n",
    "\n",
    "# Line graph for Iterations using twin axes\n",
    "ax2 = ax1.twinx()\n",
    "line, = ax2.plot(configs_combined, iterations_of_convergence_combined, color='red', marker='o', linestyle='-')\n",
    "ax2.set_ylabel('Iterations to Convergence', color='black')\n",
    "ax2.tick_params('y', colors='black')\n",
    "\n",
    "# Legend\n",
    "ax1.legend([bars[0], bars[2], line], [\"Two-layer Average Loss\", \"Three-layer Average Loss\", \"Iterations to Convergence\"], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(iterations_of_convergence_combined)\n",
    "# print(median_losses_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:16:29.651883Z",
     "start_time": "2023-10-26T04:16:29.175439500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part2 Pytorch version (20 points):\n",
    "\n",
    "<p>\n",
    "Add all code to the same threelayer.ipynb.\n",
    "Given that everything is easier with pytorch, adapt the code from class to solve the\n",
    "exact same regression problem with three layers and the same number of\n",
    "parameters. Use the ‘“nn” layers. Visualize the network architecture as well.\n",
    "Test the network 20 times with ADAM optimizer and 20 times with SGD optimizer,\n",
    "using a suitably high number of iterations. Record, plot, and compare the loss\n",
    "evaluation of the two optimizer runs. What can you say about the optimizers?\n",
    "</p>"
   ],
   "metadata": {
    "id": "xJhRwYl8YCuL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "Yf_A3FKBYCNW",
    "ExecuteTime": {
     "end_time": "2023-10-26T04:16:33.382829500Z",
     "start_time": "2023-10-26T04:16:29.650881900Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_model_pytorch(model, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "    # Choose the optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Convert data to torch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Print every 1000 epochs\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:16:33.418773900Z",
     "start_time": "2023-10-26T04:16:33.387825700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Loss: 206.7495\n",
      "Epoch [2000/10000], Loss: 111.6522\n",
      "Epoch [3000/10000], Loss: 54.8154\n",
      "Epoch [4000/10000], Loss: 23.7239\n",
      "Epoch [5000/10000], Loss: 15.7245\n",
      "Epoch [6000/10000], Loss: 12.6069\n",
      "Epoch [7000/10000], Loss: 7.4981\n",
      "Epoch [8000/10000], Loss: 3.5954\n",
      "Epoch [9000/10000], Loss: 3.3507\n",
      "Epoch [10000/10000], Loss: 3.2203\n",
      "Epoch [1000/10000], Loss: 54.2089\n",
      "Epoch [2000/10000], Loss: 26.7543\n",
      "Epoch [3000/10000], Loss: 19.4952\n",
      "Epoch [4000/10000], Loss: 17.4440\n",
      "Epoch [5000/10000], Loss: 16.4476\n",
      "Epoch [6000/10000], Loss: 13.6383\n",
      "Epoch [7000/10000], Loss: 13.1281\n",
      "Epoch [8000/10000], Loss: 12.4392\n",
      "Epoch [9000/10000], Loss: 11.3059\n",
      "Epoch [10000/10000], Loss: 9.5649\n",
      "difference of optimizers: 6.344680309295654\n",
      "Done with run 0\n",
      "Epoch [1000/10000], Loss: 193.3408\n",
      "Epoch [2000/10000], Loss: 94.7252\n",
      "Epoch [3000/10000], Loss: 39.6715\n",
      "Epoch [4000/10000], Loss: 19.4116\n",
      "Epoch [5000/10000], Loss: 14.7973\n",
      "Epoch [6000/10000], Loss: 10.3762\n",
      "Epoch [7000/10000], Loss: 6.5824\n",
      "Epoch [8000/10000], Loss: 3.4124\n",
      "Epoch [9000/10000], Loss: 2.5776\n",
      "Epoch [10000/10000], Loss: 2.3550\n",
      "Epoch [1000/10000], Loss: 70.1418\n",
      "Epoch [2000/10000], Loss: 38.0336\n",
      "Epoch [3000/10000], Loss: 25.5018\n",
      "Epoch [4000/10000], Loss: 23.8212\n",
      "Epoch [5000/10000], Loss: 21.6132\n",
      "Epoch [6000/10000], Loss: 20.4000\n",
      "Epoch [7000/10000], Loss: 16.9326\n",
      "Epoch [8000/10000], Loss: 16.2284\n",
      "Epoch [9000/10000], Loss: 15.1137\n",
      "Epoch [10000/10000], Loss: 14.4908\n",
      "difference of optimizers: 12.135705471038818\n",
      "Done with run 1\n",
      "Epoch [1000/10000], Loss: 202.5143\n",
      "Epoch [2000/10000], Loss: 98.9466\n",
      "Epoch [3000/10000], Loss: 43.9215\n",
      "Epoch [4000/10000], Loss: 11.7781\n",
      "Epoch [5000/10000], Loss: 7.4168\n",
      "Epoch [6000/10000], Loss: 5.1018\n",
      "Epoch [7000/10000], Loss: 4.3764\n",
      "Epoch [8000/10000], Loss: 2.6218\n",
      "Epoch [9000/10000], Loss: 2.0561\n",
      "Epoch [10000/10000], Loss: 1.7723\n",
      "Epoch [1000/10000], Loss: 51.7524\n",
      "Epoch [2000/10000], Loss: 27.0910\n",
      "Epoch [3000/10000], Loss: 18.5627\n",
      "Epoch [4000/10000], Loss: 14.6882\n",
      "Epoch [5000/10000], Loss: 13.2100\n",
      "Epoch [6000/10000], Loss: 13.1225\n",
      "Epoch [7000/10000], Loss: 12.4153\n",
      "Epoch [8000/10000], Loss: 10.8808\n",
      "Epoch [9000/10000], Loss: 10.3620\n",
      "Epoch [10000/10000], Loss: 9.6409\n",
      "difference of optimizers: 7.868639588356018\n",
      "Done with run 2\n",
      "Epoch [1000/10000], Loss: 218.0021\n",
      "Epoch [2000/10000], Loss: 118.5578\n",
      "Epoch [3000/10000], Loss: 50.9480\n",
      "Epoch [4000/10000], Loss: 25.6430\n",
      "Epoch [5000/10000], Loss: 21.8624\n",
      "Epoch [6000/10000], Loss: 18.6443\n",
      "Epoch [7000/10000], Loss: 11.4490\n",
      "Epoch [8000/10000], Loss: 5.4325\n",
      "Epoch [9000/10000], Loss: 4.5925\n",
      "Epoch [10000/10000], Loss: 4.1790\n",
      "Epoch [1000/10000], Loss: 66.8385\n",
      "Epoch [2000/10000], Loss: 26.8829\n",
      "Epoch [3000/10000], Loss: 16.4273\n",
      "Epoch [4000/10000], Loss: 14.5208\n",
      "Epoch [5000/10000], Loss: 12.6145\n",
      "Epoch [6000/10000], Loss: 12.9606\n",
      "Epoch [7000/10000], Loss: 11.5704\n",
      "Epoch [8000/10000], Loss: 11.0744\n",
      "Epoch [9000/10000], Loss: 10.1765\n",
      "Epoch [10000/10000], Loss: 11.7587\n",
      "difference of optimizers: 7.579662799835205\n",
      "Done with run 3\n",
      "Epoch [1000/10000], Loss: 204.2759\n",
      "Epoch [2000/10000], Loss: 90.0090\n",
      "Epoch [3000/10000], Loss: 30.9625\n",
      "Epoch [4000/10000], Loss: 10.4334\n",
      "Epoch [5000/10000], Loss: 7.4741\n",
      "Epoch [6000/10000], Loss: 6.8148\n",
      "Epoch [7000/10000], Loss: 6.2802\n",
      "Epoch [8000/10000], Loss: 5.8280\n",
      "Epoch [9000/10000], Loss: 4.9223\n",
      "Epoch [10000/10000], Loss: 3.6377\n",
      "Epoch [1000/10000], Loss: 67.9934\n",
      "Epoch [2000/10000], Loss: 32.9934\n",
      "Epoch [3000/10000], Loss: 19.1181\n",
      "Epoch [4000/10000], Loss: 14.9684\n",
      "Epoch [5000/10000], Loss: 14.7818\n",
      "Epoch [6000/10000], Loss: 13.2249\n",
      "Epoch [7000/10000], Loss: 11.7217\n",
      "Epoch [8000/10000], Loss: 11.2624\n",
      "Epoch [9000/10000], Loss: 10.5210\n",
      "Epoch [10000/10000], Loss: 10.3128\n",
      "difference of optimizers: 6.675076246261597\n",
      "Done with run 4\n",
      "Epoch [1000/10000], Loss: 169.4525\n",
      "Epoch [2000/10000], Loss: 80.9097\n",
      "Epoch [3000/10000], Loss: 31.7820\n",
      "Epoch [4000/10000], Loss: 14.9805\n",
      "Epoch [5000/10000], Loss: 11.6893\n",
      "Epoch [6000/10000], Loss: 6.2197\n",
      "Epoch [7000/10000], Loss: 3.3861\n",
      "Epoch [8000/10000], Loss: 2.7482\n",
      "Epoch [9000/10000], Loss: 2.4100\n",
      "Epoch [10000/10000], Loss: 2.0705\n",
      "Epoch [1000/10000], Loss: 62.9556\n",
      "Epoch [2000/10000], Loss: 32.1868\n",
      "Epoch [3000/10000], Loss: 15.9455\n",
      "Epoch [4000/10000], Loss: 14.5178\n",
      "Epoch [5000/10000], Loss: 13.3826\n",
      "Epoch [6000/10000], Loss: 13.7222\n",
      "Epoch [7000/10000], Loss: 14.2111\n",
      "Epoch [8000/10000], Loss: 12.0873\n",
      "Epoch [9000/10000], Loss: 10.9518\n",
      "Epoch [10000/10000], Loss: 10.1339\n",
      "difference of optimizers: 8.063390970230103\n",
      "Done with run 5\n",
      "Epoch [1000/10000], Loss: 218.8472\n",
      "Epoch [2000/10000], Loss: 97.5093\n",
      "Epoch [3000/10000], Loss: 31.8212\n",
      "Epoch [4000/10000], Loss: 18.3317\n",
      "Epoch [5000/10000], Loss: 7.0074\n",
      "Epoch [6000/10000], Loss: 5.4821\n",
      "Epoch [7000/10000], Loss: 4.5260\n",
      "Epoch [8000/10000], Loss: 3.7620\n",
      "Epoch [9000/10000], Loss: 3.2291\n",
      "Epoch [10000/10000], Loss: 2.8626\n",
      "Epoch [1000/10000], Loss: 80.3121\n",
      "Epoch [2000/10000], Loss: 30.7757\n",
      "Epoch [3000/10000], Loss: 15.9316\n",
      "Epoch [4000/10000], Loss: 11.5948\n",
      "Epoch [5000/10000], Loss: 11.0395\n",
      "Epoch [6000/10000], Loss: 11.3436\n",
      "Epoch [7000/10000], Loss: 9.7932\n",
      "Epoch [8000/10000], Loss: 8.3661\n",
      "Epoch [9000/10000], Loss: 7.9364\n",
      "Epoch [10000/10000], Loss: 7.4162\n",
      "difference of optimizers: 4.553577661514282\n",
      "Done with run 6\n",
      "Epoch [1000/10000], Loss: 207.2757\n",
      "Epoch [2000/10000], Loss: 81.9348\n",
      "Epoch [3000/10000], Loss: 41.0581\n",
      "Epoch [4000/10000], Loss: 29.0339\n",
      "Epoch [5000/10000], Loss: 19.1177\n",
      "Epoch [6000/10000], Loss: 14.2840\n",
      "Epoch [7000/10000], Loss: 11.6935\n",
      "Epoch [8000/10000], Loss: 10.5785\n",
      "Epoch [9000/10000], Loss: 8.7758\n",
      "Epoch [10000/10000], Loss: 6.0747\n",
      "Epoch [1000/10000], Loss: 87.0026\n",
      "Epoch [2000/10000], Loss: 35.8751\n",
      "Epoch [3000/10000], Loss: 26.3140\n",
      "Epoch [4000/10000], Loss: 17.2397\n",
      "Epoch [5000/10000], Loss: 14.9632\n",
      "Epoch [6000/10000], Loss: 13.1051\n",
      "Epoch [7000/10000], Loss: 11.1691\n",
      "Epoch [8000/10000], Loss: 11.0759\n",
      "Epoch [9000/10000], Loss: 9.6286\n",
      "Epoch [10000/10000], Loss: 9.0449\n",
      "difference of optimizers: 2.970189094543457\n",
      "Done with run 7\n",
      "Epoch [1000/10000], Loss: 182.5983\n",
      "Epoch [2000/10000], Loss: 85.2727\n",
      "Epoch [3000/10000], Loss: 30.3894\n",
      "Epoch [4000/10000], Loss: 13.1495\n",
      "Epoch [5000/10000], Loss: 7.8313\n",
      "Epoch [6000/10000], Loss: 5.7780\n",
      "Epoch [7000/10000], Loss: 4.9079\n",
      "Epoch [8000/10000], Loss: 3.8006\n",
      "Epoch [9000/10000], Loss: 2.8627\n",
      "Epoch [10000/10000], Loss: 2.2221\n",
      "Epoch [1000/10000], Loss: 82.1692\n",
      "Epoch [2000/10000], Loss: 39.3731\n",
      "Epoch [3000/10000], Loss: 22.8670\n",
      "Epoch [4000/10000], Loss: 18.4448\n",
      "Epoch [5000/10000], Loss: 17.8068\n",
      "Epoch [6000/10000], Loss: 16.4226\n",
      "Epoch [7000/10000], Loss: 14.3582\n",
      "Epoch [8000/10000], Loss: 14.3136\n",
      "Epoch [9000/10000], Loss: 13.4825\n",
      "Epoch [10000/10000], Loss: 13.0647\n",
      "difference of optimizers: 10.842565774917603\n",
      "Done with run 8\n",
      "Epoch [1000/10000], Loss: 181.8383\n",
      "Epoch [2000/10000], Loss: 79.1774\n",
      "Epoch [3000/10000], Loss: 36.0711\n",
      "Epoch [4000/10000], Loss: 32.8855\n",
      "Epoch [5000/10000], Loss: 27.5014\n",
      "Epoch [6000/10000], Loss: 18.1326\n",
      "Epoch [7000/10000], Loss: 8.7895\n",
      "Epoch [8000/10000], Loss: 4.8104\n",
      "Epoch [9000/10000], Loss: 3.4305\n",
      "Epoch [10000/10000], Loss: 2.5996\n",
      "Epoch [1000/10000], Loss: 89.9033\n",
      "Epoch [2000/10000], Loss: 40.2007\n",
      "Epoch [3000/10000], Loss: 22.5326\n",
      "Epoch [4000/10000], Loss: 22.5999\n",
      "Epoch [5000/10000], Loss: 18.6407\n",
      "Epoch [6000/10000], Loss: 20.3639\n",
      "Epoch [7000/10000], Loss: 18.8509\n",
      "Epoch [8000/10000], Loss: 18.3708\n",
      "Epoch [9000/10000], Loss: 17.3463\n",
      "Epoch [10000/10000], Loss: 16.1812\n",
      "difference of optimizers: 13.58160948753357\n",
      "Done with run 9\n",
      "Epoch [1000/10000], Loss: 188.1805\n",
      "Epoch [2000/10000], Loss: 109.3102\n",
      "Epoch [3000/10000], Loss: 55.3790\n",
      "Epoch [4000/10000], Loss: 26.2846\n",
      "Epoch [5000/10000], Loss: 15.9022\n",
      "Epoch [6000/10000], Loss: 12.2821\n",
      "Epoch [7000/10000], Loss: 10.1579\n",
      "Epoch [8000/10000], Loss: 7.9769\n",
      "Epoch [9000/10000], Loss: 6.0407\n",
      "Epoch [10000/10000], Loss: 4.1713\n",
      "Epoch [1000/10000], Loss: 67.3743\n",
      "Epoch [2000/10000], Loss: 33.5344\n",
      "Epoch [3000/10000], Loss: 19.3944\n",
      "Epoch [4000/10000], Loss: 16.4830\n",
      "Epoch [5000/10000], Loss: 15.5017\n",
      "Epoch [6000/10000], Loss: 15.5267\n",
      "Epoch [7000/10000], Loss: 11.7717\n",
      "Epoch [8000/10000], Loss: 13.4258\n",
      "Epoch [9000/10000], Loss: 14.4489\n",
      "Epoch [10000/10000], Loss: 13.1182\n",
      "difference of optimizers: 8.946870803833008\n",
      "Done with run 10\n",
      "Epoch [1000/10000], Loss: 197.3444\n",
      "Epoch [2000/10000], Loss: 48.4030\n",
      "Epoch [3000/10000], Loss: 16.8433\n",
      "Epoch [4000/10000], Loss: 9.2120\n",
      "Epoch [5000/10000], Loss: 6.0773\n",
      "Epoch [6000/10000], Loss: 4.8795\n",
      "Epoch [7000/10000], Loss: 4.5959\n",
      "Epoch [8000/10000], Loss: 4.5255\n",
      "Epoch [9000/10000], Loss: 4.5149\n",
      "Epoch [10000/10000], Loss: 4.5065\n",
      "Epoch [1000/10000], Loss: 54.1927\n",
      "Epoch [2000/10000], Loss: 28.5196\n",
      "Epoch [3000/10000], Loss: 19.5510\n",
      "Epoch [4000/10000], Loss: 18.7498\n",
      "Epoch [5000/10000], Loss: 15.7856\n",
      "Epoch [6000/10000], Loss: 13.1039\n",
      "Epoch [7000/10000], Loss: 11.9490\n",
      "Epoch [8000/10000], Loss: 10.5364\n",
      "Epoch [9000/10000], Loss: 9.7675\n",
      "Epoch [10000/10000], Loss: 9.1845\n",
      "difference of optimizers: 4.677913665771484\n",
      "Done with run 11\n",
      "Epoch [1000/10000], Loss: 183.2408\n",
      "Epoch [2000/10000], Loss: 74.4047\n",
      "Epoch [3000/10000], Loss: 33.2264\n",
      "Epoch [4000/10000], Loss: 20.4703\n",
      "Epoch [5000/10000], Loss: 13.6825\n",
      "Epoch [6000/10000], Loss: 8.1675\n",
      "Epoch [7000/10000], Loss: 5.1491\n",
      "Epoch [8000/10000], Loss: 4.6541\n",
      "Epoch [9000/10000], Loss: 4.6175\n",
      "Epoch [10000/10000], Loss: 3.5730\n",
      "Epoch [1000/10000], Loss: 75.0045\n",
      "Epoch [2000/10000], Loss: 39.7427\n",
      "Epoch [3000/10000], Loss: 22.2431\n",
      "Epoch [4000/10000], Loss: 17.8924\n",
      "Epoch [5000/10000], Loss: 15.5436\n",
      "Epoch [6000/10000], Loss: 14.0153\n",
      "Epoch [7000/10000], Loss: 12.4913\n",
      "Epoch [8000/10000], Loss: 16.8821\n",
      "Epoch [9000/10000], Loss: 12.7389\n",
      "Epoch [10000/10000], Loss: 11.1528\n",
      "difference of optimizers: 7.5798492431640625\n",
      "Done with run 12\n",
      "Epoch [1000/10000], Loss: 201.6943\n",
      "Epoch [2000/10000], Loss: 62.5640\n",
      "Epoch [3000/10000], Loss: 30.8268\n",
      "Epoch [4000/10000], Loss: 26.9651\n",
      "Epoch [5000/10000], Loss: 23.4868\n",
      "Epoch [6000/10000], Loss: 22.2441\n",
      "Epoch [7000/10000], Loss: 16.5270\n",
      "Epoch [8000/10000], Loss: 11.6941\n",
      "Epoch [9000/10000], Loss: 10.1488\n",
      "Epoch [10000/10000], Loss: 5.1440\n",
      "Epoch [1000/10000], Loss: 47.7783\n",
      "Epoch [2000/10000], Loss: 23.8250\n",
      "Epoch [3000/10000], Loss: 15.1340\n",
      "Epoch [4000/10000], Loss: 14.4164\n",
      "Epoch [5000/10000], Loss: 12.6384\n",
      "Epoch [6000/10000], Loss: 10.8169\n",
      "Epoch [7000/10000], Loss: 9.2769\n",
      "Epoch [8000/10000], Loss: 8.1455\n",
      "Epoch [9000/10000], Loss: 7.4736\n",
      "Epoch [10000/10000], Loss: 6.6602\n",
      "difference of optimizers: 1.5162534713745117\n",
      "Done with run 13\n",
      "Epoch [1000/10000], Loss: 188.3936\n",
      "Epoch [2000/10000], Loss: 94.0360\n",
      "Epoch [3000/10000], Loss: 34.8114\n",
      "Epoch [4000/10000], Loss: 19.8399\n",
      "Epoch [5000/10000], Loss: 12.6008\n",
      "Epoch [6000/10000], Loss: 6.6649\n",
      "Epoch [7000/10000], Loss: 5.0731\n",
      "Epoch [8000/10000], Loss: 4.5099\n",
      "Epoch [9000/10000], Loss: 4.0165\n",
      "Epoch [10000/10000], Loss: 3.7030\n",
      "Epoch [1000/10000], Loss: 227.4554\n",
      "Epoch [2000/10000], Loss: 105.8864\n",
      "Epoch [3000/10000], Loss: 65.2573\n",
      "Epoch [4000/10000], Loss: 45.7787\n",
      "Epoch [5000/10000], Loss: 31.4454\n",
      "Epoch [6000/10000], Loss: 22.6249\n",
      "Epoch [7000/10000], Loss: 10.3428\n",
      "Epoch [8000/10000], Loss: 9.4250\n",
      "Epoch [9000/10000], Loss: 8.6638\n",
      "Epoch [10000/10000], Loss: 8.5258\n",
      "difference of optimizers: 4.822717905044556\n",
      "Done with run 14\n",
      "Epoch [1000/10000], Loss: 179.3299\n",
      "Epoch [2000/10000], Loss: 74.4298\n",
      "Epoch [3000/10000], Loss: 32.8136\n",
      "Epoch [4000/10000], Loss: 24.3276\n",
      "Epoch [5000/10000], Loss: 13.2684\n",
      "Epoch [6000/10000], Loss: 6.2088\n",
      "Epoch [7000/10000], Loss: 3.4303\n",
      "Epoch [8000/10000], Loss: 2.6826\n",
      "Epoch [9000/10000], Loss: 2.4043\n",
      "Epoch [10000/10000], Loss: 2.2618\n",
      "Epoch [1000/10000], Loss: 77.5247\n",
      "Epoch [2000/10000], Loss: 19.9555\n",
      "Epoch [3000/10000], Loss: 16.5408\n",
      "Epoch [4000/10000], Loss: 15.0277\n",
      "Epoch [5000/10000], Loss: 13.9549\n",
      "Epoch [6000/10000], Loss: 10.5525\n",
      "Epoch [7000/10000], Loss: 13.1569\n",
      "Epoch [8000/10000], Loss: 12.1716\n",
      "Epoch [9000/10000], Loss: 9.8543\n",
      "Epoch [10000/10000], Loss: 9.6074\n",
      "difference of optimizers: 7.34556245803833\n",
      "Done with run 15\n",
      "Epoch [1000/10000], Loss: 198.9026\n",
      "Epoch [2000/10000], Loss: 93.1046\n",
      "Epoch [3000/10000], Loss: 37.6469\n",
      "Epoch [4000/10000], Loss: 25.2224\n",
      "Epoch [5000/10000], Loss: 16.9507\n",
      "Epoch [6000/10000], Loss: 12.5536\n",
      "Epoch [7000/10000], Loss: 9.2369\n",
      "Epoch [8000/10000], Loss: 6.3421\n",
      "Epoch [9000/10000], Loss: 5.2496\n",
      "Epoch [10000/10000], Loss: 4.1122\n",
      "Epoch [1000/10000], Loss: 103.7246\n",
      "Epoch [2000/10000], Loss: 26.3448\n",
      "Epoch [3000/10000], Loss: 18.5707\n",
      "Epoch [4000/10000], Loss: 15.6669\n",
      "Epoch [5000/10000], Loss: 15.6396\n",
      "Epoch [6000/10000], Loss: 13.4552\n",
      "Epoch [7000/10000], Loss: 12.4730\n",
      "Epoch [8000/10000], Loss: 11.0666\n",
      "Epoch [9000/10000], Loss: 10.2703\n",
      "Epoch [10000/10000], Loss: 9.5655\n",
      "difference of optimizers: 5.453365802764893\n",
      "Done with run 16\n",
      "Epoch [1000/10000], Loss: 172.5449\n",
      "Epoch [2000/10000], Loss: 89.3649\n",
      "Epoch [3000/10000], Loss: 39.0983\n",
      "Epoch [4000/10000], Loss: 16.0826\n",
      "Epoch [5000/10000], Loss: 12.0187\n",
      "Epoch [6000/10000], Loss: 8.8258\n",
      "Epoch [7000/10000], Loss: 5.3045\n",
      "Epoch [8000/10000], Loss: 3.9842\n",
      "Epoch [9000/10000], Loss: 3.0036\n",
      "Epoch [10000/10000], Loss: 2.2908\n",
      "Epoch [1000/10000], Loss: 130.3110\n",
      "Epoch [2000/10000], Loss: 59.0501\n",
      "Epoch [3000/10000], Loss: 36.0177\n",
      "Epoch [4000/10000], Loss: 26.4344\n",
      "Epoch [5000/10000], Loss: 22.6866\n",
      "Epoch [6000/10000], Loss: 21.4095\n",
      "Epoch [7000/10000], Loss: 20.3486\n",
      "Epoch [8000/10000], Loss: 19.0323\n",
      "Epoch [9000/10000], Loss: 19.4138\n",
      "Epoch [10000/10000], Loss: 21.0270\n",
      "difference of optimizers: 18.736153602600098\n",
      "Done with run 17\n",
      "Epoch [1000/10000], Loss: 182.3521\n",
      "Epoch [2000/10000], Loss: 81.3639\n",
      "Epoch [3000/10000], Loss: 34.6713\n",
      "Epoch [4000/10000], Loss: 16.5694\n",
      "Epoch [5000/10000], Loss: 12.4860\n",
      "Epoch [6000/10000], Loss: 7.5038\n",
      "Epoch [7000/10000], Loss: 4.7861\n",
      "Epoch [8000/10000], Loss: 3.9916\n",
      "Epoch [9000/10000], Loss: 3.3771\n",
      "Epoch [10000/10000], Loss: 2.9819\n",
      "Epoch [1000/10000], Loss: 69.0072\n",
      "Epoch [2000/10000], Loss: 35.6406\n",
      "Epoch [3000/10000], Loss: 27.6842\n",
      "Epoch [4000/10000], Loss: 21.2459\n",
      "Epoch [5000/10000], Loss: 16.1678\n",
      "Epoch [6000/10000], Loss: 15.5974\n",
      "Epoch [7000/10000], Loss: 14.7704\n",
      "Epoch [8000/10000], Loss: 14.7815\n",
      "Epoch [9000/10000], Loss: 14.5575\n",
      "Epoch [10000/10000], Loss: 13.2340\n",
      "difference of optimizers: 10.252135038375854\n",
      "Done with run 18\n",
      "Epoch [1000/10000], Loss: 245.0431\n",
      "Epoch [2000/10000], Loss: 126.0360\n",
      "Epoch [3000/10000], Loss: 45.6728\n",
      "Epoch [4000/10000], Loss: 32.9669\n",
      "Epoch [5000/10000], Loss: 25.1200\n",
      "Epoch [6000/10000], Loss: 14.8667\n",
      "Epoch [7000/10000], Loss: 6.0753\n",
      "Epoch [8000/10000], Loss: 4.0482\n",
      "Epoch [9000/10000], Loss: 3.2025\n",
      "Epoch [10000/10000], Loss: 2.8357\n",
      "Epoch [1000/10000], Loss: 112.9694\n",
      "Epoch [2000/10000], Loss: 48.3170\n",
      "Epoch [3000/10000], Loss: 27.8710\n",
      "Epoch [4000/10000], Loss: 19.6244\n",
      "Epoch [5000/10000], Loss: 21.7409\n",
      "Epoch [6000/10000], Loss: 23.3342\n",
      "Epoch [7000/10000], Loss: 18.8337\n",
      "Epoch [8000/10000], Loss: 17.0623\n",
      "Epoch [9000/10000], Loss: 15.3087\n",
      "Epoch [10000/10000], Loss: 13.6297\n",
      "difference of optimizers: 10.79399847984314\n",
      "Done with run 19\n",
      "--- 440.39299964904785 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Train the models with the new data using nn.Sequential\n",
    "losses_adam_seq = []\n",
    "losses_sgd_seq = []\n",
    "input_dim = X.shape[1]\n",
    "hidden_nodes = [8, 8]\n",
    "output_dim = 1\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(20):\n",
    "    # build model with adam optimizer\n",
    "    model_adam_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_adam_seq.append(train_model_pytorch(model_adam_seq, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    # build model with sgd optimizer\n",
    "    model_sgd_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_sgd_seq.append(train_model_pytorch(model_sgd_seq, X, y, optimizer_type='sgd', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    print(f'difference of optimizers: {losses_sgd_seq[-1][-1] - losses_adam_seq[-1][-1]}')\n",
    "    print(f'Done with run {_}')\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:23:53.801636200Z",
     "start_time": "2023-10-26T04:16:33.397772100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1500x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss evaluations for ADAM and SGD optimizers using the provided models\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Average loss for better visualization\n",
    "avg_losses_adam_seq = np.mean(losses_adam_seq, axis=0)\n",
    "avg_losses_sgd_seq = np.mean(losses_sgd_seq, axis=0)\n",
    "\n",
    "plt.plot(avg_losses_adam_seq, label='ADAM', color='blue')\n",
    "plt.plot(avg_losses_sgd_seq, label='SGD', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evaluation for ADAM vs. SGD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:23:54.095050800Z",
     "start_time": "2023-10-26T04:23:53.798636600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion\n",
    "\n",
    "<p>\n",
    "SGD tends to have lower loss values than ADAM.\n",
    "\n",
    "Since ADAM was supposed to be an improvement over SGD, it is surprising that SGD performs better in this case.\n",
    "\n",
    "The reason for SGD's better performance would seem to be that the dataset is relatively small, and the model is simple. \n",
    "\n",
    "ADAM is more suitable for large datasets and complex models.\n",
    " \n",
    "In this case, SGD is able to find the optimal solution faster than ADAM.\n",
    "\n",
    "The learning rate, noise, regularization factors could also could be a favorable for SGD than ADAM.\n",
    "\n",
    "In conclusion, while Adam is often favored for deep learning tasks due to its fast convergence and adaptability, it's not guaranteed to outperform SGD in all scenarios.\n",
    " \n",
    "The choice of optimizer and its parameters should ideally be based on the specific problem, dataset characteristics, and empirical validation.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T04:23:54.096049Z",
     "start_time": "2023-10-26T04:23:54.093038700Z"
    }
   }
  }
 ]
}
