{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Part1 Three layer network (60 points):\n",
    "\n",
    "<p>\n",
    "Create a notebook called threelayer.ipynb.\n",
    "Extend the fully-connected two layer perceptron shown in class for the regression\n",
    "problem by one more layer to have two hidden layers.\n",
    "For this, take a long look at the derivation of the backpropagation. As you can see,\n",
    "the derivation of the internal derivatives is always the same, no matter the number\n",
    "of hidden layers.\n",
    "Armed with this knowledge, it should be easy to extend the different functions in\n",
    "the notebook.\n",
    "The create_model function should now of course receive a list of values for the\n",
    "number of hidden_nodes in each layer - in addition, please extend the functionality,\n",
    "so that I can pass the activation function type as a string already here:\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "```python\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function = 'relu') :\n",
    "    return\n",
    "```\n",
    "\n",
    "<p>\n",
    "forward, calculate_loss, backprop should of course be extended\n",
    "accordingly to deal with the added number of layers and the activation function\n",
    "choice.\n",
    "Next, do a series of tests for the x^2+y^2+1 function using a learning rate of 0.001, a\n",
    "tolerance threshold of 0.0001, maximum iterations of 100,000, NO sgd, and NO\n",
    "regularization, and a “relu” function, comparing the “old” two-layer version with\n",
    "your “new” three-layer version as follows:\n",
    "- take 8 neurons for the two-layer version and 4 + 4 neurons for the three-layer\n",
    "version, and run each network 20 times, recording the loss and the number\n",
    "of iterations it needs\n",
    "- repeat this with 16 neurons for the two-layer version and 8+8 neurons for the\n",
    "three-layer version and 20 runs each\n",
    "- which network architecture converges “better” (earlier? lower error?). Plot\n",
    "the results nicely in one graph for errors and in another for number of\n",
    "iterations and comment on the results.\n",
    "</p>"
   ],
   "metadata": {
    "id": "89fqwYoOQ-kY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# numpy, matplotlib imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:15:16.381385700Z",
     "start_time": "2023-10-26T07:15:16.353455900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1. * (X > 0)\n",
    "\n",
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return 1.-tanh(X)**2\n",
    "\n",
    "def logistic(X):\n",
    "    return 1./(1. + np.exp(-X))\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return logistic(X)*(1. - logistic(X))\n",
    "\n",
    "# Activation functions mapping\n",
    "activation_functions = {\n",
    "    'relu': relu,\n",
    "    'tanh': tanh,\n",
    "    'logistic': logistic\n",
    "}\n",
    "\n",
    "# Activation functions derivatives mapping\n",
    "activation_derivatives = {\n",
    "    'relu': relu_derivative,\n",
    "    'tanh': tanh_derivative,\n",
    "    'logistic': logistic_derivative\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:15:17.165808500Z",
     "start_time": "2023-10-26T07:15:17.136518100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def old_create_model(X, hidden_nodes, output_dim = 2):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes) / np.sqrt(input_dim)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b1'] = np.zeros((1, hidden_nodes))\n",
    "\n",
    "    # second set of weights from hidden layer 1 to output\n",
    "    model['W2'] = np.random.randn(hidden_nodes, output_dim) / np.sqrt(hidden_nodes)\n",
    "    \n",
    "    # set of biases\n",
    "    model['b2'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def old_feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "\n",
    "    # activation function\n",
    "    #a1 = logistic(z1)\n",
    "    #a1 = tanh(z1)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "\n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = z2\n",
    "    return z1, a1, z2, out\n",
    "\n",
    "# define the regression loss\n",
    "def old_calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out = old_feed_forward(model, X)\n",
    "\n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def old_backprop(X,y,model,z1,a1,z2,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # derivative of loss function\n",
    "    delta3 = (output-y)/num_examples\n",
    "    \n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    \n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    #delta2 = delta3.dot(model['W2'].T) * logistic_derivative(a1) #if logistic\n",
    "    #delta2 = delta3.dot(model['W2'].T) * tanh_derivative(a1) #if tanh\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1) #if ReLU\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = np.dot(X.T, delta2)\n",
    "    \n",
    "    # and sum over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop\n",
    "def old_train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    \n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1,a1,z2,output = old_feed_forward(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW1, dW2, db1, db2 = old_backprop(X, y, model, z1, a1, z2, output)\n",
    "\n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "\n",
    "        loss = old_calculate_loss(model, X, y)\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"two-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n",
    "                print(np.abs((previous_loss - loss) / previous_loss), previous_loss, loss)\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "            \n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:15:17.847164800Z",
     "start_time": "2023-10-26T07:15:17.822842100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create a three-layer neural network\n",
    "def create_model(X, hidden_nodes, output_dim = 2, activation_function='relu'):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    \n",
    "    #save activation function to model\n",
    "    model['activation_function'] = activation_function\n",
    "    \n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    \n",
    "    # [i -> 1]weights and biases from input to hidden layer 1\n",
    "    model['W1'] = np.random.randn(input_dim, hidden_nodes[0]) / np.sqrt(input_dim)\n",
    "    model['b1'] = np.zeros((1, hidden_nodes[0]))\n",
    "    \n",
    "    # [1 -> 2]weights and biases  from  hidden layer 1 to hidden layer 2\n",
    "    model['W2'] = np.random.randn(hidden_nodes[0], hidden_nodes[1]) / np.sqrt(hidden_nodes[0])\n",
    "    model['b2'] = np.zeros((1, hidden_nodes[1]))\n",
    "\n",
    "    # [2 -> o]weights and biases from hidden layer 2 to output\n",
    "    model['W3'] = np.random.randn(hidden_nodes[1], output_dim) / np.sqrt(hidden_nodes[1])\n",
    "    model['b3'] = np.zeros((1, output_dim))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "    \n",
    "    # first layer\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # second layer\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = relu(z2)\n",
    "    \n",
    "    # third layer\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    out = z3\n",
    "    \n",
    "    return z1, a1, z2, a2, z3, out\n",
    "    \n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y):\n",
    "    num_examples = X.shape[0]\n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, a2, z3, out = feed_forward(model, X)\n",
    "\n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "\n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,a2,z3,output):\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "\n",
    "    # Derivative of loss function for output layer\n",
    "    delta4 = (output - y) / num_examples\n",
    "\n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW3 = a2.T.dot(delta4)\n",
    "\n",
    "    # and over all neurons\n",
    "    db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta3 = delta4.dot(model['W3'].T) * relu_derivative(a2)\n",
    "\n",
    "    # multiply this by hidden layer outputs\n",
    "    dW2 = a1.T.dot(delta3)\n",
    "\n",
    "    # and over all neurons\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "\n",
    "    # derivative of activation function\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1)\n",
    "\n",
    "    # multiply by input data\n",
    "    dW1 = X.T.dot(delta2)\n",
    "\n",
    "    # and over all neurons\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, dW2, dW3, db1, db2, db3\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, learning_rate = 0.001, tolerance = 0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "\n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z1, a1, z2, a2, z3, output = feed_forward(model, X)\n",
    "\n",
    "        # feed this into backprop\n",
    "        dW1, dW2, dW3, db1, db2, db3 = backprop(X, y, model, z1, a1, z2, a2, z3, output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        model['W3'] -= learning_rate * dW3\n",
    "        model['b3'] -= learning_rate * db3\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(\"three-layer Loss after iteration {}: {}\".format(i, loss))\n",
    "\n",
    "        # Check for convergence\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n",
    "                print(np.abs((previous_loss - loss) / previous_loss), previous_loss, loss)\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:15:18.448528800Z",
     "start_time": "2023-10-26T07:15:18.394998800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-layer Loss after iteration 0: 1490.272644561784\n",
      "two-layer Loss after iteration 1000: 26.010865033660018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_20816\\3437914713.py:112: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-layer Loss after iteration 2000: 10.180591186855196\n",
      "two-layer Loss after iteration 3000: 7.449825316105285\n",
      "two-layer Loss after iteration 4000: 6.70171778196685\n",
      "two-layer Loss after iteration 5000: 5.3174511435279666\n",
      "two-layer Loss after iteration 6000: 4.55254909704113\n",
      "two-layer Loss after iteration 7000: 4.281049548168682\n",
      "two-layer Loss after iteration 8000: 4.159493104038956\n",
      "two-layer Loss after iteration 9000: 4.070863817984685\n",
      "two-layer Loss after iteration 10000: 3.9419456984414896\n",
      "two-layer Loss after iteration 11000: 3.8332776669385296\n",
      "two-layer Loss after iteration 12000: 3.7586541151312507\n",
      "two-layer Loss after iteration 13000: 3.6927010155404627\n",
      "two-layer Loss after iteration 14000: 3.645233606204771\n",
      "two-layer Loss after iteration 15000: 3.6092909044644435\n",
      "two-layer Loss after iteration 16000: 3.5840704492466164\n",
      "two-layer Loss after iteration 17000: 3.560163105418273\n",
      "two-layer Loss after iteration 18000: 3.5442282082670875\n",
      "two-layer Loss after iteration 19000: 3.5207738625985723\n",
      "two-layer Loss after iteration 20000: 3.503155808017403\n",
      "two-layer Loss after iteration 21000: 3.4768534459085862\n",
      "two-layer Loss after iteration 22000: 3.459301962992276\n",
      "two-layer Loss after iteration 23000: 3.4454509612124586\n",
      "two-layer Loss after iteration 24000: 3.4356901968124434\n",
      "two-layer Loss after iteration 25000: 3.427453959756949\n",
      "two-layer Loss after iteration 26000: 3.423302635441575\n",
      "two-layer Loss after iteration 27000: 3.4174283822868876\n",
      "two-layer Loss after iteration 28000: 3.408962290403632\n",
      "two-layer Loss after iteration 29000: 3.3962811151004373\n",
      "two-layer Loss after iteration 30000: 3.391833028388304\n",
      "two-layer Loss after iteration 31000: 3.3872374803994365\n",
      "two-layer Loss after iteration 32000: 3.3844832397441054\n",
      "two-layer Loss after iteration 33000: 3.3826438528916665\n",
      "two-layer Loss after iteration 34000: 3.380479913750505\n",
      "two-layer Loss after iteration 35000: 3.378693020066367\n",
      "two-layer Loss after iteration 36000: 3.3769483963967337\n",
      "two-layer Loss after iteration 37000: 3.3754047349286687\n",
      "two-layer Loss after iteration 38000: 3.373995054107457\n",
      "two-layer Loss after iteration 39000: 3.37268545695221\n",
      "two-layer Loss after iteration 40000: 3.3713621285545097\n",
      "two-layer Loss after iteration 41000: 3.3700884957578636\n",
      "two-layer Loss after iteration 42000: 3.3688798543675444\n",
      "two-layer Loss after iteration 43000: 3.367724154354242\n",
      "two-layer Loss after iteration 44000: 3.3666122425765286\n",
      "two-layer Loss after iteration 45000: 3.3655397876748303\n",
      "two-layer Loss after iteration 46000: 3.364500283621654\n",
      "two-layer Loss after iteration 47000: 3.363497592056283\n",
      "two-layer Loss after iteration 48000: 3.3625694658366005\n",
      "two-layer Loss after iteration 49000: 3.3616626625309456\n",
      "two-layer Loss after iteration 50000: 3.3607690314724517\n",
      "two-layer Loss after iteration 51000: 3.359886442880368\n",
      "two-layer Loss after iteration 52000: 3.3590100475234355\n",
      "two-layer Loss after iteration 53000: 3.3581426659120797\n",
      "two-layer Loss after iteration 54000: 3.3572866871342253\n",
      "two-layer Loss after iteration 55000: 3.3564323182213456\n",
      "two-layer Loss after iteration 56000: 3.355581278199635\n",
      "two-layer Loss after iteration 57000: 3.354735710602296\n",
      "two-layer Loss after iteration 58000: 3.3538992091581283\n",
      "two-layer Loss after iteration 59000: 3.3530609341283024\n",
      "two-layer Loss after iteration 60000: 3.3522194229477136\n",
      "two-layer Loss after iteration 61000: 3.3513885913996444\n",
      "two-layer Loss after iteration 62000: 3.3505550316896677\n",
      "two-layer Loss after iteration 63000: 3.3497275900870784\n",
      "two-layer Loss after iteration 64000: 3.3488957043806082\n",
      "two-layer Loss after iteration 65000: 3.348074917447512\n",
      "two-layer Loss after iteration 66000: 3.347242408255149\n",
      "two-layer Loss after iteration 67000: 3.346418577288917\n",
      "two-layer Loss after iteration 68000: 3.3456038158800574\n",
      "two-layer Loss after iteration 69000: 3.33567412140501\n",
      "two-layer Loss after iteration 70000: 3.3340628665707035\n",
      "two-layer Loss after iteration 71000: 3.3327675616198817\n",
      "two-layer Loss after iteration 72000: 3.331527243980733\n",
      "two-layer Loss after iteration 73000: 3.3303281101587454\n",
      "two-layer Loss after iteration 74000: 3.329159118618846\n",
      "two-layer Loss after iteration 75000: 3.3280014468827472\n",
      "two-layer Loss after iteration 76000: 3.3268820519026487\n",
      "two-layer Loss after iteration 77000: 3.3257718817937914\n",
      "two-layer Loss after iteration 78000: 3.32469212932095\n",
      "two-layer Loss after iteration 79000: 3.323625291163149\n",
      "two-layer Loss after iteration 80000: 3.319681999999646\n",
      "two-layer Loss after iteration 81000: 3.316083841697462\n",
      "two-layer Loss after iteration 82000: 3.314485707109066\n",
      "two-layer Loss after iteration 83000: 3.3130408060433947\n",
      "two-layer Loss after iteration 84000: 3.311639089155802\n",
      "two-layer Loss after iteration 85000: 3.3102836808078426\n",
      "two-layer Loss after iteration 86000: 3.3090006693154494\n",
      "two-layer Loss after iteration 87000: 3.3077561934791704\n",
      "two-layer Loss after iteration 88000: 3.30657205107604\n",
      "two-layer Loss after iteration 89000: 3.305426938250934\n",
      "two-layer Loss after iteration 90000: 3.3043233822287665\n",
      "two-layer Loss after iteration 91000: 3.3032730613852133\n",
      "two-layer Loss after iteration 92000: 3.30224551724309\n",
      "two-layer Loss after iteration 93000: 3.3012628430012714\n",
      "two-layer Loss after iteration 94000: 3.3003143053435546\n",
      "two-layer Loss after iteration 95000: 3.2994051150581507\n",
      "two-layer Loss after iteration 96000: 3.2985260317629552\n",
      "two-layer Loss after iteration 97000: 3.297684059947682\n",
      "two-layer Loss after iteration 98000: 3.2968770379918086\n",
      "two-layer Loss after iteration 99000: 3.2960968311154177\n",
      "two-layer Loss after iteration 0: 1505.702007911983\n",
      "two-layer Loss after iteration 1000: 22.567946047695134\n",
      "two-layer Loss after iteration 2000: 9.040167961787569\n",
      "two-layer Loss after iteration 3000: 6.742774141778772\n",
      "two-layer Loss after iteration 4000: 5.752859389089726\n",
      "two-layer Loss after iteration 5000: 5.343845213299392\n",
      "two-layer Loss after iteration 6000: 5.046347055959767\n",
      "two-layer Loss after iteration 7000: 4.894026870417469\n",
      "two-layer Loss after iteration 8000: 4.779342480253247\n",
      "two-layer Loss after iteration 9000: 4.649904015005852\n",
      "two-layer Loss after iteration 10000: 4.50362064269964\n",
      "two-layer Loss after iteration 11000: 3.967110597324572\n",
      "two-layer Loss after iteration 12000: 3.793938491047\n",
      "two-layer Loss after iteration 13000: 3.692613991078122\n",
      "two-layer Loss after iteration 14000: 3.544549382051693\n",
      "two-layer Loss after iteration 15000: 3.2775649083408473\n",
      "two-layer Loss after iteration 16000: 3.230247820949633\n",
      "two-layer Loss after iteration 17000: 3.195718366263563\n",
      "two-layer Loss after iteration 18000: 3.173778190674224\n",
      "two-layer Loss after iteration 19000: 3.153887747832974\n",
      "two-layer Loss after iteration 20000: 3.138523588409002\n",
      "two-layer Loss after iteration 21000: 3.1271598937309792\n",
      "two-layer Loss after iteration 22000: 3.1059833241113917\n",
      "two-layer Loss after iteration 23000: 3.0944010904971706\n",
      "two-layer Loss after iteration 24000: 3.085569562668559\n",
      "two-layer Loss after iteration 25000: 3.0726485507570174\n",
      "two-layer Loss after iteration 26000: 3.055720481885567\n",
      "two-layer Loss after iteration 27000: 3.042586879575272\n",
      "two-layer Loss after iteration 28000: 3.029540544889195\n",
      "two-layer Loss after iteration 29000: 3.0195705660172125\n",
      "two-layer Loss after iteration 30000: 3.0025621559133437\n",
      "two-layer Loss after iteration 31000: 2.987870689656783\n",
      "two-layer Loss after iteration 32000: 2.980637369769606\n",
      "two-layer Loss after iteration 33000: 2.9453277700264535\n",
      "two-layer Loss after iteration 34000: 2.815565877681489\n",
      "two-layer Loss after iteration 35000: 2.7720456430381364\n",
      "two-layer Loss after iteration 36000: 2.7516556467908795\n",
      "two-layer Loss after iteration 37000: 2.738843298358094\n",
      "two-layer Loss after iteration 38000: 2.7316554363232646\n",
      "two-layer Loss after iteration 39000: 2.726756237768006\n",
      "two-layer Loss after iteration 40000: 2.7230852298953203\n",
      "two-layer Loss after iteration 41000: 2.720164350483306\n",
      "two-layer Loss after iteration 42000: 2.717657341626673\n",
      "two-layer Loss after iteration 43000: 2.715444996639704\n",
      "two-layer Loss after iteration 44000: 2.713459970977589\n",
      "two-layer Loss after iteration 45000: 2.7116611767945202\n",
      "two-layer Loss after iteration 46000: 2.7100066844067765\n",
      "two-layer Loss after iteration 47000: 2.70848104597172\n",
      "two-layer Loss after iteration 48000: 2.707062300015841\n",
      "two-layer Loss after iteration 49000: 2.705735466197775\n",
      "two-layer Loss after iteration 50000: 2.704490913794703\n",
      "two-layer Loss after iteration 51000: 2.7033162881719495\n",
      "two-layer Loss after iteration 52000: 2.7022070590716227\n",
      "two-layer Loss after iteration 53000: 2.7011552633567164\n",
      "two-layer Loss after iteration 54000: 2.700155898895164\n",
      "two-layer Loss after iteration 55000: 2.699205675073948\n",
      "two-layer Loss after iteration 56000: 2.6983017109050267\n",
      "two-layer Loss after iteration 57000: 2.6974425029169358\n",
      "two-layer Loss after iteration 58000: 2.696628358181959\n",
      "two-layer Loss after iteration 59000: 2.6958562297528603\n",
      "two-layer Loss after iteration 60000: 2.69512371308043\n",
      "two-layer Loss after iteration 61000: 2.694428542918148\n",
      "two-layer Loss after iteration 62000: 2.69376848939934\n",
      "two-layer Loss after iteration 63000: 2.693141718189495\n",
      "two-layer Loss after iteration 64000: 2.6925464960728354\n",
      "two-layer Loss after iteration 65000: 2.691981011817197\n",
      "two-layer Loss after iteration 66000: 2.6914436169913043\n",
      "two-layer Loss after iteration 67000: 2.6909329364845957\n",
      "two-layer Loss after iteration 68000: 2.690447414435626\n",
      "two-layer Loss after iteration 69000: 2.6899859435377027\n",
      "two-layer Loss after iteration 70000: 2.689547086007656\n",
      "two-layer Loss after iteration 71000: 2.6891297791844\n",
      "two-layer Loss after iteration 72000: 2.6887328652907607\n",
      "two-layer Loss after iteration 73000: 2.6883554197867365\n",
      "two-layer Loss after iteration 74000: 2.6880011684954526\n",
      "two-layer Loss after iteration 75000: 2.687664211745265\n",
      "two-layer Loss after iteration 76000: 2.687346137120113\n",
      "two-layer Loss after iteration 77000: 2.6870470851808115\n",
      "two-layer Loss after iteration 78000: 2.686763685738391\n",
      "two-layer Loss after iteration 79000: 2.686495056837814\n",
      "9.998233264913279e-05 2.686763685738391 2.686495056837814\n",
      "two-layer Loss after iteration 0: 1470.1361960400102\n",
      "two-layer Loss after iteration 1000: 24.83389373758456\n",
      "two-layer Loss after iteration 2000: 10.473421282079988\n",
      "two-layer Loss after iteration 3000: 8.205523887829115\n",
      "two-layer Loss after iteration 4000: 6.802496212318527\n",
      "two-layer Loss after iteration 5000: 5.539917188227185\n",
      "two-layer Loss after iteration 6000: 5.098183950723204\n",
      "two-layer Loss after iteration 7000: 4.702541537759614\n",
      "two-layer Loss after iteration 8000: 4.27808990612352\n",
      "two-layer Loss after iteration 9000: 4.11129789549856\n",
      "two-layer Loss after iteration 10000: 3.986880101268374\n",
      "two-layer Loss after iteration 11000: 3.8971337572209275\n",
      "two-layer Loss after iteration 12000: 3.8513687589644054\n",
      "two-layer Loss after iteration 13000: 3.816497698973687\n",
      "two-layer Loss after iteration 14000: 3.7878214389536793\n",
      "two-layer Loss after iteration 15000: 3.7644477165234997\n",
      "two-layer Loss after iteration 16000: 3.745157824867276\n",
      "two-layer Loss after iteration 17000: 3.72854045385201\n",
      "two-layer Loss after iteration 18000: 3.7140369892779717\n",
      "two-layer Loss after iteration 19000: 3.7011959153821836\n",
      "two-layer Loss after iteration 20000: 3.68936468706622\n",
      "two-layer Loss after iteration 21000: 3.6777944508526983\n",
      "two-layer Loss after iteration 22000: 3.668034669211718\n",
      "two-layer Loss after iteration 23000: 3.659576943977735\n",
      "two-layer Loss after iteration 24000: 3.651966616090362\n",
      "two-layer Loss after iteration 25000: 3.6391695205844345\n",
      "two-layer Loss after iteration 26000: 3.6227624376813803\n",
      "two-layer Loss after iteration 27000: 3.609135498656364\n",
      "two-layer Loss after iteration 28000: 3.5540590101800666\n",
      "two-layer Loss after iteration 29000: 3.5438989169342676\n",
      "two-layer Loss after iteration 30000: 3.5374947927958833\n",
      "two-layer Loss after iteration 31000: 3.53224842690557\n",
      "two-layer Loss after iteration 32000: 3.5277110697749707\n",
      "two-layer Loss after iteration 33000: 3.522873150280501\n",
      "two-layer Loss after iteration 34000: 3.5171847128780156\n",
      "two-layer Loss after iteration 35000: 3.5122941850522937\n",
      "two-layer Loss after iteration 36000: 3.508067978558341\n",
      "two-layer Loss after iteration 37000: 3.5042406984456362\n",
      "two-layer Loss after iteration 38000: 3.4769978433377533\n",
      "two-layer Loss after iteration 39000: 3.4653532886007508\n",
      "two-layer Loss after iteration 40000: 3.455480168318697\n",
      "two-layer Loss after iteration 41000: 3.4470680763922994\n",
      "two-layer Loss after iteration 42000: 3.4400142454028058\n",
      "two-layer Loss after iteration 43000: 3.432552868793404\n",
      "two-layer Loss after iteration 44000: 3.424877080167806\n",
      "two-layer Loss after iteration 45000: 3.418654588612685\n",
      "two-layer Loss after iteration 46000: 3.4133853256503475\n",
      "two-layer Loss after iteration 47000: 3.4068690458937088\n",
      "two-layer Loss after iteration 48000: 3.3999268827979607\n",
      "two-layer Loss after iteration 49000: 3.3940773727099542\n",
      "two-layer Loss after iteration 50000: 3.38901983465896\n",
      "two-layer Loss after iteration 51000: 3.384633299554097\n",
      "two-layer Loss after iteration 52000: 3.3808203765447544\n",
      "two-layer Loss after iteration 53000: 3.377499690306122\n",
      "two-layer Loss after iteration 54000: 3.3746027853947322\n",
      "two-layer Loss after iteration 55000: 3.3720718415055435\n",
      "two-layer Loss after iteration 56000: 3.3698578267406094\n",
      "two-layer Loss after iteration 57000: 3.36795431256673\n",
      "two-layer Loss after iteration 58000: 3.355752838570763\n",
      "two-layer Loss after iteration 59000: 3.3501473808741706\n",
      "two-layer Loss after iteration 60000: 3.3458234818459944\n",
      "two-layer Loss after iteration 61000: 3.342353536007023\n",
      "two-layer Loss after iteration 62000: 3.339517969776082\n",
      "two-layer Loss after iteration 63000: 3.3371635813046643\n",
      "two-layer Loss after iteration 64000: 3.3351989316047597\n",
      "two-layer Loss after iteration 65000: 3.3335557360369177\n",
      "two-layer Loss after iteration 66000: 3.332178372103337\n",
      "two-layer Loss after iteration 67000: 3.3310213364761085\n",
      "two-layer Loss after iteration 68000: 3.1415565480560734\n",
      "two-layer Loss after iteration 69000: 2.7673932455892154\n",
      "two-layer Loss after iteration 70000: 2.6517079591617563\n",
      "two-layer Loss after iteration 71000: 2.616199137951263\n",
      "two-layer Loss after iteration 72000: 2.5913075147180344\n",
      "two-layer Loss after iteration 73000: 2.5711553232522393\n",
      "two-layer Loss after iteration 74000: 2.553869557228386\n",
      "two-layer Loss after iteration 75000: 2.5405240421740003\n",
      "two-layer Loss after iteration 76000: 2.530072026592811\n",
      "two-layer Loss after iteration 77000: 2.5216998547317533\n",
      "two-layer Loss after iteration 78000: 2.51514487712821\n",
      "two-layer Loss after iteration 79000: 2.509950331464082\n",
      "two-layer Loss after iteration 80000: 2.5058116186428587\n",
      "two-layer Loss after iteration 81000: 2.5017789981887835\n",
      "two-layer Loss after iteration 82000: 2.496016406388075\n",
      "two-layer Loss after iteration 83000: 2.4913485367285513\n",
      "two-layer Loss after iteration 84000: 2.4876368605519756\n",
      "two-layer Loss after iteration 85000: 2.4850389624189866\n",
      "two-layer Loss after iteration 86000: 2.483216414782243\n",
      "two-layer Loss after iteration 87000: 2.4818868121437623\n",
      "two-layer Loss after iteration 88000: 2.4808054756165823\n",
      "two-layer Loss after iteration 89000: 2.479909727311078\n",
      "two-layer Loss after iteration 90000: 2.4791809585184175\n",
      "two-layer Loss after iteration 91000: 2.478580270012684\n",
      "two-layer Loss after iteration 92000: 2.478126640067096\n",
      "two-layer Loss after iteration 93000: 2.4777944297952823\n",
      "two-layer Loss after iteration 94000: 2.4542323312855205\n",
      "two-layer Loss after iteration 95000: 2.4517511412781237\n",
      "two-layer Loss after iteration 96000: 2.4498163422319044\n",
      "two-layer Loss after iteration 97000: 2.448242244256957\n",
      "two-layer Loss after iteration 98000: 2.446946863704429\n",
      "two-layer Loss after iteration 99000: 2.4458781013588773\n",
      "two-layer Loss after iteration 0: 1652.2265494112282\n",
      "two-layer Loss after iteration 1000: 25.355338807807986\n",
      "two-layer Loss after iteration 2000: 9.603074794490992\n",
      "two-layer Loss after iteration 3000: 7.340256970272871\n",
      "two-layer Loss after iteration 4000: 6.614228414490444\n",
      "two-layer Loss after iteration 5000: 6.422881905873576\n",
      "two-layer Loss after iteration 6000: 6.258583521178106\n",
      "two-layer Loss after iteration 7000: 6.1229975038856415\n",
      "two-layer Loss after iteration 8000: 6.014048965931169\n",
      "two-layer Loss after iteration 9000: 5.8855467156106975\n",
      "two-layer Loss after iteration 10000: 5.60689000882071\n",
      "two-layer Loss after iteration 11000: 5.523563671834904\n",
      "two-layer Loss after iteration 12000: 5.482581236960364\n",
      "two-layer Loss after iteration 13000: 5.455750841027208\n",
      "two-layer Loss after iteration 14000: 5.4303007907639556\n",
      "two-layer Loss after iteration 15000: 5.409802039225934\n",
      "two-layer Loss after iteration 16000: 5.3900503822457075\n",
      "two-layer Loss after iteration 17000: 5.375851898658877\n",
      "two-layer Loss after iteration 18000: 5.35859358114376\n",
      "two-layer Loss after iteration 19000: 5.34692800907064\n",
      "two-layer Loss after iteration 20000: 5.3353403481961745\n",
      "two-layer Loss after iteration 21000: 5.323492443473868\n",
      "two-layer Loss after iteration 22000: 5.3147967276668595\n",
      "two-layer Loss after iteration 23000: 5.30910447618414\n",
      "two-layer Loss after iteration 24000: 5.302498132421641\n",
      "two-layer Loss after iteration 25000: 5.2974234615938585\n",
      "two-layer Loss after iteration 26000: 5.2714271274620454\n",
      "two-layer Loss after iteration 27000: 5.103976344214697\n",
      "two-layer Loss after iteration 28000: 4.894713988488768\n",
      "two-layer Loss after iteration 29000: 4.833999463768225\n",
      "two-layer Loss after iteration 30000: 4.785881063916519\n",
      "two-layer Loss after iteration 31000: 4.740817312284988\n",
      "two-layer Loss after iteration 32000: 4.705561578133917\n",
      "two-layer Loss after iteration 33000: 4.676331400156077\n",
      "two-layer Loss after iteration 34000: 4.6534176177626065\n",
      "two-layer Loss after iteration 35000: 4.637375343526011\n",
      "two-layer Loss after iteration 36000: 4.631594403347325\n",
      "two-layer Loss after iteration 37000: 4.626769439060223\n",
      "two-layer Loss after iteration 38000: 4.622695577148744\n",
      "two-layer Loss after iteration 39000: 4.619258629340373\n",
      "two-layer Loss after iteration 40000: 4.616300569347898\n",
      "two-layer Loss after iteration 41000: 4.611352199400332\n",
      "two-layer Loss after iteration 42000: 4.607222144668777\n",
      "two-layer Loss after iteration 43000: 4.60321728209379\n",
      "two-layer Loss after iteration 44000: 4.599326639471576\n",
      "two-layer Loss after iteration 45000: 4.595553979466515\n",
      "two-layer Loss after iteration 46000: 4.591904309846426\n",
      "two-layer Loss after iteration 47000: 4.588383219274768\n",
      "two-layer Loss after iteration 48000: 4.5849964639636935\n",
      "two-layer Loss after iteration 49000: 4.581749621181847\n",
      "two-layer Loss after iteration 50000: 4.578647787617466\n",
      "two-layer Loss after iteration 51000: 4.575695322198233\n",
      "two-layer Loss after iteration 52000: 4.572895636979942\n",
      "two-layer Loss after iteration 53000: 4.570251039843295\n",
      "two-layer Loss after iteration 54000: 4.567762631259577\n",
      "two-layer Loss after iteration 55000: 4.565430255098275\n",
      "two-layer Loss after iteration 56000: 4.563252500892296\n",
      "two-layer Loss after iteration 57000: 4.561226752567666\n",
      "two-layer Loss after iteration 58000: 4.559349276686288\n",
      "two-layer Loss after iteration 59000: 4.557615341921877\n",
      "two-layer Loss after iteration 60000: 4.556019360856886\n",
      "two-layer Loss after iteration 61000: 4.554555045222757\n",
      "two-layer Loss after iteration 62000: 4.553215566308292\n",
      "two-layer Loss after iteration 63000: 4.55199371328982\n",
      "two-layer Loss after iteration 64000: 4.5508820435373885\n",
      "two-layer Loss after iteration 65000: 4.549873020372321\n",
      "two-layer Loss after iteration 66000: 4.54895913516644\n",
      "two-layer Loss after iteration 67000: 4.548133011981954\n",
      "two-layer Loss after iteration 68000: 4.547387494087838\n",
      "two-layer Loss after iteration 69000: 4.546715862921386\n",
      "two-layer Loss after iteration 70000: 4.54611181266158\n",
      "two-layer Loss after iteration 71000: 4.545569603038599\n",
      "two-layer Loss after iteration 72000: 4.5450825296437145\n",
      "two-layer Loss after iteration 73000: 4.544646024419433\n",
      "9.603900950855606e-05 4.5450825296437145 4.544646024419433\n",
      "two-layer Loss after iteration 0: 1527.6795291656842\n",
      "two-layer Loss after iteration 1000: 24.05450615155358\n",
      "two-layer Loss after iteration 2000: 9.225603766257453\n",
      "two-layer Loss after iteration 3000: 7.575097853808511\n",
      "two-layer Loss after iteration 4000: 6.466803383243471\n",
      "two-layer Loss after iteration 5000: 5.718915432444333\n",
      "two-layer Loss after iteration 6000: 5.321723692287356\n",
      "two-layer Loss after iteration 7000: 4.92108290338391\n",
      "two-layer Loss after iteration 8000: 4.2641765588080105\n",
      "two-layer Loss after iteration 9000: 3.9459700271274047\n",
      "two-layer Loss after iteration 10000: 3.804455655532769\n",
      "two-layer Loss after iteration 11000: 3.6956622159038246\n",
      "two-layer Loss after iteration 12000: 3.6222734054227406\n",
      "two-layer Loss after iteration 13000: 3.5394600500053506\n",
      "two-layer Loss after iteration 14000: 3.4729838444145833\n",
      "two-layer Loss after iteration 15000: 3.422685933018801\n",
      "two-layer Loss after iteration 16000: 3.3450949050394136\n",
      "two-layer Loss after iteration 17000: 3.3063811464913395\n",
      "two-layer Loss after iteration 18000: 3.2687406109962263\n",
      "two-layer Loss after iteration 19000: 3.0651374806000353\n",
      "two-layer Loss after iteration 20000: 2.8647112310571416\n",
      "two-layer Loss after iteration 21000: 2.8010599799396028\n",
      "two-layer Loss after iteration 22000: 2.765582414946727\n",
      "two-layer Loss after iteration 23000: 2.726096678503228\n",
      "two-layer Loss after iteration 24000: 2.6850165995366373\n",
      "two-layer Loss after iteration 25000: 2.3642255796074667\n",
      "two-layer Loss after iteration 26000: 2.2647376298193205\n",
      "two-layer Loss after iteration 27000: 2.1963362902739116\n",
      "two-layer Loss after iteration 28000: 2.1503393664377906\n",
      "two-layer Loss after iteration 29000: 2.1129150291599665\n",
      "two-layer Loss after iteration 30000: 2.080439881990803\n",
      "two-layer Loss after iteration 31000: 2.0604300822843498\n",
      "two-layer Loss after iteration 32000: 2.0477248263673693\n",
      "two-layer Loss after iteration 33000: 2.039273253044931\n",
      "two-layer Loss after iteration 34000: 2.0320429863196385\n",
      "two-layer Loss after iteration 35000: 2.025445453648229\n",
      "two-layer Loss after iteration 36000: 2.0031183815704416\n",
      "two-layer Loss after iteration 37000: 1.996082909539514\n",
      "two-layer Loss after iteration 38000: 1.9916018184309614\n",
      "two-layer Loss after iteration 39000: 1.9881951661153698\n",
      "two-layer Loss after iteration 40000: 1.9822404715563982\n",
      "two-layer Loss after iteration 41000: 1.97668645205311\n",
      "two-layer Loss after iteration 42000: 1.9715586107685925\n",
      "two-layer Loss after iteration 43000: 1.9679218920792048\n",
      "two-layer Loss after iteration 44000: 1.964902646747989\n",
      "two-layer Loss after iteration 45000: 1.9623196833618979\n",
      "two-layer Loss after iteration 46000: 1.96007188071967\n",
      "two-layer Loss after iteration 47000: 1.958093601277049\n",
      "two-layer Loss after iteration 48000: 1.956341556817449\n",
      "two-layer Loss after iteration 49000: 1.9547812305236343\n",
      "two-layer Loss after iteration 50000: 1.9533862406498974\n",
      "two-layer Loss after iteration 51000: 1.9521357491820193\n",
      "two-layer Loss after iteration 52000: 1.9510122514696762\n",
      "two-layer Loss after iteration 53000: 1.9500005898446044\n",
      "two-layer Loss after iteration 54000: 1.9490906591264259\n",
      "two-layer Loss after iteration 55000: 1.948270216543717\n",
      "two-layer Loss after iteration 56000: 1.9475295824538574\n",
      "two-layer Loss after iteration 57000: 1.9468619687953606\n",
      "two-layer Loss after iteration 58000: 1.9431841887903678\n",
      "two-layer Loss after iteration 59000: 1.9403168555874066\n",
      "two-layer Loss after iteration 60000: 1.9380720095844186\n",
      "two-layer Loss after iteration 61000: 1.9362207140788121\n",
      "two-layer Loss after iteration 62000: 1.9346489915944525\n",
      "two-layer Loss after iteration 63000: 1.932535248582944\n",
      "two-layer Loss after iteration 64000: 1.9303688805970398\n",
      "two-layer Loss after iteration 65000: 1.92863904195969\n",
      "two-layer Loss after iteration 66000: 1.9271761818672213\n",
      "two-layer Loss after iteration 67000: 1.9259132569544093\n",
      "two-layer Loss after iteration 68000: 1.9248064906603417\n",
      "two-layer Loss after iteration 69000: 1.9206045949141368\n",
      "two-layer Loss after iteration 70000: 1.9171752334814056\n",
      "two-layer Loss after iteration 71000: 1.9143928197486453\n",
      "two-layer Loss after iteration 72000: 1.9120512085114205\n",
      "two-layer Loss after iteration 73000: 1.9100411357995415\n",
      "two-layer Loss after iteration 74000: 1.9082882534020937\n",
      "two-layer Loss after iteration 75000: 1.9067394454311144\n",
      "two-layer Loss after iteration 76000: 1.9053556485198668\n",
      "two-layer Loss after iteration 77000: 1.9041075084585433\n",
      "two-layer Loss after iteration 78000: 1.902972570346637\n",
      "two-layer Loss after iteration 79000: 1.9019333895376695\n",
      "two-layer Loss after iteration 80000: 1.9009823356262774\n",
      "two-layer Loss after iteration 81000: 1.8989563733722536\n",
      "two-layer Loss after iteration 82000: 1.893792979098501\n",
      "two-layer Loss after iteration 83000: 1.890724223138718\n",
      "two-layer Loss after iteration 84000: 1.8881069505672423\n",
      "two-layer Loss after iteration 85000: 1.8858327247366806\n",
      "two-layer Loss after iteration 86000: 1.8838383503964304\n",
      "two-layer Loss after iteration 87000: 1.8826489226803205\n",
      "two-layer Loss after iteration 88000: 1.8816201008365414\n",
      "two-layer Loss after iteration 89000: 1.8806633477499286\n",
      "two-layer Loss after iteration 90000: 1.8797673741902003\n",
      "two-layer Loss after iteration 91000: 1.8789250799579194\n",
      "two-layer Loss after iteration 92000: 1.878131003464701\n",
      "two-layer Loss after iteration 93000: 1.8773806982217864\n",
      "two-layer Loss after iteration 94000: 1.8766704499371376\n",
      "two-layer Loss after iteration 95000: 1.8759971046093475\n",
      "two-layer Loss after iteration 96000: 1.875357948431757\n",
      "two-layer Loss after iteration 97000: 1.8747506189263796\n",
      "two-layer Loss after iteration 98000: 1.8741730374302041\n",
      "two-layer Loss after iteration 99000: 1.8736233570154375\n",
      "two-layer Loss after iteration 0: 1428.1951042671215\n",
      "two-layer Loss after iteration 1000: 23.563963682223797\n",
      "two-layer Loss after iteration 2000: 9.430241214233464\n",
      "two-layer Loss after iteration 3000: 7.187369764903346\n",
      "two-layer Loss after iteration 4000: 5.428720650074493\n",
      "two-layer Loss after iteration 5000: 5.022398057713948\n",
      "two-layer Loss after iteration 6000: 4.847062355637398\n",
      "two-layer Loss after iteration 7000: 4.7093188820942435\n",
      "two-layer Loss after iteration 8000: 4.5669168323632885\n",
      "two-layer Loss after iteration 9000: 4.4584061156614965\n",
      "two-layer Loss after iteration 10000: 4.061823464637323\n",
      "two-layer Loss after iteration 11000: 3.9100859908384957\n",
      "two-layer Loss after iteration 12000: 3.8388278082964953\n",
      "two-layer Loss after iteration 13000: 3.8007181510613286\n",
      "two-layer Loss after iteration 14000: 3.7770757433816677\n",
      "two-layer Loss after iteration 15000: 3.761695858549368\n",
      "two-layer Loss after iteration 16000: 3.7513728776050366\n",
      "two-layer Loss after iteration 17000: 3.742626022242782\n",
      "two-layer Loss after iteration 18000: 3.736212696078615\n",
      "two-layer Loss after iteration 19000: 3.7313271287785077\n",
      "two-layer Loss after iteration 20000: 3.7280295633643883\n",
      "two-layer Loss after iteration 21000: 3.72585121438355\n",
      "two-layer Loss after iteration 22000: 3.7242849604195833\n",
      "two-layer Loss after iteration 23000: 3.723069530628067\n",
      "two-layer Loss after iteration 24000: 3.72207465833994\n",
      "two-layer Loss after iteration 25000: 3.7212100500628553\n",
      "two-layer Loss after iteration 26000: 3.72044430758653\n",
      "two-layer Loss after iteration 27000: 3.7197602434135915\n",
      "two-layer Loss after iteration 28000: 3.7191271569350515\n",
      "two-layer Loss after iteration 29000: 3.7185454846067327\n",
      "two-layer Loss after iteration 30000: 3.7179940783568135\n",
      "two-layer Loss after iteration 31000: 3.7174882650933476\n",
      "two-layer Loss after iteration 32000: 3.7170010405892504\n",
      "two-layer Loss after iteration 33000: 3.71655730915795\n",
      "two-layer Loss after iteration 34000: 3.716144561594778\n",
      "two-layer Loss after iteration 35000: 3.715757789071502\n",
      "two-layer Loss after iteration 36000: 3.715385062838019\n",
      "two-layer Loss after iteration 37000: 3.7150274328964783\n",
      "9.625649441237013e-05 3.715385062838019 3.7150274328964783\n",
      "two-layer Loss after iteration 0: 1636.191509426616\n",
      "two-layer Loss after iteration 1000: 21.839870802228162\n",
      "two-layer Loss after iteration 2000: 8.974096198408251\n",
      "two-layer Loss after iteration 3000: 7.050214440999718\n",
      "two-layer Loss after iteration 4000: 6.350642803137159\n",
      "two-layer Loss after iteration 5000: 5.939669839678938\n",
      "two-layer Loss after iteration 6000: 5.646833656101806\n",
      "two-layer Loss after iteration 7000: 5.459365278262136\n",
      "two-layer Loss after iteration 8000: 5.317459023449577\n",
      "two-layer Loss after iteration 9000: 5.204029668876753\n",
      "two-layer Loss after iteration 10000: 5.1113716228525705\n",
      "two-layer Loss after iteration 11000: 5.037352815685404\n",
      "two-layer Loss after iteration 12000: 4.974176142117732\n",
      "two-layer Loss after iteration 13000: 4.922627757642755\n",
      "two-layer Loss after iteration 14000: 4.870901821741524\n",
      "two-layer Loss after iteration 15000: 4.763387457055044\n",
      "two-layer Loss after iteration 16000: 4.656464832879352\n",
      "two-layer Loss after iteration 17000: 4.528378443112791\n",
      "two-layer Loss after iteration 18000: 4.440738318538595\n",
      "two-layer Loss after iteration 19000: 4.367407043072781\n",
      "two-layer Loss after iteration 20000: 4.12546181145834\n",
      "two-layer Loss after iteration 21000: 4.082579107141736\n",
      "two-layer Loss after iteration 22000: 4.058481162876246\n",
      "two-layer Loss after iteration 23000: 4.044517768745771\n",
      "two-layer Loss after iteration 24000: 4.034230138932459\n",
      "two-layer Loss after iteration 25000: 4.005260756279236\n",
      "two-layer Loss after iteration 26000: 3.989280762311406\n",
      "two-layer Loss after iteration 27000: 3.976072872240563\n",
      "two-layer Loss after iteration 28000: 3.964990706113435\n",
      "two-layer Loss after iteration 29000: 3.9550763327144343\n",
      "two-layer Loss after iteration 30000: 3.9462202356055798\n",
      "two-layer Loss after iteration 31000: 3.938324755043648\n",
      "two-layer Loss after iteration 32000: 3.9320509059431665\n",
      "two-layer Loss after iteration 33000: 3.926045693701661\n",
      "two-layer Loss after iteration 34000: 3.920587162010181\n",
      "two-layer Loss after iteration 35000: 3.915690272485421\n",
      "two-layer Loss after iteration 36000: 3.895599744399384\n",
      "two-layer Loss after iteration 37000: 3.889220987661804\n",
      "two-layer Loss after iteration 38000: 3.881288679649415\n",
      "two-layer Loss after iteration 39000: 3.8754879993119133\n",
      "two-layer Loss after iteration 40000: 3.870762632401308\n",
      "two-layer Loss after iteration 41000: 3.8665733137083156\n",
      "two-layer Loss after iteration 42000: 3.8627099280900117\n",
      "two-layer Loss after iteration 43000: 3.8471130700914453\n",
      "two-layer Loss after iteration 44000: 3.841112724557635\n",
      "two-layer Loss after iteration 45000: 3.8372271396694275\n",
      "two-layer Loss after iteration 46000: 3.833919076327011\n",
      "two-layer Loss after iteration 47000: 3.830916707877734\n",
      "two-layer Loss after iteration 48000: 3.828182818927388\n",
      "two-layer Loss after iteration 49000: 3.824987697616813\n",
      "two-layer Loss after iteration 50000: 3.8222421005135474\n",
      "two-layer Loss after iteration 51000: 3.8199476450663647\n",
      "two-layer Loss after iteration 52000: 3.8173061928262295\n",
      "two-layer Loss after iteration 53000: 3.8145626524534744\n",
      "two-layer Loss after iteration 54000: 3.812248640409511\n",
      "two-layer Loss after iteration 55000: 3.8098356898773\n",
      "two-layer Loss after iteration 56000: 3.807750668630255\n",
      "two-layer Loss after iteration 57000: 3.80539741391914\n",
      "two-layer Loss after iteration 58000: 3.803257626198373\n",
      "two-layer Loss after iteration 59000: 3.8013127011515513\n",
      "two-layer Loss after iteration 60000: 3.7993627862589427\n",
      "two-layer Loss after iteration 61000: 3.797348631906645\n",
      "two-layer Loss after iteration 62000: 3.795675180517916\n",
      "two-layer Loss after iteration 63000: 3.7937309753579678\n",
      "two-layer Loss after iteration 64000: 3.7919308491100487\n",
      "two-layer Loss after iteration 65000: 3.790463013862595\n",
      "two-layer Loss after iteration 66000: 3.7886868591845553\n",
      "two-layer Loss after iteration 67000: 3.7870056144041566\n",
      "two-layer Loss after iteration 68000: 3.785712487472094\n",
      "two-layer Loss after iteration 69000: 3.7838427163958395\n",
      "two-layer Loss after iteration 70000: 3.7824210446085202\n",
      "two-layer Loss after iteration 71000: 3.7811009081020437\n",
      "two-layer Loss after iteration 72000: 3.7795548683846603\n",
      "two-layer Loss after iteration 73000: 3.7783656350209256\n",
      "two-layer Loss after iteration 74000: 3.777147906747903\n",
      "two-layer Loss after iteration 75000: 3.7757603984635293\n",
      "two-layer Loss after iteration 76000: 3.774531608606531\n",
      "two-layer Loss after iteration 77000: 3.7733552028889052\n",
      "two-layer Loss after iteration 78000: 3.7722083562883464\n",
      "two-layer Loss after iteration 79000: 3.7712353224243067\n",
      "two-layer Loss after iteration 80000: 3.770106759751368\n",
      "two-layer Loss after iteration 81000: 3.7690202387489564\n",
      "two-layer Loss after iteration 82000: 3.7681334138856277\n",
      "two-layer Loss after iteration 83000: 3.7670939096248537\n",
      "two-layer Loss after iteration 84000: 3.7661393678368777\n",
      "two-layer Loss after iteration 85000: 3.7652916672627987\n",
      "two-layer Loss after iteration 86000: 3.7644456170008778\n",
      "two-layer Loss after iteration 87000: 3.763468319240784\n",
      "two-layer Loss after iteration 88000: 3.7627438738803014\n",
      "two-layer Loss after iteration 89000: 3.7618799895896897\n",
      "two-layer Loss after iteration 90000: 3.7613083231155824\n",
      "two-layer Loss after iteration 91000: 3.7603361294030058\n",
      "two-layer Loss after iteration 92000: 3.7596130022994223\n",
      "two-layer Loss after iteration 93000: 3.759091711111238\n",
      "two-layer Loss after iteration 94000: 3.7581885674046065\n",
      "two-layer Loss after iteration 95000: 3.7575139845169043\n",
      "two-layer Loss after iteration 96000: 3.756898820131251\n",
      "two-layer Loss after iteration 97000: 3.7562610311519853\n",
      "two-layer Loss after iteration 98000: 3.755675553475503\n",
      "two-layer Loss after iteration 99000: 3.755064348507962\n",
      "two-layer Loss after iteration 0: 1533.7159190242019\n",
      "two-layer Loss after iteration 1000: 23.555322143988256\n",
      "two-layer Loss after iteration 2000: 9.041584599388175\n",
      "two-layer Loss after iteration 3000: 7.311450088257959\n",
      "two-layer Loss after iteration 4000: 6.45367856723549\n",
      "two-layer Loss after iteration 5000: 5.382961551816846\n",
      "two-layer Loss after iteration 6000: 5.011082312824526\n",
      "two-layer Loss after iteration 7000: 4.734725466942774\n",
      "two-layer Loss after iteration 8000: 4.544542035793415\n",
      "two-layer Loss after iteration 9000: 4.4012179965126\n",
      "two-layer Loss after iteration 10000: 4.284652512370064\n",
      "two-layer Loss after iteration 11000: 4.148421605055649\n",
      "two-layer Loss after iteration 12000: 4.049053416297957\n",
      "two-layer Loss after iteration 13000: 3.9517683887267654\n",
      "two-layer Loss after iteration 14000: 3.858543585190476\n",
      "two-layer Loss after iteration 15000: 3.7844572732164603\n",
      "two-layer Loss after iteration 16000: 3.7251208845620725\n",
      "two-layer Loss after iteration 17000: 3.66940938495661\n",
      "two-layer Loss after iteration 18000: 3.624492363842044\n",
      "two-layer Loss after iteration 19000: 3.5832654645013666\n",
      "two-layer Loss after iteration 20000: 3.549663978468631\n",
      "two-layer Loss after iteration 21000: 3.522454284583576\n",
      "two-layer Loss after iteration 22000: 3.5002327186765605\n",
      "two-layer Loss after iteration 23000: 3.481260354423219\n",
      "two-layer Loss after iteration 24000: 3.462793704078081\n",
      "two-layer Loss after iteration 25000: 3.4478225830473406\n",
      "two-layer Loss after iteration 26000: 3.4355744345680668\n",
      "two-layer Loss after iteration 27000: 3.4254609581143383\n",
      "two-layer Loss after iteration 28000: 3.4169647277620547\n",
      "two-layer Loss after iteration 29000: 3.4092117299764326\n",
      "two-layer Loss after iteration 30000: 3.402300796825785\n",
      "two-layer Loss after iteration 31000: 3.39660179008287\n",
      "two-layer Loss after iteration 32000: 3.391336612511558\n",
      "two-layer Loss after iteration 33000: 3.3869148648299623\n",
      "two-layer Loss after iteration 34000: 3.383177475998656\n",
      "two-layer Loss after iteration 35000: 3.3799867982980727\n",
      "two-layer Loss after iteration 36000: 3.358722800296276\n",
      "two-layer Loss after iteration 37000: 3.3518316235310346\n",
      "two-layer Loss after iteration 38000: 3.3467083670127624\n",
      "two-layer Loss after iteration 39000: 3.342373017966309\n",
      "two-layer Loss after iteration 40000: 3.3387695527674968\n",
      "two-layer Loss after iteration 41000: 3.3359352107602067\n",
      "two-layer Loss after iteration 42000: 3.3337149723254473\n",
      "two-layer Loss after iteration 43000: 3.3319299113815055\n",
      "two-layer Loss after iteration 44000: 3.330483284845166\n",
      "two-layer Loss after iteration 45000: 3.329086661809019\n",
      "two-layer Loss after iteration 46000: 3.32806188846276\n",
      "two-layer Loss after iteration 47000: 3.32725730990838\n",
      "two-layer Loss after iteration 48000: 3.326568444123415\n",
      "two-layer Loss after iteration 49000: 3.3259925354366158\n",
      "two-layer Loss after iteration 50000: 3.325472383056714\n",
      "two-layer Loss after iteration 51000: 3.3249621288347075\n",
      "two-layer Loss after iteration 52000: 3.324517815710922\n",
      "two-layer Loss after iteration 53000: 3.3241477489887763\n",
      "two-layer Loss after iteration 54000: 3.323821306313002\n",
      "9.82034194699653e-05 3.3241477489887763 3.323821306313002\n",
      "two-layer Loss after iteration 0: 1614.6664559097762\n",
      "two-layer Loss after iteration 1000: 23.937221693872765\n",
      "two-layer Loss after iteration 2000: 9.211936119056514\n",
      "two-layer Loss after iteration 3000: 6.974808956331856\n",
      "two-layer Loss after iteration 4000: 5.872735581229898\n",
      "two-layer Loss after iteration 5000: 5.14353285523284\n",
      "two-layer Loss after iteration 6000: 4.592815915074498\n",
      "two-layer Loss after iteration 7000: 3.4370803669348264\n",
      "two-layer Loss after iteration 8000: 3.2930148690107224\n",
      "two-layer Loss after iteration 9000: 2.9985839277767248\n",
      "two-layer Loss after iteration 10000: 2.834965427101409\n",
      "two-layer Loss after iteration 11000: 2.7424335604454972\n",
      "two-layer Loss after iteration 12000: 2.678539997626101\n",
      "two-layer Loss after iteration 13000: 2.6122946411606605\n",
      "two-layer Loss after iteration 14000: 2.564235025819678\n",
      "two-layer Loss after iteration 15000: 2.5279823481855086\n",
      "two-layer Loss after iteration 16000: 2.498375484029264\n",
      "two-layer Loss after iteration 17000: 2.4786202481245625\n",
      "two-layer Loss after iteration 18000: 2.462088038580509\n",
      "two-layer Loss after iteration 19000: 2.4510444551982777\n",
      "two-layer Loss after iteration 20000: 2.442837214156806\n",
      "two-layer Loss after iteration 21000: 2.4340321645728586\n",
      "two-layer Loss after iteration 22000: 2.427906760419047\n",
      "two-layer Loss after iteration 23000: 2.4238548046148893\n",
      "two-layer Loss after iteration 24000: 2.420612488438353\n",
      "two-layer Loss after iteration 25000: 2.4178139423708287\n",
      "two-layer Loss after iteration 26000: 2.415358057778645\n",
      "two-layer Loss after iteration 27000: 2.4131380768724755\n",
      "two-layer Loss after iteration 28000: 2.411100071247161\n",
      "two-layer Loss after iteration 29000: 2.409219894186477\n",
      "two-layer Loss after iteration 30000: 2.407502771339658\n",
      "two-layer Loss after iteration 31000: 2.405949567630335\n",
      "two-layer Loss after iteration 32000: 2.404459761900857\n",
      "two-layer Loss after iteration 33000: 2.4030766463567876\n",
      "two-layer Loss after iteration 34000: 2.401758861348742\n",
      "two-layer Loss after iteration 35000: 2.400465842177371\n",
      "two-layer Loss after iteration 36000: 2.3992856774036784\n",
      "two-layer Loss after iteration 37000: 2.398080565270114\n",
      "two-layer Loss after iteration 38000: 2.396919521894845\n",
      "two-layer Loss after iteration 39000: 2.3958513687645646\n",
      "two-layer Loss after iteration 40000: 2.3947589691209195\n",
      "two-layer Loss after iteration 41000: 2.393759898657653\n",
      "two-layer Loss after iteration 42000: 2.3928596396700867\n",
      "two-layer Loss after iteration 43000: 2.391660456298023\n",
      "two-layer Loss after iteration 44000: 2.390398363673319\n",
      "two-layer Loss after iteration 45000: 2.3890611194394955\n",
      "two-layer Loss after iteration 46000: 2.3879241297379727\n",
      "two-layer Loss after iteration 47000: 2.386977983978461\n",
      "two-layer Loss after iteration 48000: 2.386094751772447\n",
      "two-layer Loss after iteration 49000: 2.385173292016991\n",
      "two-layer Loss after iteration 50000: 2.384420681708993\n",
      "two-layer Loss after iteration 51000: 2.3835287109626857\n",
      "two-layer Loss after iteration 52000: 2.3826936976568747\n",
      "two-layer Loss after iteration 53000: 2.382044967446961\n",
      "two-layer Loss after iteration 54000: 2.3811796795090596\n",
      "two-layer Loss after iteration 55000: 2.3804275267651964\n",
      "two-layer Loss after iteration 56000: 2.379708911279759\n",
      "two-layer Loss after iteration 57000: 2.3789773970985255\n",
      "two-layer Loss after iteration 58000: 2.378271466526145\n",
      "two-layer Loss after iteration 59000: 2.377711743833287\n",
      "two-layer Loss after iteration 60000: 2.3769789180923495\n",
      "two-layer Loss after iteration 61000: 2.37631241615357\n",
      "two-layer Loss after iteration 62000: 2.3755499543378695\n",
      "two-layer Loss after iteration 63000: 2.3749982691340423\n",
      "two-layer Loss after iteration 64000: 2.3742711021908045\n",
      "two-layer Loss after iteration 65000: 2.3736802198627913\n",
      "two-layer Loss after iteration 66000: 2.373038449045202\n",
      "two-layer Loss after iteration 67000: 2.372462403123741\n",
      "two-layer Loss after iteration 68000: 2.372099984425727\n",
      "two-layer Loss after iteration 69000: 2.371404090072962\n",
      "two-layer Loss after iteration 70000: 2.370817251914453\n",
      "two-layer Loss after iteration 71000: 2.370142744425787\n",
      "two-layer Loss after iteration 72000: 2.369584700497277\n",
      "two-layer Loss after iteration 73000: 2.3690401783302497\n",
      "two-layer Loss after iteration 74000: 2.3684853057007023\n",
      "two-layer Loss after iteration 75000: 2.3679656506663074\n",
      "two-layer Loss after iteration 76000: 2.3674289148540204\n",
      "two-layer Loss after iteration 77000: 2.366908052116027\n",
      "two-layer Loss after iteration 78000: 2.366268417195498\n",
      "two-layer Loss after iteration 79000: 2.3657221886358264\n",
      "two-layer Loss after iteration 80000: 2.3656330077323857\n",
      "3.769711586131043e-05 2.3657221886358264 2.3656330077323857\n",
      "two-layer Loss after iteration 0: 1464.3166809359575\n",
      "two-layer Loss after iteration 1000: 24.31023361986448\n",
      "two-layer Loss after iteration 2000: 9.688234931748388\n",
      "two-layer Loss after iteration 3000: 7.095748459870435\n",
      "two-layer Loss after iteration 4000: 5.712441905719376\n",
      "two-layer Loss after iteration 5000: 4.497210803129966\n",
      "two-layer Loss after iteration 6000: 4.097861989570016\n",
      "two-layer Loss after iteration 7000: 3.8643562036705053\n",
      "two-layer Loss after iteration 8000: 3.4341085200216974\n",
      "two-layer Loss after iteration 9000: 3.2832496718069977\n",
      "two-layer Loss after iteration 10000: 3.1730347218837345\n",
      "two-layer Loss after iteration 11000: 3.089217103652896\n",
      "two-layer Loss after iteration 12000: 3.037609364786718\n",
      "two-layer Loss after iteration 13000: 3.004611137702975\n",
      "two-layer Loss after iteration 14000: 2.9761539168490687\n",
      "two-layer Loss after iteration 15000: 2.9559104155357314\n",
      "two-layer Loss after iteration 16000: 2.941606520565847\n",
      "two-layer Loss after iteration 17000: 2.9291095295893124\n",
      "two-layer Loss after iteration 18000: 2.9196071994462676\n",
      "two-layer Loss after iteration 19000: 2.913510860251821\n",
      "two-layer Loss after iteration 20000: 2.908698381069861\n",
      "two-layer Loss after iteration 21000: 2.9047664078618536\n",
      "two-layer Loss after iteration 22000: 2.901488764649473\n",
      "two-layer Loss after iteration 23000: 2.8987124614361415\n",
      "two-layer Loss after iteration 24000: 2.8963317057726727\n",
      "two-layer Loss after iteration 25000: 2.8935205102340555\n",
      "two-layer Loss after iteration 26000: 2.89091847611522\n",
      "two-layer Loss after iteration 27000: 2.8889887891770174\n",
      "two-layer Loss after iteration 28000: 2.8875244095688344\n",
      "two-layer Loss after iteration 29000: 2.886374229518079\n",
      "two-layer Loss after iteration 30000: 2.8854180813608963\n",
      "two-layer Loss after iteration 31000: 2.8846582194178074\n",
      "two-layer Loss after iteration 32000: 2.8840194749245507\n",
      "two-layer Loss after iteration 33000: 2.883517131096336\n",
      "two-layer Loss after iteration 34000: 2.8830574896929533\n",
      "two-layer Loss after iteration 35000: 2.8826856895571535\n",
      "two-layer Loss after iteration 36000: 2.8823775147700057\n",
      "two-layer Loss after iteration 37000: 2.8821372454077623\n",
      "8.335804765760843e-05 2.8823775147700057 2.8821372454077623\n",
      "two-layer Loss after iteration 0: 1542.5753712061455\n",
      "two-layer Loss after iteration 1000: 25.140398356264033\n",
      "two-layer Loss after iteration 2000: 10.386190197270885\n",
      "two-layer Loss after iteration 3000: 7.591581852265964\n",
      "two-layer Loss after iteration 4000: 6.783531326725813\n",
      "two-layer Loss after iteration 5000: 6.475733708990434\n",
      "two-layer Loss after iteration 6000: 6.315202837619169\n",
      "two-layer Loss after iteration 7000: 6.1797822334163355\n",
      "two-layer Loss after iteration 8000: 5.951576361079165\n",
      "two-layer Loss after iteration 9000: 5.706963895540952\n",
      "two-layer Loss after iteration 10000: 5.434749106479486\n",
      "two-layer Loss after iteration 11000: 4.885919946732479\n",
      "two-layer Loss after iteration 12000: 4.6403272438529335\n",
      "two-layer Loss after iteration 13000: 4.412469395239083\n",
      "two-layer Loss after iteration 14000: 4.251290468877511\n",
      "two-layer Loss after iteration 15000: 4.011984087598854\n",
      "two-layer Loss after iteration 16000: 3.7939650164033383\n",
      "two-layer Loss after iteration 17000: 3.3414403368489576\n",
      "two-layer Loss after iteration 18000: 2.9490337623528204\n",
      "two-layer Loss after iteration 19000: 2.748370843066828\n",
      "two-layer Loss after iteration 20000: 2.7124225548776364\n",
      "two-layer Loss after iteration 21000: 2.678537600057263\n",
      "two-layer Loss after iteration 22000: 2.6470303315467762\n",
      "two-layer Loss after iteration 23000: 2.6281635924676348\n",
      "two-layer Loss after iteration 24000: 2.6092169263534943\n",
      "two-layer Loss after iteration 25000: 2.5677452485678818\n",
      "two-layer Loss after iteration 26000: 2.473641372635069\n",
      "two-layer Loss after iteration 27000: 2.4000292593895978\n",
      "two-layer Loss after iteration 28000: 2.326665812098022\n",
      "two-layer Loss after iteration 29000: 2.255151221037171\n",
      "two-layer Loss after iteration 30000: 2.2005334784549357\n",
      "two-layer Loss after iteration 31000: 2.1703847554637954\n",
      "two-layer Loss after iteration 32000: 2.1533430032837373\n",
      "two-layer Loss after iteration 33000: 2.1401455859742833\n",
      "two-layer Loss after iteration 34000: 2.124081258470319\n",
      "two-layer Loss after iteration 35000: 2.1049909091750334\n",
      "two-layer Loss after iteration 36000: 2.084119599732694\n",
      "two-layer Loss after iteration 37000: 2.062962100805351\n",
      "two-layer Loss after iteration 38000: 2.035907005094896\n",
      "two-layer Loss after iteration 39000: 2.012695901978586\n",
      "two-layer Loss after iteration 40000: 1.9888790503553022\n",
      "two-layer Loss after iteration 41000: 1.967700955553408\n",
      "two-layer Loss after iteration 42000: 1.9438508421673102\n",
      "two-layer Loss after iteration 43000: 1.9184260312612489\n",
      "two-layer Loss after iteration 44000: 1.8995662024565838\n",
      "two-layer Loss after iteration 45000: 1.8793032495899211\n",
      "two-layer Loss after iteration 46000: 1.860944560312946\n",
      "two-layer Loss after iteration 47000: 1.8488382130053236\n",
      "two-layer Loss after iteration 48000: 1.841214404101701\n",
      "two-layer Loss after iteration 49000: 1.8365519966234567\n",
      "two-layer Loss after iteration 50000: 1.833514673740654\n",
      "two-layer Loss after iteration 51000: 1.8315159188430208\n",
      "two-layer Loss after iteration 52000: 1.8301915478676474\n",
      "two-layer Loss after iteration 53000: 1.8293058223670107\n",
      "two-layer Loss after iteration 54000: 1.8287070144589852\n",
      "two-layer Loss after iteration 55000: 1.8281133428370242\n",
      "two-layer Loss after iteration 56000: 1.8272761824921009\n",
      "two-layer Loss after iteration 57000: 1.8266787685177557\n",
      "two-layer Loss after iteration 58000: 1.8262306856202122\n",
      "two-layer Loss after iteration 59000: 1.8258833982721598\n",
      "two-layer Loss after iteration 60000: 1.825601617163789\n",
      "two-layer Loss after iteration 61000: 1.8253663020673543\n",
      "two-layer Loss after iteration 62000: 1.8251612214325004\n",
      "two-layer Loss after iteration 63000: 1.8249821216792543\n",
      "9.81281823999462e-05 1.8251612214325004 1.8249821216792543\n",
      "two-layer Loss after iteration 0: 1693.5658252199062\n",
      "two-layer Loss after iteration 1000: 23.91621346488522\n",
      "two-layer Loss after iteration 2000: 9.704391608191221\n",
      "two-layer Loss after iteration 3000: 7.933619428137989\n",
      "two-layer Loss after iteration 4000: 6.910458438520709\n",
      "two-layer Loss after iteration 5000: 6.554142573936626\n",
      "two-layer Loss after iteration 6000: 6.343113137325411\n",
      "two-layer Loss after iteration 7000: 6.193122664171147\n",
      "two-layer Loss after iteration 8000: 6.061640923857626\n",
      "two-layer Loss after iteration 9000: 5.957097542745976\n",
      "two-layer Loss after iteration 10000: 5.665382343623124\n",
      "two-layer Loss after iteration 11000: 5.5566305921576635\n",
      "two-layer Loss after iteration 12000: 5.49492766700719\n",
      "two-layer Loss after iteration 13000: 5.447663716215622\n",
      "two-layer Loss after iteration 14000: 5.4158883324659985\n",
      "two-layer Loss after iteration 15000: 5.394005095430378\n",
      "two-layer Loss after iteration 16000: 5.3770932982001005\n",
      "two-layer Loss after iteration 17000: 5.3602119222965445\n",
      "two-layer Loss after iteration 18000: 5.341943554499728\n",
      "two-layer Loss after iteration 19000: 5.330493377981005\n",
      "two-layer Loss after iteration 20000: 5.322763272485982\n",
      "two-layer Loss after iteration 21000: 5.311709363076924\n",
      "two-layer Loss after iteration 22000: 5.271483613922086\n",
      "two-layer Loss after iteration 23000: 5.140568567273229\n",
      "two-layer Loss after iteration 24000: 4.96503207122006\n",
      "two-layer Loss after iteration 25000: 4.885854864066829\n",
      "two-layer Loss after iteration 26000: 4.834146265052403\n",
      "two-layer Loss after iteration 27000: 4.792696240367203\n",
      "two-layer Loss after iteration 28000: 4.757245368176045\n",
      "two-layer Loss after iteration 29000: 4.727343957986383\n",
      "two-layer Loss after iteration 30000: 4.701736619608273\n",
      "two-layer Loss after iteration 31000: 4.671262585621389\n",
      "two-layer Loss after iteration 32000: 4.652787261491004\n",
      "two-layer Loss after iteration 33000: 4.6450880748423655\n",
      "two-layer Loss after iteration 34000: 4.6383343116039235\n",
      "two-layer Loss after iteration 35000: 4.632063231413189\n",
      "two-layer Loss after iteration 36000: 4.626503998301175\n",
      "two-layer Loss after iteration 37000: 4.621444068779319\n",
      "two-layer Loss after iteration 38000: 4.616749928952527\n",
      "two-layer Loss after iteration 39000: 4.6080337506575475\n",
      "two-layer Loss after iteration 40000: 4.601563880022377\n",
      "two-layer Loss after iteration 41000: 4.595810155400378\n",
      "two-layer Loss after iteration 42000: 4.5909048532935515\n",
      "two-layer Loss after iteration 43000: 4.586833197225977\n",
      "two-layer Loss after iteration 44000: 4.5833199660526205\n",
      "two-layer Loss after iteration 45000: 4.580030562862664\n",
      "two-layer Loss after iteration 46000: 4.57693073920028\n",
      "two-layer Loss after iteration 47000: 4.5740061300668176\n",
      "two-layer Loss after iteration 48000: 4.571247092323237\n",
      "two-layer Loss after iteration 49000: 4.568650103445634\n",
      "two-layer Loss after iteration 50000: 4.566212545236183\n",
      "two-layer Loss after iteration 51000: 4.563932611036437\n",
      "two-layer Loss after iteration 52000: 4.561807986498491\n",
      "two-layer Loss after iteration 53000: 4.559835677847256\n",
      "two-layer Loss after iteration 54000: 4.558011863774886\n",
      "two-layer Loss after iteration 55000: 4.5563317471058244\n",
      "two-layer Loss after iteration 56000: 4.554789610885453\n",
      "two-layer Loss after iteration 57000: 4.5533789621349925\n",
      "two-layer Loss after iteration 58000: 4.552093318766957\n",
      "two-layer Loss after iteration 59000: 4.550924732663286\n",
      "two-layer Loss after iteration 60000: 4.549866279231295\n",
      "two-layer Loss after iteration 61000: 4.548909380897406\n",
      "two-layer Loss after iteration 62000: 4.548047259936162\n",
      "two-layer Loss after iteration 63000: 4.54727135624371\n",
      "two-layer Loss after iteration 64000: 4.546575424527608\n",
      "two-layer Loss after iteration 65000: 4.545952678453559\n",
      "two-layer Loss after iteration 66000: 4.545395107850344\n",
      "two-layer Loss after iteration 67000: 4.544896302687995\n",
      "two-layer Loss after iteration 68000: 4.544452063669773\n",
      "9.774458835491041e-05 4.544896302687995 4.544452063669773\n",
      "two-layer Loss after iteration 0: 1578.7466439756881\n",
      "two-layer Loss after iteration 1000: 24.61941169532665\n",
      "two-layer Loss after iteration 2000: 10.377903923298598\n",
      "two-layer Loss after iteration 3000: 7.853447053105065\n",
      "two-layer Loss after iteration 4000: 6.84355031354135\n",
      "two-layer Loss after iteration 5000: 6.313995580655414\n",
      "two-layer Loss after iteration 6000: 5.97471422790695\n",
      "two-layer Loss after iteration 7000: 5.744455674174022\n",
      "two-layer Loss after iteration 8000: 5.5703324030129355\n",
      "two-layer Loss after iteration 9000: 5.3150451481741126\n",
      "two-layer Loss after iteration 10000: 5.158758840839945\n",
      "two-layer Loss after iteration 11000: 5.034094998764739\n",
      "two-layer Loss after iteration 12000: 4.9081003948345705\n",
      "two-layer Loss after iteration 13000: 4.785209219588764\n",
      "two-layer Loss after iteration 14000: 4.659568334795744\n",
      "two-layer Loss after iteration 15000: 4.569418355812128\n",
      "two-layer Loss after iteration 16000: 4.4770086554794535\n",
      "two-layer Loss after iteration 17000: 4.406191207070014\n",
      "two-layer Loss after iteration 18000: 4.350519547055659\n",
      "two-layer Loss after iteration 19000: 4.3047198640711235\n",
      "two-layer Loss after iteration 20000: 4.256967113227202\n",
      "two-layer Loss after iteration 21000: 4.221846313118983\n",
      "two-layer Loss after iteration 22000: 4.193424762984256\n",
      "two-layer Loss after iteration 23000: 4.150422130155535\n",
      "two-layer Loss after iteration 24000: 4.104355672256074\n",
      "two-layer Loss after iteration 25000: 3.931438502322553\n",
      "two-layer Loss after iteration 26000: 3.860520986428724\n",
      "two-layer Loss after iteration 27000: 3.8035506761183355\n",
      "two-layer Loss after iteration 28000: 3.75541985477267\n",
      "two-layer Loss after iteration 29000: 3.7218181573981033\n",
      "two-layer Loss after iteration 30000: 3.701877193278859\n",
      "two-layer Loss after iteration 31000: 3.6863759897842248\n",
      "two-layer Loss after iteration 32000: 3.673553003438768\n",
      "two-layer Loss after iteration 33000: 3.6627864379489594\n",
      "two-layer Loss after iteration 34000: 3.653705044785489\n",
      "two-layer Loss after iteration 35000: 3.6460208577365414\n",
      "two-layer Loss after iteration 36000: 3.639502565006351\n",
      "two-layer Loss after iteration 37000: 3.6339614275442194\n",
      "two-layer Loss after iteration 38000: 3.6292423315774824\n",
      "two-layer Loss after iteration 39000: 3.6252173088832906\n",
      "two-layer Loss after iteration 40000: 3.6217797243509406\n",
      "two-layer Loss after iteration 41000: 3.6188406435098286\n",
      "two-layer Loss after iteration 42000: 3.6163255366828677\n",
      "two-layer Loss after iteration 43000: 3.474619128118034\n",
      "two-layer Loss after iteration 44000: 3.077268518613174\n",
      "two-layer Loss after iteration 45000: 2.8895827797762172\n",
      "two-layer Loss after iteration 46000: 2.835297482584902\n",
      "two-layer Loss after iteration 47000: 2.7973291914665968\n",
      "two-layer Loss after iteration 48000: 2.7664402891124995\n",
      "two-layer Loss after iteration 49000: 2.7409517198403357\n",
      "two-layer Loss after iteration 50000: 2.7173573698643434\n",
      "two-layer Loss after iteration 51000: 2.6907968079793245\n",
      "two-layer Loss after iteration 52000: 2.672082255844095\n",
      "two-layer Loss after iteration 53000: 2.6492382169835875\n",
      "two-layer Loss after iteration 54000: 2.634171328394455\n",
      "two-layer Loss after iteration 55000: 2.6221335847650202\n",
      "two-layer Loss after iteration 56000: 2.6119129279821034\n",
      "two-layer Loss after iteration 57000: 2.604765983241087\n",
      "two-layer Loss after iteration 58000: 2.5989141019127815\n",
      "two-layer Loss after iteration 59000: 2.593981011205742\n",
      "two-layer Loss after iteration 60000: 2.5897733416356057\n",
      "two-layer Loss after iteration 61000: 2.586151361317243\n",
      "two-layer Loss after iteration 62000: 2.58300350744176\n",
      "two-layer Loss after iteration 63000: 2.5802456702658665\n",
      "two-layer Loss after iteration 64000: 2.577808330446162\n",
      "two-layer Loss after iteration 65000: 2.575546289962617\n",
      "two-layer Loss after iteration 66000: 2.573111506099091\n",
      "two-layer Loss after iteration 67000: 2.5708464172736543\n",
      "two-layer Loss after iteration 68000: 2.5687288812851987\n",
      "two-layer Loss after iteration 69000: 2.566160462786778\n",
      "two-layer Loss after iteration 70000: 2.5632578405863753\n",
      "two-layer Loss after iteration 71000: 2.5604487025633413\n",
      "two-layer Loss after iteration 72000: 2.5572613152758725\n",
      "two-layer Loss after iteration 73000: 2.5538699033198333\n",
      "two-layer Loss after iteration 74000: 2.5518881890529483\n",
      "two-layer Loss after iteration 75000: 2.5500463019608905\n",
      "two-layer Loss after iteration 76000: 2.5484985425450457\n",
      "two-layer Loss after iteration 77000: 2.5470631219387734\n",
      "two-layer Loss after iteration 78000: 2.5457299135184734\n",
      "two-layer Loss after iteration 79000: 2.544468631760799\n",
      "two-layer Loss after iteration 80000: 2.5432590091261496\n",
      "two-layer Loss after iteration 81000: 2.541767820238854\n",
      "two-layer Loss after iteration 82000: 2.5399711087499\n",
      "two-layer Loss after iteration 83000: 2.538339909776348\n",
      "two-layer Loss after iteration 84000: 2.5368149389261636\n",
      "two-layer Loss after iteration 85000: 2.5353465761640677\n",
      "two-layer Loss after iteration 86000: 2.531727905090675\n",
      "two-layer Loss after iteration 87000: 2.528786256216653\n",
      "two-layer Loss after iteration 88000: 2.5261538338519496\n",
      "two-layer Loss after iteration 89000: 2.5229325354490975\n",
      "two-layer Loss after iteration 90000: 2.5203856082353338\n",
      "two-layer Loss after iteration 91000: 2.5181423280481594\n",
      "two-layer Loss after iteration 92000: 2.514806795720023\n",
      "two-layer Loss after iteration 93000: 2.5111641262853026\n",
      "two-layer Loss after iteration 94000: 2.508083961673728\n",
      "two-layer Loss after iteration 95000: 2.5066535899850875\n",
      "two-layer Loss after iteration 96000: 2.505426350410441\n",
      "two-layer Loss after iteration 97000: 2.5043225178646007\n",
      "two-layer Loss after iteration 98000: 2.5033284537970903\n",
      "two-layer Loss after iteration 99000: 2.502430270529242\n",
      "two-layer Loss after iteration 0: 1688.1640403773474\n",
      "two-layer Loss after iteration 1000: 24.016485481874582\n",
      "two-layer Loss after iteration 2000: 10.299941271176063\n",
      "two-layer Loss after iteration 3000: 8.78350385033844\n",
      "two-layer Loss after iteration 4000: 8.066519636105406\n",
      "two-layer Loss after iteration 5000: 6.66367600233534\n",
      "two-layer Loss after iteration 6000: 5.603056732354244\n",
      "two-layer Loss after iteration 7000: 4.747767648171266\n",
      "two-layer Loss after iteration 8000: 4.616107920696714\n",
      "two-layer Loss after iteration 9000: 4.564520416957498\n",
      "two-layer Loss after iteration 10000: 4.531054746908043\n",
      "two-layer Loss after iteration 11000: 4.509318446472741\n",
      "two-layer Loss after iteration 12000: 4.494391869564222\n",
      "two-layer Loss after iteration 13000: 4.483697683165625\n",
      "two-layer Loss after iteration 14000: 4.475741367016393\n",
      "two-layer Loss after iteration 15000: 4.468869443279496\n",
      "two-layer Loss after iteration 16000: 4.463596781265998\n",
      "two-layer Loss after iteration 17000: 4.458465906042304\n",
      "two-layer Loss after iteration 18000: 4.453528014684502\n",
      "two-layer Loss after iteration 19000: 4.448538521514815\n",
      "two-layer Loss after iteration 20000: 4.443259064586903\n",
      "two-layer Loss after iteration 21000: 4.435778883927342\n",
      "two-layer Loss after iteration 22000: 4.3987280822043076\n",
      "two-layer Loss after iteration 23000: 4.380410069926446\n",
      "two-layer Loss after iteration 24000: 4.3605800328350925\n",
      "two-layer Loss after iteration 25000: 4.337965660548031\n",
      "two-layer Loss after iteration 26000: 4.313318011524475\n",
      "two-layer Loss after iteration 27000: 4.288047230934191\n",
      "two-layer Loss after iteration 28000: 4.262288542164372\n",
      "two-layer Loss after iteration 29000: 4.20327166693373\n",
      "two-layer Loss after iteration 30000: 4.160729834242472\n",
      "two-layer Loss after iteration 31000: 4.12801837813678\n",
      "two-layer Loss after iteration 32000: 4.099944567174573\n",
      "two-layer Loss after iteration 33000: 4.072557668977502\n",
      "two-layer Loss after iteration 34000: 4.041908354045645\n",
      "two-layer Loss after iteration 35000: 4.020214400489006\n",
      "two-layer Loss after iteration 36000: 4.004690795120569\n",
      "two-layer Loss after iteration 37000: 3.9931712981627148\n",
      "two-layer Loss after iteration 38000: 3.978936917939882\n",
      "two-layer Loss after iteration 39000: 3.960823294503471\n",
      "two-layer Loss after iteration 40000: 3.945722147220872\n",
      "two-layer Loss after iteration 41000: 3.9349935038904893\n",
      "two-layer Loss after iteration 42000: 3.9258267081623677\n",
      "two-layer Loss after iteration 43000: 3.9175833680212433\n",
      "two-layer Loss after iteration 44000: 3.9087818538423584\n",
      "two-layer Loss after iteration 45000: 3.9008613354510357\n",
      "two-layer Loss after iteration 46000: 3.894073775717001\n",
      "two-layer Loss after iteration 47000: 3.8873041373955464\n",
      "two-layer Loss after iteration 48000: 3.8639439148634915\n",
      "two-layer Loss after iteration 49000: 3.785494855716409\n",
      "two-layer Loss after iteration 50000: 3.751581510686008\n",
      "two-layer Loss after iteration 51000: 3.73413346971758\n",
      "two-layer Loss after iteration 52000: 3.72399459451725\n",
      "two-layer Loss after iteration 53000: 3.7169925938340933\n",
      "two-layer Loss after iteration 54000: 3.7112384896949453\n",
      "two-layer Loss after iteration 55000: 3.7064863570131417\n",
      "two-layer Loss after iteration 56000: 3.7021354135658657\n",
      "two-layer Loss after iteration 57000: 3.697983806169859\n",
      "two-layer Loss after iteration 58000: 3.6940861566162773\n",
      "two-layer Loss after iteration 59000: 3.6903458382233083\n",
      "two-layer Loss after iteration 60000: 3.6867870783957177\n",
      "two-layer Loss after iteration 61000: 3.6831409274832105\n",
      "two-layer Loss after iteration 62000: 3.6800310275882637\n",
      "two-layer Loss after iteration 63000: 3.676843816986401\n",
      "two-layer Loss after iteration 64000: 3.6736792214762266\n",
      "two-layer Loss after iteration 65000: 3.670498228280489\n",
      "two-layer Loss after iteration 66000: 3.6675735064188344\n",
      "two-layer Loss after iteration 67000: 3.6647633288911923\n",
      "two-layer Loss after iteration 68000: 3.661999449886965\n",
      "two-layer Loss after iteration 69000: 3.659547161210992\n",
      "two-layer Loss after iteration 70000: 3.6568032334913316\n",
      "two-layer Loss after iteration 71000: 3.654320277674471\n",
      "two-layer Loss after iteration 72000: 3.651994418238929\n",
      "two-layer Loss after iteration 73000: 3.6496810958447865\n",
      "two-layer Loss after iteration 74000: 3.6473227310659624\n",
      "two-layer Loss after iteration 75000: 3.645137891570493\n",
      "two-layer Loss after iteration 76000: 3.643026646229915\n",
      "two-layer Loss after iteration 77000: 3.6409656938037096\n",
      "two-layer Loss after iteration 78000: 3.638962651626442\n",
      "two-layer Loss after iteration 79000: 3.637007493209596\n",
      "two-layer Loss after iteration 80000: 3.6350994390117455\n",
      "two-layer Loss after iteration 81000: 3.633231061649297\n",
      "two-layer Loss after iteration 82000: 3.6314160071079207\n",
      "two-layer Loss after iteration 83000: 3.6296392912206996\n",
      "two-layer Loss after iteration 84000: 3.6279009317369013\n",
      "two-layer Loss after iteration 85000: 3.626204716850847\n",
      "two-layer Loss after iteration 86000: 3.624555452123984\n",
      "two-layer Loss after iteration 87000: 3.622941242828367\n",
      "two-layer Loss after iteration 88000: 3.6213621517389907\n",
      "two-layer Loss after iteration 89000: 3.619819488165661\n",
      "two-layer Loss after iteration 90000: 3.6183174949072634\n",
      "two-layer Loss after iteration 91000: 3.6168518025142444\n",
      "two-layer Loss after iteration 92000: 3.6154180647191594\n",
      "two-layer Loss after iteration 93000: 3.614018871621745\n",
      "two-layer Loss after iteration 94000: 3.6126547601505776\n",
      "two-layer Loss after iteration 95000: 3.6113216712579668\n",
      "two-layer Loss after iteration 96000: 3.6100187363217224\n",
      "two-layer Loss after iteration 97000: 3.608749336209319\n",
      "two-layer Loss after iteration 98000: 3.607510838500113\n",
      "two-layer Loss after iteration 99000: 3.6063009618568547\n",
      "two-layer Loss after iteration 0: 1563.3929801117818\n",
      "two-layer Loss after iteration 1000: 24.80680952704889\n",
      "two-layer Loss after iteration 2000: 10.1979385186287\n",
      "two-layer Loss after iteration 3000: 8.294415230594296\n",
      "two-layer Loss after iteration 4000: 7.730063411931964\n",
      "two-layer Loss after iteration 5000: 7.302206340763085\n",
      "two-layer Loss after iteration 6000: 7.0143202369825355\n",
      "two-layer Loss after iteration 7000: 6.783108957707423\n",
      "two-layer Loss after iteration 8000: 6.592552448258731\n",
      "two-layer Loss after iteration 9000: 6.41632166066992\n",
      "two-layer Loss after iteration 10000: 6.266772405810323\n",
      "two-layer Loss after iteration 11000: 6.134535189618621\n",
      "two-layer Loss after iteration 12000: 6.008805041666237\n",
      "two-layer Loss after iteration 13000: 5.904783478723602\n",
      "two-layer Loss after iteration 14000: 5.810803243614473\n",
      "two-layer Loss after iteration 15000: 5.730124401066405\n",
      "two-layer Loss after iteration 16000: 5.586627294959523\n",
      "two-layer Loss after iteration 17000: 4.969343253589702\n",
      "two-layer Loss after iteration 18000: 4.59853850621693\n",
      "two-layer Loss after iteration 19000: 4.263958170774803\n",
      "two-layer Loss after iteration 20000: 4.031749207421873\n",
      "two-layer Loss after iteration 21000: 3.897066540983236\n",
      "two-layer Loss after iteration 22000: 3.798840694680339\n",
      "two-layer Loss after iteration 23000: 3.7246509050588914\n",
      "two-layer Loss after iteration 24000: 3.6654788448081277\n",
      "two-layer Loss after iteration 25000: 3.6217223285366233\n",
      "two-layer Loss after iteration 26000: 3.5897043605141774\n",
      "two-layer Loss after iteration 27000: 3.5656332294309747\n",
      "two-layer Loss after iteration 28000: 3.547040795394134\n",
      "two-layer Loss after iteration 29000: 3.532539236237812\n",
      "two-layer Loss after iteration 30000: 3.521130186548048\n",
      "two-layer Loss after iteration 31000: 3.505608608979394\n",
      "two-layer Loss after iteration 32000: 3.4970281764505415\n",
      "two-layer Loss after iteration 33000: 3.4902624902468986\n",
      "two-layer Loss after iteration 34000: 3.4847539578828406\n",
      "two-layer Loss after iteration 35000: 3.48021232419686\n",
      "two-layer Loss after iteration 36000: 3.4764369606104215\n",
      "two-layer Loss after iteration 37000: 3.473275387328583\n",
      "two-layer Loss after iteration 38000: 3.4707509638351084\n",
      "two-layer Loss after iteration 39000: 3.468798960309459\n",
      "two-layer Loss after iteration 40000: 3.467233288640386\n",
      "two-layer Loss after iteration 41000: 3.465937930629045\n",
      "two-layer Loss after iteration 42000: 3.464863733532632\n",
      "two-layer Loss after iteration 43000: 3.463946739735976\n",
      "two-layer Loss after iteration 44000: 3.4631874013896\n",
      "two-layer Loss after iteration 45000: 3.4625277086935404\n",
      "two-layer Loss after iteration 46000: 3.4619793923477697\n",
      "two-layer Loss after iteration 47000: 3.460386925027602\n",
      "two-layer Loss after iteration 48000: 3.459538753122882\n",
      "two-layer Loss after iteration 49000: 3.4589680225453376\n",
      "two-layer Loss after iteration 50000: 3.4585437434695026\n",
      "two-layer Loss after iteration 51000: 3.458173323435366\n",
      "two-layer Loss after iteration 52000: 3.4578563644686366\n",
      "9.165502624792472e-05 3.458173323435366 3.4578563644686366\n",
      "two-layer Loss after iteration 0: 1547.7933466505958\n",
      "two-layer Loss after iteration 1000: 24.472310845339777\n",
      "two-layer Loss after iteration 2000: 9.533779180735984\n",
      "two-layer Loss after iteration 3000: 7.477902021407889\n",
      "two-layer Loss after iteration 4000: 7.002053951212365\n",
      "two-layer Loss after iteration 5000: 5.708712551175789\n",
      "two-layer Loss after iteration 6000: 4.861109765873558\n",
      "two-layer Loss after iteration 7000: 4.237435049780168\n",
      "two-layer Loss after iteration 8000: 4.0118079959364765\n",
      "two-layer Loss after iteration 9000: 3.8371033623323823\n",
      "two-layer Loss after iteration 10000: 3.738758613003241\n",
      "two-layer Loss after iteration 11000: 3.6544919241969303\n",
      "two-layer Loss after iteration 12000: 3.5794581841608957\n",
      "two-layer Loss after iteration 13000: 3.5224160874102926\n",
      "two-layer Loss after iteration 14000: 3.4624619038150612\n",
      "two-layer Loss after iteration 15000: 3.4139775598034623\n",
      "two-layer Loss after iteration 16000: 3.3761856475147964\n",
      "two-layer Loss after iteration 17000: 3.33964576777832\n",
      "two-layer Loss after iteration 18000: 3.2812671063520784\n",
      "two-layer Loss after iteration 19000: 3.048589403232774\n",
      "two-layer Loss after iteration 20000: 2.9561415064708116\n",
      "two-layer Loss after iteration 21000: 2.9090584867263383\n",
      "two-layer Loss after iteration 22000: 2.8790857889364427\n",
      "two-layer Loss after iteration 23000: 2.857714688918474\n",
      "two-layer Loss after iteration 24000: 2.832649576256061\n",
      "two-layer Loss after iteration 25000: 2.8052398974807016\n",
      "two-layer Loss after iteration 26000: 2.7525643629155905\n",
      "two-layer Loss after iteration 27000: 2.7143667485972958\n",
      "two-layer Loss after iteration 28000: 2.68913584368371\n",
      "two-layer Loss after iteration 29000: 2.669645233875339\n",
      "two-layer Loss after iteration 30000: 2.6277730882877792\n",
      "two-layer Loss after iteration 31000: 2.5953050173165635\n",
      "two-layer Loss after iteration 32000: 2.5697824689080235\n",
      "two-layer Loss after iteration 33000: 2.3321016996944293\n",
      "two-layer Loss after iteration 34000: 2.127973789359738\n",
      "two-layer Loss after iteration 35000: 2.0489632179260338\n",
      "two-layer Loss after iteration 36000: 1.9909561364036912\n",
      "two-layer Loss after iteration 37000: 1.9500250669832355\n",
      "two-layer Loss after iteration 38000: 1.9211375679665657\n",
      "two-layer Loss after iteration 39000: 1.9019230614670624\n",
      "two-layer Loss after iteration 40000: 1.887396316531221\n",
      "two-layer Loss after iteration 41000: 1.8605994836897126\n",
      "two-layer Loss after iteration 42000: 1.8462543171403538\n",
      "two-layer Loss after iteration 43000: 1.8365487000182128\n",
      "two-layer Loss after iteration 44000: 1.829647614920505\n",
      "two-layer Loss after iteration 45000: 1.8235815587399935\n",
      "two-layer Loss after iteration 46000: 1.8185325926261615\n",
      "two-layer Loss after iteration 47000: 1.8151560493269165\n",
      "two-layer Loss after iteration 48000: 1.796478187291241\n",
      "two-layer Loss after iteration 49000: 1.7890341969308121\n",
      "two-layer Loss after iteration 50000: 1.7848185686972202\n",
      "two-layer Loss after iteration 51000: 1.7817937392174237\n",
      "two-layer Loss after iteration 52000: 1.7794563634798\n",
      "two-layer Loss after iteration 53000: 1.7765188477097058\n",
      "two-layer Loss after iteration 54000: 1.7736842267053103\n",
      "two-layer Loss after iteration 55000: 1.7714977540277086\n",
      "two-layer Loss after iteration 56000: 1.769721386998447\n",
      "two-layer Loss after iteration 57000: 1.768239488553452\n",
      "two-layer Loss after iteration 58000: 1.766976593678464\n",
      "two-layer Loss after iteration 59000: 1.7658816594666769\n",
      "two-layer Loss after iteration 60000: 1.7649193567161776\n",
      "two-layer Loss after iteration 61000: 1.7640640280461075\n",
      "two-layer Loss after iteration 62000: 1.7632968133816929\n",
      "two-layer Loss after iteration 63000: 1.7626061798761108\n",
      "two-layer Loss after iteration 64000: 1.7619796663186869\n",
      "two-layer Loss after iteration 65000: 1.761409118941894\n",
      "two-layer Loss after iteration 66000: 1.7608894732778213\n",
      "two-layer Loss after iteration 67000: 1.7604135020635678\n",
      "two-layer Loss after iteration 68000: 1.759977205920121\n",
      "two-layer Loss after iteration 69000: 1.7595780369540568\n",
      "two-layer Loss after iteration 70000: 1.7592111587466164\n",
      "two-layer Loss after iteration 71000: 1.7551104173932608\n",
      "two-layer Loss after iteration 72000: 1.752790226307353\n",
      "two-layer Loss after iteration 73000: 1.7509244985769026\n",
      "two-layer Loss after iteration 74000: 1.7493627231431086\n",
      "two-layer Loss after iteration 75000: 1.7480243555069357\n",
      "two-layer Loss after iteration 76000: 1.7464947445355212\n",
      "two-layer Loss after iteration 77000: 1.745213217864182\n",
      "two-layer Loss after iteration 78000: 1.7440937336723585\n",
      "two-layer Loss after iteration 79000: 1.7431023544298432\n",
      "two-layer Loss after iteration 80000: 1.742213114874658\n",
      "two-layer Loss after iteration 81000: 1.7414101126393775\n",
      "two-layer Loss after iteration 82000: 1.7406806275403657\n",
      "two-layer Loss after iteration 83000: 1.7390319614081622\n",
      "two-layer Loss after iteration 84000: 1.7357718790546968\n",
      "two-layer Loss after iteration 85000: 1.733255704868011\n",
      "two-layer Loss after iteration 86000: 1.7311584634011847\n",
      "two-layer Loss after iteration 87000: 1.7293587007301905\n",
      "two-layer Loss after iteration 88000: 1.727780807344881\n",
      "two-layer Loss after iteration 89000: 1.7263783421557168\n",
      "two-layer Loss after iteration 90000: 1.7251185271481924\n",
      "two-layer Loss after iteration 91000: 1.7239767695263493\n",
      "two-layer Loss after iteration 92000: 1.722934250423973\n",
      "two-layer Loss after iteration 93000: 1.7219763206165968\n",
      "two-layer Loss after iteration 94000: 1.7210914003730158\n",
      "two-layer Loss after iteration 95000: 1.72027026685254\n",
      "two-layer Loss after iteration 96000: 1.7195081020122194\n",
      "two-layer Loss after iteration 97000: 1.71879964906816\n",
      "two-layer Loss after iteration 98000: 1.7143828275978321\n",
      "two-layer Loss after iteration 99000: 1.7114213029144902\n",
      "two-layer Loss after iteration 0: 1532.00826509808\n",
      "two-layer Loss after iteration 1000: 39.06097267102885\n",
      "two-layer Loss after iteration 2000: 26.68708890074211\n",
      "two-layer Loss after iteration 3000: 22.25435519269192\n",
      "two-layer Loss after iteration 4000: 20.49312067005402\n",
      "two-layer Loss after iteration 5000: 19.686211117681307\n",
      "two-layer Loss after iteration 6000: 19.055858050795624\n",
      "two-layer Loss after iteration 7000: 18.432206927548545\n",
      "two-layer Loss after iteration 8000: 17.800617372481465\n",
      "two-layer Loss after iteration 9000: 17.402756446556197\n",
      "two-layer Loss after iteration 10000: 16.890491509429108\n",
      "two-layer Loss after iteration 11000: 16.294847337241265\n",
      "two-layer Loss after iteration 12000: 15.752940005507686\n",
      "two-layer Loss after iteration 13000: 15.227267674490811\n",
      "two-layer Loss after iteration 14000: 14.692844594558668\n",
      "two-layer Loss after iteration 15000: 14.270484732183423\n",
      "two-layer Loss after iteration 16000: 13.830702389553862\n",
      "two-layer Loss after iteration 17000: 13.218654677993719\n",
      "two-layer Loss after iteration 18000: 12.754286753629838\n",
      "two-layer Loss after iteration 19000: 12.269163175202783\n",
      "two-layer Loss after iteration 20000: 11.955724349496675\n",
      "two-layer Loss after iteration 21000: 11.09857473648909\n",
      "two-layer Loss after iteration 22000: 10.506511495300163\n",
      "two-layer Loss after iteration 23000: 10.183432666507802\n",
      "two-layer Loss after iteration 24000: 9.95131464968234\n",
      "two-layer Loss after iteration 25000: 9.87091733494835\n",
      "two-layer Loss after iteration 26000: 9.853767787313206\n",
      "two-layer Loss after iteration 27000: 9.77910074303234\n",
      "two-layer Loss after iteration 28000: 9.71919563881274\n",
      "two-layer Loss after iteration 29000: 9.699845466525973\n",
      "two-layer Loss after iteration 30000: 9.693880442747204\n",
      "two-layer Loss after iteration 31000: 9.663937822068636\n",
      "two-layer Loss after iteration 32000: 9.652250826444183\n",
      "two-layer Loss after iteration 33000: 9.64214307616315\n",
      "two-layer Loss after iteration 34000: 9.59658745239811\n",
      "two-layer Loss after iteration 35000: 9.541246343599\n",
      "two-layer Loss after iteration 36000: 9.450043318293691\n",
      "two-layer Loss after iteration 37000: 9.407673510932645\n",
      "two-layer Loss after iteration 38000: 9.377115280332946\n",
      "two-layer Loss after iteration 39000: 9.341752214364472\n",
      "two-layer Loss after iteration 40000: 9.316979576222284\n",
      "two-layer Loss after iteration 41000: 9.129148064849055\n",
      "two-layer Loss after iteration 42000: 9.108258847347772\n",
      "two-layer Loss after iteration 43000: 9.094260916955722\n",
      "two-layer Loss after iteration 44000: 9.084622889626676\n",
      "two-layer Loss after iteration 45000: 9.080424944201422\n",
      "two-layer Loss after iteration 46000: 9.078041922251673\n",
      "two-layer Loss after iteration 47000: 9.076107991836595\n",
      "two-layer Loss after iteration 48000: 9.074338159933172\n",
      "two-layer Loss after iteration 49000: 9.072656661829715\n",
      "two-layer Loss after iteration 50000: 9.071037130669\n",
      "two-layer Loss after iteration 51000: 9.069468005995908\n",
      "two-layer Loss after iteration 52000: 9.067943357139566\n",
      "two-layer Loss after iteration 53000: 9.066459767740659\n",
      "two-layer Loss after iteration 54000: 9.065015042781361\n",
      "two-layer Loss after iteration 55000: 9.063607607202893\n",
      "two-layer Loss after iteration 56000: 9.062236211093635\n",
      "two-layer Loss after iteration 57000: 9.060899781735248\n",
      "two-layer Loss after iteration 58000: 9.059597348415608\n",
      "two-layer Loss after iteration 59000: 9.058328003831233\n",
      "two-layer Loss after iteration 60000: 9.057090884026222\n",
      "two-layer Loss after iteration 61000: 9.055885157771526\n",
      "two-layer Loss after iteration 62000: 9.05471002077813\n",
      "two-layer Loss after iteration 63000: 9.053564692402507\n",
      "two-layer Loss after iteration 64000: 9.052448413649872\n",
      "two-layer Loss after iteration 65000: 9.051360445862437\n",
      "two-layer Loss after iteration 66000: 9.050300069777203\n",
      "two-layer Loss after iteration 67000: 9.049266584788384\n",
      "two-layer Loss after iteration 68000: 9.04825930832832\n",
      "two-layer Loss after iteration 69000: 9.040950466927454\n",
      "two-layer Loss after iteration 70000: 9.039418790106906\n",
      "two-layer Loss after iteration 71000: 9.038124600409635\n",
      "two-layer Loss after iteration 72000: 9.036889218056062\n",
      "two-layer Loss after iteration 73000: 9.035690992501664\n",
      "two-layer Loss after iteration 74000: 9.034525128270397\n",
      "two-layer Loss after iteration 75000: 9.033389573659043\n",
      "two-layer Loss after iteration 76000: 9.032283032025894\n",
      "two-layer Loss after iteration 77000: 9.031204517684701\n",
      "two-layer Loss after iteration 78000: 9.030153199486138\n",
      "two-layer Loss after iteration 79000: 9.02912833084917\n",
      "two-layer Loss after iteration 80000: 9.02812921570877\n",
      "two-layer Loss after iteration 81000: 9.027155191419753\n",
      "two-layer Loss after iteration 82000: 9.026205620000223\n",
      "two-layer Loss after iteration 83000: 9.02527988353564\n",
      "two-layer Loss after iteration 84000: 9.024377381672672\n",
      "9.999710531030809e-05 9.02527988353564 9.024377381672672\n",
      "two-layer Loss after iteration 0: 1366.5184718021053\n",
      "two-layer Loss after iteration 1000: 22.778313905224017\n",
      "two-layer Loss after iteration 2000: 9.027678839432117\n",
      "two-layer Loss after iteration 3000: 6.5907308200474635\n",
      "two-layer Loss after iteration 4000: 5.655443212706549\n",
      "two-layer Loss after iteration 5000: 5.258848291098661\n",
      "two-layer Loss after iteration 6000: 4.710990253114771\n",
      "two-layer Loss after iteration 7000: 4.245555826007474\n",
      "two-layer Loss after iteration 8000: 3.9842378550008934\n",
      "two-layer Loss after iteration 9000: 3.839768188171541\n",
      "two-layer Loss after iteration 10000: 3.7439256942593744\n",
      "two-layer Loss after iteration 11000: 3.6632543896580487\n",
      "two-layer Loss after iteration 12000: 3.604098051416175\n",
      "two-layer Loss after iteration 13000: 3.555167707071285\n",
      "two-layer Loss after iteration 14000: 3.515126323213141\n",
      "two-layer Loss after iteration 15000: 3.481863504495093\n",
      "two-layer Loss after iteration 16000: 3.4537006313482532\n",
      "two-layer Loss after iteration 17000: 3.41958020099622\n",
      "two-layer Loss after iteration 18000: 3.385906868124857\n",
      "two-layer Loss after iteration 19000: 3.3575743069592825\n",
      "two-layer Loss after iteration 20000: 3.336636660903559\n",
      "two-layer Loss after iteration 21000: 3.31974895077063\n",
      "two-layer Loss after iteration 22000: 3.305156111666244\n",
      "two-layer Loss after iteration 23000: 3.273524369297064\n",
      "two-layer Loss after iteration 24000: 3.2521672868301708\n",
      "two-layer Loss after iteration 25000: 3.2222861825653704\n",
      "two-layer Loss after iteration 26000: 3.1920285959886567\n",
      "two-layer Loss after iteration 27000: 3.1767586543267123\n",
      "two-layer Loss after iteration 28000: 3.151338498902271\n",
      "two-layer Loss after iteration 29000: 3.1189939352721754\n",
      "two-layer Loss after iteration 30000: 3.102149612818805\n",
      "two-layer Loss after iteration 31000: 3.0781269104489604\n",
      "two-layer Loss after iteration 32000: 3.0580211441056844\n",
      "two-layer Loss after iteration 33000: 3.041763285621058\n",
      "two-layer Loss after iteration 34000: 3.0232287758197405\n",
      "two-layer Loss after iteration 35000: 3.011398861318811\n",
      "two-layer Loss after iteration 36000: 3.0027443635693767\n",
      "two-layer Loss after iteration 37000: 2.99530181447217\n",
      "two-layer Loss after iteration 38000: 2.9895519261970005\n",
      "two-layer Loss after iteration 39000: 2.984600465232479\n",
      "two-layer Loss after iteration 40000: 2.98027402471105\n",
      "two-layer Loss after iteration 41000: 2.976449195335349\n",
      "two-layer Loss after iteration 42000: 2.9730532432811922\n",
      "two-layer Loss after iteration 43000: 2.9648857192727966\n",
      "two-layer Loss after iteration 44000: 2.9616133628115784\n",
      "two-layer Loss after iteration 45000: 2.959010536403486\n",
      "two-layer Loss after iteration 46000: 2.9567460182639005\n",
      "two-layer Loss after iteration 47000: 2.9522233438097554\n",
      "two-layer Loss after iteration 48000: 2.949572150289997\n",
      "two-layer Loss after iteration 49000: 2.9473133913502747\n",
      "two-layer Loss after iteration 50000: 2.945332835491598\n",
      "two-layer Loss after iteration 51000: 2.9436619760599867\n",
      "two-layer Loss after iteration 52000: 2.940913579331422\n",
      "two-layer Loss after iteration 53000: 2.9366358134299375\n",
      "two-layer Loss after iteration 54000: 2.932849984461812\n",
      "two-layer Loss after iteration 55000: 2.9295200183422225\n",
      "two-layer Loss after iteration 56000: 2.9265558125605295\n",
      "two-layer Loss after iteration 57000: 2.922180464880355\n",
      "two-layer Loss after iteration 58000: 2.9177757263080455\n",
      "two-layer Loss after iteration 59000: 2.9137901815591487\n",
      "two-layer Loss after iteration 60000: 2.9104522018539014\n",
      "two-layer Loss after iteration 61000: 2.9085300183627063\n",
      "two-layer Loss after iteration 62000: 2.9068004679374377\n",
      "two-layer Loss after iteration 63000: 2.905228053173672\n",
      "two-layer Loss after iteration 64000: 2.9021007432492034\n",
      "two-layer Loss after iteration 65000: 2.9001522706480287\n",
      "two-layer Loss after iteration 66000: 2.8986377133952463\n",
      "two-layer Loss after iteration 67000: 2.8976333103694127\n",
      "two-layer Loss after iteration 68000: 2.8968024627613467\n",
      "two-layer Loss after iteration 69000: 2.8960637644811884\n",
      "two-layer Loss after iteration 70000: 2.8953944255016792\n",
      "two-layer Loss after iteration 71000: 2.8947850054220234\n",
      "two-layer Loss after iteration 72000: 2.894228028209448\n",
      "two-layer Loss after iteration 73000: 2.893717335603719\n",
      "two-layer Loss after iteration 74000: 2.8932477785000668\n",
      "two-layer Loss after iteration 75000: 2.8928150052051715\n",
      "two-layer Loss after iteration 76000: 2.8924153038891958\n",
      "two-layer Loss after iteration 77000: 2.892045481669424\n",
      "two-layer Loss after iteration 78000: 2.8917027703974867\n",
      "two-layer Loss after iteration 79000: 2.8913993862270733\n",
      "two-layer Loss after iteration 80000: 2.8911891463878923\n",
      "7.271214076563148e-05 2.8913993862270733 2.8911891463878923\n",
      "two-layer Loss after iteration 0: 1666.6216265713517\n",
      "two-layer Loss after iteration 1000: 24.990227703846823\n",
      "two-layer Loss after iteration 2000: 10.722524144089816\n",
      "two-layer Loss after iteration 3000: 8.672076614684352\n",
      "two-layer Loss after iteration 4000: 7.540747045501512\n",
      "two-layer Loss after iteration 5000: 6.704852653077507\n",
      "two-layer Loss after iteration 6000: 6.274984917222266\n",
      "two-layer Loss after iteration 7000: 6.013858476390764\n",
      "two-layer Loss after iteration 8000: 5.856239563744257\n",
      "two-layer Loss after iteration 9000: 5.702650965667323\n",
      "two-layer Loss after iteration 10000: 5.574779717037245\n",
      "two-layer Loss after iteration 11000: 5.5023844597047535\n",
      "two-layer Loss after iteration 12000: 5.429824673085384\n",
      "two-layer Loss after iteration 13000: 5.364586782038007\n",
      "two-layer Loss after iteration 14000: 5.316653575546509\n",
      "two-layer Loss after iteration 15000: 5.266058667105509\n",
      "two-layer Loss after iteration 16000: 5.227274507033908\n",
      "two-layer Loss after iteration 17000: 5.194284469667851\n",
      "two-layer Loss after iteration 18000: 5.167999059732798\n",
      "two-layer Loss after iteration 19000: 5.1459640133754405\n",
      "two-layer Loss after iteration 20000: 5.120939461122092\n",
      "two-layer Loss after iteration 21000: 5.100968410133088\n",
      "two-layer Loss after iteration 22000: 5.081001099952941\n",
      "two-layer Loss after iteration 23000: 5.0667866135869986\n",
      "two-layer Loss after iteration 24000: 5.056115972467052\n",
      "two-layer Loss after iteration 25000: 5.04744501148972\n",
      "two-layer Loss after iteration 26000: 5.040325271374485\n",
      "two-layer Loss after iteration 27000: 5.034476955836572\n",
      "two-layer Loss after iteration 28000: 5.029829439401114\n",
      "two-layer Loss after iteration 29000: 5.024523527346793\n",
      "two-layer Loss after iteration 30000: 5.016146381878577\n",
      "two-layer Loss after iteration 31000: 5.011646762425836\n",
      "two-layer Loss after iteration 32000: 5.007938156950912\n",
      "two-layer Loss after iteration 33000: 4.863535398013367\n",
      "two-layer Loss after iteration 34000: 4.667406400568378\n",
      "two-layer Loss after iteration 35000: 4.470175898475971\n",
      "two-layer Loss after iteration 36000: 4.400900400694762\n",
      "two-layer Loss after iteration 37000: 4.364066877440391\n",
      "two-layer Loss after iteration 38000: 4.335459995049035\n",
      "two-layer Loss after iteration 39000: 4.288174324814891\n",
      "two-layer Loss after iteration 40000: 4.126372180081084\n",
      "two-layer Loss after iteration 41000: 4.033031068268514\n",
      "two-layer Loss after iteration 42000: 3.9744858163205508\n",
      "two-layer Loss after iteration 43000: 3.9280653287544864\n",
      "two-layer Loss after iteration 44000: 3.894797380870639\n",
      "two-layer Loss after iteration 45000: 3.850314319388699\n",
      "two-layer Loss after iteration 46000: 3.8170454793116684\n",
      "two-layer Loss after iteration 47000: 3.7943026933984587\n",
      "two-layer Loss after iteration 48000: 3.779192010259333\n",
      "two-layer Loss after iteration 49000: 3.767760656561491\n",
      "two-layer Loss after iteration 50000: 3.7586216766152756\n",
      "two-layer Loss after iteration 51000: 3.751413745802932\n",
      "two-layer Loss after iteration 52000: 3.745563949213746\n",
      "two-layer Loss after iteration 53000: 3.7408090333735067\n",
      "two-layer Loss after iteration 54000: 3.73694036912718\n",
      "two-layer Loss after iteration 55000: 3.733791558499745\n",
      "two-layer Loss after iteration 56000: 3.7312236384539443\n",
      "two-layer Loss after iteration 57000: 3.7291263473588567\n",
      "two-layer Loss after iteration 58000: 3.7274084636224156\n",
      "two-layer Loss after iteration 59000: 3.725998398473048\n",
      "two-layer Loss after iteration 60000: 3.7248369067437643\n",
      "two-layer Loss after iteration 61000: 3.7238755223682776\n",
      "two-layer Loss after iteration 62000: 3.7230765627572193\n",
      "two-layer Loss after iteration 63000: 3.722408960865975\n",
      "two-layer Loss after iteration 64000: 3.721847428186978\n",
      "two-layer Loss after iteration 65000: 3.721372182703738\n",
      "two-layer Loss after iteration 66000: 3.7209652188660685\n",
      "two-layer Loss after iteration 67000: 3.720615318719514\n",
      "9.40347802179106e-05 3.7209652188660685 3.720615318719514\n",
      "two-layer Loss after iteration 0: 1411.283987948925\n",
      "two-layer Loss after iteration 1000: 24.68282789774121\n",
      "two-layer Loss after iteration 2000: 9.94286313848663\n",
      "two-layer Loss after iteration 3000: 7.614323590943777\n",
      "two-layer Loss after iteration 4000: 6.472294991703972\n",
      "two-layer Loss after iteration 5000: 5.64742236147788\n",
      "two-layer Loss after iteration 6000: 5.259925441716146\n",
      "two-layer Loss after iteration 7000: 5.083811938662121\n",
      "two-layer Loss after iteration 8000: 4.972279857688425\n",
      "two-layer Loss after iteration 9000: 4.899035256270405\n",
      "two-layer Loss after iteration 10000: 4.840590797631882\n",
      "two-layer Loss after iteration 11000: 4.777285253989593\n",
      "two-layer Loss after iteration 12000: 4.709835762587225\n",
      "two-layer Loss after iteration 13000: 4.646407734320152\n",
      "two-layer Loss after iteration 14000: 4.5965274971533585\n",
      "two-layer Loss after iteration 15000: 4.542254196531818\n",
      "two-layer Loss after iteration 16000: 4.507888177641298\n",
      "two-layer Loss after iteration 17000: 4.454954603045437\n",
      "two-layer Loss after iteration 18000: 4.413858539175773\n",
      "two-layer Loss after iteration 19000: 4.3784629426983095\n",
      "two-layer Loss after iteration 20000: 4.348299211879416\n",
      "two-layer Loss after iteration 21000: 4.201026210637868\n",
      "two-layer Loss after iteration 22000: 4.0793375504674945\n",
      "two-layer Loss after iteration 23000: 3.985957679734062\n",
      "two-layer Loss after iteration 24000: 3.9521508965693726\n",
      "two-layer Loss after iteration 25000: 3.926787295408197\n",
      "two-layer Loss after iteration 26000: 3.905701551701022\n",
      "two-layer Loss after iteration 27000: 3.887676608284455\n",
      "two-layer Loss after iteration 28000: 3.8723561610852175\n",
      "two-layer Loss after iteration 29000: 3.8586079802660875\n",
      "two-layer Loss after iteration 30000: 3.8463205993792453\n",
      "two-layer Loss after iteration 31000: 3.835934030806978\n",
      "two-layer Loss after iteration 32000: 3.8270069360442993\n",
      "two-layer Loss after iteration 33000: 3.8194046585108925\n",
      "two-layer Loss after iteration 34000: 3.8129605785599305\n",
      "two-layer Loss after iteration 35000: 3.807386940214293\n",
      "two-layer Loss after iteration 36000: 3.8025538290466456\n",
      "two-layer Loss after iteration 37000: 3.79806526193172\n",
      "two-layer Loss after iteration 38000: 3.7929736919000065\n",
      "two-layer Loss after iteration 39000: 3.789139529403402\n",
      "two-layer Loss after iteration 40000: 3.7861199593728947\n",
      "two-layer Loss after iteration 41000: 3.7837788307681546\n",
      "two-layer Loss after iteration 42000: 3.7817342798928144\n",
      "two-layer Loss after iteration 43000: 3.7799712077046337\n",
      "two-layer Loss after iteration 44000: 3.778490951290729\n",
      "two-layer Loss after iteration 45000: 3.7761038567278273\n",
      "two-layer Loss after iteration 46000: 3.773809981740235\n",
      "two-layer Loss after iteration 47000: 3.7725853407540972\n",
      "two-layer Loss after iteration 48000: 3.7716351239309307\n",
      "two-layer Loss after iteration 49000: 3.770815457485974\n",
      "two-layer Loss after iteration 50000: 3.7701066791525397\n",
      "two-layer Loss after iteration 51000: 3.769490181008042\n",
      "two-layer Loss after iteration 52000: 3.7689520416507336\n",
      "two-layer Loss after iteration 53000: 3.7684806414682592\n",
      "two-layer Loss after iteration 54000: 3.7675985640064606\n",
      "two-layer Loss after iteration 55000: 3.6528949810900406\n",
      "two-layer Loss after iteration 56000: 3.4206157349689383\n",
      "two-layer Loss after iteration 57000: 3.1291120223255993\n",
      "two-layer Loss after iteration 58000: 3.0350934917995476\n",
      "two-layer Loss after iteration 59000: 3.0001073436038217\n",
      "two-layer Loss after iteration 60000: 2.979767312777225\n",
      "two-layer Loss after iteration 61000: 2.967680711280597\n",
      "two-layer Loss after iteration 62000: 2.9602670157718958\n",
      "two-layer Loss after iteration 63000: 2.9555489641385084\n",
      "two-layer Loss after iteration 64000: 2.9488780040775784\n",
      "two-layer Loss after iteration 65000: 2.943730630977016\n",
      "two-layer Loss after iteration 66000: 2.9395019600902397\n",
      "two-layer Loss after iteration 67000: 2.935846361947112\n",
      "two-layer Loss after iteration 68000: 2.932603482135367\n",
      "two-layer Loss after iteration 69000: 2.9296887756333896\n",
      "two-layer Loss after iteration 70000: 2.9270429686074313\n",
      "two-layer Loss after iteration 71000: 2.924625452080963\n",
      "two-layer Loss after iteration 72000: 2.9224129076587584\n",
      "two-layer Loss after iteration 73000: 2.9203837158266177\n",
      "two-layer Loss after iteration 74000: 2.9185185222610945\n",
      "two-layer Loss after iteration 75000: 2.916803352718505\n",
      "two-layer Loss after iteration 76000: 2.9152237272959396\n",
      "two-layer Loss after iteration 77000: 2.913765847544202\n",
      "two-layer Loss after iteration 78000: 2.9124201166160306\n",
      "two-layer Loss after iteration 79000: 2.911174884752411\n",
      "two-layer Loss after iteration 80000: 2.9100201421459815\n",
      "two-layer Loss after iteration 81000: 2.9089485816879552\n",
      "two-layer Loss after iteration 82000: 2.907954650531877\n",
      "two-layer Loss after iteration 83000: 2.9070309024319783\n",
      "two-layer Loss after iteration 84000: 2.9061704818181515\n",
      "two-layer Loss after iteration 85000: 2.905367484775736\n",
      "two-layer Loss after iteration 86000: 2.904617089078546\n",
      "two-layer Loss after iteration 87000: 2.903912575635053\n",
      "two-layer Loss after iteration 88000: 2.903252732416407\n",
      "two-layer Loss after iteration 89000: 2.9026300579601143\n",
      "two-layer Loss after iteration 90000: 2.902044192340878\n",
      "two-layer Loss after iteration 91000: 2.9014909695871323\n",
      "two-layer Loss after iteration 92000: 2.9009662671636307\n",
      "two-layer Loss after iteration 93000: 2.900469463358647\n",
      "two-layer Loss after iteration 94000: 2.899996149403475\n",
      "two-layer Loss after iteration 95000: 2.89954621120694\n",
      "two-layer Loss after iteration 96000: 2.899116799095658\n",
      "two-layer Loss after iteration 97000: 2.8987057208163827\n",
      "two-layer Loss after iteration 98000: 2.898312142314008\n",
      "two-layer Loss after iteration 99000: 2.89793472933845\n",
      "three-layer Loss after iteration 0: 1421.4960139672094\n",
      "three-layer Loss after iteration 1000: 34.66132711097035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_20816\\2957246329.py:131: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < 0.0001:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three-layer Loss after iteration 2000: 29.20447985964855\n",
      "three-layer Loss after iteration 3000: 26.417472327763996\n",
      "three-layer Loss after iteration 4000: 25.50246121302901\n",
      "three-layer Loss after iteration 5000: 25.421519739660848\n",
      "three-layer Loss after iteration 6000: 25.11705531279063\n",
      "three-layer Loss after iteration 7000: 25.0664972361905\n",
      "three-layer Loss after iteration 8000: 24.037160521203713\n",
      "three-layer Loss after iteration 9000: 22.451822780105964\n",
      "three-layer Loss after iteration 10000: 22.672401560988387\n",
      "three-layer Loss after iteration 11000: 22.524288480014736\n",
      "three-layer Loss after iteration 12000: 21.75109601800643\n",
      "three-layer Loss after iteration 13000: 20.82045535730079\n",
      "three-layer Loss after iteration 14000: 20.64432327509546\n",
      "three-layer Loss after iteration 15000: 19.87236838225023\n",
      "three-layer Loss after iteration 16000: 19.078703100674527\n",
      "three-layer Loss after iteration 17000: 18.485249066673816\n",
      "three-layer Loss after iteration 18000: 20.34402866805453\n",
      "three-layer Loss after iteration 19000: 15.421004660978195\n",
      "three-layer Loss after iteration 20000: 19.194021067632914\n",
      "three-layer Loss after iteration 21000: 19.123438952780354\n",
      "three-layer Loss after iteration 22000: 18.397809013666233\n",
      "three-layer Loss after iteration 23000: 18.648606910689686\n",
      "three-layer Loss after iteration 24000: 16.99212871437202\n",
      "three-layer Loss after iteration 25000: 12.046090590240949\n",
      "three-layer Loss after iteration 26000: 34.59804826329959\n",
      "three-layer Loss after iteration 27000: 16.579031266320786\n",
      "three-layer Loss after iteration 28000: 14.47045406558177\n",
      "three-layer Loss after iteration 29000: 12.684750378819816\n",
      "three-layer Loss after iteration 30000: 13.036727599064452\n",
      "three-layer Loss after iteration 31000: 14.515828598720702\n",
      "three-layer Loss after iteration 32000: 14.946543855901645\n",
      "three-layer Loss after iteration 33000: 15.578715373984148\n",
      "three-layer Loss after iteration 34000: 15.59555898610816\n",
      "three-layer Loss after iteration 35000: 14.505146244968039\n",
      "three-layer Loss after iteration 36000: 13.758701622262132\n",
      "three-layer Loss after iteration 37000: 13.605045597606688\n",
      "three-layer Loss after iteration 38000: 13.636462595137315\n",
      "three-layer Loss after iteration 39000: 13.375276572597652\n",
      "three-layer Loss after iteration 40000: 13.429019390099274\n",
      "three-layer Loss after iteration 41000: 13.237321578527336\n",
      "three-layer Loss after iteration 42000: 13.010379184605858\n",
      "three-layer Loss after iteration 43000: 13.254525603196882\n",
      "three-layer Loss after iteration 44000: 13.419403125758226\n",
      "three-layer Loss after iteration 45000: 13.204467110725819\n",
      "three-layer Loss after iteration 46000: 13.577372098114228\n",
      "three-layer Loss after iteration 47000: 12.71148562311128\n",
      "three-layer Loss after iteration 48000: 12.65540019899206\n",
      "three-layer Loss after iteration 49000: 12.561529875641938\n",
      "three-layer Loss after iteration 50000: 12.42340370732936\n",
      "three-layer Loss after iteration 51000: 12.244925163772479\n",
      "three-layer Loss after iteration 52000: 12.07115430524002\n",
      "three-layer Loss after iteration 53000: 11.96997327480849\n",
      "three-layer Loss after iteration 54000: 11.90488631742405\n",
      "three-layer Loss after iteration 55000: 11.813528840430566\n",
      "three-layer Loss after iteration 56000: 11.726403556077768\n",
      "three-layer Loss after iteration 57000: 11.796433398703075\n",
      "three-layer Loss after iteration 58000: 12.211610691062944\n",
      "three-layer Loss after iteration 59000: 11.482919896515513\n",
      "three-layer Loss after iteration 60000: 11.034903935499072\n",
      "three-layer Loss after iteration 61000: 11.841276127107204\n",
      "three-layer Loss after iteration 62000: 11.71962282838677\n",
      "three-layer Loss after iteration 63000: 11.674439921783799\n",
      "three-layer Loss after iteration 64000: 11.573396981381824\n",
      "three-layer Loss after iteration 65000: 11.339015747847201\n",
      "three-layer Loss after iteration 66000: 11.347384570926321\n",
      "three-layer Loss after iteration 67000: 10.991658014166617\n",
      "three-layer Loss after iteration 68000: 10.993183021830907\n",
      "three-layer Loss after iteration 69000: 10.993200418489621\n",
      "1.5824951408456864e-06 10.993183021830907 10.993200418489621\n",
      "three-layer Loss after iteration 0: 1719.5429846591155\n",
      "three-layer Loss after iteration 1000: 22.344916798626894\n",
      "three-layer Loss after iteration 2000: 29.424558325970995\n",
      "three-layer Loss after iteration 3000: 24.077233681545447\n",
      "three-layer Loss after iteration 4000: 26.249366707433765\n",
      "three-layer Loss after iteration 5000: 23.195742832278913\n",
      "three-layer Loss after iteration 6000: 19.31106761714375\n",
      "three-layer Loss after iteration 7000: 19.32505826547512\n",
      "three-layer Loss after iteration 8000: 18.228909194790976\n",
      "three-layer Loss after iteration 9000: 17.494113925063036\n",
      "three-layer Loss after iteration 10000: 16.932175398158567\n",
      "three-layer Loss after iteration 11000: 16.227360632746407\n",
      "three-layer Loss after iteration 12000: 14.689003360585309\n",
      "three-layer Loss after iteration 13000: 20.261357682321478\n",
      "three-layer Loss after iteration 14000: 16.22540591167457\n",
      "three-layer Loss after iteration 15000: 15.745357503949\n",
      "three-layer Loss after iteration 16000: 22.283836266272466\n",
      "three-layer Loss after iteration 17000: 19.97722252830053\n",
      "three-layer Loss after iteration 18000: 17.30261210216325\n",
      "three-layer Loss after iteration 19000: 16.69597521745891\n",
      "three-layer Loss after iteration 20000: 16.262790444909136\n",
      "three-layer Loss after iteration 21000: 16.993717653147403\n",
      "three-layer Loss after iteration 22000: 16.604317028250023\n",
      "three-layer Loss after iteration 23000: 13.049090503895524\n",
      "three-layer Loss after iteration 24000: 12.976692278256207\n",
      "three-layer Loss after iteration 25000: 25.25563923781137\n",
      "three-layer Loss after iteration 26000: 13.284369931425951\n",
      "three-layer Loss after iteration 27000: 14.414082740729963\n",
      "three-layer Loss after iteration 28000: 13.864174969528605\n",
      "three-layer Loss after iteration 29000: 13.775526653695666\n",
      "three-layer Loss after iteration 30000: 14.423530651588242\n",
      "three-layer Loss after iteration 31000: 12.927792184853086\n",
      "three-layer Loss after iteration 32000: 12.830536304520093\n",
      "three-layer Loss after iteration 33000: 12.763878151002565\n",
      "three-layer Loss after iteration 34000: 12.732026856249023\n",
      "three-layer Loss after iteration 35000: 41.642390356581714\n",
      "three-layer Loss after iteration 36000: 12.908585343288916\n",
      "three-layer Loss after iteration 37000: 13.56608845134946\n",
      "three-layer Loss after iteration 38000: 13.07149046093803\n",
      "three-layer Loss after iteration 39000: 13.228138988077784\n",
      "three-layer Loss after iteration 40000: 18.60459743254315\n",
      "three-layer Loss after iteration 41000: 14.082516328836094\n",
      "three-layer Loss after iteration 42000: 14.483816018619255\n",
      "three-layer Loss after iteration 43000: 13.24281303926954\n",
      "three-layer Loss after iteration 44000: 12.603652132682157\n",
      "three-layer Loss after iteration 45000: 16.309092258065395\n",
      "three-layer Loss after iteration 46000: 13.7388678991623\n",
      "three-layer Loss after iteration 47000: 15.092880875935686\n",
      "three-layer Loss after iteration 48000: 12.794188544555464\n",
      "three-layer Loss after iteration 49000: 14.753078844942785\n",
      "three-layer Loss after iteration 50000: 12.828590565586376\n",
      "three-layer Loss after iteration 51000: 12.87189781141119\n",
      "three-layer Loss after iteration 52000: 12.905755594932403\n",
      "three-layer Loss after iteration 53000: 12.879082818523429\n",
      "three-layer Loss after iteration 54000: 12.85833872682264\n",
      "three-layer Loss after iteration 55000: 12.897422698347542\n",
      "three-layer Loss after iteration 56000: 12.937153454887136\n",
      "three-layer Loss after iteration 57000: 12.96518370537585\n",
      "three-layer Loss after iteration 58000: 12.987316236703636\n",
      "three-layer Loss after iteration 59000: 13.03440741087716\n",
      "three-layer Loss after iteration 60000: 13.089726839323514\n",
      "three-layer Loss after iteration 61000: 13.16768695591004\n",
      "three-layer Loss after iteration 62000: 13.192935347179755\n",
      "three-layer Loss after iteration 63000: 13.072420319600294\n",
      "three-layer Loss after iteration 64000: 12.992195146032639\n",
      "three-layer Loss after iteration 65000: 12.92587827342281\n",
      "three-layer Loss after iteration 66000: 12.869344310014158\n",
      "three-layer Loss after iteration 67000: 12.820834207450378\n",
      "three-layer Loss after iteration 68000: 12.779106627849991\n",
      "three-layer Loss after iteration 69000: 12.743155514763073\n",
      "three-layer Loss after iteration 70000: 12.712141270422217\n",
      "three-layer Loss after iteration 71000: 12.685356801899793\n",
      "three-layer Loss after iteration 72000: 12.662203568915263\n",
      "three-layer Loss after iteration 73000: 12.642172984917108\n",
      "three-layer Loss after iteration 74000: 12.624831556699958\n",
      "three-layer Loss after iteration 75000: 12.609808838986828\n",
      "three-layer Loss after iteration 76000: 12.596787569634843\n",
      "three-layer Loss after iteration 77000: 12.585495523825067\n",
      "three-layer Loss after iteration 78000: 12.575698741783938\n",
      "three-layer Loss after iteration 79000: 12.567195866300455\n",
      "three-layer Loss after iteration 80000: 12.55981338536368\n",
      "three-layer Loss after iteration 81000: 12.553401618824674\n",
      "three-layer Loss after iteration 82000: 12.547831320786418\n",
      "three-layer Loss after iteration 83000: 12.542990794473145\n",
      "three-layer Loss after iteration 84000: 12.538783435769998\n",
      "three-layer Loss after iteration 85000: 12.5351256368687\n",
      "three-layer Loss after iteration 86000: 12.531944993559039\n",
      "three-layer Loss after iteration 87000: 12.529178769406926\n",
      "three-layer Loss after iteration 88000: 12.52677257788613\n",
      "three-layer Loss after iteration 89000: 12.524679249911124\n",
      "three-layer Loss after iteration 90000: 12.522857859440077\n",
      "three-layer Loss after iteration 91000: 12.52127288412053\n",
      "three-layer Loss after iteration 92000: 12.51989348151572\n",
      "three-layer Loss after iteration 93000: 12.518692864415698\n",
      "9.589675038323605e-05 12.51989348151572 12.518692864415698\n",
      "three-layer Loss after iteration 0: 1696.0801131207313\n",
      "three-layer Loss after iteration 1000: 19.34646251050462\n",
      "three-layer Loss after iteration 2000: 10.285751107508137\n",
      "three-layer Loss after iteration 3000: 11.894460178060104\n",
      "three-layer Loss after iteration 4000: 11.534222292567117\n",
      "three-layer Loss after iteration 5000: 7.5943926653080505\n",
      "three-layer Loss after iteration 6000: 7.2670603371689895\n",
      "three-layer Loss after iteration 7000: 7.060142988783856\n",
      "three-layer Loss after iteration 8000: 8.89528959968473\n",
      "three-layer Loss after iteration 9000: 8.915263717876337\n",
      "three-layer Loss after iteration 10000: 11.974577447299433\n",
      "three-layer Loss after iteration 11000: 10.98845851496394\n",
      "three-layer Loss after iteration 12000: 9.840592796440392\n",
      "three-layer Loss after iteration 13000: 8.976145284320689\n",
      "three-layer Loss after iteration 14000: 8.588752116071268\n",
      "three-layer Loss after iteration 15000: 8.349784219505553\n",
      "three-layer Loss after iteration 16000: 8.914548794951992\n",
      "three-layer Loss after iteration 17000: 8.556253878373573\n",
      "three-layer Loss after iteration 18000: 5.612706611494528\n",
      "three-layer Loss after iteration 19000: 5.3056109594169785\n",
      "three-layer Loss after iteration 20000: 3.9944475437582505\n",
      "three-layer Loss after iteration 21000: 4.565471391686968\n",
      "three-layer Loss after iteration 22000: 3.7287647140325144\n",
      "three-layer Loss after iteration 23000: 3.4173140630923307\n",
      "three-layer Loss after iteration 24000: 3.444342015700138\n",
      "three-layer Loss after iteration 25000: 3.2755537874467024\n",
      "three-layer Loss after iteration 26000: 3.114235024993851\n",
      "three-layer Loss after iteration 27000: 2.870542342484826\n",
      "three-layer Loss after iteration 28000: 2.7896904904956017\n",
      "three-layer Loss after iteration 29000: 2.7687233089891596\n",
      "three-layer Loss after iteration 30000: 2.7570612533686765\n",
      "three-layer Loss after iteration 31000: 2.7981389175205664\n",
      "three-layer Loss after iteration 32000: 2.687476764191037\n",
      "three-layer Loss after iteration 33000: 2.6753108140739297\n",
      "three-layer Loss after iteration 34000: 2.6649326947018666\n",
      "three-layer Loss after iteration 35000: 2.659065010260948\n",
      "three-layer Loss after iteration 36000: 2.6555812042395384\n",
      "three-layer Loss after iteration 37000: 2.6524628813695834\n",
      "three-layer Loss after iteration 38000: 2.649030279786818\n",
      "three-layer Loss after iteration 39000: 2.6472995749729207\n",
      "three-layer Loss after iteration 40000: 2.6467578260075975\n",
      "three-layer Loss after iteration 41000: 2.6447346254136037\n",
      "three-layer Loss after iteration 42000: 2.6423274176835556\n",
      "three-layer Loss after iteration 43000: 2.641213398529598\n",
      "three-layer Loss after iteration 44000: 2.6391773700795893\n",
      "three-layer Loss after iteration 45000: 2.6389228918846745\n",
      "9.642330136647055e-05 2.6391773700795893 2.6389228918846745\n",
      "three-layer Loss after iteration 0: 1637.1305329904653\n",
      "three-layer Loss after iteration 1000: 11.482658681037542\n",
      "three-layer Loss after iteration 2000: 8.533824626727236\n",
      "three-layer Loss after iteration 3000: 7.0614770628425445\n",
      "three-layer Loss after iteration 4000: 6.2240314814166915\n",
      "three-layer Loss after iteration 5000: 5.14362313961777\n",
      "three-layer Loss after iteration 6000: 5.282185657749853\n",
      "three-layer Loss after iteration 7000: 4.953157097362976\n",
      "three-layer Loss after iteration 8000: 4.736174680249644\n",
      "three-layer Loss after iteration 9000: 4.331282944940066\n",
      "three-layer Loss after iteration 10000: 4.3521977751741545\n",
      "three-layer Loss after iteration 11000: 4.392425433856813\n",
      "three-layer Loss after iteration 12000: 4.0473215608429145\n",
      "three-layer Loss after iteration 13000: 3.857585298165759\n",
      "three-layer Loss after iteration 14000: 3.506096865156564\n",
      "three-layer Loss after iteration 15000: 3.559960805291842\n",
      "three-layer Loss after iteration 16000: 2.954243826218367\n",
      "three-layer Loss after iteration 17000: 2.9490734599803727\n",
      "three-layer Loss after iteration 18000: 2.7160598948726014\n",
      "three-layer Loss after iteration 19000: 2.5752830748284947\n",
      "three-layer Loss after iteration 20000: 2.508687857631874\n",
      "three-layer Loss after iteration 21000: 2.485557556654305\n",
      "three-layer Loss after iteration 22000: 2.5568699688870797\n",
      "three-layer Loss after iteration 23000: 2.5791462006098804\n",
      "three-layer Loss after iteration 24000: 2.5684454508543473\n",
      "three-layer Loss after iteration 25000: 2.435152300589688\n",
      "three-layer Loss after iteration 26000: 2.448126520219938\n",
      "three-layer Loss after iteration 27000: 2.4874240856084513\n",
      "three-layer Loss after iteration 28000: 2.485496949907759\n",
      "three-layer Loss after iteration 29000: 2.4755997658782434\n",
      "three-layer Loss after iteration 30000: 2.462853796337599\n",
      "three-layer Loss after iteration 31000: 2.4499533939189977\n",
      "three-layer Loss after iteration 32000: 2.438599578928586\n",
      "three-layer Loss after iteration 33000: 2.428864085993642\n",
      "three-layer Loss after iteration 34000: 2.4208017814824374\n",
      "three-layer Loss after iteration 35000: 2.4141234518945547\n",
      "three-layer Loss after iteration 36000: 2.408729332540702\n",
      "three-layer Loss after iteration 37000: 2.4041740719679274\n",
      "three-layer Loss after iteration 38000: 2.4005816949964274\n",
      "three-layer Loss after iteration 39000: 2.397439054157588\n",
      "three-layer Loss after iteration 40000: 2.3947717216506166\n",
      "three-layer Loss after iteration 41000: 2.392614283512441\n",
      "three-layer Loss after iteration 42000: 2.3907282732615265\n",
      "three-layer Loss after iteration 43000: 2.3889155700453446\n",
      "three-layer Loss after iteration 44000: 2.3874024948473864\n",
      "three-layer Loss after iteration 45000: 2.386141587581655\n",
      "three-layer Loss after iteration 46000: 2.384835477327807\n",
      "three-layer Loss after iteration 47000: 2.383722467207617\n",
      "three-layer Loss after iteration 48000: 2.382654003729558\n",
      "three-layer Loss after iteration 49000: 2.381736640614876\n",
      "three-layer Loss after iteration 50000: 2.3807108762780214\n",
      "three-layer Loss after iteration 51000: 2.3798144640289096\n",
      "three-layer Loss after iteration 52000: 2.3790342981753665\n",
      "three-layer Loss after iteration 53000: 2.3781216919057018\n",
      "three-layer Loss after iteration 54000: 2.37743596795429\n",
      "three-layer Loss after iteration 55000: 2.3766096951953966\n",
      "three-layer Loss after iteration 56000: 2.3758765762047345\n",
      "three-layer Loss after iteration 57000: 2.375114866619727\n",
      "three-layer Loss after iteration 58000: 2.3744414613875517\n",
      "three-layer Loss after iteration 59000: 2.3737344298031835\n",
      "three-layer Loss after iteration 60000: 2.372991381342684\n",
      "three-layer Loss after iteration 61000: 2.372327576888823\n",
      "three-layer Loss after iteration 62000: 2.371624068129816\n",
      "three-layer Loss after iteration 63000: 2.370997874037528\n",
      "three-layer Loss after iteration 64000: 2.370328936198386\n",
      "three-layer Loss after iteration 65000: 2.3697315648984913\n",
      "three-layer Loss after iteration 66000: 2.3690883135102254\n",
      "three-layer Loss after iteration 67000: 2.3683998038084084\n",
      "three-layer Loss after iteration 68000: 2.367780044038012\n",
      "three-layer Loss after iteration 69000: 2.3672258099663916\n",
      "three-layer Loss after iteration 70000: 2.366622257551251\n",
      "three-layer Loss after iteration 71000: 2.3659693207053745\n",
      "three-layer Loss after iteration 72000: 2.36538117993851\n",
      "three-layer Loss after iteration 73000: 2.364844045580888\n",
      "three-layer Loss after iteration 74000: 2.3642543801654896\n",
      "three-layer Loss after iteration 75000: 2.3637260323031377\n",
      "three-layer Loss after iteration 76000: 2.3631485686276092\n",
      "three-layer Loss after iteration 77000: 2.362626216715676\n",
      "three-layer Loss after iteration 78000: 2.3620465468505523\n",
      "three-layer Loss after iteration 79000: 2.3615191766531964\n",
      "three-layer Loss after iteration 80000: 2.3610438207890394\n",
      "three-layer Loss after iteration 81000: 2.360510264066223\n",
      "three-layer Loss after iteration 82000: 2.359916916116158\n",
      "three-layer Loss after iteration 83000: 2.3594818334848924\n",
      "three-layer Loss after iteration 84000: 2.3589854389438805\n",
      "three-layer Loss after iteration 85000: 2.3584283298671935\n",
      "three-layer Loss after iteration 86000: 2.3579166422509137\n",
      "three-layer Loss after iteration 87000: 2.357449671041841\n",
      "three-layer Loss after iteration 88000: 2.357025452088865\n",
      "three-layer Loss after iteration 89000: 2.356537809189133\n",
      "three-layer Loss after iteration 90000: 2.3560919091706265\n",
      "three-layer Loss after iteration 91000: 2.3555786449168408\n",
      "three-layer Loss after iteration 92000: 2.355106027264906\n",
      "three-layer Loss after iteration 93000: 2.354673267027362\n",
      "three-layer Loss after iteration 94000: 2.3541725597295438\n",
      "three-layer Loss after iteration 95000: 2.3538105788087806\n",
      "three-layer Loss after iteration 96000: 2.353379930835584\n",
      "three-layer Loss after iteration 97000: 2.35288111604745\n",
      "three-layer Loss after iteration 98000: 2.3525198329592323\n",
      "three-layer Loss after iteration 99000: 2.352087583440563\n",
      "three-layer Loss after iteration 0: 1611.6362602519657\n",
      "three-layer Loss after iteration 1000: 19.57949311944609\n",
      "three-layer Loss after iteration 2000: 30.27483714405275\n",
      "three-layer Loss after iteration 3000: 26.09628297301338\n",
      "three-layer Loss after iteration 4000: 20.17312909705194\n",
      "three-layer Loss after iteration 5000: 17.610637214804413\n",
      "three-layer Loss after iteration 6000: 15.330107062187748\n",
      "three-layer Loss after iteration 7000: 13.859114913988014\n",
      "three-layer Loss after iteration 8000: 13.11396098505592\n",
      "three-layer Loss after iteration 9000: 12.515375029081806\n",
      "three-layer Loss after iteration 10000: 11.001887312727863\n",
      "three-layer Loss after iteration 11000: 11.49059753844669\n",
      "three-layer Loss after iteration 12000: 9.453418371988292\n",
      "three-layer Loss after iteration 13000: 10.296272731552035\n",
      "three-layer Loss after iteration 14000: 9.511796773240567\n",
      "three-layer Loss after iteration 15000: 9.437880837331072\n",
      "three-layer Loss after iteration 16000: 8.793277760342677\n",
      "three-layer Loss after iteration 17000: 8.859434000687306\n",
      "three-layer Loss after iteration 18000: 8.853240015990352\n",
      "three-layer Loss after iteration 19000: 8.771282936448072\n",
      "three-layer Loss after iteration 20000: 8.58501819389937\n",
      "three-layer Loss after iteration 21000: 8.312489751846798\n",
      "three-layer Loss after iteration 22000: 8.075996343680751\n",
      "three-layer Loss after iteration 23000: 8.19193741773783\n",
      "three-layer Loss after iteration 24000: 7.96424044333784\n",
      "three-layer Loss after iteration 25000: 7.784419771418741\n",
      "three-layer Loss after iteration 26000: 7.991732578361468\n",
      "three-layer Loss after iteration 27000: 8.062113187783218\n",
      "three-layer Loss after iteration 28000: 7.575430782133181\n",
      "three-layer Loss after iteration 29000: 7.863200117489875\n",
      "three-layer Loss after iteration 30000: 7.565676777141816\n",
      "three-layer Loss after iteration 31000: 7.613835139815755\n",
      "three-layer Loss after iteration 32000: 7.730641380047582\n",
      "three-layer Loss after iteration 33000: 7.201356906713903\n",
      "three-layer Loss after iteration 34000: 7.296853236016193\n",
      "three-layer Loss after iteration 35000: 5.438031724227666\n",
      "three-layer Loss after iteration 36000: 7.862380282884444\n",
      "three-layer Loss after iteration 37000: 7.377434213533494\n",
      "three-layer Loss after iteration 38000: 6.62342561826237\n",
      "three-layer Loss after iteration 39000: 6.212772842425897\n",
      "three-layer Loss after iteration 40000: 7.996897868467496\n",
      "three-layer Loss after iteration 41000: 7.014249825088399\n",
      "three-layer Loss after iteration 42000: 7.257481664869596\n",
      "three-layer Loss after iteration 43000: 6.1627027644898815\n",
      "three-layer Loss after iteration 44000: 6.712176070382109\n",
      "three-layer Loss after iteration 45000: 5.756538858904443\n",
      "three-layer Loss after iteration 46000: 6.57301673216088\n",
      "three-layer Loss after iteration 47000: 6.348127110810369\n",
      "three-layer Loss after iteration 48000: 7.0966591672944945\n",
      "three-layer Loss after iteration 49000: 6.037995238062576\n",
      "three-layer Loss after iteration 50000: 4.966272993807443\n",
      "three-layer Loss after iteration 51000: 4.5843925733872934\n",
      "three-layer Loss after iteration 52000: 5.00547176264513\n",
      "three-layer Loss after iteration 53000: 5.351912711498259\n",
      "three-layer Loss after iteration 54000: 5.096025502302748\n",
      "three-layer Loss after iteration 55000: 4.892866358901549\n",
      "three-layer Loss after iteration 56000: 4.752883234215482\n",
      "three-layer Loss after iteration 57000: 6.560407266793415\n",
      "three-layer Loss after iteration 58000: 4.747684649459313\n",
      "three-layer Loss after iteration 59000: 5.155673801149399\n",
      "three-layer Loss after iteration 60000: 4.786966027959777\n",
      "three-layer Loss after iteration 61000: 6.3279512705873335\n",
      "three-layer Loss after iteration 62000: 5.012032942773101\n",
      "three-layer Loss after iteration 63000: 4.795219182413154\n",
      "three-layer Loss after iteration 64000: 4.891260698272686\n",
      "three-layer Loss after iteration 65000: 5.0416318413171215\n",
      "three-layer Loss after iteration 66000: 5.204421811128466\n",
      "three-layer Loss after iteration 67000: 5.127714501099535\n",
      "three-layer Loss after iteration 68000: 5.054893975372957\n",
      "three-layer Loss after iteration 69000: 5.019148785815429\n",
      "three-layer Loss after iteration 70000: 4.972675858924928\n",
      "three-layer Loss after iteration 71000: 4.933314670337758\n",
      "three-layer Loss after iteration 72000: 4.903830151456608\n",
      "three-layer Loss after iteration 73000: 4.8800401925832375\n",
      "three-layer Loss after iteration 74000: 4.856032822829586\n",
      "three-layer Loss after iteration 75000: 4.823664063180096\n",
      "three-layer Loss after iteration 76000: 4.802844450078008\n",
      "three-layer Loss after iteration 77000: 4.77583373729948\n",
      "three-layer Loss after iteration 78000: 4.7481128498683045\n",
      "three-layer Loss after iteration 79000: 4.708379225473354\n",
      "three-layer Loss after iteration 80000: 4.702680250323282\n",
      "three-layer Loss after iteration 81000: 4.679019726030161\n",
      "three-layer Loss after iteration 82000: 4.65337210071283\n",
      "three-layer Loss after iteration 83000: 4.63525145361519\n",
      "three-layer Loss after iteration 84000: 4.626585583143387\n",
      "three-layer Loss after iteration 85000: 4.606247118553186\n",
      "three-layer Loss after iteration 86000: 4.593326293420777\n",
      "three-layer Loss after iteration 87000: 4.582542016559611\n",
      "three-layer Loss after iteration 88000: 4.571600694363321\n",
      "three-layer Loss after iteration 89000: 4.564287302848887\n",
      "three-layer Loss after iteration 90000: 4.556559854670262\n",
      "three-layer Loss after iteration 91000: 4.545845891765449\n",
      "three-layer Loss after iteration 92000: 4.53333322040387\n",
      "three-layer Loss after iteration 93000: 4.522215922706325\n",
      "three-layer Loss after iteration 94000: 4.514061420247794\n",
      "three-layer Loss after iteration 95000: 4.5157142212215575\n",
      "three-layer Loss after iteration 96000: 4.512611860574403\n",
      "three-layer Loss after iteration 97000: 4.49983008024539\n",
      "three-layer Loss after iteration 98000: 4.4981693831349325\n",
      "three-layer Loss after iteration 99000: 4.493967609616839\n",
      "three-layer Loss after iteration 0: 1630.1270940307877\n",
      "three-layer Loss after iteration 1000: 27.681755869641545\n",
      "three-layer Loss after iteration 2000: 22.735863270802582\n",
      "three-layer Loss after iteration 3000: 21.30803139209969\n",
      "three-layer Loss after iteration 4000: 18.790952108768863\n",
      "three-layer Loss after iteration 5000: 17.322300047293705\n",
      "three-layer Loss after iteration 6000: 16.57459838679849\n",
      "three-layer Loss after iteration 7000: 15.790322000473138\n",
      "three-layer Loss after iteration 8000: 14.83918766558361\n",
      "three-layer Loss after iteration 9000: 14.324394952341212\n",
      "three-layer Loss after iteration 10000: 13.7246476060683\n",
      "three-layer Loss after iteration 11000: 13.29536995082267\n",
      "three-layer Loss after iteration 12000: 12.729787540377433\n",
      "three-layer Loss after iteration 13000: 11.728430713966166\n",
      "three-layer Loss after iteration 14000: 11.872604993666567\n",
      "three-layer Loss after iteration 15000: 11.68277691488008\n",
      "three-layer Loss after iteration 16000: 11.650591772696425\n",
      "three-layer Loss after iteration 17000: 11.554793481990353\n",
      "three-layer Loss after iteration 18000: 10.72963010953551\n",
      "three-layer Loss after iteration 19000: 9.473952986233009\n",
      "three-layer Loss after iteration 20000: 9.027929396551695\n",
      "three-layer Loss after iteration 21000: 11.565736529144088\n",
      "three-layer Loss after iteration 22000: 8.566843754917885\n",
      "three-layer Loss after iteration 23000: 9.733690145519564\n",
      "three-layer Loss after iteration 24000: 7.966141238556033\n",
      "three-layer Loss after iteration 25000: 11.588420772382921\n",
      "three-layer Loss after iteration 26000: 10.694301619776835\n",
      "three-layer Loss after iteration 27000: 8.255946277666567\n",
      "three-layer Loss after iteration 28000: 8.175856098622628\n",
      "three-layer Loss after iteration 29000: 9.427324129635624\n",
      "three-layer Loss after iteration 30000: 10.657178871139617\n",
      "three-layer Loss after iteration 31000: 9.130838454847597\n",
      "three-layer Loss after iteration 32000: 8.038736780774483\n",
      "three-layer Loss after iteration 33000: 8.434131523165478\n",
      "three-layer Loss after iteration 34000: 8.053297926984902\n",
      "three-layer Loss after iteration 35000: 8.015321179207403\n",
      "three-layer Loss after iteration 36000: 7.953889494461105\n",
      "three-layer Loss after iteration 37000: 7.874366297647149\n",
      "three-layer Loss after iteration 38000: 8.101191942707048\n",
      "three-layer Loss after iteration 39000: 8.255185416532367\n",
      "three-layer Loss after iteration 40000: 7.7126322860276115\n",
      "three-layer Loss after iteration 41000: 9.235949552554645\n",
      "three-layer Loss after iteration 42000: 7.695732867701929\n",
      "three-layer Loss after iteration 43000: 9.054793760509789\n",
      "three-layer Loss after iteration 44000: 8.038228261197816\n",
      "three-layer Loss after iteration 45000: 8.323034401459786\n",
      "three-layer Loss after iteration 46000: 8.428698695893223\n",
      "three-layer Loss after iteration 47000: 8.005378096347508\n",
      "three-layer Loss after iteration 48000: 8.024268150471121\n",
      "three-layer Loss after iteration 49000: 8.317636286470334\n",
      "three-layer Loss after iteration 50000: 8.095612772238393\n",
      "three-layer Loss after iteration 51000: 8.057740149466127\n",
      "three-layer Loss after iteration 52000: 8.53769548484334\n",
      "three-layer Loss after iteration 53000: 8.16113036903647\n",
      "three-layer Loss after iteration 54000: 7.949055019787771\n",
      "three-layer Loss after iteration 55000: 8.017580153584959\n",
      "three-layer Loss after iteration 56000: 9.455289238125097\n",
      "three-layer Loss after iteration 57000: 7.774326216651663\n",
      "three-layer Loss after iteration 58000: 7.6607281550238735\n",
      "three-layer Loss after iteration 59000: 9.527165107762844\n",
      "three-layer Loss after iteration 60000: 7.51275038334911\n",
      "three-layer Loss after iteration 61000: 7.856489597927265\n",
      "three-layer Loss after iteration 62000: 7.46924633874978\n",
      "three-layer Loss after iteration 63000: 7.779359342855753\n",
      "three-layer Loss after iteration 64000: 7.89734909702776\n",
      "three-layer Loss after iteration 65000: 7.88686992218192\n",
      "three-layer Loss after iteration 66000: 7.8714113296607735\n",
      "three-layer Loss after iteration 67000: 7.868834460417004\n",
      "three-layer Loss after iteration 68000: 7.695438404678989\n",
      "three-layer Loss after iteration 69000: 8.833166704512005\n",
      "three-layer Loss after iteration 70000: 7.809932034061624\n",
      "three-layer Loss after iteration 71000: 8.648326833148857\n",
      "three-layer Loss after iteration 72000: 7.797284658933918\n",
      "three-layer Loss after iteration 73000: 7.7819777293234536\n",
      "three-layer Loss after iteration 74000: 7.778898750106111\n",
      "three-layer Loss after iteration 75000: 7.805067389238724\n",
      "three-layer Loss after iteration 76000: 7.702215809077666\n",
      "three-layer Loss after iteration 77000: 7.768225778163244\n",
      "three-layer Loss after iteration 78000: 7.70730865117868\n",
      "three-layer Loss after iteration 79000: 7.714429839137609\n",
      "three-layer Loss after iteration 80000: 7.716830292317044\n",
      "three-layer Loss after iteration 81000: 7.683218356329468\n",
      "three-layer Loss after iteration 82000: 7.6934655615169465\n",
      "three-layer Loss after iteration 83000: 7.695751518079731\n",
      "three-layer Loss after iteration 84000: 8.117786635879789\n",
      "three-layer Loss after iteration 85000: 7.807560772411935\n",
      "three-layer Loss after iteration 86000: 7.774981516787101\n",
      "three-layer Loss after iteration 87000: 7.928494099595716\n",
      "three-layer Loss after iteration 88000: 7.83080278385192\n",
      "three-layer Loss after iteration 89000: 7.812426555577626\n",
      "three-layer Loss after iteration 90000: 7.811905752583267\n",
      "6.666340997305585e-05 7.812426555577626 7.811905752583267\n",
      "three-layer Loss after iteration 0: 1740.434167074184\n",
      "three-layer Loss after iteration 1000: 29.804065062817763\n",
      "three-layer Loss after iteration 2000: 24.785465483282138\n",
      "three-layer Loss after iteration 3000: 22.379484417106028\n",
      "three-layer Loss after iteration 4000: 21.806190526521668\n",
      "three-layer Loss after iteration 5000: 20.177744836905752\n",
      "three-layer Loss after iteration 6000: 19.76451594309983\n",
      "three-layer Loss after iteration 7000: 20.425441406898482\n",
      "three-layer Loss after iteration 8000: 18.878812269736688\n",
      "three-layer Loss after iteration 9000: 18.79499582315248\n",
      "three-layer Loss after iteration 10000: 19.24922512489767\n",
      "three-layer Loss after iteration 11000: 17.007635925659287\n",
      "three-layer Loss after iteration 12000: 17.166355782659323\n",
      "three-layer Loss after iteration 13000: 16.680579188620218\n",
      "three-layer Loss after iteration 14000: 16.329893660689546\n",
      "three-layer Loss after iteration 15000: 16.360491220613174\n",
      "three-layer Loss after iteration 16000: 15.198620414249069\n",
      "three-layer Loss after iteration 17000: 19.358204498856608\n",
      "three-layer Loss after iteration 18000: 17.524209836155812\n",
      "three-layer Loss after iteration 19000: 18.01372498294534\n",
      "three-layer Loss after iteration 20000: 15.4213835470611\n",
      "three-layer Loss after iteration 21000: 15.98553919184285\n",
      "three-layer Loss after iteration 22000: 15.346503504888522\n",
      "three-layer Loss after iteration 23000: 14.516042433289929\n",
      "three-layer Loss after iteration 24000: 15.788999906161088\n",
      "three-layer Loss after iteration 25000: 16.182416540616735\n",
      "three-layer Loss after iteration 26000: 15.6771230557108\n",
      "three-layer Loss after iteration 27000: 13.661397174152917\n",
      "three-layer Loss after iteration 28000: 14.703151610991437\n",
      "three-layer Loss after iteration 29000: 14.248759627687901\n",
      "three-layer Loss after iteration 30000: 13.43089544465324\n",
      "three-layer Loss after iteration 31000: 13.538303695102767\n",
      "three-layer Loss after iteration 32000: 13.410710275225135\n",
      "three-layer Loss after iteration 33000: 13.314969987527268\n",
      "three-layer Loss after iteration 34000: 13.188271125386446\n",
      "three-layer Loss after iteration 35000: 13.465646843673438\n",
      "three-layer Loss after iteration 36000: 13.180820335080575\n",
      "three-layer Loss after iteration 37000: 13.545980129318309\n",
      "three-layer Loss after iteration 38000: 13.174238732694892\n",
      "three-layer Loss after iteration 39000: 13.150579031431763\n",
      "three-layer Loss after iteration 40000: 12.81123614941249\n",
      "three-layer Loss after iteration 41000: 12.831846602946353\n",
      "three-layer Loss after iteration 42000: 12.711958598859326\n",
      "three-layer Loss after iteration 43000: 12.63250977808587\n",
      "three-layer Loss after iteration 44000: 12.558096382588815\n",
      "three-layer Loss after iteration 45000: 12.50996171516851\n",
      "three-layer Loss after iteration 46000: 12.51407618755486\n",
      "three-layer Loss after iteration 47000: 12.445757704634724\n",
      "three-layer Loss after iteration 48000: 12.386577336086722\n",
      "three-layer Loss after iteration 49000: 12.314982642782772\n",
      "three-layer Loss after iteration 50000: 12.18098802498527\n",
      "three-layer Loss after iteration 51000: 12.0827212898567\n",
      "three-layer Loss after iteration 52000: 12.020519450031564\n",
      "three-layer Loss after iteration 53000: 11.95525756270691\n",
      "three-layer Loss after iteration 54000: 11.893230831939109\n",
      "three-layer Loss after iteration 55000: 11.832498337278864\n",
      "three-layer Loss after iteration 56000: 11.76765024811486\n",
      "three-layer Loss after iteration 57000: 11.908232307667774\n",
      "three-layer Loss after iteration 58000: 11.972197951576666\n",
      "three-layer Loss after iteration 59000: 11.88603573243186\n",
      "three-layer Loss after iteration 60000: 11.799305714986096\n",
      "three-layer Loss after iteration 61000: 11.768462709866116\n",
      "three-layer Loss after iteration 62000: 11.690663722884414\n",
      "three-layer Loss after iteration 63000: 11.625476681945397\n",
      "three-layer Loss after iteration 64000: 11.543607633063687\n",
      "three-layer Loss after iteration 65000: 11.601623944097062\n",
      "three-layer Loss after iteration 66000: 11.50662845028517\n",
      "three-layer Loss after iteration 67000: 11.458393549355133\n",
      "three-layer Loss after iteration 68000: 11.370882625490129\n",
      "three-layer Loss after iteration 69000: 11.31178757652598\n",
      "three-layer Loss after iteration 70000: 11.271504073326058\n",
      "three-layer Loss after iteration 71000: 11.235015967252714\n",
      "three-layer Loss after iteration 72000: 11.190815989822902\n",
      "three-layer Loss after iteration 73000: 11.168895342502976\n",
      "three-layer Loss after iteration 74000: 11.134855572291618\n",
      "three-layer Loss after iteration 75000: 11.106279200055884\n",
      "three-layer Loss after iteration 76000: 11.072856286144116\n",
      "three-layer Loss after iteration 77000: 11.042849267544147\n",
      "three-layer Loss after iteration 78000: 11.017439931766837\n",
      "three-layer Loss after iteration 79000: 10.987920466260698\n",
      "three-layer Loss after iteration 80000: 10.970392274933973\n",
      "three-layer Loss after iteration 81000: 10.935800237476691\n",
      "three-layer Loss after iteration 82000: 10.907527337682362\n",
      "three-layer Loss after iteration 83000: 10.879556478751212\n",
      "three-layer Loss after iteration 84000: 10.856143542940373\n",
      "three-layer Loss after iteration 85000: 10.789217900177606\n",
      "three-layer Loss after iteration 86000: 10.761622848921126\n",
      "three-layer Loss after iteration 87000: 10.722555365229576\n",
      "three-layer Loss after iteration 88000: 10.70477256269037\n",
      "three-layer Loss after iteration 89000: 10.687636813192404\n",
      "three-layer Loss after iteration 90000: 10.668599298112749\n",
      "three-layer Loss after iteration 91000: 10.64660562559607\n",
      "three-layer Loss after iteration 92000: 10.622359632103095\n",
      "three-layer Loss after iteration 93000: 10.606079522073932\n",
      "three-layer Loss after iteration 94000: 10.410726492876352\n",
      "three-layer Loss after iteration 95000: 10.659300443913144\n",
      "three-layer Loss after iteration 96000: 10.415141878837414\n",
      "three-layer Loss after iteration 97000: 10.457098984754706\n",
      "three-layer Loss after iteration 98000: 10.438683951648043\n",
      "three-layer Loss after iteration 99000: 10.416046573266362\n",
      "three-layer Loss after iteration 0: 1640.204425430942\n",
      "three-layer Loss after iteration 1000: 30.885591254259325\n",
      "three-layer Loss after iteration 2000: 32.58340505855133\n",
      "three-layer Loss after iteration 3000: 28.597181298111206\n",
      "three-layer Loss after iteration 4000: 24.62566739310918\n",
      "three-layer Loss after iteration 5000: 23.419067439277836\n",
      "three-layer Loss after iteration 6000: 21.263839646092425\n",
      "three-layer Loss after iteration 7000: 20.249480457960914\n",
      "three-layer Loss after iteration 8000: 18.790857175682213\n",
      "three-layer Loss after iteration 9000: 17.542702835422457\n",
      "three-layer Loss after iteration 10000: 16.61254094362507\n",
      "three-layer Loss after iteration 11000: 16.53259424303805\n",
      "three-layer Loss after iteration 12000: 15.801394642847553\n",
      "three-layer Loss after iteration 13000: 15.2244232697196\n",
      "three-layer Loss after iteration 14000: 14.729896984180963\n",
      "three-layer Loss after iteration 15000: 14.462579235014147\n",
      "three-layer Loss after iteration 16000: 14.113880322440867\n",
      "three-layer Loss after iteration 17000: 13.863134093536509\n",
      "three-layer Loss after iteration 18000: 13.643598721856325\n",
      "three-layer Loss after iteration 19000: 13.489113333284408\n",
      "three-layer Loss after iteration 20000: 13.329681395243691\n",
      "three-layer Loss after iteration 21000: 13.209200056826832\n",
      "three-layer Loss after iteration 22000: 13.097631697755432\n",
      "three-layer Loss after iteration 23000: 12.993560059575293\n",
      "three-layer Loss after iteration 24000: 12.909003185635\n",
      "three-layer Loss after iteration 25000: 12.826275307226288\n",
      "three-layer Loss after iteration 26000: 12.781243634760555\n",
      "three-layer Loss after iteration 27000: 12.735201625259563\n",
      "three-layer Loss after iteration 28000: 12.698240474321652\n",
      "three-layer Loss after iteration 29000: 12.650490079832458\n",
      "three-layer Loss after iteration 30000: 12.61272192377087\n",
      "three-layer Loss after iteration 31000: 12.57619320841163\n",
      "three-layer Loss after iteration 32000: 12.532351132078484\n",
      "three-layer Loss after iteration 33000: 12.500695831178264\n",
      "three-layer Loss after iteration 34000: 12.399594896624606\n",
      "three-layer Loss after iteration 35000: 12.34458058774671\n",
      "three-layer Loss after iteration 36000: 12.29854021612728\n",
      "three-layer Loss after iteration 37000: 12.25464398690935\n",
      "three-layer Loss after iteration 38000: 12.225347059213778\n",
      "three-layer Loss after iteration 39000: 12.19836407049004\n",
      "three-layer Loss after iteration 40000: 12.172263284488231\n",
      "three-layer Loss after iteration 41000: 12.146651626242216\n",
      "three-layer Loss after iteration 42000: 12.121660405231\n",
      "three-layer Loss after iteration 43000: 12.097459794192204\n",
      "three-layer Loss after iteration 44000: 12.073732182995437\n",
      "three-layer Loss after iteration 45000: 12.050763048492573\n",
      "three-layer Loss after iteration 46000: 12.028385885161095\n",
      "three-layer Loss after iteration 47000: 12.00783121923947\n",
      "three-layer Loss after iteration 48000: 11.985380154377705\n",
      "three-layer Loss after iteration 49000: 11.964512318415693\n",
      "three-layer Loss after iteration 50000: 11.944377002867359\n",
      "three-layer Loss after iteration 51000: 11.924520209877079\n",
      "three-layer Loss after iteration 52000: 11.905651673866354\n",
      "three-layer Loss after iteration 53000: 11.893062123895813\n",
      "three-layer Loss after iteration 54000: 11.887507228042804\n",
      "three-layer Loss after iteration 55000: 11.906548407728655\n",
      "three-layer Loss after iteration 56000: 11.90834728842808\n",
      "three-layer Loss after iteration 57000: 11.886014213725087\n",
      "three-layer Loss after iteration 58000: 11.813599234144215\n",
      "three-layer Loss after iteration 59000: 12.035187600663347\n",
      "three-layer Loss after iteration 60000: 11.893999999810344\n",
      "three-layer Loss after iteration 61000: 11.910811246450434\n",
      "three-layer Loss after iteration 62000: 11.848779512149825\n",
      "three-layer Loss after iteration 63000: 11.867466746596353\n",
      "three-layer Loss after iteration 64000: 11.86792313481236\n",
      "3.845708825243442e-05 11.867466746596353 11.86792313481236\n",
      "three-layer Loss after iteration 0: 1766.4062361799035\n",
      "three-layer Loss after iteration 1000: 13.454512165860708\n",
      "three-layer Loss after iteration 2000: 12.936749961087044\n",
      "three-layer Loss after iteration 3000: 11.610798390521207\n",
      "three-layer Loss after iteration 4000: 10.597313275559552\n",
      "three-layer Loss after iteration 5000: 10.644295219912633\n",
      "three-layer Loss after iteration 6000: 10.245582597287184\n",
      "three-layer Loss after iteration 7000: 10.180905888952214\n",
      "three-layer Loss after iteration 8000: 9.916701105373152\n",
      "three-layer Loss after iteration 9000: 10.012491181402309\n",
      "three-layer Loss after iteration 10000: 10.08031081907338\n",
      "three-layer Loss after iteration 11000: 9.49083924573066\n",
      "three-layer Loss after iteration 12000: 11.707035882664073\n",
      "three-layer Loss after iteration 13000: 10.567899000955581\n",
      "three-layer Loss after iteration 14000: 9.805795869829593\n",
      "three-layer Loss after iteration 15000: 9.488528966067816\n",
      "three-layer Loss after iteration 16000: 9.27344150730941\n",
      "three-layer Loss after iteration 17000: 9.011503480820508\n",
      "three-layer Loss after iteration 18000: 7.838564906789403\n",
      "three-layer Loss after iteration 19000: 8.14785506250114\n",
      "three-layer Loss after iteration 20000: 8.073717590727668\n",
      "three-layer Loss after iteration 21000: 7.778207707566851\n",
      "three-layer Loss after iteration 22000: 7.486163753732449\n",
      "three-layer Loss after iteration 23000: 7.9868119231960435\n",
      "three-layer Loss after iteration 24000: 7.2065226031807965\n",
      "three-layer Loss after iteration 25000: 7.479337695314167\n",
      "three-layer Loss after iteration 26000: 7.440085296500893\n",
      "three-layer Loss after iteration 27000: 6.9194438428802085\n",
      "three-layer Loss after iteration 28000: 8.103554155724256\n",
      "three-layer Loss after iteration 29000: 7.071629201570627\n",
      "three-layer Loss after iteration 30000: 7.123745673287588\n",
      "three-layer Loss after iteration 31000: 7.404001917852711\n",
      "three-layer Loss after iteration 32000: 7.113718698502125\n",
      "three-layer Loss after iteration 33000: 7.02987809455189\n",
      "three-layer Loss after iteration 34000: 6.956564839840216\n",
      "three-layer Loss after iteration 35000: 6.904877345508347\n",
      "three-layer Loss after iteration 36000: 6.876685792708544\n",
      "three-layer Loss after iteration 37000: 6.812938004338095\n",
      "three-layer Loss after iteration 38000: 6.8003860801746745\n",
      "three-layer Loss after iteration 39000: 6.885036953978667\n",
      "three-layer Loss after iteration 40000: 6.830596124393952\n",
      "three-layer Loss after iteration 41000: 6.7609030692557885\n",
      "three-layer Loss after iteration 42000: 6.715342625937107\n",
      "three-layer Loss after iteration 43000: 6.668331999980963\n",
      "three-layer Loss after iteration 44000: 6.634399020499487\n",
      "three-layer Loss after iteration 45000: 6.607234356606294\n",
      "three-layer Loss after iteration 46000: 6.5863801581447925\n",
      "three-layer Loss after iteration 47000: 6.573614910649196\n",
      "three-layer Loss after iteration 48000: 6.553527389562704\n",
      "three-layer Loss after iteration 49000: 6.538558113752695\n",
      "three-layer Loss after iteration 50000: 6.526899238607122\n",
      "three-layer Loss after iteration 51000: 6.515458732278783\n",
      "three-layer Loss after iteration 52000: 6.505273792466061\n",
      "three-layer Loss after iteration 53000: 6.493241688922426\n",
      "three-layer Loss after iteration 54000: 6.484465753717551\n",
      "three-layer Loss after iteration 55000: 6.474677123718296\n",
      "three-layer Loss after iteration 56000: 6.469250620662201\n",
      "three-layer Loss after iteration 57000: 6.463739572151303\n",
      "three-layer Loss after iteration 58000: 6.459424173080663\n",
      "three-layer Loss after iteration 59000: 6.450265990127556\n",
      "three-layer Loss after iteration 60000: 6.4445250326935595\n",
      "three-layer Loss after iteration 61000: 6.439409060683809\n",
      "three-layer Loss after iteration 62000: 6.435193365923737\n",
      "three-layer Loss after iteration 63000: 6.430150226049575\n",
      "three-layer Loss after iteration 64000: 6.427161176474549\n",
      "three-layer Loss after iteration 65000: 6.423258671057332\n",
      "three-layer Loss after iteration 66000: 6.419761614547283\n",
      "three-layer Loss after iteration 67000: 6.417846523454221\n",
      "three-layer Loss after iteration 68000: 6.4147162407647755\n",
      "three-layer Loss after iteration 69000: 6.413112650457323\n",
      "three-layer Loss after iteration 70000: 6.41102498261673\n",
      "three-layer Loss after iteration 71000: 6.408096415534799\n",
      "three-layer Loss after iteration 72000: 6.407193510615984\n",
      "three-layer Loss after iteration 73000: 6.405144908401061\n",
      "three-layer Loss after iteration 74000: 6.404251259468302\n",
      "three-layer Loss after iteration 75000: 6.405629168618331\n",
      "three-layer Loss after iteration 76000: 6.411827140049579\n",
      "three-layer Loss after iteration 77000: 6.411681078446581\n",
      "2.2780028189627704e-05 6.411827140049579 6.411681078446581\n",
      "three-layer Loss after iteration 0: 1687.2582111285833\n",
      "three-layer Loss after iteration 1000: 22.378038128617266\n",
      "three-layer Loss after iteration 2000: 17.120762526422286\n",
      "three-layer Loss after iteration 3000: 15.030937212325593\n",
      "three-layer Loss after iteration 4000: 14.718005093126923\n",
      "three-layer Loss after iteration 5000: 12.346516986970116\n",
      "three-layer Loss after iteration 6000: 11.58105345023764\n",
      "three-layer Loss after iteration 7000: 10.867918918999045\n",
      "three-layer Loss after iteration 8000: 11.091691935245219\n",
      "three-layer Loss after iteration 9000: 10.244916471891132\n",
      "three-layer Loss after iteration 10000: 10.1470449427936\n",
      "three-layer Loss after iteration 11000: 9.368930681516778\n",
      "three-layer Loss after iteration 12000: 8.057059393229181\n",
      "three-layer Loss after iteration 13000: 8.558525645669004\n",
      "three-layer Loss after iteration 14000: 8.05780827056003\n",
      "three-layer Loss after iteration 15000: 7.806772064195114\n",
      "three-layer Loss after iteration 16000: 7.6190035119660875\n",
      "three-layer Loss after iteration 17000: 7.1588514249770965\n",
      "three-layer Loss after iteration 18000: 6.713108350220847\n",
      "three-layer Loss after iteration 19000: 6.8311232927720535\n",
      "three-layer Loss after iteration 20000: 6.7755798244963765\n",
      "three-layer Loss after iteration 21000: 6.5970741642076325\n",
      "three-layer Loss after iteration 22000: 6.4862720179887665\n",
      "three-layer Loss after iteration 23000: 6.259641482910431\n",
      "three-layer Loss after iteration 24000: 6.0629262113356726\n",
      "three-layer Loss after iteration 25000: 6.626877000140892\n",
      "three-layer Loss after iteration 26000: 5.288706759455403\n",
      "three-layer Loss after iteration 27000: 6.43093772361063\n",
      "three-layer Loss after iteration 28000: 5.301131865059186\n",
      "three-layer Loss after iteration 29000: 5.4274142524833\n",
      "three-layer Loss after iteration 30000: 5.331639059895796\n",
      "three-layer Loss after iteration 31000: 5.230394668507625\n",
      "three-layer Loss after iteration 32000: 5.803123066292899\n",
      "three-layer Loss after iteration 33000: 5.378345997185739\n",
      "three-layer Loss after iteration 34000: 5.54696604503068\n",
      "three-layer Loss after iteration 35000: 5.487450246977957\n",
      "three-layer Loss after iteration 36000: 5.380643506581466\n",
      "three-layer Loss after iteration 37000: 5.305065580486241\n",
      "three-layer Loss after iteration 38000: 5.2197858889100095\n",
      "three-layer Loss after iteration 39000: 5.130173893433657\n",
      "three-layer Loss after iteration 40000: 5.053244096338692\n",
      "three-layer Loss after iteration 41000: 4.9802134562357505\n",
      "three-layer Loss after iteration 42000: 4.920044119860027\n",
      "three-layer Loss after iteration 43000: 4.848918506584679\n",
      "three-layer Loss after iteration 44000: 4.8974217519514305\n",
      "three-layer Loss after iteration 45000: 4.858064026418077\n",
      "three-layer Loss after iteration 46000: 4.788013626458522\n",
      "three-layer Loss after iteration 47000: 4.732389338800635\n",
      "three-layer Loss after iteration 48000: 4.67520944689842\n",
      "three-layer Loss after iteration 49000: 4.624263499405843\n",
      "three-layer Loss after iteration 50000: 4.581305282660602\n",
      "three-layer Loss after iteration 51000: 4.535692433035434\n",
      "three-layer Loss after iteration 52000: 4.49193280367625\n",
      "three-layer Loss after iteration 53000: 5.152002963421002\n",
      "three-layer Loss after iteration 54000: 4.730001400312905\n",
      "three-layer Loss after iteration 55000: 5.675194907067996\n",
      "three-layer Loss after iteration 56000: 3.9309845057031363\n",
      "three-layer Loss after iteration 57000: 4.671463341036466\n",
      "three-layer Loss after iteration 58000: 3.8818010719113953\n",
      "three-layer Loss after iteration 59000: 4.201045250907748\n",
      "three-layer Loss after iteration 60000: 4.951162290204134\n",
      "three-layer Loss after iteration 61000: 3.8717379832182095\n",
      "three-layer Loss after iteration 62000: 5.31402561033474\n",
      "three-layer Loss after iteration 63000: 3.8704453587253016\n",
      "three-layer Loss after iteration 64000: 4.399935378520267\n",
      "three-layer Loss after iteration 65000: 4.092904912133347\n",
      "three-layer Loss after iteration 66000: 3.8321840879979434\n",
      "three-layer Loss after iteration 67000: 3.8338638775563267\n",
      "three-layer Loss after iteration 68000: 4.651084134606154\n",
      "three-layer Loss after iteration 69000: 5.071269519467605\n",
      "three-layer Loss after iteration 70000: 4.826443569658124\n",
      "three-layer Loss after iteration 71000: 4.637527524237409\n",
      "three-layer Loss after iteration 72000: 4.511862427657293\n",
      "three-layer Loss after iteration 73000: 4.322625620220245\n",
      "three-layer Loss after iteration 74000: 4.306063420027432\n",
      "three-layer Loss after iteration 75000: 4.243813108609823\n",
      "three-layer Loss after iteration 76000: 4.224838865096649\n",
      "three-layer Loss after iteration 77000: 4.561573989413857\n",
      "three-layer Loss after iteration 78000: 4.806504472707515\n",
      "three-layer Loss after iteration 79000: 4.032121714461195\n",
      "three-layer Loss after iteration 80000: 3.8072375481731897\n",
      "three-layer Loss after iteration 81000: 4.140622579728413\n",
      "three-layer Loss after iteration 82000: 4.003772907243641\n",
      "three-layer Loss after iteration 83000: 3.876906978163747\n",
      "three-layer Loss after iteration 84000: 4.120766421416799\n",
      "three-layer Loss after iteration 85000: 4.0407460669895725\n",
      "three-layer Loss after iteration 86000: 3.806688192971467\n",
      "three-layer Loss after iteration 87000: 3.7979512375111324\n",
      "three-layer Loss after iteration 88000: 4.436946155625531\n",
      "three-layer Loss after iteration 89000: 3.8056572599077247\n",
      "three-layer Loss after iteration 90000: 4.124121743145794\n",
      "three-layer Loss after iteration 91000: 3.8035358476255996\n",
      "three-layer Loss after iteration 92000: 3.9825574643686434\n",
      "three-layer Loss after iteration 93000: 3.807239970240666\n",
      "three-layer Loss after iteration 94000: 4.139728514057536\n",
      "three-layer Loss after iteration 95000: 3.8030787352986732\n",
      "three-layer Loss after iteration 96000: 3.821231156100835\n",
      "three-layer Loss after iteration 97000: 3.8029228921936915\n",
      "three-layer Loss after iteration 98000: 3.805365030521515\n",
      "three-layer Loss after iteration 99000: 3.813007071587286\n",
      "three-layer Loss after iteration 0: 1698.9263617008874\n",
      "three-layer Loss after iteration 1000: 245.24045587440662\n",
      "three-layer Loss after iteration 2000: 244.0944716132015\n",
      "three-layer Loss after iteration 3000: 243.04282970223693\n",
      "three-layer Loss after iteration 4000: 242.29519305036965\n",
      "three-layer Loss after iteration 5000: 240.83434975883787\n",
      "three-layer Loss after iteration 6000: 24.585609645296156\n",
      "three-layer Loss after iteration 7000: 17.493225413013235\n",
      "three-layer Loss after iteration 8000: 15.148671065220277\n",
      "three-layer Loss after iteration 9000: 13.49180622328268\n",
      "three-layer Loss after iteration 10000: 12.449550794545374\n",
      "three-layer Loss after iteration 11000: 12.807330802952604\n",
      "three-layer Loss after iteration 12000: 7.930744597058348\n",
      "three-layer Loss after iteration 13000: 14.720835186351344\n",
      "three-layer Loss after iteration 14000: 10.89561590495047\n",
      "three-layer Loss after iteration 15000: 9.080499487244941\n",
      "three-layer Loss after iteration 16000: 11.930127415400506\n",
      "three-layer Loss after iteration 17000: 9.005306058543246\n",
      "three-layer Loss after iteration 18000: 8.362940984813939\n",
      "three-layer Loss after iteration 19000: 8.19953555490156\n",
      "three-layer Loss after iteration 20000: 7.648075071265426\n",
      "three-layer Loss after iteration 21000: 7.075424775995918\n",
      "three-layer Loss after iteration 22000: 7.446236495876641\n",
      "three-layer Loss after iteration 23000: 7.298458682521214\n",
      "three-layer Loss after iteration 24000: 7.243410142379715\n",
      "three-layer Loss after iteration 25000: 7.081960437354493\n",
      "three-layer Loss after iteration 26000: 7.031807338448052\n",
      "three-layer Loss after iteration 27000: 7.1009118282232\n",
      "three-layer Loss after iteration 28000: 7.094137913341718\n",
      "three-layer Loss after iteration 29000: 6.911990594674625\n",
      "three-layer Loss after iteration 30000: 6.748363050679384\n",
      "three-layer Loss after iteration 31000: 6.745768447280199\n",
      "three-layer Loss after iteration 32000: 6.52841534199528\n",
      "three-layer Loss after iteration 33000: 6.437083481282674\n",
      "three-layer Loss after iteration 34000: 6.318471586699878\n",
      "three-layer Loss after iteration 35000: 6.257925355803667\n",
      "three-layer Loss after iteration 36000: 6.173084088696175\n",
      "three-layer Loss after iteration 37000: 6.235086951041887\n",
      "three-layer Loss after iteration 38000: 6.510959734218882\n",
      "three-layer Loss after iteration 39000: 6.440894307973382\n",
      "three-layer Loss after iteration 40000: 6.414954842801853\n",
      "three-layer Loss after iteration 41000: 6.401792454608739\n",
      "three-layer Loss after iteration 42000: 6.682696041659624\n",
      "three-layer Loss after iteration 43000: 6.711937029223518\n",
      "three-layer Loss after iteration 44000: 5.649787621245061\n",
      "three-layer Loss after iteration 45000: 5.164622462917205\n",
      "three-layer Loss after iteration 46000: 5.115470998977908\n",
      "three-layer Loss after iteration 47000: 4.886353706374108\n",
      "three-layer Loss after iteration 48000: 6.060069174420488\n",
      "three-layer Loss after iteration 49000: 7.235707394393137\n",
      "three-layer Loss after iteration 50000: 7.109839262190564\n",
      "three-layer Loss after iteration 51000: 4.909389848888666\n",
      "three-layer Loss after iteration 52000: 6.591556385972933\n",
      "three-layer Loss after iteration 53000: 4.528085108641275\n",
      "three-layer Loss after iteration 54000: 4.235401488390367\n",
      "three-layer Loss after iteration 55000: 4.162996450167308\n",
      "three-layer Loss after iteration 56000: 4.02380353309913\n",
      "three-layer Loss after iteration 57000: 4.060912953124899\n",
      "three-layer Loss after iteration 58000: 4.43896731837658\n",
      "three-layer Loss after iteration 59000: 4.023395518864809\n",
      "three-layer Loss after iteration 60000: 4.153307402462938\n",
      "three-layer Loss after iteration 61000: 4.088015689889574\n",
      "three-layer Loss after iteration 62000: 5.683531390816647\n",
      "three-layer Loss after iteration 63000: 4.7969623127662695\n",
      "three-layer Loss after iteration 64000: 4.053734717735932\n",
      "three-layer Loss after iteration 65000: 4.8311602469034005\n",
      "three-layer Loss after iteration 66000: 4.410687561719162\n",
      "three-layer Loss after iteration 67000: 4.659701006772932\n",
      "three-layer Loss after iteration 68000: 4.096818816933721\n",
      "three-layer Loss after iteration 69000: 4.229192650033719\n",
      "three-layer Loss after iteration 70000: 5.5372494949986075\n",
      "three-layer Loss after iteration 71000: 4.146282101530363\n",
      "three-layer Loss after iteration 72000: 4.293687333100643\n",
      "three-layer Loss after iteration 73000: 4.232034389076517\n",
      "three-layer Loss after iteration 74000: 4.084733633348139\n",
      "three-layer Loss after iteration 75000: 4.140134796125676\n",
      "three-layer Loss after iteration 76000: 4.2896077185319434\n",
      "three-layer Loss after iteration 77000: 6.162121181470985\n",
      "three-layer Loss after iteration 78000: 4.102492307508527\n",
      "three-layer Loss after iteration 79000: 5.016392886947916\n",
      "three-layer Loss after iteration 80000: 4.200442631039676\n",
      "three-layer Loss after iteration 81000: 5.423796695155325\n",
      "three-layer Loss after iteration 82000: 4.363501220323452\n",
      "three-layer Loss after iteration 83000: 5.056162677174815\n",
      "three-layer Loss after iteration 84000: 4.297547456789121\n",
      "three-layer Loss after iteration 85000: 4.800696740000231\n",
      "three-layer Loss after iteration 86000: 4.4399244616647735\n",
      "three-layer Loss after iteration 87000: 4.561243723300128\n",
      "three-layer Loss after iteration 88000: 4.4985442270688525\n",
      "three-layer Loss after iteration 89000: 4.986122093170193\n",
      "three-layer Loss after iteration 90000: 4.036143889249896\n",
      "three-layer Loss after iteration 91000: 5.215007418015289\n",
      "three-layer Loss after iteration 92000: 5.112495075931771\n",
      "three-layer Loss after iteration 93000: 4.313981625258274\n",
      "three-layer Loss after iteration 94000: 4.618634196890836\n",
      "three-layer Loss after iteration 95000: 4.005058979467781\n",
      "three-layer Loss after iteration 96000: 4.355735257758438\n",
      "three-layer Loss after iteration 97000: 4.195400640636204\n",
      "three-layer Loss after iteration 98000: 5.207743313000184\n",
      "three-layer Loss after iteration 99000: 3.999027370875976\n",
      "three-layer Loss after iteration 0: 1712.5326904172439\n",
      "three-layer Loss after iteration 1000: 14.750202247265298\n",
      "three-layer Loss after iteration 2000: 15.393068515408244\n",
      "three-layer Loss after iteration 3000: 10.669149145411563\n",
      "three-layer Loss after iteration 4000: 10.444043275000947\n",
      "three-layer Loss after iteration 5000: 9.60058798045601\n",
      "three-layer Loss after iteration 6000: 8.990430449476676\n",
      "three-layer Loss after iteration 7000: 8.715192569542877\n",
      "three-layer Loss after iteration 8000: 8.605299920607683\n",
      "three-layer Loss after iteration 9000: 8.40909755943392\n",
      "three-layer Loss after iteration 10000: 8.09319768527351\n",
      "three-layer Loss after iteration 11000: 8.800196504587687\n",
      "three-layer Loss after iteration 12000: 8.607994264048816\n",
      "three-layer Loss after iteration 13000: 8.321493819323331\n",
      "three-layer Loss after iteration 14000: 8.558554336285098\n",
      "three-layer Loss after iteration 15000: 7.813782260053979\n",
      "three-layer Loss after iteration 16000: 7.876341244658606\n",
      "three-layer Loss after iteration 17000: 9.198875918516666\n",
      "three-layer Loss after iteration 18000: 8.216352355779229\n",
      "three-layer Loss after iteration 19000: 7.742173245723512\n",
      "three-layer Loss after iteration 20000: 7.567874392463363\n",
      "three-layer Loss after iteration 21000: 7.249010441376514\n",
      "three-layer Loss after iteration 22000: 7.031646174649539\n",
      "three-layer Loss after iteration 23000: 8.561941987680987\n",
      "three-layer Loss after iteration 24000: 7.486360264703364\n",
      "three-layer Loss after iteration 25000: 6.9498931281202445\n",
      "three-layer Loss after iteration 26000: 7.203866422315555\n",
      "three-layer Loss after iteration 27000: 6.887271409629398\n",
      "three-layer Loss after iteration 28000: 6.389579656314213\n",
      "three-layer Loss after iteration 29000: 6.708391192597523\n",
      "three-layer Loss after iteration 30000: 6.5887364953732765\n",
      "three-layer Loss after iteration 31000: 6.382990080502322\n",
      "three-layer Loss after iteration 32000: 6.189008775508074\n",
      "three-layer Loss after iteration 33000: 6.141909242712578\n",
      "three-layer Loss after iteration 34000: 5.924595392130811\n",
      "three-layer Loss after iteration 35000: 5.9797822145763595\n",
      "three-layer Loss after iteration 36000: 5.9552495123928875\n",
      "three-layer Loss after iteration 37000: 5.882400823314426\n",
      "three-layer Loss after iteration 38000: 5.821115283923981\n",
      "three-layer Loss after iteration 39000: 5.7712936497384115\n",
      "three-layer Loss after iteration 40000: 5.7269237074781305\n",
      "three-layer Loss after iteration 41000: 5.6870913214084515\n",
      "three-layer Loss after iteration 42000: 5.651608852520528\n",
      "three-layer Loss after iteration 43000: 5.381766502914743\n",
      "three-layer Loss after iteration 44000: 5.8592489594711115\n",
      "three-layer Loss after iteration 45000: 5.328169702951306\n",
      "three-layer Loss after iteration 46000: 5.398303499820171\n",
      "three-layer Loss after iteration 47000: 5.922921783553818\n",
      "three-layer Loss after iteration 48000: 5.684932582877017\n",
      "three-layer Loss after iteration 49000: 5.831299577660478\n",
      "three-layer Loss after iteration 50000: 6.127190155993524\n",
      "three-layer Loss after iteration 51000: 5.39583127421737\n",
      "three-layer Loss after iteration 52000: 5.825285839522987\n",
      "three-layer Loss after iteration 53000: 5.859447052166037\n",
      "three-layer Loss after iteration 54000: 5.565301706622186\n",
      "three-layer Loss after iteration 55000: 6.004838978890788\n",
      "three-layer Loss after iteration 56000: 5.64146994291999\n",
      "three-layer Loss after iteration 57000: 8.106828087089443\n",
      "three-layer Loss after iteration 58000: 6.2333024882972685\n",
      "three-layer Loss after iteration 59000: 6.1187107591340055\n",
      "three-layer Loss after iteration 60000: 6.80835990768331\n",
      "three-layer Loss after iteration 61000: 6.738525139420181\n",
      "three-layer Loss after iteration 62000: 6.742854199269728\n",
      "three-layer Loss after iteration 63000: 6.571382663395719\n",
      "three-layer Loss after iteration 64000: 5.925832760892816\n",
      "three-layer Loss after iteration 65000: 6.160569313697769\n",
      "three-layer Loss after iteration 66000: 6.05594716463088\n",
      "three-layer Loss after iteration 67000: 5.941226791222056\n",
      "three-layer Loss after iteration 68000: 6.110375977823666\n",
      "three-layer Loss after iteration 69000: 6.623605715668213\n",
      "three-layer Loss after iteration 70000: 6.01965650730067\n",
      "three-layer Loss after iteration 71000: 6.25774679835565\n",
      "three-layer Loss after iteration 72000: 5.853447565821609\n",
      "three-layer Loss after iteration 73000: 5.535760130640052\n",
      "three-layer Loss after iteration 74000: 5.7218605657320305\n",
      "three-layer Loss after iteration 75000: 5.760778749215229\n",
      "three-layer Loss after iteration 76000: 5.555346296219548\n",
      "three-layer Loss after iteration 77000: 6.22833984940735\n",
      "three-layer Loss after iteration 78000: 6.654579258130145\n",
      "three-layer Loss after iteration 79000: 6.20520458825737\n",
      "three-layer Loss after iteration 80000: 6.127819962944792\n",
      "three-layer Loss after iteration 81000: 5.98973838264682\n",
      "three-layer Loss after iteration 82000: 5.898247892575889\n",
      "three-layer Loss after iteration 83000: 5.9198655397329\n",
      "three-layer Loss after iteration 84000: 5.776550974849074\n",
      "three-layer Loss after iteration 85000: 5.679110375710342\n",
      "three-layer Loss after iteration 86000: 5.840579738769948\n",
      "three-layer Loss after iteration 87000: 5.868131771432103\n",
      "three-layer Loss after iteration 88000: 5.3111809052708985\n",
      "three-layer Loss after iteration 89000: 5.6771318577687815\n",
      "three-layer Loss after iteration 90000: 6.0035124895111345\n",
      "three-layer Loss after iteration 91000: 6.208921284653831\n",
      "three-layer Loss after iteration 92000: 6.430803655746669\n",
      "three-layer Loss after iteration 93000: 6.08461108325443\n",
      "three-layer Loss after iteration 94000: 5.294686517140442\n",
      "three-layer Loss after iteration 95000: 6.836421013976691\n",
      "three-layer Loss after iteration 96000: 6.67691220692139\n",
      "three-layer Loss after iteration 97000: 6.9093900454244785\n",
      "three-layer Loss after iteration 98000: 5.435010443413745\n",
      "three-layer Loss after iteration 99000: 5.105173453430588\n",
      "three-layer Loss after iteration 0: 1543.8005397087538\n",
      "three-layer Loss after iteration 1000: 27.137119607630286\n",
      "three-layer Loss after iteration 2000: 28.9203835133614\n",
      "three-layer Loss after iteration 3000: 23.651484605380944\n",
      "three-layer Loss after iteration 4000: 22.85511001103473\n",
      "three-layer Loss after iteration 5000: 25.685460728751888\n",
      "three-layer Loss after iteration 6000: 22.98014042723569\n",
      "three-layer Loss after iteration 7000: 19.75710355705383\n",
      "three-layer Loss after iteration 8000: 19.76979987387828\n",
      "three-layer Loss after iteration 9000: 18.290179207455097\n",
      "three-layer Loss after iteration 10000: 21.133777546382916\n",
      "three-layer Loss after iteration 11000: 21.762021227324116\n",
      "three-layer Loss after iteration 12000: 20.834159110546082\n",
      "three-layer Loss after iteration 13000: 25.646053016224045\n",
      "three-layer Loss after iteration 14000: 25.602568414510106\n",
      "three-layer Loss after iteration 15000: 19.078770107098187\n",
      "three-layer Loss after iteration 16000: 17.833472085012094\n",
      "three-layer Loss after iteration 17000: 18.185749128853335\n",
      "three-layer Loss after iteration 18000: 16.4853922544768\n",
      "three-layer Loss after iteration 19000: 16.894185384110546\n",
      "three-layer Loss after iteration 20000: 16.708861952904215\n",
      "three-layer Loss after iteration 21000: 16.022401926584696\n",
      "three-layer Loss after iteration 22000: 15.638256711853\n",
      "three-layer Loss after iteration 23000: 15.696260049576203\n",
      "three-layer Loss after iteration 24000: 15.256625974739917\n",
      "three-layer Loss after iteration 25000: 14.92159349417311\n",
      "three-layer Loss after iteration 26000: 14.370471242967527\n",
      "three-layer Loss after iteration 27000: 14.437969457098088\n",
      "three-layer Loss after iteration 28000: 13.897568379798773\n",
      "three-layer Loss after iteration 29000: 13.72946278873422\n",
      "three-layer Loss after iteration 30000: 13.375046230096892\n",
      "three-layer Loss after iteration 31000: 13.199943975878346\n",
      "three-layer Loss after iteration 32000: 12.982559080778907\n",
      "three-layer Loss after iteration 33000: 12.85797143073178\n",
      "three-layer Loss after iteration 34000: 12.65501759950773\n",
      "three-layer Loss after iteration 35000: 12.757637211885138\n",
      "three-layer Loss after iteration 36000: 12.404921525427156\n",
      "three-layer Loss after iteration 37000: 12.306175302695694\n",
      "three-layer Loss after iteration 38000: 12.189256241632087\n",
      "three-layer Loss after iteration 39000: 12.03440480360858\n",
      "three-layer Loss after iteration 40000: 11.669458567335129\n",
      "three-layer Loss after iteration 41000: 10.366364735452638\n",
      "three-layer Loss after iteration 42000: 10.918333639220315\n",
      "three-layer Loss after iteration 43000: 10.58090319580636\n",
      "three-layer Loss after iteration 44000: 11.712964454102055\n",
      "three-layer Loss after iteration 45000: 11.274092742357675\n",
      "three-layer Loss after iteration 46000: 10.861283069465758\n",
      "three-layer Loss after iteration 47000: 12.190029711707833\n",
      "three-layer Loss after iteration 48000: 11.240130828076907\n",
      "three-layer Loss after iteration 49000: 10.546225538673426\n",
      "three-layer Loss after iteration 50000: 10.804042138803027\n",
      "three-layer Loss after iteration 51000: 11.012853933393247\n",
      "three-layer Loss after iteration 52000: 10.817472622061182\n",
      "three-layer Loss after iteration 53000: 10.701041069396325\n",
      "three-layer Loss after iteration 54000: 11.05885822539352\n",
      "three-layer Loss after iteration 55000: 10.910525997060383\n",
      "three-layer Loss after iteration 56000: 10.782757634701696\n",
      "three-layer Loss after iteration 57000: 10.681860252764693\n",
      "three-layer Loss after iteration 58000: 10.591016722067927\n",
      "three-layer Loss after iteration 59000: 10.507883049012644\n",
      "three-layer Loss after iteration 60000: 10.301767050232394\n",
      "three-layer Loss after iteration 61000: 10.4778338149853\n",
      "three-layer Loss after iteration 62000: 10.43293870315457\n",
      "three-layer Loss after iteration 63000: 10.368380272694791\n",
      "three-layer Loss after iteration 64000: 10.301906785348514\n",
      "three-layer Loss after iteration 65000: 10.386771728013818\n",
      "three-layer Loss after iteration 66000: 10.36644334506462\n",
      "three-layer Loss after iteration 67000: 10.314047085094515\n",
      "three-layer Loss after iteration 68000: 10.299615783636614\n",
      "three-layer Loss after iteration 69000: 10.234786098229662\n",
      "three-layer Loss after iteration 70000: 10.169526631003446\n",
      "three-layer Loss after iteration 71000: 10.10824008280229\n",
      "three-layer Loss after iteration 72000: 10.050325882145529\n",
      "three-layer Loss after iteration 73000: 9.995380297213297\n",
      "three-layer Loss after iteration 74000: 9.94313014034766\n",
      "three-layer Loss after iteration 75000: 9.893364212334985\n",
      "three-layer Loss after iteration 76000: 9.845907972839733\n",
      "three-layer Loss after iteration 77000: 9.80061208017339\n",
      "three-layer Loss after iteration 78000: 9.757345752726291\n",
      "three-layer Loss after iteration 79000: 9.715992396751606\n",
      "three-layer Loss after iteration 80000: 9.676446587891135\n",
      "three-layer Loss after iteration 81000: 9.638611945620323\n",
      "three-layer Loss after iteration 82000: 9.600503280696483\n",
      "three-layer Loss after iteration 83000: 9.563254489087425\n",
      "three-layer Loss after iteration 84000: 9.528529323963776\n",
      "three-layer Loss after iteration 85000: 9.495458557444948\n",
      "three-layer Loss after iteration 86000: 9.463977318598287\n",
      "three-layer Loss after iteration 87000: 9.434026330425759\n",
      "three-layer Loss after iteration 88000: 9.404935916470349\n",
      "three-layer Loss after iteration 89000: 9.37772282618111\n",
      "three-layer Loss after iteration 90000: 9.350913250545075\n",
      "three-layer Loss after iteration 91000: 9.325656399544568\n",
      "three-layer Loss after iteration 92000: 9.300955194403775\n",
      "three-layer Loss after iteration 93000: 9.278217884122729\n",
      "three-layer Loss after iteration 94000: 9.255531052104116\n",
      "three-layer Loss after iteration 95000: 9.234431555446022\n",
      "three-layer Loss after iteration 96000: 9.213837567623818\n",
      "three-layer Loss after iteration 97000: 9.194383248638369\n",
      "three-layer Loss after iteration 98000: 9.175569792939786\n",
      "three-layer Loss after iteration 99000: 9.157078659370166\n",
      "three-layer Loss after iteration 0: 1719.3480707114481\n",
      "three-layer Loss after iteration 1000: 26.629966064393447\n",
      "three-layer Loss after iteration 2000: 23.64592611208232\n",
      "three-layer Loss after iteration 3000: 21.741809715593913\n",
      "three-layer Loss after iteration 4000: 21.892884342355803\n",
      "three-layer Loss after iteration 5000: 21.673665600496452\n",
      "three-layer Loss after iteration 6000: 20.66679569023474\n",
      "three-layer Loss after iteration 7000: 21.03593662240536\n",
      "three-layer Loss after iteration 8000: 19.416961579243118\n",
      "three-layer Loss after iteration 9000: 17.84325220347167\n",
      "three-layer Loss after iteration 10000: 17.129274194526005\n",
      "three-layer Loss after iteration 11000: 16.183895719709604\n",
      "three-layer Loss after iteration 12000: 15.558400544770354\n",
      "three-layer Loss after iteration 13000: 15.164444409414582\n",
      "three-layer Loss after iteration 14000: 14.955572733614602\n",
      "three-layer Loss after iteration 15000: 14.368602058707697\n",
      "three-layer Loss after iteration 16000: 13.95713473638918\n",
      "three-layer Loss after iteration 17000: 13.591600687508762\n",
      "three-layer Loss after iteration 18000: 13.187714798780375\n",
      "three-layer Loss after iteration 19000: 12.83358103647038\n",
      "three-layer Loss after iteration 20000: 13.059777159883447\n",
      "three-layer Loss after iteration 21000: 12.167302584400852\n",
      "three-layer Loss after iteration 22000: 12.424109118185847\n",
      "three-layer Loss after iteration 23000: 12.08880377907103\n",
      "three-layer Loss after iteration 24000: 11.863724388618829\n",
      "three-layer Loss after iteration 25000: 11.82448599929431\n",
      "three-layer Loss after iteration 26000: 11.489248111793515\n",
      "three-layer Loss after iteration 27000: 11.369759070338043\n",
      "three-layer Loss after iteration 28000: 11.318692117492533\n",
      "three-layer Loss after iteration 29000: 11.145023068307601\n",
      "three-layer Loss after iteration 30000: 10.958450514522578\n",
      "three-layer Loss after iteration 31000: 10.796370076566106\n",
      "three-layer Loss after iteration 32000: 11.333042650602353\n",
      "three-layer Loss after iteration 33000: 10.79302034266411\n",
      "three-layer Loss after iteration 34000: 10.696820200526114\n",
      "three-layer Loss after iteration 35000: 10.405985924947947\n",
      "three-layer Loss after iteration 36000: 10.318102406615713\n",
      "three-layer Loss after iteration 37000: 10.209227401521716\n",
      "three-layer Loss after iteration 38000: 10.129713556543372\n",
      "three-layer Loss after iteration 39000: 10.04704520229004\n",
      "three-layer Loss after iteration 40000: 9.960454109313401\n",
      "three-layer Loss after iteration 41000: 9.856426716286704\n",
      "three-layer Loss after iteration 42000: 9.796763826832862\n",
      "three-layer Loss after iteration 43000: 9.74223564565801\n",
      "three-layer Loss after iteration 44000: 8.999491612207851\n",
      "three-layer Loss after iteration 45000: 9.455711473821589\n",
      "three-layer Loss after iteration 46000: 9.70620718098498\n",
      "three-layer Loss after iteration 47000: 10.419794610007884\n",
      "three-layer Loss after iteration 48000: 10.083417647242689\n",
      "three-layer Loss after iteration 49000: 9.97657794271832\n",
      "three-layer Loss after iteration 50000: 9.864020939263007\n",
      "three-layer Loss after iteration 51000: 9.820452934261535\n",
      "three-layer Loss after iteration 52000: 9.705131895653764\n",
      "three-layer Loss after iteration 53000: 9.575875546410007\n",
      "three-layer Loss after iteration 54000: 9.493185108243882\n",
      "three-layer Loss after iteration 55000: 9.494458707219229\n",
      "three-layer Loss after iteration 56000: 9.426790483672642\n",
      "three-layer Loss after iteration 57000: 9.458138583547065\n",
      "three-layer Loss after iteration 58000: 9.500536493844374\n",
      "three-layer Loss after iteration 59000: 9.46568687436259\n",
      "three-layer Loss after iteration 60000: 9.715944481978864\n",
      "three-layer Loss after iteration 61000: 9.59356917650743\n",
      "three-layer Loss after iteration 62000: 10.045286563938308\n",
      "three-layer Loss after iteration 63000: 9.466552072028126\n",
      "three-layer Loss after iteration 64000: 9.64953268292532\n",
      "three-layer Loss after iteration 65000: 9.787645560328786\n",
      "three-layer Loss after iteration 66000: 9.597111971612044\n",
      "three-layer Loss after iteration 67000: 9.52239651964078\n",
      "three-layer Loss after iteration 68000: 9.33005372167029\n",
      "three-layer Loss after iteration 69000: 11.087202131998767\n",
      "three-layer Loss after iteration 70000: 9.393453895356373\n",
      "three-layer Loss after iteration 71000: 9.45682593602656\n",
      "three-layer Loss after iteration 72000: 9.549839549513145\n",
      "three-layer Loss after iteration 73000: 9.281254889755706\n",
      "three-layer Loss after iteration 74000: 9.679435999621461\n",
      "three-layer Loss after iteration 75000: 10.009195233542908\n",
      "three-layer Loss after iteration 76000: 9.564267227357897\n",
      "three-layer Loss after iteration 77000: 9.443883487265804\n",
      "three-layer Loss after iteration 78000: 9.345030280596657\n",
      "three-layer Loss after iteration 79000: 9.303715347136876\n",
      "three-layer Loss after iteration 80000: 9.505089442209004\n",
      "three-layer Loss after iteration 81000: 9.10911735489932\n",
      "three-layer Loss after iteration 82000: 8.652153374130478\n",
      "three-layer Loss after iteration 83000: 8.589486194153848\n",
      "three-layer Loss after iteration 84000: 8.389416155748073\n",
      "three-layer Loss after iteration 85000: 7.801545623146696\n",
      "three-layer Loss after iteration 86000: 8.120142059888693\n",
      "three-layer Loss after iteration 87000: 7.934703708194898\n",
      "three-layer Loss after iteration 88000: 8.444948659302963\n",
      "three-layer Loss after iteration 89000: 7.928626661866738\n",
      "three-layer Loss after iteration 90000: 7.960885268695225\n",
      "three-layer Loss after iteration 91000: 8.249756110075385\n",
      "three-layer Loss after iteration 92000: 8.761143810151585\n",
      "three-layer Loss after iteration 93000: 9.679391565492553\n",
      "three-layer Loss after iteration 94000: 8.776064199306344\n",
      "three-layer Loss after iteration 95000: 8.690999585088921\n",
      "three-layer Loss after iteration 96000: 8.633860757580534\n",
      "three-layer Loss after iteration 97000: 8.767462071931106\n",
      "three-layer Loss after iteration 98000: 8.93756844379524\n",
      "three-layer Loss after iteration 99000: 8.68953208689747\n",
      "three-layer Loss after iteration 0: 1679.336050302068\n",
      "three-layer Loss after iteration 1000: 33.806833339040764\n",
      "three-layer Loss after iteration 2000: 28.197924472102436\n",
      "three-layer Loss after iteration 3000: 23.541884527422518\n",
      "three-layer Loss after iteration 4000: 23.890262043864308\n",
      "three-layer Loss after iteration 5000: 22.882241170018922\n",
      "three-layer Loss after iteration 6000: 22.232343916709905\n",
      "three-layer Loss after iteration 7000: 14.756612849941504\n",
      "three-layer Loss after iteration 8000: 15.739347984259862\n",
      "three-layer Loss after iteration 9000: 12.07416586701174\n",
      "three-layer Loss after iteration 10000: 11.96095237139918\n",
      "three-layer Loss after iteration 11000: 10.529506452778067\n",
      "three-layer Loss after iteration 12000: 9.96843837745412\n",
      "three-layer Loss after iteration 13000: 9.709892181633995\n",
      "three-layer Loss after iteration 14000: 9.13762185076724\n",
      "three-layer Loss after iteration 15000: 8.882027375368759\n",
      "three-layer Loss after iteration 16000: 9.141650992120855\n",
      "three-layer Loss after iteration 17000: 8.83980971507738\n",
      "three-layer Loss after iteration 18000: 8.587662147621897\n",
      "three-layer Loss after iteration 19000: 8.706865372775743\n",
      "three-layer Loss after iteration 20000: 8.38616687345884\n",
      "three-layer Loss after iteration 21000: 8.237880523253162\n",
      "three-layer Loss after iteration 22000: 7.639070192099622\n",
      "three-layer Loss after iteration 23000: 8.202628479804678\n",
      "three-layer Loss after iteration 24000: 6.7967842842899415\n",
      "three-layer Loss after iteration 25000: 6.858632127032627\n",
      "three-layer Loss after iteration 26000: 6.6841864327872935\n",
      "three-layer Loss after iteration 27000: 7.35098355878479\n",
      "three-layer Loss after iteration 28000: 7.289202131767531\n",
      "three-layer Loss after iteration 29000: 7.2161189475399965\n",
      "three-layer Loss after iteration 30000: 6.949493576268656\n",
      "three-layer Loss after iteration 31000: 7.460832300084383\n",
      "three-layer Loss after iteration 32000: 7.319731237544477\n",
      "three-layer Loss after iteration 33000: 7.348666168346777\n",
      "three-layer Loss after iteration 34000: 6.871028784632729\n",
      "three-layer Loss after iteration 35000: 7.175203070688425\n",
      "three-layer Loss after iteration 36000: 7.118588494452987\n",
      "three-layer Loss after iteration 37000: 7.143982194425316\n",
      "three-layer Loss after iteration 38000: 7.205085260137569\n",
      "three-layer Loss after iteration 39000: 6.060906198740587\n",
      "three-layer Loss after iteration 40000: 6.868858145697766\n",
      "three-layer Loss after iteration 41000: 6.731737058247948\n",
      "three-layer Loss after iteration 42000: 6.638341025187319\n",
      "three-layer Loss after iteration 43000: 6.659185860116975\n",
      "three-layer Loss after iteration 44000: 6.5587917782283185\n",
      "three-layer Loss after iteration 45000: 6.691747890716866\n",
      "three-layer Loss after iteration 46000: 6.388825661697646\n",
      "three-layer Loss after iteration 47000: 6.254830596994128\n",
      "three-layer Loss after iteration 48000: 6.306918738016129\n",
      "three-layer Loss after iteration 49000: 6.4713300739116875\n",
      "three-layer Loss after iteration 50000: 6.5748865558761524\n",
      "three-layer Loss after iteration 51000: 6.40660671520744\n",
      "three-layer Loss after iteration 52000: 6.08501689357923\n",
      "three-layer Loss after iteration 53000: 6.4533845419251845\n",
      "three-layer Loss after iteration 54000: 6.102789806553883\n",
      "three-layer Loss after iteration 55000: 6.296421943298422\n",
      "three-layer Loss after iteration 56000: 5.992550006015077\n",
      "three-layer Loss after iteration 57000: 5.95829361702147\n",
      "three-layer Loss after iteration 58000: 6.006290494312369\n",
      "three-layer Loss after iteration 59000: 6.008523441970302\n",
      "three-layer Loss after iteration 60000: 5.920916473578248\n",
      "three-layer Loss after iteration 61000: 5.942866550245834\n",
      "three-layer Loss after iteration 62000: 5.823470984314256\n",
      "three-layer Loss after iteration 63000: 5.643494283631318\n",
      "three-layer Loss after iteration 64000: 5.902643548174433\n",
      "three-layer Loss after iteration 65000: 5.883944342040959\n",
      "three-layer Loss after iteration 66000: 5.839906540225675\n",
      "three-layer Loss after iteration 67000: 5.779299769917602\n",
      "three-layer Loss after iteration 68000: 5.73299913796632\n",
      "three-layer Loss after iteration 69000: 5.445654838821282\n",
      "three-layer Loss after iteration 70000: 5.798489217985404\n",
      "three-layer Loss after iteration 71000: 5.791198861729671\n",
      "three-layer Loss after iteration 72000: 5.751709842660855\n",
      "three-layer Loss after iteration 73000: 5.664731715126912\n",
      "three-layer Loss after iteration 74000: 5.61719594530519\n",
      "three-layer Loss after iteration 75000: 5.669767374228356\n",
      "three-layer Loss after iteration 76000: 5.698513983639817\n",
      "three-layer Loss after iteration 77000: 5.528469373119373\n",
      "three-layer Loss after iteration 78000: 5.440556805945124\n",
      "three-layer Loss after iteration 79000: 5.54918892457907\n",
      "three-layer Loss after iteration 80000: 5.357663453186571\n",
      "three-layer Loss after iteration 81000: 5.548027402119072\n",
      "three-layer Loss after iteration 82000: 5.503085750191589\n",
      "three-layer Loss after iteration 83000: 5.343371056876228\n",
      "three-layer Loss after iteration 84000: 5.317340620448389\n",
      "three-layer Loss after iteration 85000: 5.449715968941554\n",
      "three-layer Loss after iteration 86000: 5.448027861087321\n",
      "three-layer Loss after iteration 87000: 5.404915435926535\n",
      "three-layer Loss after iteration 88000: 5.403933346397603\n",
      "three-layer Loss after iteration 89000: 5.467766522327388\n",
      "three-layer Loss after iteration 90000: 5.386461404404292\n",
      "three-layer Loss after iteration 91000: 5.52777794211287\n",
      "three-layer Loss after iteration 92000: 5.495949796597042\n",
      "three-layer Loss after iteration 93000: 5.4744240622304705\n",
      "three-layer Loss after iteration 94000: 5.420376807535809\n",
      "three-layer Loss after iteration 95000: 5.644051006190843\n",
      "three-layer Loss after iteration 96000: 5.923987453536085\n",
      "three-layer Loss after iteration 97000: 5.694134951726475\n",
      "three-layer Loss after iteration 98000: 5.61763479398438\n",
      "three-layer Loss after iteration 99000: 5.595165772763855\n",
      "three-layer Loss after iteration 0: 1734.0682000886086\n",
      "three-layer Loss after iteration 1000: 29.533836305981144\n",
      "three-layer Loss after iteration 2000: 15.576083761021788\n",
      "three-layer Loss after iteration 3000: 14.006342024770177\n",
      "three-layer Loss after iteration 4000: 11.365836799056417\n",
      "three-layer Loss after iteration 5000: 11.073668841264874\n",
      "three-layer Loss after iteration 6000: 9.761630666412579\n",
      "three-layer Loss after iteration 7000: 9.793081546114061\n",
      "three-layer Loss after iteration 8000: 9.238350143433502\n",
      "three-layer Loss after iteration 9000: 8.914305162475165\n",
      "three-layer Loss after iteration 10000: 8.661092070875775\n",
      "three-layer Loss after iteration 11000: 8.387663976849446\n",
      "three-layer Loss after iteration 12000: 8.176758161354956\n",
      "three-layer Loss after iteration 13000: 8.014856009536638\n",
      "three-layer Loss after iteration 14000: 7.874373745760551\n",
      "three-layer Loss after iteration 15000: 7.702316742949972\n",
      "three-layer Loss after iteration 16000: 7.451084900976785\n",
      "three-layer Loss after iteration 17000: 7.389038528859024\n",
      "three-layer Loss after iteration 18000: 7.518380167321896\n",
      "three-layer Loss after iteration 19000: 7.487337310968748\n",
      "three-layer Loss after iteration 20000: 7.423514092241623\n",
      "three-layer Loss after iteration 21000: 7.392336027561316\n",
      "three-layer Loss after iteration 22000: 7.290110484019877\n",
      "three-layer Loss after iteration 23000: 7.239768082743782\n",
      "three-layer Loss after iteration 24000: 7.217433477094777\n",
      "three-layer Loss after iteration 25000: 7.204467103553344\n",
      "three-layer Loss after iteration 26000: 7.188769213805954\n",
      "three-layer Loss after iteration 27000: 7.178257461649126\n",
      "three-layer Loss after iteration 28000: 7.171530988198302\n",
      "three-layer Loss after iteration 29000: 7.163767111511468\n",
      "three-layer Loss after iteration 30000: 7.156480366804773\n",
      "three-layer Loss after iteration 31000: 7.147267022312133\n",
      "three-layer Loss after iteration 32000: 7.143804070686305\n",
      "three-layer Loss after iteration 33000: 7.137734405948078\n",
      "three-layer Loss after iteration 34000: 7.133872930729158\n",
      "three-layer Loss after iteration 35000: 7.1273241407641965\n",
      "three-layer Loss after iteration 36000: 8.833470788959524\n",
      "three-layer Loss after iteration 37000: 7.546066127470434\n",
      "three-layer Loss after iteration 38000: 7.728818065310318\n",
      "three-layer Loss after iteration 39000: 7.836265747243784\n",
      "three-layer Loss after iteration 40000: 7.678588929847097\n",
      "three-layer Loss after iteration 41000: 7.5549524208933425\n",
      "three-layer Loss after iteration 42000: 7.475175784080938\n",
      "three-layer Loss after iteration 43000: 7.400763148625925\n",
      "three-layer Loss after iteration 44000: 7.337345244908105\n",
      "three-layer Loss after iteration 45000: 7.586956355099807\n",
      "three-layer Loss after iteration 46000: 7.655746754077294\n",
      "three-layer Loss after iteration 47000: 7.577238905847318\n",
      "three-layer Loss after iteration 48000: 7.699638404832804\n",
      "three-layer Loss after iteration 49000: 7.672711562305528\n",
      "three-layer Loss after iteration 50000: 7.512082266764139\n",
      "three-layer Loss after iteration 51000: 7.391414874269378\n",
      "three-layer Loss after iteration 52000: 7.305294873425381\n",
      "three-layer Loss after iteration 53000: 7.248877022720009\n",
      "three-layer Loss after iteration 54000: 7.196331145748516\n",
      "three-layer Loss after iteration 55000: 7.153471571885374\n",
      "three-layer Loss after iteration 56000: 7.1014968705002985\n",
      "three-layer Loss after iteration 57000: 7.055868865718595\n",
      "three-layer Loss after iteration 58000: 7.020616418234887\n",
      "three-layer Loss after iteration 59000: 6.9895072366621225\n",
      "three-layer Loss after iteration 60000: 6.969265086745515\n",
      "three-layer Loss after iteration 61000: 6.947985358091418\n",
      "three-layer Loss after iteration 62000: 6.931500208840961\n",
      "three-layer Loss after iteration 63000: 6.915644362394997\n",
      "three-layer Loss after iteration 64000: 6.907300028723864\n",
      "three-layer Loss after iteration 65000: 6.8939244701640545\n",
      "three-layer Loss after iteration 66000: 6.885043262413225\n",
      "three-layer Loss after iteration 67000: 6.8785433792136335\n",
      "three-layer Loss after iteration 68000: 6.871140582098716\n",
      "three-layer Loss after iteration 69000: 6.864042119944414\n",
      "three-layer Loss after iteration 70000: 6.8587948891341455\n",
      "three-layer Loss after iteration 71000: 6.852964655342202\n",
      "three-layer Loss after iteration 72000: 6.848841238432932\n",
      "three-layer Loss after iteration 73000: 6.844403630693214\n",
      "three-layer Loss after iteration 74000: 6.840735152466668\n",
      "three-layer Loss after iteration 75000: 6.836818838544336\n",
      "three-layer Loss after iteration 76000: 6.834285635065337\n",
      "three-layer Loss after iteration 77000: 6.8317210137517534\n",
      "three-layer Loss after iteration 78000: 6.829100317533264\n",
      "three-layer Loss after iteration 79000: 6.827427399289877\n",
      "three-layer Loss after iteration 80000: 6.825913473153392\n",
      "three-layer Loss after iteration 81000: 6.823047492380772\n",
      "three-layer Loss after iteration 82000: 6.822550680637237\n",
      "7.28137601400912e-05 6.823047492380772 6.822550680637237\n",
      "three-layer Loss after iteration 0: 1661.0198126624882\n",
      "three-layer Loss after iteration 1000: 247.5922989662656\n",
      "three-layer Loss after iteration 2000: 246.3252160307558\n",
      "three-layer Loss after iteration 3000: 245.61676227338376\n",
      "three-layer Loss after iteration 4000: 245.0701125890173\n",
      "three-layer Loss after iteration 5000: 243.87412839330045\n",
      "three-layer Loss after iteration 6000: 242.38505236399115\n",
      "three-layer Loss after iteration 7000: 240.0968027444359\n",
      "three-layer Loss after iteration 8000: 241.73878325533283\n",
      "three-layer Loss after iteration 9000: 241.22869707099673\n",
      "three-layer Loss after iteration 10000: 239.9917567920735\n",
      "three-layer Loss after iteration 11000: 239.55188919386433\n",
      "three-layer Loss after iteration 12000: 239.20593166057188\n",
      "three-layer Loss after iteration 13000: 239.08159174446777\n",
      "three-layer Loss after iteration 14000: 239.00749631731713\n",
      "three-layer Loss after iteration 15000: 238.93988008358977\n",
      "three-layer Loss after iteration 16000: 238.90135082392928\n",
      "three-layer Loss after iteration 17000: 238.8723975483528\n",
      "three-layer Loss after iteration 18000: 238.86521823364492\n",
      "3.005502009260125e-05 238.8723975483528 238.86521823364492\n",
      "three-layer Loss after iteration 0: 1651.644724646009\n",
      "three-layer Loss after iteration 1000: 28.439004679271854\n",
      "three-layer Loss after iteration 2000: 30.28577528414084\n",
      "three-layer Loss after iteration 3000: 26.331826422986296\n",
      "three-layer Loss after iteration 4000: 23.363328687228947\n",
      "three-layer Loss after iteration 5000: 19.873136765935755\n",
      "three-layer Loss after iteration 6000: 20.121592077646287\n",
      "three-layer Loss after iteration 7000: 18.614406589238445\n",
      "three-layer Loss after iteration 8000: 20.10019552695682\n",
      "three-layer Loss after iteration 9000: 15.679147942387038\n",
      "three-layer Loss after iteration 10000: 15.87500784469301\n",
      "three-layer Loss after iteration 11000: 15.652341983913171\n",
      "three-layer Loss after iteration 12000: 15.813148563882136\n",
      "three-layer Loss after iteration 13000: 15.11401123652424\n",
      "three-layer Loss after iteration 14000: 16.64296247291547\n",
      "three-layer Loss after iteration 15000: 15.227253784634938\n",
      "three-layer Loss after iteration 16000: 16.362770589019853\n",
      "three-layer Loss after iteration 17000: 16.626793505504786\n",
      "three-layer Loss after iteration 18000: 15.991889922236929\n",
      "three-layer Loss after iteration 19000: 15.990237493954146\n",
      "three-layer Loss after iteration 20000: 15.925577770224628\n",
      "three-layer Loss after iteration 21000: 15.234574293186565\n",
      "three-layer Loss after iteration 22000: 14.808279747006823\n",
      "three-layer Loss after iteration 23000: 14.337941322749048\n",
      "three-layer Loss after iteration 24000: 14.148706748146521\n",
      "three-layer Loss after iteration 25000: 13.700598167815736\n",
      "three-layer Loss after iteration 26000: 13.239567625484645\n",
      "three-layer Loss after iteration 27000: 12.891390958981708\n",
      "three-layer Loss after iteration 28000: 12.551184472726172\n",
      "three-layer Loss after iteration 29000: 12.217501767750194\n",
      "three-layer Loss after iteration 30000: 11.856897538511735\n",
      "three-layer Loss after iteration 31000: 11.682280359652724\n",
      "three-layer Loss after iteration 32000: 11.501718299484725\n",
      "three-layer Loss after iteration 33000: 10.864258643618042\n",
      "three-layer Loss after iteration 34000: 12.547799226695696\n",
      "three-layer Loss after iteration 35000: 10.182110561690989\n",
      "three-layer Loss after iteration 36000: 9.998807702123688\n",
      "three-layer Loss after iteration 37000: 10.92222448632045\n",
      "three-layer Loss after iteration 38000: 9.806289690358394\n",
      "three-layer Loss after iteration 39000: 10.44704391765088\n",
      "three-layer Loss after iteration 40000: 9.520371195487911\n",
      "three-layer Loss after iteration 41000: 9.494479805941136\n",
      "three-layer Loss after iteration 42000: 9.30821393631437\n",
      "three-layer Loss after iteration 43000: 9.19993661434685\n",
      "three-layer Loss after iteration 44000: 10.327364183868124\n",
      "three-layer Loss after iteration 45000: 8.083374275835588\n",
      "three-layer Loss after iteration 46000: 7.637244607507748\n",
      "three-layer Loss after iteration 47000: 9.560719540353873\n",
      "three-layer Loss after iteration 48000: 12.530691535375045\n",
      "three-layer Loss after iteration 49000: 10.072986215234417\n",
      "three-layer Loss after iteration 50000: 7.35384326370064\n",
      "three-layer Loss after iteration 51000: 6.590165465526087\n",
      "three-layer Loss after iteration 52000: 6.217998731320245\n",
      "three-layer Loss after iteration 53000: 6.009484144935775\n",
      "three-layer Loss after iteration 54000: 6.483635464467625\n",
      "three-layer Loss after iteration 55000: 7.532218507564865\n",
      "three-layer Loss after iteration 56000: 8.32220807186222\n",
      "three-layer Loss after iteration 57000: 8.081302758003307\n",
      "three-layer Loss after iteration 58000: 8.153738106451888\n",
      "three-layer Loss after iteration 59000: 7.927228941457758\n",
      "three-layer Loss after iteration 60000: 7.698732753762941\n",
      "three-layer Loss after iteration 61000: 7.7618102008141925\n",
      "three-layer Loss after iteration 62000: 7.413826390336651\n",
      "three-layer Loss after iteration 63000: 6.968198380231131\n",
      "three-layer Loss after iteration 64000: 7.183714138654577\n",
      "three-layer Loss after iteration 65000: 7.1433534722772585\n",
      "three-layer Loss after iteration 66000: 7.005072751000063\n",
      "three-layer Loss after iteration 67000: 7.004008360255058\n",
      "three-layer Loss after iteration 68000: 6.8989022195721414\n",
      "three-layer Loss after iteration 69000: 6.8281681345933745\n",
      "three-layer Loss after iteration 70000: 6.741753256982552\n",
      "three-layer Loss after iteration 71000: 6.700672482495591\n",
      "three-layer Loss after iteration 72000: 6.604681627754343\n",
      "three-layer Loss after iteration 73000: 6.51430401560498\n",
      "three-layer Loss after iteration 74000: 6.581654710236354\n",
      "three-layer Loss after iteration 75000: 6.496637342269873\n",
      "three-layer Loss after iteration 76000: 6.454762588203826\n",
      "three-layer Loss after iteration 77000: 6.394737319162717\n",
      "three-layer Loss after iteration 78000: 6.3321694328172695\n",
      "three-layer Loss after iteration 79000: 6.288297029463671\n",
      "three-layer Loss after iteration 80000: 6.625408764422565\n",
      "three-layer Loss after iteration 81000: 6.25704236452914\n",
      "three-layer Loss after iteration 82000: 6.2098434164994\n",
      "three-layer Loss after iteration 83000: 6.160510980722882\n",
      "three-layer Loss after iteration 84000: 6.111648873301366\n",
      "three-layer Loss after iteration 85000: 6.065244771559602\n",
      "three-layer Loss after iteration 86000: 6.322070835864414\n",
      "three-layer Loss after iteration 87000: 6.051665865903055\n",
      "three-layer Loss after iteration 88000: 5.998073588063438\n",
      "three-layer Loss after iteration 89000: 5.956817979925412\n",
      "three-layer Loss after iteration 90000: 5.921981691077031\n",
      "three-layer Loss after iteration 91000: 5.892724959909166\n",
      "three-layer Loss after iteration 92000: 5.966842091507992\n",
      "three-layer Loss after iteration 93000: 6.06129033963199\n",
      "three-layer Loss after iteration 94000: 6.014116246998268\n",
      "three-layer Loss after iteration 95000: 5.97490293530073\n",
      "three-layer Loss after iteration 96000: 5.933546517211616\n",
      "three-layer Loss after iteration 97000: 5.896738847592983\n",
      "three-layer Loss after iteration 98000: 5.861861146567608\n",
      "three-layer Loss after iteration 99000: 5.82268233259982\n",
      "three-layer Loss after iteration 0: 1713.8449201578674\n",
      "three-layer Loss after iteration 1000: 14.576255720131403\n",
      "three-layer Loss after iteration 2000: 10.922361333789238\n",
      "three-layer Loss after iteration 3000: 12.09157509936128\n",
      "three-layer Loss after iteration 4000: 11.022597500732457\n",
      "three-layer Loss after iteration 5000: 10.269609618117832\n",
      "three-layer Loss after iteration 6000: 9.979266716730145\n",
      "three-layer Loss after iteration 7000: 9.798126699076757\n",
      "three-layer Loss after iteration 8000: 9.688447826363614\n",
      "three-layer Loss after iteration 9000: 9.532599980075485\n",
      "three-layer Loss after iteration 10000: 9.012965164397496\n",
      "three-layer Loss after iteration 11000: 10.01809312659914\n",
      "three-layer Loss after iteration 12000: 9.5037933681068\n",
      "three-layer Loss after iteration 13000: 9.259430554978312\n",
      "three-layer Loss after iteration 14000: 8.801658069298723\n",
      "three-layer Loss after iteration 15000: 8.267361281892768\n",
      "three-layer Loss after iteration 16000: 9.183643333728453\n",
      "three-layer Loss after iteration 17000: 9.162193972821989\n",
      "three-layer Loss after iteration 18000: 8.741445473395792\n",
      "three-layer Loss after iteration 19000: 8.506816013566551\n",
      "three-layer Loss after iteration 20000: 8.25452959207172\n",
      "three-layer Loss after iteration 21000: 8.066284330464732\n",
      "three-layer Loss after iteration 22000: 8.0307530865701\n",
      "three-layer Loss after iteration 23000: 8.049242569933567\n",
      "three-layer Loss after iteration 24000: 7.968629917328098\n",
      "three-layer Loss after iteration 25000: 7.7320865488825\n",
      "three-layer Loss after iteration 26000: 7.5705828972528675\n",
      "three-layer Loss after iteration 27000: 7.012352464627318\n",
      "three-layer Loss after iteration 28000: 6.260446620835527\n",
      "three-layer Loss after iteration 29000: 7.000687927510359\n",
      "three-layer Loss after iteration 30000: 7.03926187264219\n",
      "three-layer Loss after iteration 31000: 7.102904274778563\n",
      "three-layer Loss after iteration 32000: 7.070481441407466\n",
      "three-layer Loss after iteration 33000: 6.343281552201355\n",
      "three-layer Loss after iteration 34000: 7.8323615333646135\n",
      "three-layer Loss after iteration 35000: 7.3996408097855255\n",
      "three-layer Loss after iteration 36000: 6.827441706781298\n",
      "three-layer Loss after iteration 37000: 6.834126488139284\n",
      "three-layer Loss after iteration 38000: 6.716589952002451\n",
      "three-layer Loss after iteration 39000: 6.806301988666007\n",
      "three-layer Loss after iteration 40000: 6.689009494347716\n",
      "three-layer Loss after iteration 41000: 6.246867147423365\n",
      "three-layer Loss after iteration 42000: 6.683212658453993\n",
      "three-layer Loss after iteration 43000: 7.533020994867137\n",
      "three-layer Loss after iteration 44000: 6.254345684148526\n",
      "three-layer Loss after iteration 45000: 6.604968323919118\n",
      "three-layer Loss after iteration 46000: 6.555925635974165\n",
      "three-layer Loss after iteration 47000: 6.434381929913223\n",
      "three-layer Loss after iteration 48000: 6.351770381253286\n",
      "three-layer Loss after iteration 49000: 6.2652905959384775\n",
      "three-layer Loss after iteration 50000: 6.181878097487186\n",
      "three-layer Loss after iteration 51000: 6.107373433842237\n",
      "three-layer Loss after iteration 52000: 5.991048365439072\n",
      "three-layer Loss after iteration 53000: 6.035982653825223\n",
      "three-layer Loss after iteration 54000: 6.044935220162982\n",
      "three-layer Loss after iteration 55000: 5.036172653718992\n",
      "three-layer Loss after iteration 56000: 6.149635840527254\n",
      "three-layer Loss after iteration 57000: 6.366460499400032\n",
      "three-layer Loss after iteration 58000: 6.299605655509141\n",
      "three-layer Loss after iteration 59000: 6.059927106091148\n",
      "three-layer Loss after iteration 60000: 4.720229228229783\n",
      "three-layer Loss after iteration 61000: 5.74734338894791\n",
      "three-layer Loss after iteration 62000: 7.463329681026807\n",
      "three-layer Loss after iteration 63000: 5.009024439327402\n",
      "three-layer Loss after iteration 64000: 6.805077251723473\n",
      "three-layer Loss after iteration 65000: 5.681185858079041\n",
      "three-layer Loss after iteration 66000: 5.096290085341025\n",
      "three-layer Loss after iteration 67000: 5.030829651154941\n",
      "three-layer Loss after iteration 68000: 4.921116738039634\n",
      "three-layer Loss after iteration 69000: 4.937721933037763\n",
      "three-layer Loss after iteration 70000: 4.900040973274176\n",
      "three-layer Loss after iteration 71000: 4.762477646957585\n",
      "three-layer Loss after iteration 72000: 4.511802219698658\n",
      "three-layer Loss after iteration 73000: 3.983133440385917\n",
      "three-layer Loss after iteration 74000: 6.363576034302402\n",
      "three-layer Loss after iteration 75000: 5.003338846595564\n",
      "three-layer Loss after iteration 76000: 4.580892501016136\n",
      "three-layer Loss after iteration 77000: 4.550968510755582\n",
      "three-layer Loss after iteration 78000: 4.519238398609271\n",
      "three-layer Loss after iteration 79000: 4.5566348507784245\n",
      "three-layer Loss after iteration 80000: 4.424313633089783\n",
      "three-layer Loss after iteration 81000: 4.41101551861885\n",
      "three-layer Loss after iteration 82000: 4.58665090305827\n",
      "three-layer Loss after iteration 83000: 4.0184611648337745\n",
      "three-layer Loss after iteration 84000: 4.239763451691746\n",
      "three-layer Loss after iteration 85000: 4.736387753660019\n",
      "three-layer Loss after iteration 86000: 4.846893773425752\n",
      "three-layer Loss after iteration 87000: 4.3308870769932275\n",
      "three-layer Loss after iteration 88000: 4.245901863389962\n",
      "three-layer Loss after iteration 89000: 5.154014566021134\n",
      "three-layer Loss after iteration 90000: 4.210516949422201\n",
      "three-layer Loss after iteration 91000: 4.16422294350899\n",
      "three-layer Loss after iteration 92000: 4.5551969636617855\n",
      "three-layer Loss after iteration 93000: 4.357609095758652\n",
      "three-layer Loss after iteration 94000: 4.346048387263544\n",
      "three-layer Loss after iteration 95000: 4.317550655135056\n",
      "three-layer Loss after iteration 96000: 4.289490034713013\n",
      "three-layer Loss after iteration 97000: 4.262160433933154\n",
      "three-layer Loss after iteration 98000: 4.186846146725253\n",
      "three-layer Loss after iteration 99000: 4.28674406448485\n",
      "three-layer Loss after iteration 0: 1594.8267106667158\n",
      "three-layer Loss after iteration 1000: 14.530933143669005\n",
      "three-layer Loss after iteration 2000: 14.801369557033379\n",
      "three-layer Loss after iteration 3000: 11.96803653474218\n",
      "three-layer Loss after iteration 4000: 10.17997567197418\n",
      "three-layer Loss after iteration 5000: 10.167291151643738\n",
      "three-layer Loss after iteration 6000: 9.083620651909346\n",
      "three-layer Loss after iteration 7000: 8.014899714337034\n",
      "three-layer Loss after iteration 8000: 6.58989229821176\n",
      "three-layer Loss after iteration 9000: 9.02360184909061\n",
      "three-layer Loss after iteration 10000: 7.7921721135139626\n",
      "three-layer Loss after iteration 11000: 7.809661663287528\n",
      "three-layer Loss after iteration 12000: 7.310567319883659\n",
      "three-layer Loss after iteration 13000: 7.529987451503997\n",
      "three-layer Loss after iteration 14000: 6.430346788313218\n",
      "three-layer Loss after iteration 15000: 6.1688464908124825\n",
      "three-layer Loss after iteration 16000: 5.8423555314788835\n",
      "three-layer Loss after iteration 17000: 5.500206446738155\n",
      "three-layer Loss after iteration 18000: 6.830148297563042\n",
      "three-layer Loss after iteration 19000: 5.428409891538821\n",
      "three-layer Loss after iteration 20000: 5.581514181676438\n",
      "three-layer Loss after iteration 21000: 4.864234976930531\n",
      "three-layer Loss after iteration 22000: 4.360020751609805\n",
      "three-layer Loss after iteration 23000: 4.423119489202719\n",
      "three-layer Loss after iteration 24000: 4.120353513300339\n",
      "three-layer Loss after iteration 25000: 4.738323309202464\n",
      "three-layer Loss after iteration 26000: 5.676099195929465\n",
      "three-layer Loss after iteration 27000: 3.174466708965301\n",
      "three-layer Loss after iteration 28000: 2.9789914200654346\n",
      "three-layer Loss after iteration 29000: 2.973946026405456\n",
      "three-layer Loss after iteration 30000: 2.926008010237491\n",
      "three-layer Loss after iteration 31000: 3.5007970173912084\n",
      "three-layer Loss after iteration 32000: 3.4891270322088324\n",
      "three-layer Loss after iteration 33000: 3.786747480370711\n",
      "three-layer Loss after iteration 34000: 3.8481941250355653\n",
      "three-layer Loss after iteration 35000: 3.8286979567547315\n",
      "three-layer Loss after iteration 36000: 3.7456381800893914\n",
      "three-layer Loss after iteration 37000: 3.683757105423263\n",
      "three-layer Loss after iteration 38000: 3.6762618007420422\n",
      "three-layer Loss after iteration 39000: 3.5962996026192227\n",
      "three-layer Loss after iteration 40000: 3.5406508414996853\n",
      "three-layer Loss after iteration 41000: 3.4967627936162913\n",
      "three-layer Loss after iteration 42000: 3.4532041584726367\n",
      "three-layer Loss after iteration 43000: 3.410674554306517\n",
      "three-layer Loss after iteration 44000: 3.3716437938535297\n",
      "three-layer Loss after iteration 45000: 3.329786181188961\n",
      "three-layer Loss after iteration 46000: 3.3251774148650743\n",
      "three-layer Loss after iteration 47000: 3.272169362360731\n",
      "three-layer Loss after iteration 48000: 3.200653781663449\n",
      "three-layer Loss after iteration 49000: 3.008551661440007\n",
      "three-layer Loss after iteration 50000: 3.628509092016564\n",
      "three-layer Loss after iteration 51000: 3.1816022092173784\n",
      "three-layer Loss after iteration 52000: 2.8563329881131185\n",
      "three-layer Loss after iteration 53000: 2.8775370201038553\n",
      "three-layer Loss after iteration 54000: 3.093185106798382\n",
      "three-layer Loss after iteration 55000: 3.3705809335246797\n",
      "three-layer Loss after iteration 56000: 3.1162332939653843\n",
      "three-layer Loss after iteration 57000: 2.9761093428875918\n",
      "three-layer Loss after iteration 58000: 3.0652181418585465\n",
      "three-layer Loss after iteration 59000: 3.023833594727118\n",
      "three-layer Loss after iteration 60000: 2.9950985578181206\n",
      "three-layer Loss after iteration 61000: 2.9907575469993177\n",
      "three-layer Loss after iteration 62000: 2.9856136055401437\n",
      "three-layer Loss after iteration 63000: 2.9692359679054015\n",
      "three-layer Loss after iteration 64000: 2.9161066683574193\n",
      "three-layer Loss after iteration 65000: 2.861384312475113\n",
      "three-layer Loss after iteration 66000: 2.8610210488082846\n",
      "three-layer Loss after iteration 67000: 2.732612119183215\n",
      "three-layer Loss after iteration 68000: 2.6999461242095455\n",
      "three-layer Loss after iteration 69000: 2.688082064208409\n",
      "three-layer Loss after iteration 70000: 2.6707939127705127\n",
      "three-layer Loss after iteration 71000: 2.650843801631807\n",
      "three-layer Loss after iteration 72000: 2.6192399711327075\n",
      "three-layer Loss after iteration 73000: 2.5872278141153724\n",
      "three-layer Loss after iteration 74000: 2.545875079430482\n",
      "three-layer Loss after iteration 75000: 2.513855874918003\n",
      "three-layer Loss after iteration 76000: 2.4861136833657533\n",
      "three-layer Loss after iteration 77000: 2.462221784543552\n",
      "three-layer Loss after iteration 78000: 2.4281555710321774\n",
      "three-layer Loss after iteration 79000: 2.402698000262453\n",
      "three-layer Loss after iteration 80000: 2.370986694816814\n",
      "three-layer Loss after iteration 81000: 2.3519550467395542\n",
      "three-layer Loss after iteration 82000: 2.339050146571852\n",
      "three-layer Loss after iteration 83000: 2.34268133517159\n",
      "three-layer Loss after iteration 84000: 2.335831370558895\n",
      "three-layer Loss after iteration 85000: 2.329196555531275\n",
      "three-layer Loss after iteration 86000: 2.3274797436936945\n",
      "three-layer Loss after iteration 87000: 2.325242365036791\n",
      "three-layer Loss after iteration 88000: 2.3241659895543516\n",
      "three-layer Loss after iteration 89000: 2.323711963579266\n",
      "three-layer Loss after iteration 90000: 2.3232318027605476\n",
      "three-layer Loss after iteration 91000: 2.323141385983939\n",
      "3.8918534302537234e-05 2.3232318027605476 2.323141385983939\n",
      "{'8': {'losses': [3.2960968311154177, 2.686495056837814, 2.4458781013588773, 4.544646024419433, 1.8736233570154375, 3.7150274328964783, 3.755064348507962, 3.323821306313002, 2.3656330077323857, 2.8821372454077623, 1.8249821216792543, 4.544452063669773, 2.502430270529242, 3.6063009618568547, 3.4578563644686366, 1.7114213029144902, 9.024377381672672, 2.8911891463878923, 3.720615318719514, 2.89793472933845], 'iterations': [100000, 79001, 100000, 73001, 100000, 37001, 100000, 54001, 80001, 37001, 63001, 68001, 100000, 100000, 52001, 100000, 84001, 80001, 67001, 100000]}} {'4+4': {'losses': [10.993200418489621, 12.518692864415698, 2.6389228918846745, 2.352087583440563, 4.493967609616839, 7.811905752583267, 10.416046573266362, 11.86792313481236, 6.411681078446581, 3.813007071587286, 3.999027370875976, 5.105173453430588, 9.157078659370166, 8.68953208689747, 5.595165772763855, 6.822550680637237, 238.86521823364492, 5.82268233259982, 4.28674406448485, 2.323141385983939], 'iterations': [69001, 93001, 45001, 100000, 100000, 90001, 100000, 64001, 77001, 100000, 100000, 100000, 100000, 100000, 100000, 82001, 18001, 100000, 100000, 91001]}}\n",
      "two-layer Loss after iteration 0: 1548.495883064527\n",
      "two-layer Loss after iteration 1000: 23.631129549767923\n",
      "two-layer Loss after iteration 2000: 9.345156061883253\n",
      "two-layer Loss after iteration 3000: 6.427456156483745\n",
      "two-layer Loss after iteration 4000: 5.138804625015189\n",
      "two-layer Loss after iteration 5000: 4.176569399661721\n",
      "two-layer Loss after iteration 6000: 3.413280211579596\n",
      "two-layer Loss after iteration 7000: 2.736096566514228\n",
      "two-layer Loss after iteration 8000: 2.1361670904004746\n",
      "two-layer Loss after iteration 9000: 1.8626413336751086\n",
      "two-layer Loss after iteration 10000: 1.702547615935175\n",
      "two-layer Loss after iteration 11000: 1.5498351686941745\n",
      "two-layer Loss after iteration 12000: 1.4464047604825527\n",
      "two-layer Loss after iteration 13000: 1.371585222997337\n",
      "two-layer Loss after iteration 14000: 1.3152755215928584\n",
      "two-layer Loss after iteration 15000: 1.2543955181301438\n",
      "two-layer Loss after iteration 16000: 1.207158765706784\n",
      "two-layer Loss after iteration 17000: 1.1706571046863479\n",
      "two-layer Loss after iteration 18000: 1.1361840696044125\n",
      "two-layer Loss after iteration 19000: 1.1150052580189747\n",
      "two-layer Loss after iteration 20000: 1.0992062990783256\n",
      "two-layer Loss after iteration 21000: 1.0866552548651813\n",
      "two-layer Loss after iteration 22000: 1.0745297739741986\n",
      "two-layer Loss after iteration 23000: 1.0604300164988185\n",
      "two-layer Loss after iteration 24000: 1.0493720152255017\n",
      "two-layer Loss after iteration 25000: 1.0401582877800886\n",
      "two-layer Loss after iteration 26000: 1.0335455090906664\n",
      "two-layer Loss after iteration 27000: 1.02787419454858\n",
      "two-layer Loss after iteration 28000: 1.0229439918958525\n",
      "two-layer Loss after iteration 29000: 1.0177586472674682\n",
      "two-layer Loss after iteration 30000: 1.0131876406363798\n",
      "two-layer Loss after iteration 31000: 1.0084924819216867\n",
      "two-layer Loss after iteration 32000: 1.0043980735078586\n",
      "two-layer Loss after iteration 33000: 1.0007698068846174\n",
      "two-layer Loss after iteration 34000: 0.9974919772176902\n",
      "two-layer Loss after iteration 35000: 0.9941345987782478\n",
      "two-layer Loss after iteration 36000: 0.9906847927959621\n",
      "two-layer Loss after iteration 37000: 0.9862169165390684\n",
      "two-layer Loss after iteration 38000: 0.9831564919789286\n",
      "two-layer Loss after iteration 39000: 0.9786894482769997\n",
      "two-layer Loss after iteration 40000: 0.9752294390982891\n",
      "two-layer Loss after iteration 41000: 0.9717818221019715\n",
      "two-layer Loss after iteration 42000: 0.9687595610833307\n",
      "two-layer Loss after iteration 43000: 0.9659337285116065\n",
      "two-layer Loss after iteration 44000: 0.9621398798854446\n",
      "two-layer Loss after iteration 45000: 0.9589402125804635\n",
      "two-layer Loss after iteration 46000: 0.9555606637105025\n",
      "two-layer Loss after iteration 47000: 0.9525727841863922\n",
      "two-layer Loss after iteration 48000: 0.9498701727959173\n",
      "two-layer Loss after iteration 49000: 0.9472647712643786\n",
      "two-layer Loss after iteration 50000: 0.9448410312991822\n",
      "two-layer Loss after iteration 51000: 0.9341597666050173\n",
      "two-layer Loss after iteration 52000: 0.9299444754799348\n",
      "two-layer Loss after iteration 53000: 0.9266539298495514\n",
      "two-layer Loss after iteration 54000: 0.9240538492174639\n",
      "two-layer Loss after iteration 55000: 0.9218300002267796\n",
      "two-layer Loss after iteration 56000: 0.9198554432023621\n",
      "two-layer Loss after iteration 57000: 0.9143351287474519\n",
      "two-layer Loss after iteration 58000: 0.9114511014883508\n",
      "two-layer Loss after iteration 59000: 0.9089104635194972\n",
      "two-layer Loss after iteration 60000: 0.9066447092233644\n",
      "two-layer Loss after iteration 61000: 0.9046030087358107\n",
      "two-layer Loss after iteration 62000: 0.9026989920585502\n",
      "two-layer Loss after iteration 63000: 0.90094722378375\n",
      "two-layer Loss after iteration 64000: 0.8992830273255386\n",
      "two-layer Loss after iteration 65000: 0.897696124070305\n",
      "two-layer Loss after iteration 66000: 0.8961748450614533\n",
      "two-layer Loss after iteration 67000: 0.8946968601632332\n",
      "two-layer Loss after iteration 68000: 0.8932669249078005\n",
      "two-layer Loss after iteration 69000: 0.8918935975920231\n",
      "two-layer Loss after iteration 70000: 0.8903556907425967\n",
      "two-layer Loss after iteration 71000: 0.8889905406761164\n",
      "two-layer Loss after iteration 72000: 0.887674072609451\n",
      "two-layer Loss after iteration 73000: 0.886402891653554\n",
      "two-layer Loss after iteration 74000: 0.8851750702208556\n",
      "two-layer Loss after iteration 75000: 0.8839966721238162\n",
      "two-layer Loss after iteration 76000: 0.8828545223908363\n",
      "two-layer Loss after iteration 77000: 0.8813400579162088\n",
      "two-layer Loss after iteration 78000: 0.8800462256846119\n",
      "two-layer Loss after iteration 79000: 0.8788552627815184\n",
      "two-layer Loss after iteration 80000: 0.8777178305815535\n",
      "two-layer Loss after iteration 81000: 0.876628152692556\n",
      "two-layer Loss after iteration 82000: 0.8755797861982804\n",
      "two-layer Loss after iteration 83000: 0.87456661678116\n",
      "two-layer Loss after iteration 84000: 0.8735782392817569\n",
      "two-layer Loss after iteration 85000: 0.8726307975217173\n",
      "two-layer Loss after iteration 86000: 0.8717058882011328\n",
      "two-layer Loss after iteration 87000: 0.870820577029798\n",
      "two-layer Loss after iteration 88000: 0.869966010626703\n",
      "two-layer Loss after iteration 89000: 0.8691183441954692\n",
      "two-layer Loss after iteration 90000: 0.8673619063370678\n",
      "two-layer Loss after iteration 91000: 0.866155484890092\n",
      "two-layer Loss after iteration 92000: 0.865105212398038\n",
      "two-layer Loss after iteration 93000: 0.8641499763058326\n",
      "two-layer Loss after iteration 94000: 0.8629214352003184\n",
      "two-layer Loss after iteration 95000: 0.8616668749689207\n",
      "two-layer Loss after iteration 96000: 0.8607814609276093\n",
      "two-layer Loss after iteration 97000: 0.854016662255708\n",
      "two-layer Loss after iteration 98000: 0.8518906135341866\n",
      "two-layer Loss after iteration 99000: 0.8505529278327828\n",
      "two-layer Loss after iteration 0: 1406.6186614129895\n",
      "two-layer Loss after iteration 1000: 24.798308076332578\n",
      "two-layer Loss after iteration 2000: 9.11285974751006\n",
      "two-layer Loss after iteration 3000: 6.421217937580861\n",
      "two-layer Loss after iteration 4000: 5.349547233993453\n",
      "two-layer Loss after iteration 5000: 4.682080096475681\n",
      "two-layer Loss after iteration 6000: 3.287145237470794\n",
      "two-layer Loss after iteration 7000: 2.48574753435893\n",
      "two-layer Loss after iteration 8000: 1.5432666340130017\n",
      "two-layer Loss after iteration 9000: 1.2852137587204226\n",
      "two-layer Loss after iteration 10000: 1.161860153915201\n",
      "two-layer Loss after iteration 11000: 1.0925527119505132\n",
      "two-layer Loss after iteration 12000: 1.0208132937792092\n",
      "two-layer Loss after iteration 13000: 0.9768402280018524\n",
      "two-layer Loss after iteration 14000: 0.9538183848643713\n",
      "two-layer Loss after iteration 15000: 0.9391411614570011\n",
      "two-layer Loss after iteration 16000: 0.9285073419695844\n",
      "two-layer Loss after iteration 17000: 0.9207455758746309\n",
      "two-layer Loss after iteration 18000: 0.9145173837075562\n",
      "two-layer Loss after iteration 19000: 0.9092174137059756\n",
      "two-layer Loss after iteration 20000: 0.9045524089141466\n",
      "two-layer Loss after iteration 21000: 0.9003731952966434\n",
      "two-layer Loss after iteration 22000: 0.8964531994216846\n",
      "two-layer Loss after iteration 23000: 0.8928578448606463\n",
      "two-layer Loss after iteration 24000: 0.8892406387505093\n",
      "two-layer Loss after iteration 25000: 0.8857348067582943\n",
      "two-layer Loss after iteration 26000: 0.8823876274237937\n",
      "two-layer Loss after iteration 27000: 0.8789688886202656\n",
      "two-layer Loss after iteration 28000: 0.8754451838614088\n",
      "two-layer Loss after iteration 29000: 0.872558503568502\n",
      "two-layer Loss after iteration 30000: 0.8699589523032122\n",
      "two-layer Loss after iteration 31000: 0.8675370470971717\n",
      "two-layer Loss after iteration 32000: 0.8652759733717887\n",
      "two-layer Loss after iteration 33000: 0.8632046036313457\n",
      "two-layer Loss after iteration 34000: 0.8612663641696853\n",
      "two-layer Loss after iteration 35000: 0.8594920496741899\n",
      "two-layer Loss after iteration 36000: 0.8580352069220573\n",
      "two-layer Loss after iteration 37000: 0.8566648984837947\n",
      "two-layer Loss after iteration 38000: 0.855369661139496\n",
      "two-layer Loss after iteration 39000: 0.854144988416582\n",
      "two-layer Loss after iteration 40000: 0.8529839317081388\n",
      "two-layer Loss after iteration 41000: 0.8518855788093537\n",
      "two-layer Loss after iteration 42000: 0.8508429217497897\n",
      "two-layer Loss after iteration 43000: 0.8498538881854121\n",
      "two-layer Loss after iteration 44000: 0.8488467327743566\n",
      "two-layer Loss after iteration 45000: 0.847731432839486\n",
      "two-layer Loss after iteration 46000: 0.8464457958947151\n",
      "two-layer Loss after iteration 47000: 0.8454013377138646\n",
      "two-layer Loss after iteration 48000: 0.8445780247604681\n",
      "two-layer Loss after iteration 49000: 0.8438268487861117\n",
      "two-layer Loss after iteration 50000: 0.8429686260739547\n",
      "two-layer Loss after iteration 51000: 0.842170601872669\n",
      "two-layer Loss after iteration 52000: 0.8414410516459265\n",
      "two-layer Loss after iteration 53000: 0.8407645271709692\n",
      "two-layer Loss after iteration 54000: 0.8400908468669513\n",
      "two-layer Loss after iteration 55000: 0.8392469380644418\n",
      "two-layer Loss after iteration 56000: 0.8385073125905587\n",
      "two-layer Loss after iteration 57000: 0.8377760872610222\n",
      "two-layer Loss after iteration 58000: 0.8367590409854185\n",
      "two-layer Loss after iteration 59000: 0.8351067424726762\n",
      "two-layer Loss after iteration 60000: 0.8344000575862939\n",
      "two-layer Loss after iteration 61000: 0.8337743243220423\n",
      "two-layer Loss after iteration 62000: 0.833209512392019\n",
      "two-layer Loss after iteration 63000: 0.8324947549122336\n",
      "two-layer Loss after iteration 64000: 0.8314455837611984\n",
      "two-layer Loss after iteration 65000: 0.8308161160921705\n",
      "two-layer Loss after iteration 66000: 0.8303387899294785\n",
      "two-layer Loss after iteration 67000: 0.8299385680214911\n",
      "two-layer Loss after iteration 68000: 0.8295903638789782\n",
      "two-layer Loss after iteration 69000: 0.8292831670183853\n",
      "two-layer Loss after iteration 70000: 0.8290079415590945\n",
      "two-layer Loss after iteration 71000: 0.8287588330290739\n",
      "two-layer Loss after iteration 72000: 0.8285322745986267\n",
      "two-layer Loss after iteration 73000: 0.8283443722986673\n",
      "two-layer Loss after iteration 74000: 0.828179130937212\n",
      "two-layer Loss after iteration 75000: 0.8280262292409205\n",
      "two-layer Loss after iteration 76000: 0.8278889777777817\n",
      "two-layer Loss after iteration 77000: 0.8277654015292859\n",
      "two-layer Loss after iteration 78000: 0.8276494926687484\n",
      "two-layer Loss after iteration 79000: 0.8275420970261043\n",
      "two-layer Loss after iteration 80000: 0.8274403121368904\n",
      "two-layer Loss after iteration 81000: 0.8273435943140968\n",
      "two-layer Loss after iteration 82000: 0.8272547851865069\n",
      "two-layer Loss after iteration 83000: 0.8271711661636523\n",
      "two-layer Loss after iteration 84000: 0.8270884128379539\n",
      "two-layer Loss after iteration 85000: 0.8270121257547873\n",
      "9.223570537613597e-05 0.8270884128379539 0.8270121257547873\n",
      "two-layer Loss after iteration 0: 1430.2308421175762\n",
      "two-layer Loss after iteration 1000: 25.372805885820593\n",
      "two-layer Loss after iteration 2000: 9.871741618941655\n",
      "two-layer Loss after iteration 3000: 7.016087776735288\n",
      "two-layer Loss after iteration 4000: 5.741745844807186\n",
      "two-layer Loss after iteration 5000: 4.752011700616891\n",
      "two-layer Loss after iteration 6000: 3.7659152314100903\n",
      "two-layer Loss after iteration 7000: 3.05779225643638\n",
      "two-layer Loss after iteration 8000: 2.743251805859528\n",
      "two-layer Loss after iteration 9000: 2.5043233939689156\n",
      "two-layer Loss after iteration 10000: 2.0657922870497454\n",
      "two-layer Loss after iteration 11000: 1.7249633865955132\n",
      "two-layer Loss after iteration 12000: 1.5392947958238425\n",
      "two-layer Loss after iteration 13000: 1.4041304287875265\n",
      "two-layer Loss after iteration 14000: 1.2944319827188895\n",
      "two-layer Loss after iteration 15000: 1.197552270329439\n",
      "two-layer Loss after iteration 16000: 1.1103785242460236\n",
      "two-layer Loss after iteration 17000: 1.0387462967047305\n",
      "two-layer Loss after iteration 18000: 0.9775832671228513\n",
      "two-layer Loss after iteration 19000: 0.9246882850474948\n",
      "two-layer Loss after iteration 20000: 0.878289248293116\n",
      "two-layer Loss after iteration 21000: 0.8377481020619808\n",
      "two-layer Loss after iteration 22000: 0.7918821008313234\n",
      "two-layer Loss after iteration 23000: 0.7405099888221944\n",
      "two-layer Loss after iteration 24000: 0.6992436261962457\n",
      "two-layer Loss after iteration 25000: 0.6692233651634707\n",
      "two-layer Loss after iteration 26000: 0.643984876778877\n",
      "two-layer Loss after iteration 27000: 0.6208768764143298\n",
      "two-layer Loss after iteration 28000: 0.6017067276967099\n",
      "two-layer Loss after iteration 29000: 0.581857600058453\n",
      "two-layer Loss after iteration 30000: 0.5622966688204495\n",
      "two-layer Loss after iteration 31000: 0.5476506805658464\n",
      "two-layer Loss after iteration 32000: 0.5342047467035895\n",
      "two-layer Loss after iteration 33000: 0.5220773345714239\n",
      "two-layer Loss after iteration 34000: 0.506712705486661\n",
      "two-layer Loss after iteration 35000: 0.48494315087479317\n",
      "two-layer Loss after iteration 36000: 0.448940480824143\n",
      "two-layer Loss after iteration 37000: 0.43939369614118157\n",
      "two-layer Loss after iteration 38000: 0.434670573759272\n",
      "two-layer Loss after iteration 39000: 0.4296552109813825\n",
      "two-layer Loss after iteration 40000: 0.4267586281655057\n",
      "two-layer Loss after iteration 41000: 0.4238918275402045\n",
      "two-layer Loss after iteration 42000: 0.41995478578625733\n",
      "two-layer Loss after iteration 43000: 0.4174793316985621\n",
      "two-layer Loss after iteration 44000: 0.415431144083376\n",
      "two-layer Loss after iteration 45000: 0.41367401904445383\n",
      "two-layer Loss after iteration 46000: 0.4121512754000436\n",
      "two-layer Loss after iteration 47000: 0.41082037961670015\n",
      "two-layer Loss after iteration 48000: 0.409382175396795\n",
      "two-layer Loss after iteration 49000: 0.4080017797456671\n",
      "two-layer Loss after iteration 50000: 0.4067634475337309\n",
      "two-layer Loss after iteration 51000: 0.4057998500276405\n",
      "two-layer Loss after iteration 52000: 0.4041110436745987\n",
      "two-layer Loss after iteration 53000: 0.40323657255614126\n",
      "two-layer Loss after iteration 54000: 0.4025203181142056\n",
      "two-layer Loss after iteration 55000: 0.4018808391155475\n",
      "two-layer Loss after iteration 56000: 0.40130652525614663\n",
      "two-layer Loss after iteration 57000: 0.3999945935409355\n",
      "two-layer Loss after iteration 58000: 0.3991511935177181\n",
      "two-layer Loss after iteration 59000: 0.39845607471638944\n",
      "two-layer Loss after iteration 60000: 0.39785842248013575\n",
      "two-layer Loss after iteration 61000: 0.3973335078019037\n",
      "two-layer Loss after iteration 62000: 0.39686443927916465\n",
      "two-layer Loss after iteration 63000: 0.3964391547344803\n",
      "two-layer Loss after iteration 64000: 0.39605529714475074\n",
      "two-layer Loss after iteration 65000: 0.3956926647711942\n",
      "two-layer Loss after iteration 66000: 0.39523378407603815\n",
      "two-layer Loss after iteration 67000: 0.3948595421636349\n",
      "two-layer Loss after iteration 68000: 0.3945281744834968\n",
      "two-layer Loss after iteration 69000: 0.3942205798577735\n",
      "two-layer Loss after iteration 70000: 0.3939434589874783\n",
      "two-layer Loss after iteration 71000: 0.39192908612847227\n",
      "two-layer Loss after iteration 72000: 0.39120173588494594\n",
      "two-layer Loss after iteration 73000: 0.39084463260092595\n",
      "two-layer Loss after iteration 74000: 0.3905574612695231\n",
      "two-layer Loss after iteration 75000: 0.3903084297495177\n",
      "two-layer Loss after iteration 76000: 0.39007992113999623\n",
      "two-layer Loss after iteration 77000: 0.38987299779320134\n",
      "two-layer Loss after iteration 78000: 0.3896841167095455\n",
      "two-layer Loss after iteration 79000: 0.3895123377575332\n",
      "two-layer Loss after iteration 80000: 0.3893478780269712\n",
      "two-layer Loss after iteration 81000: 0.38919880110385213\n",
      "two-layer Loss after iteration 82000: 0.38905776650568985\n",
      "two-layer Loss after iteration 83000: 0.388926450168758\n",
      "two-layer Loss after iteration 84000: 0.3888054348396209\n",
      "two-layer Loss after iteration 85000: 0.3886886678318461\n",
      "two-layer Loss after iteration 86000: 0.3885772550721337\n",
      "two-layer Loss after iteration 87000: 0.3884717255723726\n",
      "two-layer Loss after iteration 88000: 0.38837149222119083\n",
      "two-layer Loss after iteration 89000: 0.3882814364688964\n",
      "two-layer Loss after iteration 90000: 0.3881900673404825\n",
      "two-layer Loss after iteration 91000: 0.38810534445661954\n",
      "two-layer Loss after iteration 92000: 0.3880207580325297\n",
      "two-layer Loss after iteration 93000: 0.3879442766486386\n",
      "two-layer Loss after iteration 94000: 0.38786390295557005\n",
      "two-layer Loss after iteration 95000: 0.3877589813728047\n",
      "two-layer Loss after iteration 96000: 0.38766805186139586\n",
      "two-layer Loss after iteration 97000: 0.3875803567375858\n",
      "two-layer Loss after iteration 98000: 0.38749434870550675\n",
      "two-layer Loss after iteration 99000: 0.38741576049150755\n",
      "two-layer Loss after iteration 0: 1406.1841182729918\n",
      "two-layer Loss after iteration 1000: 24.083909755651682\n",
      "two-layer Loss after iteration 2000: 9.101073875584802\n",
      "two-layer Loss after iteration 3000: 6.634122083417484\n",
      "two-layer Loss after iteration 4000: 5.743434302930319\n",
      "two-layer Loss after iteration 5000: 4.812835844186446\n",
      "two-layer Loss after iteration 6000: 3.9810206253725138\n",
      "two-layer Loss after iteration 7000: 3.261593592088258\n",
      "two-layer Loss after iteration 8000: 2.904032654116846\n",
      "two-layer Loss after iteration 9000: 2.343222102423697\n",
      "two-layer Loss after iteration 10000: 2.0282458686471747\n",
      "two-layer Loss after iteration 11000: 1.842114386602618\n",
      "two-layer Loss after iteration 12000: 1.7253109647491027\n",
      "two-layer Loss after iteration 13000: 1.6004723918334953\n",
      "two-layer Loss after iteration 14000: 1.5004094596169928\n",
      "two-layer Loss after iteration 15000: 1.4288827072324866\n",
      "two-layer Loss after iteration 16000: 1.3604710059448124\n",
      "two-layer Loss after iteration 17000: 1.325067529458407\n",
      "two-layer Loss after iteration 18000: 1.3041271755102333\n",
      "two-layer Loss after iteration 19000: 1.2910539481984775\n",
      "two-layer Loss after iteration 20000: 1.2748757584619306\n",
      "two-layer Loss after iteration 21000: 1.2634792490116593\n",
      "two-layer Loss after iteration 22000: 1.2552128019133317\n",
      "two-layer Loss after iteration 23000: 1.2466868095938264\n",
      "two-layer Loss after iteration 24000: 1.2369764800930407\n",
      "two-layer Loss after iteration 25000: 1.2296729955873729\n",
      "two-layer Loss after iteration 26000: 1.224256404000997\n",
      "two-layer Loss after iteration 27000: 1.2185749223275475\n",
      "two-layer Loss after iteration 28000: 1.2142529853217066\n",
      "two-layer Loss after iteration 29000: 1.2107922021149482\n",
      "two-layer Loss after iteration 30000: 1.2079083444042447\n",
      "two-layer Loss after iteration 31000: 1.2047645270677876\n",
      "two-layer Loss after iteration 32000: 1.199790097851583\n",
      "two-layer Loss after iteration 33000: 1.1964072976220916\n",
      "two-layer Loss after iteration 34000: 1.1935595137892343\n",
      "two-layer Loss after iteration 35000: 1.1906488023836275\n",
      "two-layer Loss after iteration 36000: 1.1866267385448928\n",
      "two-layer Loss after iteration 37000: 1.1831032080160109\n",
      "two-layer Loss after iteration 38000: 1.1802189238341443\n",
      "two-layer Loss after iteration 39000: 1.1775857039002942\n",
      "two-layer Loss after iteration 40000: 1.1750296375014573\n",
      "two-layer Loss after iteration 41000: 1.1725961313812425\n",
      "two-layer Loss after iteration 42000: 1.1705091466618942\n",
      "two-layer Loss after iteration 43000: 1.168591266050129\n",
      "two-layer Loss after iteration 44000: 1.1590757052544214\n",
      "two-layer Loss after iteration 45000: 1.1562079316570328\n",
      "two-layer Loss after iteration 46000: 1.153206846375767\n",
      "two-layer Loss after iteration 47000: 1.1490845344058305\n",
      "two-layer Loss after iteration 48000: 1.1462395870391473\n",
      "two-layer Loss after iteration 49000: 1.143849923742088\n",
      "two-layer Loss after iteration 50000: 1.141652852419995\n",
      "two-layer Loss after iteration 51000: 1.1396140757658557\n",
      "two-layer Loss after iteration 52000: 1.1375079311771417\n",
      "two-layer Loss after iteration 53000: 1.1353793357274204\n",
      "two-layer Loss after iteration 54000: 1.1339164397303023\n",
      "two-layer Loss after iteration 55000: 1.1326637636132317\n",
      "two-layer Loss after iteration 56000: 1.130865556673482\n",
      "two-layer Loss after iteration 57000: 1.128927581366196\n",
      "two-layer Loss after iteration 58000: 1.1273928911695346\n",
      "two-layer Loss after iteration 59000: 1.1260758611156843\n",
      "two-layer Loss after iteration 60000: 1.124913668302242\n",
      "two-layer Loss after iteration 61000: 1.1236118008775209\n",
      "two-layer Loss after iteration 62000: 1.1220510944832025\n",
      "two-layer Loss after iteration 63000: 1.1208721633252359\n",
      "two-layer Loss after iteration 64000: 1.119872838034416\n",
      "two-layer Loss after iteration 65000: 1.1189925568548071\n",
      "two-layer Loss after iteration 66000: 1.117900508732707\n",
      "two-layer Loss after iteration 67000: 1.11673203474106\n",
      "two-layer Loss after iteration 68000: 1.1155379628278854\n",
      "two-layer Loss after iteration 69000: 1.1144744955313386\n",
      "two-layer Loss after iteration 70000: 1.1135795655552585\n",
      "two-layer Loss after iteration 71000: 1.1129885299340563\n",
      "two-layer Loss after iteration 72000: 1.1122407489993074\n",
      "two-layer Loss after iteration 73000: 1.1117142572337986\n",
      "two-layer Loss after iteration 74000: 1.111262615615496\n",
      "two-layer Loss after iteration 75000: 1.1108536647624958\n",
      "two-layer Loss after iteration 76000: 1.1104727089233715\n",
      "two-layer Loss after iteration 77000: 1.1101340855303528\n",
      "two-layer Loss after iteration 78000: 1.1098210130238189\n",
      "two-layer Loss after iteration 79000: 1.1095606854291309\n",
      "two-layer Loss after iteration 80000: 1.1093202004562743\n",
      "two-layer Loss after iteration 81000: 1.1090918550589133\n",
      "two-layer Loss after iteration 82000: 1.108868970807469\n",
      "two-layer Loss after iteration 83000: 1.1086564019458909\n",
      "two-layer Loss after iteration 84000: 1.108451436192333\n",
      "two-layer Loss after iteration 85000: 1.1082446381606357\n",
      "two-layer Loss after iteration 86000: 1.1080523771745352\n",
      "two-layer Loss after iteration 87000: 1.107866047272855\n",
      "two-layer Loss after iteration 88000: 1.1076867764849687\n",
      "two-layer Loss after iteration 89000: 1.1075065001874054\n",
      "two-layer Loss after iteration 90000: 1.107335406609968\n",
      "two-layer Loss after iteration 91000: 1.1071564903834825\n",
      "two-layer Loss after iteration 92000: 1.1069881797598038\n",
      "two-layer Loss after iteration 93000: 1.1068249081566326\n",
      "two-layer Loss after iteration 94000: 1.106668106335167\n",
      "two-layer Loss after iteration 95000: 1.106513284290436\n",
      "two-layer Loss after iteration 96000: 1.1063648854244428\n",
      "two-layer Loss after iteration 97000: 1.1062202080832226\n",
      "two-layer Loss after iteration 98000: 1.106081018312322\n",
      "two-layer Loss after iteration 99000: 1.1059465033382587\n",
      "two-layer Loss after iteration 0: 1349.4760473968192\n",
      "two-layer Loss after iteration 1000: 24.732496329470823\n",
      "two-layer Loss after iteration 2000: 10.001114892694709\n",
      "two-layer Loss after iteration 3000: 6.911491587812507\n",
      "two-layer Loss after iteration 4000: 5.0942450213937\n",
      "two-layer Loss after iteration 5000: 4.253928956125937\n",
      "two-layer Loss after iteration 6000: 3.414191649617464\n",
      "two-layer Loss after iteration 7000: 2.735530380750882\n",
      "two-layer Loss after iteration 8000: 2.289520033607988\n",
      "two-layer Loss after iteration 9000: 1.7029122335814206\n",
      "two-layer Loss after iteration 10000: 1.3892161446714175\n",
      "two-layer Loss after iteration 11000: 1.0593763992860872\n",
      "two-layer Loss after iteration 12000: 0.9514651199210544\n",
      "two-layer Loss after iteration 13000: 0.8852438331013066\n",
      "two-layer Loss after iteration 14000: 0.8380101683402189\n",
      "two-layer Loss after iteration 15000: 0.8049277771250665\n",
      "two-layer Loss after iteration 16000: 0.7786159341770023\n",
      "two-layer Loss after iteration 17000: 0.7586171132981705\n",
      "two-layer Loss after iteration 18000: 0.7416328170374598\n",
      "two-layer Loss after iteration 19000: 0.72870011153376\n",
      "two-layer Loss after iteration 20000: 0.7171729819211325\n",
      "two-layer Loss after iteration 21000: 0.707091605371938\n",
      "two-layer Loss after iteration 22000: 0.6981367879962956\n",
      "two-layer Loss after iteration 23000: 0.6894151318212776\n",
      "two-layer Loss after iteration 24000: 0.6821381778977642\n",
      "two-layer Loss after iteration 25000: 0.6759292045609829\n",
      "two-layer Loss after iteration 26000: 0.6705064225667532\n",
      "two-layer Loss after iteration 27000: 0.6655337925861012\n",
      "two-layer Loss after iteration 28000: 0.6611767682188114\n",
      "two-layer Loss after iteration 29000: 0.6535701731560961\n",
      "two-layer Loss after iteration 30000: 0.649460570414973\n",
      "two-layer Loss after iteration 31000: 0.6446857641231064\n",
      "two-layer Loss after iteration 32000: 0.6412116584399278\n",
      "two-layer Loss after iteration 33000: 0.6381600700922586\n",
      "two-layer Loss after iteration 34000: 0.635457121330126\n",
      "two-layer Loss after iteration 35000: 0.6329661292004192\n",
      "two-layer Loss after iteration 36000: 0.6305084302896538\n",
      "two-layer Loss after iteration 37000: 0.6281442676731857\n",
      "two-layer Loss after iteration 38000: 0.6260195936306417\n",
      "two-layer Loss after iteration 39000: 0.6240884080985936\n",
      "two-layer Loss after iteration 40000: 0.6223390158402186\n",
      "two-layer Loss after iteration 41000: 0.6198475050113735\n",
      "two-layer Loss after iteration 42000: 0.6178433638875908\n",
      "two-layer Loss after iteration 43000: 0.616047903249501\n",
      "two-layer Loss after iteration 44000: 0.6144155088319009\n",
      "two-layer Loss after iteration 45000: 0.6128470208335758\n",
      "two-layer Loss after iteration 46000: 0.6114357288563911\n",
      "two-layer Loss after iteration 47000: 0.6101411046073579\n",
      "two-layer Loss after iteration 48000: 0.608956706166424\n",
      "two-layer Loss after iteration 49000: 0.6079196571696188\n",
      "two-layer Loss after iteration 50000: 0.6070147058684876\n",
      "two-layer Loss after iteration 51000: 0.6061845546088775\n",
      "two-layer Loss after iteration 52000: 0.6053932438523093\n",
      "two-layer Loss after iteration 53000: 0.6046518492710405\n",
      "two-layer Loss after iteration 54000: 0.6039485132055552\n",
      "two-layer Loss after iteration 55000: 0.6032858030928715\n",
      "two-layer Loss after iteration 56000: 0.6026613190990971\n",
      "two-layer Loss after iteration 57000: 0.6020715031887892\n",
      "two-layer Loss after iteration 58000: 0.601513735805048\n",
      "two-layer Loss after iteration 59000: 0.5997277993124601\n",
      "two-layer Loss after iteration 60000: 0.5990619688252271\n",
      "two-layer Loss after iteration 61000: 0.5984424918969894\n",
      "two-layer Loss after iteration 62000: 0.5978139866858717\n",
      "two-layer Loss after iteration 63000: 0.5971055305522244\n",
      "two-layer Loss after iteration 64000: 0.5959490659371434\n",
      "two-layer Loss after iteration 65000: 0.5952390265704839\n",
      "two-layer Loss after iteration 66000: 0.5945575298378389\n",
      "two-layer Loss after iteration 67000: 0.5939115580620299\n",
      "two-layer Loss after iteration 68000: 0.5932866475020534\n",
      "two-layer Loss after iteration 69000: 0.5926883151208941\n",
      "two-layer Loss after iteration 70000: 0.5921148924702927\n",
      "two-layer Loss after iteration 71000: 0.5908685707816247\n",
      "two-layer Loss after iteration 72000: 0.5899555075631583\n",
      "two-layer Loss after iteration 73000: 0.5891182153541901\n",
      "two-layer Loss after iteration 74000: 0.5883255099220572\n",
      "two-layer Loss after iteration 75000: 0.5875751205219095\n",
      "two-layer Loss after iteration 76000: 0.5868580334204115\n",
      "two-layer Loss after iteration 77000: 0.5860440253831911\n",
      "two-layer Loss after iteration 78000: 0.5847539805552742\n",
      "two-layer Loss after iteration 79000: 0.5834490178507581\n",
      "two-layer Loss after iteration 80000: 0.5825447578479112\n",
      "two-layer Loss after iteration 81000: 0.5817574809224861\n",
      "two-layer Loss after iteration 82000: 0.5810567131950465\n",
      "two-layer Loss after iteration 83000: 0.5802196717829264\n",
      "two-layer Loss after iteration 84000: 0.5795252423629009\n",
      "two-layer Loss after iteration 85000: 0.5788860373957979\n",
      "two-layer Loss after iteration 86000: 0.5782736618886816\n",
      "two-layer Loss after iteration 87000: 0.577689618428426\n",
      "two-layer Loss after iteration 88000: 0.5771308488163063\n",
      "two-layer Loss after iteration 89000: 0.5765937626959273\n",
      "two-layer Loss after iteration 90000: 0.5760784375866485\n",
      "two-layer Loss after iteration 91000: 0.5719238498999056\n",
      "two-layer Loss after iteration 92000: 0.5709631122998448\n",
      "two-layer Loss after iteration 93000: 0.5699959967508711\n",
      "two-layer Loss after iteration 94000: 0.5691999519534091\n",
      "two-layer Loss after iteration 95000: 0.5681550618332195\n",
      "two-layer Loss after iteration 96000: 0.567392014576955\n",
      "two-layer Loss after iteration 97000: 0.5667246083598845\n",
      "two-layer Loss after iteration 98000: 0.5661222269629711\n",
      "two-layer Loss after iteration 99000: 0.5655733964191837\n",
      "two-layer Loss after iteration 0: 1354.4335061197921\n",
      "two-layer Loss after iteration 1000: 23.690133406003184\n",
      "two-layer Loss after iteration 2000: 9.023170530342242\n",
      "two-layer Loss after iteration 3000: 6.642894023630233\n",
      "two-layer Loss after iteration 4000: 5.450368487568438\n",
      "two-layer Loss after iteration 5000: 4.118201183995428\n",
      "two-layer Loss after iteration 6000: 3.12383401453337\n",
      "two-layer Loss after iteration 7000: 2.679707344100755\n",
      "two-layer Loss after iteration 8000: 2.3424098191934526\n",
      "two-layer Loss after iteration 9000: 2.1214305045838056\n",
      "two-layer Loss after iteration 10000: 1.9737316754185243\n",
      "two-layer Loss after iteration 11000: 1.8532505944080087\n",
      "two-layer Loss after iteration 12000: 1.7567227978305346\n",
      "two-layer Loss after iteration 13000: 1.6816005915563994\n",
      "two-layer Loss after iteration 14000: 1.618340861910719\n",
      "two-layer Loss after iteration 15000: 1.5649520159084755\n",
      "two-layer Loss after iteration 16000: 1.495690776883269\n",
      "two-layer Loss after iteration 17000: 1.4494605593041479\n",
      "two-layer Loss after iteration 18000: 1.4169749045979412\n",
      "two-layer Loss after iteration 19000: 1.3914299054600088\n",
      "two-layer Loss after iteration 20000: 1.3725394715209667\n",
      "two-layer Loss after iteration 21000: 1.3520544878063558\n",
      "two-layer Loss after iteration 22000: 1.3395029446759237\n",
      "two-layer Loss after iteration 23000: 1.3253171307302358\n",
      "two-layer Loss after iteration 24000: 1.3159675229250656\n",
      "two-layer Loss after iteration 25000: 1.3093603743972348\n",
      "two-layer Loss after iteration 26000: 1.303950396229008\n",
      "two-layer Loss after iteration 27000: 1.2939273604831736\n",
      "two-layer Loss after iteration 28000: 1.2862601941985774\n",
      "two-layer Loss after iteration 29000: 1.281474141321202\n",
      "two-layer Loss after iteration 30000: 1.277517227762832\n",
      "two-layer Loss after iteration 31000: 1.2741219556951004\n",
      "two-layer Loss after iteration 32000: 1.2705901514592655\n",
      "two-layer Loss after iteration 33000: 1.2675685036977318\n",
      "two-layer Loss after iteration 34000: 1.2648047681108079\n",
      "two-layer Loss after iteration 35000: 1.262393816334302\n",
      "two-layer Loss after iteration 36000: 1.259235264211043\n",
      "two-layer Loss after iteration 37000: 1.2568737866643485\n",
      "two-layer Loss after iteration 38000: 1.2548245326769518\n",
      "two-layer Loss after iteration 39000: 1.2529800422191946\n",
      "two-layer Loss after iteration 40000: 1.2512981267328445\n",
      "two-layer Loss after iteration 41000: 1.2496854248159546\n",
      "two-layer Loss after iteration 42000: 1.2481804522157565\n",
      "two-layer Loss after iteration 43000: 1.2467711382922706\n",
      "two-layer Loss after iteration 44000: 1.2454241216679818\n",
      "two-layer Loss after iteration 45000: 1.2437909753415826\n",
      "two-layer Loss after iteration 46000: 1.242505448596686\n",
      "two-layer Loss after iteration 47000: 1.2413029915214226\n",
      "two-layer Loss after iteration 48000: 1.2401507920760622\n",
      "two-layer Loss after iteration 49000: 1.2390656927088266\n",
      "two-layer Loss after iteration 50000: 1.238025027469807\n",
      "two-layer Loss after iteration 51000: 1.237029288495969\n",
      "two-layer Loss after iteration 52000: 1.236081933800904\n",
      "two-layer Loss after iteration 53000: 1.235158047408562\n",
      "two-layer Loss after iteration 54000: 1.2342940300312244\n",
      "two-layer Loss after iteration 55000: 1.2334393868256697\n",
      "two-layer Loss after iteration 56000: 1.2326293848169665\n",
      "two-layer Loss after iteration 57000: 1.231843054614978\n",
      "two-layer Loss after iteration 58000: 1.2310920842106596\n",
      "two-layer Loss after iteration 59000: 1.2303589703479696\n",
      "two-layer Loss after iteration 60000: 1.2282366316185696\n",
      "two-layer Loss after iteration 61000: 1.2268652776052562\n",
      "two-layer Loss after iteration 62000: 1.225336507008352\n",
      "two-layer Loss after iteration 63000: 1.2235604886267086\n",
      "two-layer Loss after iteration 64000: 1.2220742892493228\n",
      "two-layer Loss after iteration 65000: 1.22083390005177\n",
      "two-layer Loss after iteration 66000: 1.2195657880473578\n",
      "two-layer Loss after iteration 67000: 1.218406629679253\n",
      "two-layer Loss after iteration 68000: 1.2174783825564448\n",
      "two-layer Loss after iteration 69000: 1.2166224704485336\n",
      "two-layer Loss after iteration 70000: 1.2158145348739782\n",
      "two-layer Loss after iteration 71000: 1.2150633740976808\n",
      "two-layer Loss after iteration 72000: 1.2143662522851089\n",
      "two-layer Loss after iteration 73000: 1.2137045244150457\n",
      "two-layer Loss after iteration 74000: 1.2130806058871628\n",
      "two-layer Loss after iteration 75000: 1.2125183448811938\n",
      "two-layer Loss after iteration 76000: 1.211973252159174\n",
      "two-layer Loss after iteration 77000: 1.2114699097289983\n",
      "two-layer Loss after iteration 78000: 1.2109916232710851\n",
      "two-layer Loss after iteration 79000: 1.2105487546098908\n",
      "two-layer Loss after iteration 80000: 1.2101033206284322\n",
      "two-layer Loss after iteration 81000: 1.209696700667879\n",
      "two-layer Loss after iteration 82000: 1.209286246802549\n",
      "two-layer Loss after iteration 83000: 1.2089020726297752\n",
      "two-layer Loss after iteration 84000: 1.2084427144799184\n",
      "two-layer Loss after iteration 85000: 1.207618677541018\n",
      "two-layer Loss after iteration 86000: 1.2071474964364128\n",
      "two-layer Loss after iteration 87000: 1.206737798425311\n",
      "two-layer Loss after iteration 88000: 1.2063600058698576\n",
      "two-layer Loss after iteration 89000: 1.2060029084753996\n",
      "two-layer Loss after iteration 90000: 1.205669469459226\n",
      "two-layer Loss after iteration 91000: 1.2053478011172472\n",
      "two-layer Loss after iteration 92000: 1.205032135233123\n",
      "two-layer Loss after iteration 93000: 1.2047280874533122\n",
      "two-layer Loss after iteration 94000: 1.2044423053217912\n",
      "two-layer Loss after iteration 95000: 1.2041671852641307\n",
      "two-layer Loss after iteration 96000: 1.2039005547915098\n",
      "two-layer Loss after iteration 97000: 1.2036307925661571\n",
      "two-layer Loss after iteration 98000: 1.203394359944848\n",
      "two-layer Loss after iteration 99000: 1.2031323535257308\n",
      "two-layer Loss after iteration 0: 1526.0824545364362\n",
      "two-layer Loss after iteration 1000: 22.935194128714958\n",
      "two-layer Loss after iteration 2000: 8.76591710817684\n",
      "two-layer Loss after iteration 3000: 6.052794846349151\n",
      "two-layer Loss after iteration 4000: 4.454133004272767\n",
      "two-layer Loss after iteration 5000: 3.5759238290313142\n",
      "two-layer Loss after iteration 6000: 2.8277297525192866\n",
      "two-layer Loss after iteration 7000: 2.389799641824531\n",
      "two-layer Loss after iteration 8000: 1.908441728064903\n",
      "two-layer Loss after iteration 9000: 1.6539047557836135\n",
      "two-layer Loss after iteration 10000: 1.4618553944272012\n",
      "two-layer Loss after iteration 11000: 1.2880575303453377\n",
      "two-layer Loss after iteration 12000: 1.1288535577686516\n",
      "two-layer Loss after iteration 13000: 0.9637631968650556\n",
      "two-layer Loss after iteration 14000: 0.8625663027543174\n",
      "two-layer Loss after iteration 15000: 0.7667277477895513\n",
      "two-layer Loss after iteration 16000: 0.7114515643603938\n",
      "two-layer Loss after iteration 17000: 0.6709402981994087\n",
      "two-layer Loss after iteration 18000: 0.6357475169090796\n",
      "two-layer Loss after iteration 19000: 0.6163789291711332\n",
      "two-layer Loss after iteration 20000: 0.6012092656935597\n",
      "two-layer Loss after iteration 21000: 0.5852196690787665\n",
      "two-layer Loss after iteration 22000: 0.5712452321078323\n",
      "two-layer Loss after iteration 23000: 0.560695225537859\n",
      "two-layer Loss after iteration 24000: 0.5514634909353153\n",
      "two-layer Loss after iteration 25000: 0.5427758561872604\n",
      "two-layer Loss after iteration 26000: 0.5351380080273737\n",
      "two-layer Loss after iteration 27000: 0.5264893076083533\n",
      "two-layer Loss after iteration 28000: 0.5202948378785064\n",
      "two-layer Loss after iteration 29000: 0.5137453291938566\n",
      "two-layer Loss after iteration 30000: 0.5072481171972082\n",
      "two-layer Loss after iteration 31000: 0.4969661762604783\n",
      "two-layer Loss after iteration 32000: 0.48901513433966287\n",
      "two-layer Loss after iteration 33000: 0.4798507102395328\n",
      "two-layer Loss after iteration 34000: 0.4721930914361416\n",
      "two-layer Loss after iteration 35000: 0.46151828324187083\n",
      "two-layer Loss after iteration 36000: 0.4537010001712616\n",
      "two-layer Loss after iteration 37000: 0.44898376835096393\n",
      "two-layer Loss after iteration 38000: 0.4450742724010052\n",
      "two-layer Loss after iteration 39000: 0.440534195461687\n",
      "two-layer Loss after iteration 40000: 0.43553376707969965\n",
      "two-layer Loss after iteration 41000: 0.4310364985930042\n",
      "two-layer Loss after iteration 42000: 0.4275752752586089\n",
      "two-layer Loss after iteration 43000: 0.4244085542604962\n",
      "two-layer Loss after iteration 44000: 0.4215516038323681\n",
      "two-layer Loss after iteration 45000: 0.4189223852743576\n",
      "two-layer Loss after iteration 46000: 0.4164947855384859\n",
      "two-layer Loss after iteration 47000: 0.4092506601411635\n",
      "two-layer Loss after iteration 48000: 0.39758430297131625\n",
      "two-layer Loss after iteration 49000: 0.3932870715920775\n",
      "two-layer Loss after iteration 50000: 0.3895773015165062\n",
      "two-layer Loss after iteration 51000: 0.3865380360435068\n",
      "two-layer Loss after iteration 52000: 0.3809402085556925\n",
      "two-layer Loss after iteration 53000: 0.3782564167338551\n",
      "two-layer Loss after iteration 54000: 0.37609252945400157\n",
      "two-layer Loss after iteration 55000: 0.3742346840735801\n",
      "two-layer Loss after iteration 56000: 0.3725932447424647\n",
      "two-layer Loss after iteration 57000: 0.37116966733444673\n",
      "two-layer Loss after iteration 58000: 0.3698260434602494\n",
      "two-layer Loss after iteration 59000: 0.3663821236269013\n",
      "two-layer Loss after iteration 60000: 0.36490422926339156\n",
      "two-layer Loss after iteration 61000: 0.3637473903958547\n",
      "two-layer Loss after iteration 62000: 0.3627304771837084\n",
      "two-layer Loss after iteration 63000: 0.36159422422062737\n",
      "two-layer Loss after iteration 64000: 0.36066876559757804\n",
      "two-layer Loss after iteration 65000: 0.3597823302132702\n",
      "two-layer Loss after iteration 66000: 0.3590044056381728\n",
      "two-layer Loss after iteration 67000: 0.3582676204478777\n",
      "two-layer Loss after iteration 68000: 0.3576013915605118\n",
      "two-layer Loss after iteration 69000: 0.3569986151658395\n",
      "two-layer Loss after iteration 70000: 0.35645320001587355\n",
      "two-layer Loss after iteration 71000: 0.355699736147626\n",
      "two-layer Loss after iteration 72000: 0.35518985084205684\n",
      "two-layer Loss after iteration 73000: 0.35468986398891145\n",
      "two-layer Loss after iteration 74000: 0.3542250784345418\n",
      "two-layer Loss after iteration 75000: 0.3538273741576464\n",
      "two-layer Loss after iteration 76000: 0.35343875722521195\n",
      "two-layer Loss after iteration 77000: 0.35308716423146874\n",
      "two-layer Loss after iteration 78000: 0.35276106057141743\n",
      "two-layer Loss after iteration 79000: 0.35246537097681724\n",
      "two-layer Loss after iteration 80000: 0.3521773131852354\n",
      "two-layer Loss after iteration 81000: 0.3519168028721387\n",
      "two-layer Loss after iteration 82000: 0.35163168372371856\n",
      "two-layer Loss after iteration 83000: 0.3514274271142656\n",
      "two-layer Loss after iteration 84000: 0.35117555777611625\n",
      "two-layer Loss after iteration 85000: 0.35097994454891324\n",
      "two-layer Loss after iteration 86000: 0.35064150845214476\n",
      "two-layer Loss after iteration 87000: 0.35043928486411835\n",
      "two-layer Loss after iteration 88000: 0.3502200446368721\n",
      "two-layer Loss after iteration 89000: 0.35004013030675185\n",
      "two-layer Loss after iteration 90000: 0.34982927279404363\n",
      "two-layer Loss after iteration 91000: 0.34965287671875533\n",
      "two-layer Loss after iteration 92000: 0.3494905595556543\n",
      "two-layer Loss after iteration 93000: 0.3493315964217972\n",
      "two-layer Loss after iteration 94000: 0.34918824258963366\n",
      "two-layer Loss after iteration 95000: 0.349067539985609\n",
      "two-layer Loss after iteration 96000: 0.3489108879049428\n",
      "two-layer Loss after iteration 97000: 0.34877560451999723\n",
      "two-layer Loss after iteration 98000: 0.34864745067480307\n",
      "two-layer Loss after iteration 99000: 0.34851881778852445\n",
      "two-layer Loss after iteration 0: 1513.7823275410574\n",
      "two-layer Loss after iteration 1000: 25.43294494450857\n",
      "two-layer Loss after iteration 2000: 9.35966425694544\n",
      "two-layer Loss after iteration 3000: 6.458048451980841\n",
      "two-layer Loss after iteration 4000: 5.219651483368205\n",
      "two-layer Loss after iteration 5000: 4.596573983730153\n",
      "two-layer Loss after iteration 6000: 4.128887923539569\n",
      "two-layer Loss after iteration 7000: 3.818747187535688\n",
      "two-layer Loss after iteration 8000: 3.544713009093768\n",
      "two-layer Loss after iteration 9000: 3.3299472106410986\n",
      "two-layer Loss after iteration 10000: 2.9991831475276065\n",
      "two-layer Loss after iteration 11000: 2.1935895868735957\n",
      "two-layer Loss after iteration 12000: 1.704735081222749\n",
      "two-layer Loss after iteration 13000: 1.450912041822278\n",
      "two-layer Loss after iteration 14000: 1.285780600691531\n",
      "two-layer Loss after iteration 15000: 1.1724999652644874\n",
      "two-layer Loss after iteration 16000: 1.0969434159924207\n",
      "two-layer Loss after iteration 17000: 1.0520966532736764\n",
      "two-layer Loss after iteration 18000: 0.9586685345726845\n",
      "two-layer Loss after iteration 19000: 0.9061180233482232\n",
      "two-layer Loss after iteration 20000: 0.880732501968027\n",
      "two-layer Loss after iteration 21000: 0.849671371807334\n",
      "two-layer Loss after iteration 22000: 0.8254020914882815\n",
      "two-layer Loss after iteration 23000: 0.805647450935834\n",
      "two-layer Loss after iteration 24000: 0.7747804971558218\n",
      "two-layer Loss after iteration 25000: 0.7524016283767798\n",
      "two-layer Loss after iteration 26000: 0.7177182360139615\n",
      "two-layer Loss after iteration 27000: 0.6892133699084565\n",
      "two-layer Loss after iteration 28000: 0.67166789698246\n",
      "two-layer Loss after iteration 29000: 0.6533888404040877\n",
      "two-layer Loss after iteration 30000: 0.6360663031871775\n",
      "two-layer Loss after iteration 31000: 0.623314526844852\n",
      "two-layer Loss after iteration 32000: 0.6130020310194982\n",
      "two-layer Loss after iteration 33000: 0.6024460406254218\n",
      "two-layer Loss after iteration 34000: 0.5902698528048894\n",
      "two-layer Loss after iteration 35000: 0.5826370324088114\n",
      "two-layer Loss after iteration 36000: 0.5769573183981362\n",
      "two-layer Loss after iteration 37000: 0.5687959010711525\n",
      "two-layer Loss after iteration 38000: 0.5639560582285167\n",
      "two-layer Loss after iteration 39000: 0.5604909115021197\n",
      "two-layer Loss after iteration 40000: 0.5579175083551127\n",
      "two-layer Loss after iteration 41000: 0.5535807886566592\n",
      "two-layer Loss after iteration 42000: 0.5502995586598162\n",
      "two-layer Loss after iteration 43000: 0.5479139755443964\n",
      "two-layer Loss after iteration 44000: 0.5452550287304043\n",
      "two-layer Loss after iteration 45000: 0.5377000638055434\n",
      "two-layer Loss after iteration 46000: 0.5330272656455941\n",
      "two-layer Loss after iteration 47000: 0.529240372687965\n",
      "two-layer Loss after iteration 48000: 0.5260505026913125\n",
      "two-layer Loss after iteration 49000: 0.523539240934068\n",
      "two-layer Loss after iteration 50000: 0.5213352452108396\n",
      "two-layer Loss after iteration 51000: 0.5195244942894439\n",
      "two-layer Loss after iteration 52000: 0.5179833649893194\n",
      "two-layer Loss after iteration 53000: 0.5165768173288668\n",
      "two-layer Loss after iteration 54000: 0.5153001507437468\n",
      "two-layer Loss after iteration 55000: 0.5141541154739412\n",
      "two-layer Loss after iteration 56000: 0.5131075088860598\n",
      "two-layer Loss after iteration 57000: 0.5121080033130689\n",
      "two-layer Loss after iteration 58000: 0.5111936204937605\n",
      "two-layer Loss after iteration 59000: 0.5101529114956386\n",
      "two-layer Loss after iteration 60000: 0.509141935663671\n",
      "two-layer Loss after iteration 61000: 0.5082308627613291\n",
      "two-layer Loss after iteration 62000: 0.5073976259693607\n",
      "two-layer Loss after iteration 63000: 0.5066198395921975\n",
      "two-layer Loss after iteration 64000: 0.5058787561292918\n",
      "two-layer Loss after iteration 65000: 0.5052122352542059\n",
      "two-layer Loss after iteration 66000: 0.5045639982995366\n",
      "two-layer Loss after iteration 67000: 0.5039194181367452\n",
      "two-layer Loss after iteration 68000: 0.5033159479800631\n",
      "two-layer Loss after iteration 69000: 0.502767127665864\n",
      "two-layer Loss after iteration 70000: 0.5022669930164668\n",
      "two-layer Loss after iteration 71000: 0.5017430891397182\n",
      "two-layer Loss after iteration 72000: 0.5012911754735695\n",
      "two-layer Loss after iteration 73000: 0.500848203409656\n",
      "two-layer Loss after iteration 74000: 0.5004276124025321\n",
      "two-layer Loss after iteration 75000: 0.5000363658872556\n",
      "two-layer Loss after iteration 76000: 0.499638945654995\n",
      "two-layer Loss after iteration 77000: 0.4992726805805185\n",
      "two-layer Loss after iteration 78000: 0.49891336455645907\n",
      "two-layer Loss after iteration 79000: 0.49856674443551524\n",
      "two-layer Loss after iteration 80000: 0.4982297960003793\n",
      "two-layer Loss after iteration 81000: 0.49787772877371395\n",
      "two-layer Loss after iteration 82000: 0.49752925754964017\n",
      "two-layer Loss after iteration 83000: 0.4971945910023572\n",
      "two-layer Loss after iteration 84000: 0.49686629559943307\n",
      "two-layer Loss after iteration 85000: 0.4965508991868002\n",
      "two-layer Loss after iteration 86000: 0.49624569859831885\n",
      "two-layer Loss after iteration 87000: 0.495958358789603\n",
      "two-layer Loss after iteration 88000: 0.495691695465655\n",
      "two-layer Loss after iteration 89000: 0.49542091423271645\n",
      "two-layer Loss after iteration 90000: 0.49513191312516613\n",
      "two-layer Loss after iteration 91000: 0.49490480225155337\n",
      "two-layer Loss after iteration 92000: 0.4946213452175165\n",
      "two-layer Loss after iteration 93000: 0.4944083446486542\n",
      "two-layer Loss after iteration 94000: 0.4941872511334928\n",
      "two-layer Loss after iteration 95000: 0.4939750969685406\n",
      "two-layer Loss after iteration 96000: 0.4937667002671362\n",
      "two-layer Loss after iteration 97000: 0.4935867815925603\n",
      "two-layer Loss after iteration 98000: 0.4933494712391323\n",
      "two-layer Loss after iteration 99000: 0.49317906542681655\n",
      "two-layer Loss after iteration 0: 1455.9385353503474\n",
      "two-layer Loss after iteration 1000: 24.804600535164493\n",
      "two-layer Loss after iteration 2000: 9.212827783674747\n",
      "two-layer Loss after iteration 3000: 6.10171999753163\n",
      "two-layer Loss after iteration 4000: 4.50119324890191\n",
      "two-layer Loss after iteration 5000: 3.5186304416090297\n",
      "two-layer Loss after iteration 6000: 2.8655159504242738\n",
      "two-layer Loss after iteration 7000: 2.433943552188548\n",
      "two-layer Loss after iteration 8000: 2.1400018671238517\n",
      "two-layer Loss after iteration 9000: 1.8825539093761727\n",
      "two-layer Loss after iteration 10000: 1.688364206614305\n",
      "two-layer Loss after iteration 11000: 1.548479311960618\n",
      "two-layer Loss after iteration 12000: 1.4374171404693041\n",
      "two-layer Loss after iteration 13000: 1.3732580533562526\n",
      "two-layer Loss after iteration 14000: 1.3203530940770596\n",
      "two-layer Loss after iteration 15000: 1.2784142623372594\n",
      "two-layer Loss after iteration 16000: 1.2374564781190516\n",
      "two-layer Loss after iteration 17000: 1.2017920452685433\n",
      "two-layer Loss after iteration 18000: 1.1730535861398723\n",
      "two-layer Loss after iteration 19000: 1.1524839451489342\n",
      "two-layer Loss after iteration 20000: 1.136800258997386\n",
      "two-layer Loss after iteration 21000: 1.1240634583238671\n",
      "two-layer Loss after iteration 22000: 1.114641609509992\n",
      "two-layer Loss after iteration 23000: 1.0988111487905514\n",
      "two-layer Loss after iteration 24000: 1.0920207894592042\n",
      "two-layer Loss after iteration 25000: 1.0839343515099071\n",
      "two-layer Loss after iteration 26000: 1.0791602230738002\n",
      "two-layer Loss after iteration 27000: 1.07134689426496\n",
      "two-layer Loss after iteration 28000: 1.0658968606332613\n",
      "two-layer Loss after iteration 29000: 1.0610645448230527\n",
      "two-layer Loss after iteration 30000: 1.0554359138664313\n",
      "two-layer Loss after iteration 31000: 1.0516399658449744\n",
      "two-layer Loss after iteration 32000: 1.0488925891679635\n",
      "two-layer Loss after iteration 33000: 1.0467051241361658\n",
      "two-layer Loss after iteration 34000: 1.044920650529074\n",
      "two-layer Loss after iteration 35000: 1.0351807157220647\n",
      "two-layer Loss after iteration 36000: 1.0327844128202035\n",
      "two-layer Loss after iteration 37000: 1.0311093936102886\n",
      "two-layer Loss after iteration 38000: 1.0298607568699627\n",
      "two-layer Loss after iteration 39000: 1.0278517617102672\n",
      "two-layer Loss after iteration 40000: 1.0266555222315292\n",
      "two-layer Loss after iteration 41000: 1.0251378162006939\n",
      "two-layer Loss after iteration 42000: 1.0239820927294037\n",
      "two-layer Loss after iteration 43000: 1.0231513878869\n",
      "two-layer Loss after iteration 44000: 1.0225342698988806\n",
      "two-layer Loss after iteration 45000: 1.022019197957399\n",
      "two-layer Loss after iteration 46000: 1.0215811187237733\n",
      "two-layer Loss after iteration 47000: 1.0207113617149512\n",
      "two-layer Loss after iteration 48000: 1.0201939004223326\n",
      "two-layer Loss after iteration 49000: 1.019639785719608\n",
      "two-layer Loss after iteration 50000: 1.019240555684987\n",
      "two-layer Loss after iteration 51000: 1.0188946398079637\n",
      "two-layer Loss after iteration 52000: 1.0185914729249939\n",
      "two-layer Loss after iteration 53000: 1.0183227878366885\n",
      "two-layer Loss after iteration 54000: 1.0180739801716452\n",
      "two-layer Loss after iteration 55000: 1.017850981069723\n",
      "two-layer Loss after iteration 56000: 1.0176433367561988\n",
      "two-layer Loss after iteration 57000: 1.0174493459225222\n",
      "two-layer Loss after iteration 58000: 1.0172666570274458\n",
      "two-layer Loss after iteration 59000: 1.0171056725881504\n",
      "two-layer Loss after iteration 60000: 1.0169551842964284\n",
      "two-layer Loss after iteration 61000: 1.0168177587162506\n",
      "two-layer Loss after iteration 62000: 1.0166865894128019\n",
      "two-layer Loss after iteration 63000: 1.0165626574491704\n",
      "two-layer Loss after iteration 64000: 1.0164437250619722\n",
      "two-layer Loss after iteration 65000: 1.0163333197155922\n",
      "two-layer Loss after iteration 66000: 1.0162263892074608\n",
      "two-layer Loss after iteration 67000: 1.0161271003154295\n",
      "9.770351674166936e-05 1.0162263892074608 1.0161271003154295\n",
      "two-layer Loss after iteration 0: 1325.0630225738162\n",
      "two-layer Loss after iteration 1000: 23.354188207696385\n",
      "two-layer Loss after iteration 2000: 9.009125948583934\n",
      "two-layer Loss after iteration 3000: 6.260999516230632\n",
      "two-layer Loss after iteration 4000: 4.4158256599007855\n",
      "two-layer Loss after iteration 5000: 3.3704455290502966\n",
      "two-layer Loss after iteration 6000: 2.647799055866969\n",
      "two-layer Loss after iteration 7000: 2.125688733780342\n",
      "two-layer Loss after iteration 8000: 1.8216467976201225\n",
      "two-layer Loss after iteration 9000: 1.5122851839042142\n",
      "two-layer Loss after iteration 10000: 1.1667155553186412\n",
      "two-layer Loss after iteration 11000: 0.7910949276422254\n",
      "two-layer Loss after iteration 12000: 0.6738322736155535\n",
      "two-layer Loss after iteration 13000: 0.5936705891874356\n",
      "two-layer Loss after iteration 14000: 0.5388828734793316\n",
      "two-layer Loss after iteration 15000: 0.49810872098490716\n",
      "two-layer Loss after iteration 16000: 0.4566296847931282\n",
      "two-layer Loss after iteration 17000: 0.4215930340495328\n",
      "two-layer Loss after iteration 18000: 0.39387529420654976\n",
      "two-layer Loss after iteration 19000: 0.37303651927976134\n",
      "two-layer Loss after iteration 20000: 0.353765108827357\n",
      "two-layer Loss after iteration 21000: 0.33707843159641854\n",
      "two-layer Loss after iteration 22000: 0.32288652825270975\n",
      "two-layer Loss after iteration 23000: 0.31191751814446766\n",
      "two-layer Loss after iteration 24000: 0.3031335507226948\n",
      "two-layer Loss after iteration 25000: 0.2958573382972742\n",
      "two-layer Loss after iteration 26000: 0.289726765466636\n",
      "two-layer Loss after iteration 27000: 0.2833413068811832\n",
      "two-layer Loss after iteration 28000: 0.277314504262994\n",
      "two-layer Loss after iteration 29000: 0.2696895363276956\n",
      "two-layer Loss after iteration 30000: 0.26431123884353447\n",
      "two-layer Loss after iteration 31000: 0.2530208147888705\n",
      "two-layer Loss after iteration 32000: 0.24341066596599617\n",
      "two-layer Loss after iteration 33000: 0.2375458934633028\n",
      "two-layer Loss after iteration 34000: 0.23230484850876656\n",
      "two-layer Loss after iteration 35000: 0.22818299475334503\n",
      "two-layer Loss after iteration 36000: 0.2235693417870132\n",
      "two-layer Loss after iteration 37000: 0.214695963399664\n",
      "two-layer Loss after iteration 38000: 0.20843097989043713\n",
      "two-layer Loss after iteration 39000: 0.2018502313197936\n",
      "two-layer Loss after iteration 40000: 0.19696609821661518\n",
      "two-layer Loss after iteration 41000: 0.19231965063149015\n",
      "two-layer Loss after iteration 42000: 0.18889093951889474\n",
      "two-layer Loss after iteration 43000: 0.18478736646961516\n",
      "two-layer Loss after iteration 44000: 0.17902182533565786\n",
      "two-layer Loss after iteration 45000: 0.1764066306096563\n",
      "two-layer Loss after iteration 46000: 0.17421516265813813\n",
      "two-layer Loss after iteration 47000: 0.17107478855703448\n",
      "two-layer Loss after iteration 48000: 0.1690132226301992\n",
      "two-layer Loss after iteration 49000: 0.1673628931200592\n",
      "two-layer Loss after iteration 50000: 0.16587149411689373\n",
      "two-layer Loss after iteration 51000: 0.16451161000821565\n",
      "two-layer Loss after iteration 52000: 0.16326493681343268\n",
      "two-layer Loss after iteration 53000: 0.16211994106310304\n",
      "two-layer Loss after iteration 54000: 0.16105801149064583\n",
      "two-layer Loss after iteration 55000: 0.15998547570910363\n",
      "two-layer Loss after iteration 56000: 0.1589050180004717\n",
      "two-layer Loss after iteration 57000: 0.157916095123896\n",
      "two-layer Loss after iteration 58000: 0.15699907222725054\n",
      "two-layer Loss after iteration 59000: 0.1561464660154999\n",
      "two-layer Loss after iteration 60000: 0.15534738392953298\n",
      "two-layer Loss after iteration 61000: 0.15458865999907764\n",
      "two-layer Loss after iteration 62000: 0.15388309893525076\n",
      "two-layer Loss after iteration 63000: 0.1532212157941571\n",
      "two-layer Loss after iteration 64000: 0.15259873577512603\n",
      "two-layer Loss after iteration 65000: 0.15201145792076784\n",
      "two-layer Loss after iteration 66000: 0.1514595830153573\n",
      "two-layer Loss after iteration 67000: 0.15093566429881816\n",
      "two-layer Loss after iteration 68000: 0.1504409215703663\n",
      "two-layer Loss after iteration 69000: 0.1499690948900489\n",
      "two-layer Loss after iteration 70000: 0.14952531781061792\n",
      "two-layer Loss after iteration 71000: 0.14910231938687704\n",
      "two-layer Loss after iteration 72000: 0.14869969316946152\n",
      "two-layer Loss after iteration 73000: 0.14831687104375527\n",
      "two-layer Loss after iteration 74000: 0.14795227785350157\n",
      "two-layer Loss after iteration 75000: 0.1476043538082578\n",
      "two-layer Loss after iteration 76000: 0.14727500808523755\n",
      "two-layer Loss after iteration 77000: 0.14695891468556582\n",
      "two-layer Loss after iteration 78000: 0.1466597810657873\n",
      "two-layer Loss after iteration 79000: 0.14637270927631107\n",
      "two-layer Loss after iteration 80000: 0.1460991411138921\n",
      "two-layer Loss after iteration 81000: 0.14583871493609554\n",
      "two-layer Loss after iteration 82000: 0.1454122819221764\n",
      "two-layer Loss after iteration 83000: 0.14516022971450215\n",
      "two-layer Loss after iteration 84000: 0.14492053637431346\n",
      "two-layer Loss after iteration 85000: 0.14469241333242913\n",
      "two-layer Loss after iteration 86000: 0.1444740039305009\n",
      "two-layer Loss after iteration 87000: 0.14426518126107832\n",
      "two-layer Loss after iteration 88000: 0.13414250063421382\n",
      "two-layer Loss after iteration 89000: 0.12937581657600963\n",
      "two-layer Loss after iteration 90000: 0.12707039577192572\n",
      "two-layer Loss after iteration 91000: 0.12514897063149164\n",
      "two-layer Loss after iteration 92000: 0.12357053761016301\n",
      "two-layer Loss after iteration 93000: 0.12238635801912642\n",
      "two-layer Loss after iteration 94000: 0.12149858342030095\n",
      "two-layer Loss after iteration 95000: 0.12085652509452338\n",
      "two-layer Loss after iteration 96000: 0.12042434476146746\n",
      "two-layer Loss after iteration 97000: 0.12004904879882557\n",
      "two-layer Loss after iteration 98000: 0.11974587969738486\n",
      "two-layer Loss after iteration 99000: 0.11947003747853555\n",
      "two-layer Loss after iteration 0: 1389.5101591568953\n",
      "two-layer Loss after iteration 1000: 25.893202066384653\n",
      "two-layer Loss after iteration 2000: 9.808035475595462\n",
      "two-layer Loss after iteration 3000: 7.0978460931566465\n",
      "two-layer Loss after iteration 4000: 5.713930272321509\n",
      "two-layer Loss after iteration 5000: 4.887536558398477\n",
      "two-layer Loss after iteration 6000: 4.300978608262757\n",
      "two-layer Loss after iteration 7000: 3.910449281369235\n",
      "two-layer Loss after iteration 8000: 3.3458688246575417\n",
      "two-layer Loss after iteration 9000: 3.1082568250107916\n",
      "two-layer Loss after iteration 10000: 2.8826916303871415\n",
      "two-layer Loss after iteration 11000: 2.709610166913954\n",
      "two-layer Loss after iteration 12000: 2.4202193504812084\n",
      "two-layer Loss after iteration 13000: 2.1549389265069014\n",
      "two-layer Loss after iteration 14000: 2.0005446453321096\n",
      "two-layer Loss after iteration 15000: 1.8535227759701887\n",
      "two-layer Loss after iteration 16000: 1.6454628472264656\n",
      "two-layer Loss after iteration 17000: 1.4558984895900497\n",
      "two-layer Loss after iteration 18000: 1.375270996082275\n",
      "two-layer Loss after iteration 19000: 1.3466630375977253\n",
      "two-layer Loss after iteration 20000: 1.3285213213100526\n",
      "two-layer Loss after iteration 21000: 1.3137951365026874\n",
      "two-layer Loss after iteration 22000: 1.3015361583719123\n",
      "two-layer Loss after iteration 23000: 1.2916799800582406\n",
      "two-layer Loss after iteration 24000: 1.2827147533114032\n",
      "two-layer Loss after iteration 25000: 1.2745633728653785\n",
      "two-layer Loss after iteration 26000: 1.2670036197420271\n",
      "two-layer Loss after iteration 27000: 1.2596080299378216\n",
      "two-layer Loss after iteration 28000: 1.2518164327002121\n",
      "two-layer Loss after iteration 29000: 1.2447306058575827\n",
      "two-layer Loss after iteration 30000: 1.238415105449513\n",
      "two-layer Loss after iteration 31000: 1.2327195859271298\n",
      "two-layer Loss after iteration 32000: 1.2278715466446648\n",
      "two-layer Loss after iteration 33000: 1.223712327559605\n",
      "two-layer Loss after iteration 34000: 1.220013589858147\n",
      "two-layer Loss after iteration 35000: 1.21614083420403\n",
      "two-layer Loss after iteration 36000: 1.2126674534345687\n",
      "two-layer Loss after iteration 37000: 1.2094206116851092\n",
      "two-layer Loss after iteration 38000: 1.2064373969893194\n",
      "two-layer Loss after iteration 39000: 1.2036199719072112\n",
      "two-layer Loss after iteration 40000: 1.2009788542577915\n",
      "two-layer Loss after iteration 41000: 1.1985686378586025\n",
      "two-layer Loss after iteration 42000: 1.1962638355621737\n",
      "two-layer Loss after iteration 43000: 1.1941081969421283\n",
      "two-layer Loss after iteration 44000: 1.19204787373343\n",
      "two-layer Loss after iteration 45000: 1.1901064775702253\n",
      "two-layer Loss after iteration 46000: 1.1882428192058292\n",
      "two-layer Loss after iteration 47000: 1.1864910559119606\n",
      "two-layer Loss after iteration 48000: 1.184823041109685\n",
      "two-layer Loss after iteration 49000: 1.1832307957449537\n",
      "two-layer Loss after iteration 50000: 1.1817558042642766\n",
      "two-layer Loss after iteration 51000: 1.180317087916278\n",
      "two-layer Loss after iteration 52000: 1.1789607733848049\n",
      "two-layer Loss after iteration 53000: 1.177563227164294\n",
      "two-layer Loss after iteration 54000: 1.176095207170013\n",
      "two-layer Loss after iteration 55000: 1.1417203581926687\n",
      "two-layer Loss after iteration 56000: 0.7944146164481263\n",
      "two-layer Loss after iteration 57000: 0.7051420747364566\n",
      "two-layer Loss after iteration 58000: 0.6803588318433004\n",
      "two-layer Loss after iteration 59000: 0.6711616138392122\n",
      "two-layer Loss after iteration 60000: 0.6661393384581037\n",
      "two-layer Loss after iteration 61000: 0.6601915183181729\n",
      "two-layer Loss after iteration 62000: 0.6574001580833008\n",
      "two-layer Loss after iteration 63000: 0.6553203667231671\n",
      "two-layer Loss after iteration 64000: 0.6537063711840655\n",
      "two-layer Loss after iteration 65000: 0.6524027251182636\n",
      "two-layer Loss after iteration 66000: 0.6512828081374261\n",
      "two-layer Loss after iteration 67000: 0.6503401396823089\n",
      "two-layer Loss after iteration 68000: 0.649509286330016\n",
      "two-layer Loss after iteration 69000: 0.6487834793304786\n",
      "two-layer Loss after iteration 70000: 0.6481370719610482\n",
      "two-layer Loss after iteration 71000: 0.6474655237293659\n",
      "two-layer Loss after iteration 72000: 0.6467580156663577\n",
      "two-layer Loss after iteration 73000: 0.646048270476374\n",
      "two-layer Loss after iteration 74000: 0.6440899593940903\n",
      "two-layer Loss after iteration 75000: 0.6432946285908971\n",
      "two-layer Loss after iteration 76000: 0.6426907536249369\n",
      "two-layer Loss after iteration 77000: 0.6421143351033304\n",
      "two-layer Loss after iteration 78000: 0.6415535208608814\n",
      "two-layer Loss after iteration 79000: 0.640997918153672\n",
      "two-layer Loss after iteration 80000: 0.6403004076255174\n",
      "two-layer Loss after iteration 81000: 0.6396266996614631\n",
      "two-layer Loss after iteration 82000: 0.6390120528637888\n",
      "two-layer Loss after iteration 83000: 0.6383665236273967\n",
      "two-layer Loss after iteration 84000: 0.6377691324479742\n",
      "two-layer Loss after iteration 85000: 0.6371898376831564\n",
      "two-layer Loss after iteration 86000: 0.6366662472945526\n",
      "two-layer Loss after iteration 87000: 0.6360676333273699\n",
      "two-layer Loss after iteration 88000: 0.6354743134970684\n",
      "two-layer Loss after iteration 89000: 0.6349923316691678\n",
      "two-layer Loss after iteration 90000: 0.6345617320606526\n",
      "two-layer Loss after iteration 91000: 0.6341104863389357\n",
      "two-layer Loss after iteration 92000: 0.6337045193278977\n",
      "two-layer Loss after iteration 93000: 0.6333385675451244\n",
      "two-layer Loss after iteration 94000: 0.6330020089444836\n",
      "two-layer Loss after iteration 95000: 0.6326247326788818\n",
      "two-layer Loss after iteration 96000: 0.6322780794445809\n",
      "two-layer Loss after iteration 97000: 0.631941526625421\n",
      "two-layer Loss after iteration 98000: 0.6315984929792184\n",
      "two-layer Loss after iteration 99000: 0.6312674566872153\n",
      "two-layer Loss after iteration 0: 1518.827121558953\n",
      "two-layer Loss after iteration 1000: 23.839691977264344\n",
      "two-layer Loss after iteration 2000: 9.333190982854159\n",
      "two-layer Loss after iteration 3000: 6.7019480944318435\n",
      "two-layer Loss after iteration 4000: 5.513395102515759\n",
      "two-layer Loss after iteration 5000: 4.270529537471527\n",
      "two-layer Loss after iteration 6000: 3.436724360494741\n",
      "two-layer Loss after iteration 7000: 2.56285427717692\n",
      "two-layer Loss after iteration 8000: 1.8826446241796995\n",
      "two-layer Loss after iteration 9000: 1.4279329594117327\n",
      "two-layer Loss after iteration 10000: 1.1175272202835056\n",
      "two-layer Loss after iteration 11000: 0.9940822970578163\n",
      "two-layer Loss after iteration 12000: 0.9360706276686253\n",
      "two-layer Loss after iteration 13000: 0.9005988191781874\n",
      "two-layer Loss after iteration 14000: 0.8756902627017087\n",
      "two-layer Loss after iteration 15000: 0.8513345282919635\n",
      "two-layer Loss after iteration 16000: 0.8288910677781095\n",
      "two-layer Loss after iteration 17000: 0.8129532731294297\n",
      "two-layer Loss after iteration 18000: 0.7987555398344838\n",
      "two-layer Loss after iteration 19000: 0.784845526866233\n",
      "two-layer Loss after iteration 20000: 0.7732432185131864\n",
      "two-layer Loss after iteration 21000: 0.7634508505641422\n",
      "two-layer Loss after iteration 22000: 0.7548489696835667\n",
      "two-layer Loss after iteration 23000: 0.7468565807615652\n",
      "two-layer Loss after iteration 24000: 0.7397168351887045\n",
      "two-layer Loss after iteration 25000: 0.7332732573551521\n",
      "two-layer Loss after iteration 26000: 0.727575815278068\n",
      "two-layer Loss after iteration 27000: 0.7225295046987216\n",
      "two-layer Loss after iteration 28000: 0.7179257932425985\n",
      "two-layer Loss after iteration 29000: 0.7131846263438106\n",
      "two-layer Loss after iteration 30000: 0.7090042897393523\n",
      "two-layer Loss after iteration 31000: 0.7053396758555194\n",
      "two-layer Loss after iteration 32000: 0.7018590933150985\n",
      "two-layer Loss after iteration 33000: 0.6958855158441121\n",
      "two-layer Loss after iteration 34000: 0.683871499147075\n",
      "two-layer Loss after iteration 35000: 0.679119070637129\n",
      "two-layer Loss after iteration 36000: 0.6753762810543051\n",
      "two-layer Loss after iteration 37000: 0.6719964214175522\n",
      "two-layer Loss after iteration 38000: 0.6687759086132261\n",
      "two-layer Loss after iteration 39000: 0.6658362543471975\n",
      "two-layer Loss after iteration 40000: 0.6631171395802705\n",
      "two-layer Loss after iteration 41000: 0.6563654700807037\n",
      "two-layer Loss after iteration 42000: 0.6498189002647089\n",
      "two-layer Loss after iteration 43000: 0.6453458042539221\n",
      "two-layer Loss after iteration 44000: 0.6417880152272101\n",
      "two-layer Loss after iteration 45000: 0.6371650500582857\n",
      "two-layer Loss after iteration 46000: 0.63366971192751\n",
      "two-layer Loss after iteration 47000: 0.6246224204941869\n",
      "two-layer Loss after iteration 48000: 0.6163739503297085\n",
      "two-layer Loss after iteration 49000: 0.6101782570784947\n",
      "two-layer Loss after iteration 50000: 0.6041077794819812\n",
      "two-layer Loss after iteration 51000: 0.5940615265376635\n",
      "two-layer Loss after iteration 52000: 0.5864471909952655\n",
      "two-layer Loss after iteration 53000: 0.5804865836325661\n",
      "two-layer Loss after iteration 54000: 0.5751469998421949\n",
      "two-layer Loss after iteration 55000: 0.5695671130195765\n",
      "two-layer Loss after iteration 56000: 0.5609871978663019\n",
      "two-layer Loss after iteration 57000: 0.5474743285788195\n",
      "two-layer Loss after iteration 58000: 0.5346525576386004\n",
      "two-layer Loss after iteration 59000: 0.5204796080509221\n",
      "two-layer Loss after iteration 60000: 0.5098963972599164\n",
      "two-layer Loss after iteration 61000: 0.49983288757474953\n",
      "two-layer Loss after iteration 62000: 0.4881803467211563\n",
      "two-layer Loss after iteration 63000: 0.4795185766579645\n",
      "two-layer Loss after iteration 64000: 0.47308499173034446\n",
      "two-layer Loss after iteration 65000: 0.46757418303631376\n",
      "two-layer Loss after iteration 66000: 0.4621720981004858\n",
      "two-layer Loss after iteration 67000: 0.4579014796032493\n",
      "two-layer Loss after iteration 68000: 0.45464402769060697\n",
      "two-layer Loss after iteration 69000: 0.45201448839701475\n",
      "two-layer Loss after iteration 70000: 0.4490783050124217\n",
      "two-layer Loss after iteration 71000: 0.4469618152170902\n",
      "two-layer Loss after iteration 72000: 0.44514581438357415\n",
      "two-layer Loss after iteration 73000: 0.4430033201799166\n",
      "two-layer Loss after iteration 74000: 0.4403758823921809\n",
      "two-layer Loss after iteration 75000: 0.43814516219106914\n",
      "two-layer Loss after iteration 76000: 0.435547033762886\n",
      "two-layer Loss after iteration 77000: 0.43417639664547597\n",
      "two-layer Loss after iteration 78000: 0.4330737937628645\n",
      "two-layer Loss after iteration 79000: 0.43170290638898906\n",
      "two-layer Loss after iteration 80000: 0.4302706533160131\n",
      "two-layer Loss after iteration 81000: 0.4293051128974955\n",
      "two-layer Loss after iteration 82000: 0.4257365138470951\n",
      "two-layer Loss after iteration 83000: 0.4243178785533868\n",
      "two-layer Loss after iteration 84000: 0.42314766321069264\n",
      "two-layer Loss after iteration 85000: 0.4221544657080287\n",
      "two-layer Loss after iteration 86000: 0.4213476175640625\n",
      "two-layer Loss after iteration 87000: 0.4206691331988346\n",
      "two-layer Loss after iteration 88000: 0.4200945602536346\n",
      "two-layer Loss after iteration 89000: 0.41960400972752004\n",
      "two-layer Loss after iteration 90000: 0.419175914470985\n",
      "two-layer Loss after iteration 91000: 0.4187945366927878\n",
      "two-layer Loss after iteration 92000: 0.4184487486944566\n",
      "two-layer Loss after iteration 93000: 0.41813667167310786\n",
      "two-layer Loss after iteration 94000: 0.4178517661241239\n",
      "two-layer Loss after iteration 95000: 0.417586092757742\n",
      "two-layer Loss after iteration 96000: 0.4173381947197152\n",
      "two-layer Loss after iteration 97000: 0.4171052862385072\n",
      "two-layer Loss after iteration 98000: 0.41688751263284746\n",
      "two-layer Loss after iteration 99000: 0.41668216531978486\n",
      "two-layer Loss after iteration 0: 1388.3778411409266\n",
      "two-layer Loss after iteration 1000: 24.031093995552997\n",
      "two-layer Loss after iteration 2000: 9.05989228694294\n",
      "two-layer Loss after iteration 3000: 6.0335159607287245\n",
      "two-layer Loss after iteration 4000: 4.513400244675486\n",
      "two-layer Loss after iteration 5000: 3.1454048204567333\n",
      "two-layer Loss after iteration 6000: 2.3064259009575574\n",
      "two-layer Loss after iteration 7000: 1.9451610898513017\n",
      "two-layer Loss after iteration 8000: 1.7501606346805145\n",
      "two-layer Loss after iteration 9000: 1.529468886625567\n",
      "two-layer Loss after iteration 10000: 1.2476649569412464\n",
      "two-layer Loss after iteration 11000: 1.0977839319214273\n",
      "two-layer Loss after iteration 12000: 1.0062942876592404\n",
      "two-layer Loss after iteration 13000: 0.9421849463309894\n",
      "two-layer Loss after iteration 14000: 0.8913252273628381\n",
      "two-layer Loss after iteration 15000: 0.8548828656920816\n",
      "two-layer Loss after iteration 16000: 0.8276997673584123\n",
      "two-layer Loss after iteration 17000: 0.8037080392733217\n",
      "two-layer Loss after iteration 18000: 0.7816065293256078\n",
      "two-layer Loss after iteration 19000: 0.7543156075358044\n",
      "two-layer Loss after iteration 20000: 0.7251607251948365\n",
      "two-layer Loss after iteration 21000: 0.7017387913096246\n",
      "two-layer Loss after iteration 22000: 0.6790386871818841\n",
      "two-layer Loss after iteration 23000: 0.662712424313951\n",
      "two-layer Loss after iteration 24000: 0.6444387808465935\n",
      "two-layer Loss after iteration 25000: 0.6307353892433272\n",
      "two-layer Loss after iteration 26000: 0.6186538635290854\n",
      "two-layer Loss after iteration 27000: 0.5875053054153341\n",
      "two-layer Loss after iteration 28000: 0.5702326905730906\n",
      "two-layer Loss after iteration 29000: 0.5525496147105137\n",
      "two-layer Loss after iteration 30000: 0.5395441430571312\n",
      "two-layer Loss after iteration 31000: 0.5267638966862014\n",
      "two-layer Loss after iteration 32000: 0.5157114625512264\n",
      "two-layer Loss after iteration 33000: 0.5067635807509575\n",
      "two-layer Loss after iteration 34000: 0.4988524569803941\n",
      "two-layer Loss after iteration 35000: 0.4922256858936688\n",
      "two-layer Loss after iteration 36000: 0.4859892422745157\n",
      "two-layer Loss after iteration 37000: 0.4775650882944406\n",
      "two-layer Loss after iteration 38000: 0.4712394156826429\n",
      "two-layer Loss after iteration 39000: 0.46331838876930986\n",
      "two-layer Loss after iteration 40000: 0.45694284495529663\n",
      "two-layer Loss after iteration 41000: 0.45235535691785217\n",
      "two-layer Loss after iteration 42000: 0.4480932826055765\n",
      "two-layer Loss after iteration 43000: 0.4396995413231126\n",
      "two-layer Loss after iteration 44000: 0.4327214335117813\n",
      "two-layer Loss after iteration 45000: 0.427075903949419\n",
      "two-layer Loss after iteration 46000: 0.4197476148406914\n",
      "two-layer Loss after iteration 47000: 0.41370394592211324\n",
      "two-layer Loss after iteration 48000: 0.40891461849392285\n",
      "two-layer Loss after iteration 49000: 0.4043950766192504\n",
      "two-layer Loss after iteration 50000: 0.4000032354977472\n",
      "two-layer Loss after iteration 51000: 0.39502046063824536\n",
      "two-layer Loss after iteration 52000: 0.3905815665556468\n",
      "two-layer Loss after iteration 53000: 0.3851065947654094\n",
      "two-layer Loss after iteration 54000: 0.38111358221064723\n",
      "two-layer Loss after iteration 55000: 0.3765651024631115\n",
      "two-layer Loss after iteration 56000: 0.3730469024685408\n",
      "two-layer Loss after iteration 57000: 0.36985402143649443\n",
      "two-layer Loss after iteration 58000: 0.36751500572343276\n",
      "two-layer Loss after iteration 59000: 0.3656020889059583\n",
      "two-layer Loss after iteration 60000: 0.3639771325746189\n",
      "two-layer Loss after iteration 61000: 0.36256841238158555\n",
      "two-layer Loss after iteration 62000: 0.36132460724688753\n",
      "two-layer Loss after iteration 63000: 0.36021361922091044\n",
      "two-layer Loss after iteration 64000: 0.3592102525534936\n",
      "two-layer Loss after iteration 65000: 0.3571015257434987\n",
      "two-layer Loss after iteration 66000: 0.35595984638877187\n",
      "two-layer Loss after iteration 67000: 0.35503146230648663\n",
      "two-layer Loss after iteration 68000: 0.35421457653386507\n",
      "two-layer Loss after iteration 69000: 0.3533617039851605\n",
      "two-layer Loss after iteration 70000: 0.35255383031789655\n",
      "two-layer Loss after iteration 71000: 0.35186545622287346\n",
      "two-layer Loss after iteration 72000: 0.3509816029802248\n",
      "two-layer Loss after iteration 73000: 0.35024624163008483\n",
      "two-layer Loss after iteration 74000: 0.34892359212943114\n",
      "two-layer Loss after iteration 75000: 0.34801506257993775\n",
      "two-layer Loss after iteration 76000: 0.3472179296387421\n",
      "two-layer Loss after iteration 77000: 0.3464924872018618\n",
      "two-layer Loss after iteration 78000: 0.34582546131451897\n",
      "two-layer Loss after iteration 79000: 0.3452081862109166\n",
      "two-layer Loss after iteration 80000: 0.3446342355279377\n",
      "two-layer Loss after iteration 81000: 0.3437105745609828\n",
      "two-layer Loss after iteration 82000: 0.34286488981537905\n",
      "two-layer Loss after iteration 83000: 0.34222567344505744\n",
      "two-layer Loss after iteration 84000: 0.3416465240753934\n",
      "two-layer Loss after iteration 85000: 0.34110917910498584\n",
      "two-layer Loss after iteration 86000: 0.34060725210739784\n",
      "two-layer Loss after iteration 87000: 0.3400882867139445\n",
      "two-layer Loss after iteration 88000: 0.3395116875300409\n",
      "two-layer Loss after iteration 89000: 0.33898104629314163\n",
      "two-layer Loss after iteration 90000: 0.33848859998536895\n",
      "two-layer Loss after iteration 91000: 0.33802996289373655\n",
      "two-layer Loss after iteration 92000: 0.337601725621261\n",
      "two-layer Loss after iteration 93000: 0.33720105144009455\n",
      "two-layer Loss after iteration 94000: 0.3368271193027847\n",
      "two-layer Loss after iteration 95000: 0.33648526398006995\n",
      "two-layer Loss after iteration 96000: 0.3361715429282435\n",
      "two-layer Loss after iteration 97000: 0.3358823363546763\n",
      "two-layer Loss after iteration 98000: 0.3356140525913241\n",
      "two-layer Loss after iteration 99000: 0.33536437726154544\n",
      "two-layer Loss after iteration 0: 1532.4602064202054\n",
      "two-layer Loss after iteration 1000: 23.471632212951295\n",
      "two-layer Loss after iteration 2000: 8.904354340025835\n",
      "two-layer Loss after iteration 3000: 6.1961931953376395\n",
      "two-layer Loss after iteration 4000: 4.695723134702203\n",
      "two-layer Loss after iteration 5000: 3.6659805212050083\n",
      "two-layer Loss after iteration 6000: 2.6033286169893346\n",
      "two-layer Loss after iteration 7000: 2.0379157232940486\n",
      "two-layer Loss after iteration 8000: 1.4359573451309755\n",
      "two-layer Loss after iteration 9000: 1.0147138166572929\n",
      "two-layer Loss after iteration 10000: 0.7997261766720815\n",
      "two-layer Loss after iteration 11000: 0.6689996995643062\n",
      "two-layer Loss after iteration 12000: 0.5694673565294622\n",
      "two-layer Loss after iteration 13000: 0.4994285445655694\n",
      "two-layer Loss after iteration 14000: 0.44340713538782023\n",
      "two-layer Loss after iteration 15000: 0.38642419640563525\n",
      "two-layer Loss after iteration 16000: 0.3484758687466936\n",
      "two-layer Loss after iteration 17000: 0.32644493101838346\n",
      "two-layer Loss after iteration 18000: 0.3082358963338325\n",
      "two-layer Loss after iteration 19000: 0.2942393753476859\n",
      "two-layer Loss after iteration 20000: 0.28324960691375717\n",
      "two-layer Loss after iteration 21000: 0.2744698886315874\n",
      "two-layer Loss after iteration 22000: 0.26824953976451854\n",
      "two-layer Loss after iteration 23000: 0.2631127219813288\n",
      "two-layer Loss after iteration 24000: 0.25873160347648994\n",
      "two-layer Loss after iteration 25000: 0.2549999504711459\n",
      "two-layer Loss after iteration 26000: 0.25147905712190216\n",
      "two-layer Loss after iteration 27000: 0.24749040961632462\n",
      "two-layer Loss after iteration 28000: 0.24345233913008735\n",
      "two-layer Loss after iteration 29000: 0.2397553977192808\n",
      "two-layer Loss after iteration 30000: 0.23599618622874066\n",
      "two-layer Loss after iteration 31000: 0.23042962325018948\n",
      "two-layer Loss after iteration 32000: 0.22652345561797252\n",
      "two-layer Loss after iteration 33000: 0.22226064510442373\n",
      "two-layer Loss after iteration 34000: 0.2199481183530422\n",
      "two-layer Loss after iteration 35000: 0.21576977082308052\n",
      "two-layer Loss after iteration 36000: 0.2130414595702564\n",
      "two-layer Loss after iteration 37000: 0.20982394602234938\n",
      "two-layer Loss after iteration 38000: 0.20676944729202723\n",
      "two-layer Loss after iteration 39000: 0.20540019964784953\n",
      "two-layer Loss after iteration 40000: 0.20431450254772224\n",
      "two-layer Loss after iteration 41000: 0.20335329377364966\n",
      "two-layer Loss after iteration 42000: 0.20241521073997087\n",
      "two-layer Loss after iteration 43000: 0.20143958721412086\n",
      "two-layer Loss after iteration 44000: 0.20059951706144832\n",
      "two-layer Loss after iteration 45000: 0.19984782174540203\n",
      "two-layer Loss after iteration 46000: 0.19916655083986287\n",
      "two-layer Loss after iteration 47000: 0.19842247303365462\n",
      "two-layer Loss after iteration 48000: 0.19734712820263206\n",
      "two-layer Loss after iteration 49000: 0.19677184084248067\n",
      "two-layer Loss after iteration 50000: 0.1962731029748641\n",
      "two-layer Loss after iteration 51000: 0.1958331712397681\n",
      "two-layer Loss after iteration 52000: 0.19544057275643723\n",
      "two-layer Loss after iteration 53000: 0.19508609744044306\n",
      "two-layer Loss after iteration 54000: 0.19476450263665085\n",
      "two-layer Loss after iteration 55000: 0.19447184958934627\n",
      "two-layer Loss after iteration 56000: 0.19420458042154584\n",
      "two-layer Loss after iteration 57000: 0.19395994789735924\n",
      "two-layer Loss after iteration 58000: 0.19373549864852782\n",
      "two-layer Loss after iteration 59000: 0.19352942026018322\n",
      "two-layer Loss after iteration 60000: 0.19333936966774412\n",
      "two-layer Loss after iteration 61000: 0.19316435184068698\n",
      "two-layer Loss after iteration 62000: 0.19300261878214087\n",
      "two-layer Loss after iteration 63000: 0.19285288900242145\n",
      "two-layer Loss after iteration 64000: 0.19271405111654058\n",
      "two-layer Loss after iteration 65000: 0.19258543384345927\n",
      "two-layer Loss after iteration 66000: 0.1924657718116028\n",
      "two-layer Loss after iteration 67000: 0.19235446129883718\n",
      "two-layer Loss after iteration 68000: 0.1922507469082704\n",
      "two-layer Loss after iteration 69000: 0.19215390110063216\n",
      "two-layer Loss after iteration 70000: 0.19206347534572576\n",
      "two-layer Loss after iteration 71000: 0.19197890029883652\n",
      "two-layer Loss after iteration 72000: 0.1918997332198115\n",
      "two-layer Loss after iteration 73000: 0.19182541043259776\n",
      "two-layer Loss after iteration 74000: 0.1917557031122517\n",
      "two-layer Loss after iteration 75000: 0.1916902180372965\n",
      "two-layer Loss after iteration 76000: 0.19162860329649928\n",
      "two-layer Loss after iteration 77000: 0.19157054843417598\n",
      "two-layer Loss after iteration 78000: 0.1915156893966047\n",
      "two-layer Loss after iteration 79000: 0.19146385450732395\n",
      "two-layer Loss after iteration 80000: 0.1914127272025115\n",
      "two-layer Loss after iteration 81000: 0.1913637589530959\n",
      "two-layer Loss after iteration 82000: 0.19131753621244632\n",
      "two-layer Loss after iteration 83000: 0.1912739379573155\n",
      "two-layer Loss after iteration 84000: 0.19123262366430924\n",
      "two-layer Loss after iteration 85000: 0.19119365901130134\n",
      "two-layer Loss after iteration 86000: 0.19115663411630143\n",
      "two-layer Loss after iteration 87000: 0.19112161953325407\n",
      "two-layer Loss after iteration 88000: 0.1910883671614222\n",
      "two-layer Loss after iteration 89000: 0.19105681418822315\n",
      "two-layer Loss after iteration 90000: 0.19102673891425265\n",
      "two-layer Loss after iteration 91000: 0.19099817707163166\n",
      "two-layer Loss after iteration 92000: 0.19097100709503384\n",
      "two-layer Loss after iteration 93000: 0.19094511772174586\n",
      "two-layer Loss after iteration 94000: 0.19092034472307387\n",
      "two-layer Loss after iteration 95000: 0.1908967085409107\n",
      "two-layer Loss after iteration 96000: 0.1908740525539977\n",
      "two-layer Loss after iteration 97000: 0.19085243723534986\n",
      "two-layer Loss after iteration 98000: 0.19083175638758834\n",
      "two-layer Loss after iteration 99000: 0.19081193877421462\n",
      "two-layer Loss after iteration 0: 1592.6174073347377\n",
      "two-layer Loss after iteration 1000: 21.96730621509693\n",
      "two-layer Loss after iteration 2000: 8.513338817031824\n",
      "two-layer Loss after iteration 3000: 5.3883601571630155\n",
      "two-layer Loss after iteration 4000: 3.880483348381036\n",
      "two-layer Loss after iteration 5000: 3.1153587245351164\n",
      "two-layer Loss after iteration 6000: 2.642639901594756\n",
      "two-layer Loss after iteration 7000: 2.319057943725416\n",
      "two-layer Loss after iteration 8000: 2.0140574311476214\n",
      "two-layer Loss after iteration 9000: 1.7502107787457457\n",
      "two-layer Loss after iteration 10000: 1.2739265714124055\n",
      "two-layer Loss after iteration 11000: 1.0690010695625907\n",
      "two-layer Loss after iteration 12000: 0.9163253036138987\n",
      "two-layer Loss after iteration 13000: 0.8146847743387666\n",
      "two-layer Loss after iteration 14000: 0.7446059987014835\n",
      "two-layer Loss after iteration 15000: 0.6912682053674035\n",
      "two-layer Loss after iteration 16000: 0.6429869802010567\n",
      "two-layer Loss after iteration 17000: 0.6042976153402552\n",
      "two-layer Loss after iteration 18000: 0.5667329146334618\n",
      "two-layer Loss after iteration 19000: 0.5338457170128904\n",
      "two-layer Loss after iteration 20000: 0.5046979751709969\n",
      "two-layer Loss after iteration 21000: 0.4748973481336453\n",
      "two-layer Loss after iteration 22000: 0.45091264224476946\n",
      "two-layer Loss after iteration 23000: 0.42719478296065033\n",
      "two-layer Loss after iteration 24000: 0.40868740474433196\n",
      "two-layer Loss after iteration 25000: 0.39257418073158096\n",
      "two-layer Loss after iteration 26000: 0.37694876937430977\n",
      "two-layer Loss after iteration 27000: 0.361370786553821\n",
      "two-layer Loss after iteration 28000: 0.34959029672371217\n",
      "two-layer Loss after iteration 29000: 0.3375435015897141\n",
      "two-layer Loss after iteration 30000: 0.3266420861946075\n",
      "two-layer Loss after iteration 31000: 0.31774716195951525\n",
      "two-layer Loss after iteration 32000: 0.30769754893838713\n",
      "two-layer Loss after iteration 33000: 0.2964922282605172\n",
      "two-layer Loss after iteration 34000: 0.2869799747087185\n",
      "two-layer Loss after iteration 35000: 0.2765771918943555\n",
      "two-layer Loss after iteration 36000: 0.26830537303694896\n",
      "two-layer Loss after iteration 37000: 0.261995342832458\n",
      "two-layer Loss after iteration 38000: 0.2556190930614823\n",
      "two-layer Loss after iteration 39000: 0.2503633190093998\n",
      "two-layer Loss after iteration 40000: 0.24605745208249266\n",
      "two-layer Loss after iteration 41000: 0.24241018033875522\n",
      "two-layer Loss after iteration 42000: 0.23834874022259303\n",
      "two-layer Loss after iteration 43000: 0.2353247894532023\n",
      "two-layer Loss after iteration 44000: 0.23167532168812208\n",
      "two-layer Loss after iteration 45000: 0.228492738303454\n",
      "two-layer Loss after iteration 46000: 0.22581630136140735\n",
      "two-layer Loss after iteration 47000: 0.22356609061416033\n",
      "two-layer Loss after iteration 48000: 0.2216582909289148\n",
      "two-layer Loss after iteration 49000: 0.2200232198652381\n",
      "two-layer Loss after iteration 50000: 0.2186107474257333\n",
      "two-layer Loss after iteration 51000: 0.21739507278464001\n",
      "two-layer Loss after iteration 52000: 0.21634420208798189\n",
      "two-layer Loss after iteration 53000: 0.21543272988841144\n",
      "two-layer Loss after iteration 54000: 0.2145609290990835\n",
      "two-layer Loss after iteration 55000: 0.21345682146362915\n",
      "two-layer Loss after iteration 56000: 0.212412358882988\n",
      "two-layer Loss after iteration 57000: 0.21163723644699436\n",
      "two-layer Loss after iteration 58000: 0.21100791968072183\n",
      "two-layer Loss after iteration 59000: 0.21047298892435268\n",
      "two-layer Loss after iteration 60000: 0.21001317691330598\n",
      "two-layer Loss after iteration 61000: 0.20947892172716126\n",
      "two-layer Loss after iteration 62000: 0.2090598902168871\n",
      "two-layer Loss after iteration 63000: 0.20777584795643791\n",
      "two-layer Loss after iteration 64000: 0.2068263227755529\n",
      "two-layer Loss after iteration 65000: 0.20639432141259817\n",
      "two-layer Loss after iteration 66000: 0.20611108682188134\n",
      "two-layer Loss after iteration 67000: 0.20587456498026865\n",
      "two-layer Loss after iteration 68000: 0.20567007889777483\n",
      "two-layer Loss after iteration 69000: 0.20549057498464\n",
      "two-layer Loss after iteration 70000: 0.2053315508519001\n",
      "two-layer Loss after iteration 71000: 0.20519136450170045\n",
      "two-layer Loss after iteration 72000: 0.20506347732394514\n",
      "two-layer Loss after iteration 73000: 0.20494899901263194\n",
      "two-layer Loss after iteration 74000: 0.20484411455063392\n",
      "two-layer Loss after iteration 75000: 0.2047491446172497\n",
      "two-layer Loss after iteration 76000: 0.20466333518097415\n",
      "two-layer Loss after iteration 77000: 0.20458330106793732\n",
      "two-layer Loss after iteration 78000: 0.20450952494705563\n",
      "two-layer Loss after iteration 79000: 0.20444123738100495\n",
      "two-layer Loss after iteration 80000: 0.2043780735480725\n",
      "two-layer Loss after iteration 81000: 0.20431988074819302\n",
      "two-layer Loss after iteration 82000: 0.2042643867404962\n",
      "two-layer Loss after iteration 83000: 0.20421437633313505\n",
      "two-layer Loss after iteration 84000: 0.20416518387315516\n",
      "two-layer Loss after iteration 85000: 0.20412065167209217\n",
      "two-layer Loss after iteration 86000: 0.20407814987049733\n",
      "two-layer Loss after iteration 87000: 0.2040390944815275\n",
      "two-layer Loss after iteration 88000: 0.20400086267812045\n",
      "two-layer Loss after iteration 89000: 0.20396589674792343\n",
      "two-layer Loss after iteration 90000: 0.20393241155956396\n",
      "two-layer Loss after iteration 91000: 0.20389954799757362\n",
      "two-layer Loss after iteration 92000: 0.20387015893763544\n",
      "two-layer Loss after iteration 93000: 0.20384089673197497\n",
      "two-layer Loss after iteration 94000: 0.20381329761570136\n",
      "two-layer Loss after iteration 95000: 0.20378686097139953\n",
      "two-layer Loss after iteration 96000: 0.20376204368691656\n",
      "two-layer Loss after iteration 97000: 0.20373949458991147\n",
      "two-layer Loss after iteration 98000: 0.20371546430898635\n",
      "two-layer Loss after iteration 99000: 0.2036937898266589\n",
      "two-layer Loss after iteration 0: 1487.816628449717\n",
      "two-layer Loss after iteration 1000: 22.392550833742906\n",
      "two-layer Loss after iteration 2000: 8.681636708518827\n",
      "two-layer Loss after iteration 3000: 6.303771924738958\n",
      "two-layer Loss after iteration 4000: 4.972977085572331\n",
      "two-layer Loss after iteration 5000: 4.298487941310192\n",
      "two-layer Loss after iteration 6000: 3.4781208086860795\n",
      "two-layer Loss after iteration 7000: 2.5868167316772066\n",
      "two-layer Loss after iteration 8000: 2.232961089342369\n",
      "two-layer Loss after iteration 9000: 1.9605981287658512\n",
      "two-layer Loss after iteration 10000: 1.7089955886716215\n",
      "two-layer Loss after iteration 11000: 1.4912380334094566\n",
      "two-layer Loss after iteration 12000: 1.282082255241535\n",
      "two-layer Loss after iteration 13000: 1.1339428242625518\n",
      "two-layer Loss after iteration 14000: 1.0397437735836428\n",
      "two-layer Loss after iteration 15000: 0.9737354901276795\n",
      "two-layer Loss after iteration 16000: 0.9248105248012848\n",
      "two-layer Loss after iteration 17000: 0.8775529551142273\n",
      "two-layer Loss after iteration 18000: 0.8350003435613292\n",
      "two-layer Loss after iteration 19000: 0.8002589189768982\n",
      "two-layer Loss after iteration 20000: 0.7723163884236266\n",
      "two-layer Loss after iteration 21000: 0.7490612069426114\n",
      "two-layer Loss after iteration 22000: 0.7293089658791884\n",
      "two-layer Loss after iteration 23000: 0.7095708439689847\n",
      "two-layer Loss after iteration 24000: 0.6923440222056714\n",
      "two-layer Loss after iteration 25000: 0.6768363211770094\n",
      "two-layer Loss after iteration 26000: 0.6636265330169901\n",
      "two-layer Loss after iteration 27000: 0.6514002104856641\n",
      "two-layer Loss after iteration 28000: 0.6377028095187535\n",
      "two-layer Loss after iteration 29000: 0.626362341803851\n",
      "two-layer Loss after iteration 30000: 0.6168995168939007\n",
      "two-layer Loss after iteration 31000: 0.6086329423143679\n",
      "two-layer Loss after iteration 32000: 0.6014722490809982\n",
      "two-layer Loss after iteration 33000: 0.5942243361688817\n",
      "two-layer Loss after iteration 34000: 0.588017920149902\n",
      "two-layer Loss after iteration 35000: 0.5822552081826122\n",
      "two-layer Loss after iteration 36000: 0.5762098092745613\n",
      "two-layer Loss after iteration 37000: 0.5695650755850261\n",
      "two-layer Loss after iteration 38000: 0.5642909984529225\n",
      "two-layer Loss after iteration 39000: 0.5592822030048316\n",
      "two-layer Loss after iteration 40000: 0.5547112379404605\n",
      "two-layer Loss after iteration 41000: 0.5502105133419157\n",
      "two-layer Loss after iteration 42000: 0.5461708809769186\n",
      "two-layer Loss after iteration 43000: 0.5425091085937701\n",
      "two-layer Loss after iteration 44000: 0.5383993462732156\n",
      "two-layer Loss after iteration 45000: 0.534634387676919\n",
      "two-layer Loss after iteration 46000: 0.5313422043695701\n",
      "two-layer Loss after iteration 47000: 0.5280345942151046\n",
      "two-layer Loss after iteration 48000: 0.525119974004242\n",
      "two-layer Loss after iteration 49000: 0.5222951109114593\n",
      "two-layer Loss after iteration 50000: 0.5191168367397013\n",
      "two-layer Loss after iteration 51000: 0.5160194955195145\n",
      "two-layer Loss after iteration 52000: 0.5132691392049449\n",
      "two-layer Loss after iteration 53000: 0.5107140925021733\n",
      "two-layer Loss after iteration 54000: 0.5083397222976866\n",
      "two-layer Loss after iteration 55000: 0.5061823451535317\n",
      "two-layer Loss after iteration 56000: 0.5041515317563138\n",
      "two-layer Loss after iteration 57000: 0.5024182177505027\n",
      "two-layer Loss after iteration 58000: 0.5005701515782591\n",
      "two-layer Loss after iteration 59000: 0.49670266019506054\n",
      "two-layer Loss after iteration 60000: 0.49295811753156804\n",
      "two-layer Loss after iteration 61000: 0.49009469644009374\n",
      "two-layer Loss after iteration 62000: 0.4879374448121058\n",
      "two-layer Loss after iteration 63000: 0.4859853272871119\n",
      "two-layer Loss after iteration 64000: 0.48419672029169886\n",
      "two-layer Loss after iteration 65000: 0.48191195775648726\n",
      "two-layer Loss after iteration 66000: 0.4801745254557577\n",
      "two-layer Loss after iteration 67000: 0.4785446734952351\n",
      "two-layer Loss after iteration 68000: 0.4769531309793727\n",
      "two-layer Loss after iteration 69000: 0.47452371530521614\n",
      "two-layer Loss after iteration 70000: 0.47185620510522147\n",
      "two-layer Loss after iteration 71000: 0.46988242433436883\n",
      "two-layer Loss after iteration 72000: 0.4681722690498578\n",
      "two-layer Loss after iteration 73000: 0.46644122523190645\n",
      "two-layer Loss after iteration 74000: 0.46485023611851567\n",
      "two-layer Loss after iteration 75000: 0.4633445439685024\n",
      "two-layer Loss after iteration 76000: 0.46188761319985766\n",
      "two-layer Loss after iteration 77000: 0.4604612533493337\n",
      "two-layer Loss after iteration 78000: 0.4590717474064569\n",
      "two-layer Loss after iteration 79000: 0.45771353946796006\n",
      "two-layer Loss after iteration 80000: 0.4563763247396368\n",
      "two-layer Loss after iteration 81000: 0.4550689144382869\n",
      "two-layer Loss after iteration 82000: 0.45378738228689136\n",
      "two-layer Loss after iteration 83000: 0.4525319380370167\n",
      "two-layer Loss after iteration 84000: 0.4512200745168179\n",
      "two-layer Loss after iteration 85000: 0.4499525084052303\n",
      "two-layer Loss after iteration 86000: 0.44871888187228276\n",
      "two-layer Loss after iteration 87000: 0.4475243196161302\n",
      "two-layer Loss after iteration 88000: 0.4463460911783596\n",
      "two-layer Loss after iteration 89000: 0.4452040819614334\n",
      "two-layer Loss after iteration 90000: 0.44409021954972483\n",
      "two-layer Loss after iteration 91000: 0.44299650035127003\n",
      "two-layer Loss after iteration 92000: 0.4419336133625785\n",
      "two-layer Loss after iteration 93000: 0.4407291947686643\n",
      "two-layer Loss after iteration 94000: 0.43966143534158675\n",
      "two-layer Loss after iteration 95000: 0.4386359014259265\n",
      "two-layer Loss after iteration 96000: 0.4376354855810569\n",
      "two-layer Loss after iteration 97000: 0.4362422576586711\n",
      "two-layer Loss after iteration 98000: 0.43513747637819944\n",
      "two-layer Loss after iteration 99000: 0.434113104549623\n",
      "two-layer Loss after iteration 0: 1509.2004869532657\n",
      "two-layer Loss after iteration 1000: 23.079583590790023\n",
      "two-layer Loss after iteration 2000: 8.71382658482675\n",
      "two-layer Loss after iteration 3000: 5.630996680488651\n",
      "two-layer Loss after iteration 4000: 4.0057152562796885\n",
      "two-layer Loss after iteration 5000: 3.311908240275394\n",
      "two-layer Loss after iteration 6000: 2.8958571144081127\n",
      "two-layer Loss after iteration 7000: 2.5064836547491276\n",
      "two-layer Loss after iteration 8000: 2.112619011951081\n",
      "two-layer Loss after iteration 9000: 1.6521429452242478\n",
      "two-layer Loss after iteration 10000: 1.3810462550592324\n",
      "two-layer Loss after iteration 11000: 1.1640891152722868\n",
      "two-layer Loss after iteration 12000: 0.9751181492959122\n",
      "two-layer Loss after iteration 13000: 0.8225266414193246\n",
      "two-layer Loss after iteration 14000: 0.7352399198363083\n",
      "two-layer Loss after iteration 15000: 0.6551602527228766\n",
      "two-layer Loss after iteration 16000: 0.5883670905469176\n",
      "two-layer Loss after iteration 17000: 0.5286891306708326\n",
      "two-layer Loss after iteration 18000: 0.49220441116016017\n",
      "two-layer Loss after iteration 19000: 0.4673487436911398\n",
      "two-layer Loss after iteration 20000: 0.4461036334802098\n",
      "two-layer Loss after iteration 21000: 0.43073654205592077\n",
      "two-layer Loss after iteration 22000: 0.4171522519556963\n",
      "two-layer Loss after iteration 23000: 0.40246425991068174\n",
      "two-layer Loss after iteration 24000: 0.3891521259405028\n",
      "two-layer Loss after iteration 25000: 0.37917164087082156\n",
      "two-layer Loss after iteration 26000: 0.36563302887230836\n",
      "two-layer Loss after iteration 27000: 0.35466704467304\n",
      "two-layer Loss after iteration 28000: 0.34672624546334835\n",
      "two-layer Loss after iteration 29000: 0.3407116314198826\n",
      "two-layer Loss after iteration 30000: 0.3360057417490287\n",
      "two-layer Loss after iteration 31000: 0.33135012389833834\n",
      "two-layer Loss after iteration 32000: 0.3274713895432948\n",
      "two-layer Loss after iteration 33000: 0.32457011105932987\n",
      "two-layer Loss after iteration 34000: 0.3222676976763046\n",
      "two-layer Loss after iteration 35000: 0.31864303034460884\n",
      "two-layer Loss after iteration 36000: 0.31635626472743555\n",
      "two-layer Loss after iteration 37000: 0.31445829455551483\n",
      "two-layer Loss after iteration 38000: 0.3127156116515251\n",
      "two-layer Loss after iteration 39000: 0.3110140646310468\n",
      "two-layer Loss after iteration 40000: 0.3095622927825555\n",
      "two-layer Loss after iteration 41000: 0.3083169549770773\n",
      "two-layer Loss after iteration 42000: 0.3072305945781905\n",
      "two-layer Loss after iteration 43000: 0.30588663152279155\n",
      "two-layer Loss after iteration 44000: 0.30487222242427253\n",
      "two-layer Loss after iteration 45000: 0.3039954437493544\n",
      "two-layer Loss after iteration 46000: 0.3032193432661207\n",
      "two-layer Loss after iteration 47000: 0.30252440822419746\n",
      "two-layer Loss after iteration 48000: 0.3015291922198613\n",
      "two-layer Loss after iteration 49000: 0.30069768396098145\n",
      "two-layer Loss after iteration 50000: 0.2997279235201913\n",
      "two-layer Loss after iteration 51000: 0.2989371707798328\n",
      "two-layer Loss after iteration 52000: 0.29827858346316777\n",
      "two-layer Loss after iteration 53000: 0.2977030944306104\n",
      "two-layer Loss after iteration 54000: 0.2970133302579859\n",
      "two-layer Loss after iteration 55000: 0.29619677133721733\n",
      "two-layer Loss after iteration 56000: 0.2952667379627714\n",
      "two-layer Loss after iteration 57000: 0.29341150062297555\n",
      "two-layer Loss after iteration 58000: 0.29234183842457734\n",
      "two-layer Loss after iteration 59000: 0.29128700355688286\n",
      "two-layer Loss after iteration 60000: 0.2905078826343041\n",
      "two-layer Loss after iteration 61000: 0.2898427853371032\n",
      "two-layer Loss after iteration 62000: 0.28925217802769\n",
      "two-layer Loss after iteration 63000: 0.2887174795331453\n",
      "two-layer Loss after iteration 64000: 0.2882242465541543\n",
      "two-layer Loss after iteration 65000: 0.28776521248479614\n",
      "two-layer Loss after iteration 66000: 0.2873399868521622\n",
      "two-layer Loss after iteration 67000: 0.28693995315365406\n",
      "two-layer Loss after iteration 68000: 0.28656271633924385\n",
      "two-layer Loss after iteration 69000: 0.286206833963365\n",
      "two-layer Loss after iteration 70000: 0.2858727328639132\n",
      "two-layer Loss after iteration 71000: 0.2855539946492518\n",
      "two-layer Loss after iteration 72000: 0.2852520950469079\n",
      "two-layer Loss after iteration 73000: 0.2849640980723786\n",
      "two-layer Loss after iteration 74000: 0.2846911086599654\n",
      "two-layer Loss after iteration 75000: 0.28443116064868545\n",
      "two-layer Loss after iteration 76000: 0.2841827489568011\n",
      "two-layer Loss after iteration 77000: 0.2839487553008902\n",
      "two-layer Loss after iteration 78000: 0.2837263990312438\n",
      "two-layer Loss after iteration 79000: 0.2835190763432076\n",
      "two-layer Loss after iteration 80000: 0.28331953868141907\n",
      "two-layer Loss after iteration 81000: 0.28313072590578525\n",
      "two-layer Loss after iteration 82000: 0.28295148872044024\n",
      "two-layer Loss after iteration 83000: 0.28260560033348237\n",
      "two-layer Loss after iteration 84000: 0.2822777367555387\n",
      "two-layer Loss after iteration 85000: 0.2819930302579155\n",
      "two-layer Loss after iteration 86000: 0.28174135814775836\n",
      "two-layer Loss after iteration 87000: 0.28151074708178436\n",
      "two-layer Loss after iteration 88000: 0.2813011454263546\n",
      "two-layer Loss after iteration 89000: 0.28110530697445557\n",
      "two-layer Loss after iteration 90000: 0.28091983256065833\n",
      "two-layer Loss after iteration 91000: 0.28074594836708233\n",
      "two-layer Loss after iteration 92000: 0.2805266874160711\n",
      "two-layer Loss after iteration 93000: 0.280168764979053\n",
      "two-layer Loss after iteration 94000: 0.2787200880094427\n",
      "two-layer Loss after iteration 95000: 0.27814647499541983\n",
      "two-layer Loss after iteration 96000: 0.2777677428518659\n",
      "two-layer Loss after iteration 97000: 0.2774836730141005\n",
      "two-layer Loss after iteration 98000: 0.277248463692624\n",
      "two-layer Loss after iteration 99000: 0.27704362986726544\n",
      "two-layer Loss after iteration 0: 1649.8727237630656\n",
      "two-layer Loss after iteration 1000: 23.78050818671047\n",
      "two-layer Loss after iteration 2000: 9.564993285295136\n",
      "two-layer Loss after iteration 3000: 7.184700070535649\n",
      "two-layer Loss after iteration 4000: 5.850458446986413\n",
      "two-layer Loss after iteration 5000: 5.007110524262517\n",
      "two-layer Loss after iteration 6000: 4.326613439756339\n",
      "two-layer Loss after iteration 7000: 3.9010158283084544\n",
      "two-layer Loss after iteration 8000: 3.632106681681205\n",
      "two-layer Loss after iteration 9000: 3.4290077427851147\n",
      "two-layer Loss after iteration 10000: 3.2679182961481406\n",
      "two-layer Loss after iteration 11000: 3.0828855719503916\n",
      "two-layer Loss after iteration 12000: 2.968053420067523\n",
      "two-layer Loss after iteration 13000: 2.8603195145751124\n",
      "two-layer Loss after iteration 14000: 2.7596239692671998\n",
      "two-layer Loss after iteration 15000: 2.6808623266936293\n",
      "two-layer Loss after iteration 16000: 2.6369734792154276\n",
      "two-layer Loss after iteration 17000: 2.601937473069858\n",
      "two-layer Loss after iteration 18000: 2.578265093941683\n",
      "two-layer Loss after iteration 19000: 2.5597551653449853\n",
      "two-layer Loss after iteration 20000: 2.54438087413291\n",
      "two-layer Loss after iteration 21000: 2.496022000046496\n",
      "two-layer Loss after iteration 22000: 2.4713334991646203\n",
      "two-layer Loss after iteration 23000: 2.4538699141136804\n",
      "two-layer Loss after iteration 24000: 2.437891443761847\n",
      "two-layer Loss after iteration 25000: 2.4250454900815637\n",
      "two-layer Loss after iteration 26000: 2.4141914376968185\n",
      "two-layer Loss after iteration 27000: 2.405124740732412\n",
      "two-layer Loss after iteration 28000: 2.397334865462551\n",
      "two-layer Loss after iteration 29000: 2.390517466947924\n",
      "two-layer Loss after iteration 30000: 2.384456033330837\n",
      "two-layer Loss after iteration 31000: 2.3791604341465677\n",
      "two-layer Loss after iteration 32000: 2.3745202113427664\n",
      "two-layer Loss after iteration 33000: 2.365136950931008\n",
      "two-layer Loss after iteration 34000: 2.3597830608932857\n",
      "two-layer Loss after iteration 35000: 2.3554165609999456\n",
      "two-layer Loss after iteration 36000: 2.3490960211130134\n",
      "two-layer Loss after iteration 37000: 2.345261162285847\n",
      "two-layer Loss after iteration 38000: 2.3421876856941606\n",
      "two-layer Loss after iteration 39000: 2.3394016285235506\n",
      "two-layer Loss after iteration 40000: 2.3367960463003574\n",
      "two-layer Loss after iteration 41000: 2.3343067567361953\n",
      "two-layer Loss after iteration 42000: 2.331679233592339\n",
      "two-layer Loss after iteration 43000: 2.325507508809984\n",
      "two-layer Loss after iteration 44000: 2.322475189135645\n",
      "two-layer Loss after iteration 45000: 2.3196149972236246\n",
      "two-layer Loss after iteration 46000: 2.316964328806484\n",
      "two-layer Loss after iteration 47000: 2.3144614764768545\n",
      "two-layer Loss after iteration 48000: 2.3120897711966326\n",
      "two-layer Loss after iteration 49000: 2.3098298971339912\n",
      "two-layer Loss after iteration 50000: 2.307258745935861\n",
      "two-layer Loss after iteration 51000: 2.3050537884920597\n",
      "two-layer Loss after iteration 52000: 2.30297521757223\n",
      "two-layer Loss after iteration 53000: 2.3009590957655175\n",
      "two-layer Loss after iteration 54000: 2.2955914103657302\n",
      "two-layer Loss after iteration 55000: 2.2937901357113364\n",
      "two-layer Loss after iteration 56000: 2.2921200938089945\n",
      "two-layer Loss after iteration 57000: 2.2904995770981684\n",
      "two-layer Loss after iteration 58000: 2.2889411985625547\n",
      "two-layer Loss after iteration 59000: 2.287448721354194\n",
      "two-layer Loss after iteration 60000: 2.286071794172289\n",
      "two-layer Loss after iteration 61000: 2.2847178914314634\n",
      "two-layer Loss after iteration 62000: 2.2834501123939597\n",
      "two-layer Loss after iteration 63000: 2.2823189892365883\n",
      "two-layer Loss after iteration 64000: 2.2811774125949644\n",
      "two-layer Loss after iteration 65000: 2.2798866840825136\n",
      "two-layer Loss after iteration 66000: 2.27877519218472\n",
      "two-layer Loss after iteration 67000: 2.277772061881984\n",
      "two-layer Loss after iteration 68000: 2.2767503815369126\n",
      "two-layer Loss after iteration 69000: 2.2758409800843054\n",
      "two-layer Loss after iteration 70000: 2.2749419806004076\n",
      "two-layer Loss after iteration 71000: 2.2741262715019235\n",
      "two-layer Loss after iteration 72000: 2.2733366492686997\n",
      "two-layer Loss after iteration 73000: 2.27258679115502\n",
      "two-layer Loss after iteration 74000: 2.2718765418398728\n",
      "two-layer Loss after iteration 75000: 2.2711673026409103\n",
      "two-layer Loss after iteration 76000: 2.270537259592669\n",
      "two-layer Loss after iteration 77000: 2.269881209282284\n",
      "two-layer Loss after iteration 78000: 2.269226767974875\n",
      "two-layer Loss after iteration 79000: 2.2686717795180242\n",
      "two-layer Loss after iteration 80000: 2.2681019095029225\n",
      "two-layer Loss after iteration 81000: 2.2675723826989547\n",
      "two-layer Loss after iteration 82000: 2.267042693120403\n",
      "two-layer Loss after iteration 83000: 2.2665149958470554\n",
      "two-layer Loss after iteration 84000: 1.8510129465772245\n",
      "two-layer Loss after iteration 85000: 1.4290004992740284\n",
      "two-layer Loss after iteration 86000: 1.2470393471680181\n",
      "two-layer Loss after iteration 87000: 1.1467870781469658\n",
      "two-layer Loss after iteration 88000: 1.096294216584765\n",
      "two-layer Loss after iteration 89000: 1.0629475925660112\n",
      "two-layer Loss after iteration 90000: 1.0434897754559656\n",
      "two-layer Loss after iteration 91000: 1.0290359565198812\n",
      "two-layer Loss after iteration 92000: 1.010896183637088\n",
      "two-layer Loss after iteration 93000: 0.9873638360393914\n",
      "two-layer Loss after iteration 94000: 0.969899657870109\n",
      "two-layer Loss after iteration 95000: 0.6612296899697946\n",
      "two-layer Loss after iteration 96000: 0.5913983308577201\n",
      "two-layer Loss after iteration 97000: 0.5513217381715817\n",
      "two-layer Loss after iteration 98000: 0.5185633285010314\n",
      "two-layer Loss after iteration 99000: 0.4756775760892486\n",
      "two-layer Loss after iteration 0: 1694.9768575385672\n",
      "two-layer Loss after iteration 1000: 23.12916662014087\n",
      "two-layer Loss after iteration 2000: 8.928161403717862\n",
      "two-layer Loss after iteration 3000: 6.540036440794875\n",
      "two-layer Loss after iteration 4000: 4.833977173837441\n",
      "two-layer Loss after iteration 5000: 3.701737727593539\n",
      "two-layer Loss after iteration 6000: 2.499802772421989\n",
      "two-layer Loss after iteration 7000: 1.897077571364404\n",
      "two-layer Loss after iteration 8000: 1.5821692252667117\n",
      "two-layer Loss after iteration 9000: 1.3118068305757955\n",
      "two-layer Loss after iteration 10000: 1.045580055628626\n",
      "two-layer Loss after iteration 11000: 0.9245977812437249\n",
      "two-layer Loss after iteration 12000: 0.841898690474116\n",
      "two-layer Loss after iteration 13000: 0.7829948229604522\n",
      "two-layer Loss after iteration 14000: 0.7279373687632618\n",
      "two-layer Loss after iteration 15000: 0.6824079617262543\n",
      "two-layer Loss after iteration 16000: 0.6549689453653279\n",
      "two-layer Loss after iteration 17000: 0.6337156065169688\n",
      "two-layer Loss after iteration 18000: 0.613623818608538\n",
      "two-layer Loss after iteration 19000: 0.5892986888638634\n",
      "two-layer Loss after iteration 20000: 0.5705742640635076\n",
      "two-layer Loss after iteration 21000: 0.5582062877848136\n",
      "two-layer Loss after iteration 22000: 0.5476364615068939\n",
      "two-layer Loss after iteration 23000: 0.5351816137326886\n",
      "two-layer Loss after iteration 24000: 0.5261406501912639\n",
      "two-layer Loss after iteration 25000: 0.5185732842679116\n",
      "two-layer Loss after iteration 26000: 0.5121914819323937\n",
      "two-layer Loss after iteration 27000: 0.5067786750239995\n",
      "two-layer Loss after iteration 28000: 0.5010417808591665\n",
      "two-layer Loss after iteration 29000: 0.49588929626404443\n",
      "two-layer Loss after iteration 30000: 0.49175630317923014\n",
      "two-layer Loss after iteration 31000: 0.4879551175266751\n",
      "two-layer Loss after iteration 32000: 0.4844467107689817\n",
      "two-layer Loss after iteration 33000: 0.4815427680159477\n",
      "two-layer Loss after iteration 34000: 0.4787112629470549\n",
      "two-layer Loss after iteration 35000: 0.4754095572266033\n",
      "two-layer Loss after iteration 36000: 0.4724612302518126\n",
      "two-layer Loss after iteration 37000: 0.4696456612590493\n",
      "two-layer Loss after iteration 38000: 0.4671904535743105\n",
      "two-layer Loss after iteration 39000: 0.464761553170385\n",
      "two-layer Loss after iteration 40000: 0.4627699149050679\n",
      "two-layer Loss after iteration 41000: 0.46104191992343796\n",
      "two-layer Loss after iteration 42000: 0.45952304041006653\n",
      "two-layer Loss after iteration 43000: 0.4582102960435783\n",
      "two-layer Loss after iteration 44000: 0.457095060391378\n",
      "two-layer Loss after iteration 45000: 0.4560815486162477\n",
      "two-layer Loss after iteration 46000: 0.45517833303387106\n",
      "two-layer Loss after iteration 47000: 0.4539930092447685\n",
      "two-layer Loss after iteration 48000: 0.4527693997107805\n",
      "two-layer Loss after iteration 49000: 0.4519293663352499\n",
      "two-layer Loss after iteration 50000: 0.4512148640701756\n",
      "two-layer Loss after iteration 51000: 0.4505877687797878\n",
      "two-layer Loss after iteration 52000: 0.45003504599872346\n",
      "two-layer Loss after iteration 53000: 0.4494088590806697\n",
      "two-layer Loss after iteration 54000: 0.44872731837996915\n",
      "two-layer Loss after iteration 55000: 0.44811056654158987\n",
      "two-layer Loss after iteration 56000: 0.44754469797821417\n",
      "two-layer Loss after iteration 57000: 0.4470208814890196\n",
      "two-layer Loss after iteration 58000: 0.4465280482270322\n",
      "two-layer Loss after iteration 59000: 0.4460631635482408\n",
      "two-layer Loss after iteration 60000: 0.4456199444836522\n",
      "two-layer Loss after iteration 61000: 0.44519484227159084\n",
      "two-layer Loss after iteration 62000: 0.44479649053863873\n",
      "two-layer Loss after iteration 63000: 0.44441702064086824\n",
      "two-layer Loss after iteration 64000: 0.4440552844129468\n",
      "two-layer Loss after iteration 65000: 0.4437060365121558\n",
      "two-layer Loss after iteration 66000: 0.4433650321884398\n",
      "two-layer Loss after iteration 67000: 0.4430346343606406\n",
      "two-layer Loss after iteration 68000: 0.44271472373964865\n",
      "two-layer Loss after iteration 69000: 0.44241209192347647\n",
      "two-layer Loss after iteration 70000: 0.44199309076245213\n",
      "two-layer Loss after iteration 71000: 0.44042636028797966\n",
      "two-layer Loss after iteration 72000: 0.43328688240401964\n",
      "two-layer Loss after iteration 73000: 0.42934258105109\n",
      "two-layer Loss after iteration 74000: 0.4266397450925366\n",
      "two-layer Loss after iteration 75000: 0.4243689520192111\n",
      "two-layer Loss after iteration 76000: 0.4223859402061132\n",
      "two-layer Loss after iteration 77000: 0.42062143192243195\n",
      "two-layer Loss after iteration 78000: 0.4190349414300038\n",
      "two-layer Loss after iteration 79000: 0.4175976441770086\n",
      "two-layer Loss after iteration 80000: 0.41628671380841803\n",
      "two-layer Loss after iteration 81000: 0.4150880676438886\n",
      "two-layer Loss after iteration 82000: 0.4139840362442754\n",
      "two-layer Loss after iteration 83000: 0.41296571605875504\n",
      "two-layer Loss after iteration 84000: 0.41202110071505055\n",
      "two-layer Loss after iteration 85000: 0.4111449356552872\n",
      "two-layer Loss after iteration 86000: 0.4103377300268045\n",
      "two-layer Loss after iteration 87000: 0.4095887576840481\n",
      "two-layer Loss after iteration 88000: 0.40889446536215573\n",
      "two-layer Loss after iteration 89000: 0.4082516139121036\n",
      "two-layer Loss after iteration 90000: 0.4076552577248304\n",
      "two-layer Loss after iteration 91000: 0.4071012110295329\n",
      "two-layer Loss after iteration 92000: 0.4065852943427681\n",
      "two-layer Loss after iteration 93000: 0.40610466043647775\n",
      "two-layer Loss after iteration 94000: 0.40565762401801886\n",
      "two-layer Loss after iteration 95000: 0.4052411995945681\n",
      "two-layer Loss after iteration 96000: 0.40484927815292165\n",
      "two-layer Loss after iteration 97000: 0.4044851905193041\n",
      "two-layer Loss after iteration 98000: 0.4041446855119297\n",
      "two-layer Loss after iteration 99000: 0.4038232597900414\n",
      "two-layer Loss after iteration 0: 1469.2179584861356\n",
      "two-layer Loss after iteration 1000: 23.997677615214197\n",
      "two-layer Loss after iteration 2000: 9.060192643832986\n",
      "two-layer Loss after iteration 3000: 6.368771376631739\n",
      "two-layer Loss after iteration 4000: 4.818819026326385\n",
      "two-layer Loss after iteration 5000: 3.4461677329026297\n",
      "two-layer Loss after iteration 6000: 2.510760867108837\n",
      "two-layer Loss after iteration 7000: 2.017036928615797\n",
      "two-layer Loss after iteration 8000: 1.6389445309712471\n",
      "two-layer Loss after iteration 9000: 1.4166702660340655\n",
      "two-layer Loss after iteration 10000: 1.2708822136585352\n",
      "two-layer Loss after iteration 11000: 1.1643019923605404\n",
      "two-layer Loss after iteration 12000: 1.0745696214108982\n",
      "two-layer Loss after iteration 13000: 0.9979889312442219\n",
      "two-layer Loss after iteration 14000: 0.8473215219453917\n",
      "two-layer Loss after iteration 15000: 0.7972374786963499\n",
      "two-layer Loss after iteration 16000: 0.7689909696899279\n",
      "two-layer Loss after iteration 17000: 0.7462197923309155\n",
      "two-layer Loss after iteration 18000: 0.7293208686529067\n",
      "two-layer Loss after iteration 19000: 0.7117983655323095\n",
      "two-layer Loss after iteration 20000: 0.6998602889622141\n",
      "two-layer Loss after iteration 21000: 0.6909774894299238\n",
      "two-layer Loss after iteration 22000: 0.6838325759904945\n",
      "two-layer Loss after iteration 23000: 0.6777335858648004\n",
      "two-layer Loss after iteration 24000: 0.6702496770878285\n",
      "two-layer Loss after iteration 25000: 0.6618102469578978\n",
      "two-layer Loss after iteration 26000: 0.6539509972193984\n",
      "two-layer Loss after iteration 27000: 0.6484228380677292\n",
      "two-layer Loss after iteration 28000: 0.6438800566981986\n",
      "two-layer Loss after iteration 29000: 0.6399174230660474\n",
      "two-layer Loss after iteration 30000: 0.6360971632927032\n",
      "two-layer Loss after iteration 31000: 0.6308062696860447\n",
      "two-layer Loss after iteration 32000: 0.6246491498778768\n",
      "two-layer Loss after iteration 33000: 0.6210308816696688\n",
      "two-layer Loss after iteration 34000: 0.6180280267168768\n",
      "two-layer Loss after iteration 35000: 0.6154388877902103\n",
      "two-layer Loss after iteration 36000: 0.6130069210212872\n",
      "two-layer Loss after iteration 37000: 0.6108025949538424\n",
      "two-layer Loss after iteration 38000: 0.6087605662019372\n",
      "two-layer Loss after iteration 39000: 0.6068329731157633\n",
      "two-layer Loss after iteration 40000: 0.6049829493120948\n",
      "two-layer Loss after iteration 41000: 0.6032480231567094\n",
      "two-layer Loss after iteration 42000: 0.6016132973268459\n",
      "two-layer Loss after iteration 43000: 0.6000629520375393\n",
      "two-layer Loss after iteration 44000: 0.5985843486363119\n",
      "two-layer Loss after iteration 45000: 0.597171562118183\n",
      "two-layer Loss after iteration 46000: 0.595817934756947\n",
      "two-layer Loss after iteration 47000: 0.5945237325513261\n",
      "two-layer Loss after iteration 48000: 0.5932782450477415\n",
      "two-layer Loss after iteration 49000: 0.5920806632471549\n",
      "two-layer Loss after iteration 50000: 0.5909272148919894\n",
      "two-layer Loss after iteration 51000: 0.5898157271016208\n",
      "two-layer Loss after iteration 52000: 0.5887441347725862\n",
      "two-layer Loss after iteration 53000: 0.5877059739705343\n",
      "two-layer Loss after iteration 54000: 0.5867054309346065\n",
      "two-layer Loss after iteration 55000: 0.5857370294458931\n",
      "two-layer Loss after iteration 56000: 0.584801454769413\n",
      "two-layer Loss after iteration 57000: 0.5838983010398081\n",
      "two-layer Loss after iteration 58000: 0.5830234448041616\n",
      "two-layer Loss after iteration 59000: 0.5821778268629546\n",
      "two-layer Loss after iteration 60000: 0.581114690383185\n",
      "two-layer Loss after iteration 61000: 0.580071892199529\n",
      "two-layer Loss after iteration 62000: 0.5792763912461845\n",
      "two-layer Loss after iteration 63000: 0.5785127879548445\n",
      "two-layer Loss after iteration 64000: 0.5777784331817828\n",
      "two-layer Loss after iteration 65000: 0.5769343169335232\n",
      "two-layer Loss after iteration 66000: 0.5761408675505445\n",
      "two-layer Loss after iteration 67000: 0.5753906058369936\n",
      "two-layer Loss after iteration 68000: 0.5746701473465112\n",
      "two-layer Loss after iteration 69000: 0.5739747083984078\n",
      "two-layer Loss after iteration 70000: 0.5733053274706528\n",
      "two-layer Loss after iteration 71000: 0.5726572876756699\n",
      "two-layer Loss after iteration 72000: 0.5720308327531622\n",
      "two-layer Loss after iteration 73000: 0.5714262396038102\n",
      "two-layer Loss after iteration 74000: 0.5708421705091804\n",
      "two-layer Loss after iteration 75000: 0.5702750323118847\n",
      "two-layer Loss after iteration 76000: 0.5697236760826718\n",
      "two-layer Loss after iteration 77000: 0.5691932417239799\n",
      "two-layer Loss after iteration 78000: 0.5686762182527239\n",
      "two-layer Loss after iteration 79000: 0.5681759256209068\n",
      "two-layer Loss after iteration 80000: 0.5675784476617834\n",
      "two-layer Loss after iteration 81000: 0.5670340793820262\n",
      "two-layer Loss after iteration 82000: 0.5665165596043361\n",
      "two-layer Loss after iteration 83000: 0.56601830630522\n",
      "two-layer Loss after iteration 84000: 0.5655412385504078\n",
      "two-layer Loss after iteration 85000: 0.5650798198201453\n",
      "two-layer Loss after iteration 86000: 0.5646396997569687\n",
      "two-layer Loss after iteration 87000: 0.5642164955904766\n",
      "two-layer Loss after iteration 88000: 0.563810464267292\n",
      "two-layer Loss after iteration 89000: 0.5608815667152866\n",
      "two-layer Loss after iteration 90000: 0.5595970127296959\n",
      "two-layer Loss after iteration 91000: 0.5582950035812133\n",
      "two-layer Loss after iteration 92000: 0.5573665023437016\n",
      "two-layer Loss after iteration 93000: 0.5567753950805719\n",
      "two-layer Loss after iteration 94000: 0.5563606126456531\n",
      "two-layer Loss after iteration 95000: 0.5542230400035111\n",
      "two-layer Loss after iteration 96000: 0.5535820119153043\n",
      "two-layer Loss after iteration 97000: 0.5531500110377366\n",
      "two-layer Loss after iteration 98000: 0.5527897819952821\n",
      "two-layer Loss after iteration 99000: 0.552467015010619\n",
      "three-layer Loss after iteration 0: 1367.6325814877903\n",
      "three-layer Loss after iteration 1000: 12.812239691154105\n",
      "three-layer Loss after iteration 2000: 7.964232691779423\n",
      "three-layer Loss after iteration 3000: 5.606820422560008\n",
      "three-layer Loss after iteration 4000: 5.02320126133717\n",
      "three-layer Loss after iteration 5000: 4.867039180348257\n",
      "three-layer Loss after iteration 6000: 3.775187731243753\n",
      "three-layer Loss after iteration 7000: 4.160543380915275\n",
      "three-layer Loss after iteration 8000: 3.189218980120366\n",
      "three-layer Loss after iteration 9000: 2.8559922099236554\n",
      "three-layer Loss after iteration 10000: 3.4757468187726084\n",
      "three-layer Loss after iteration 11000: 2.5133369902172333\n",
      "three-layer Loss after iteration 12000: 2.9526794827271767\n",
      "three-layer Loss after iteration 13000: 2.2576517937047935\n",
      "three-layer Loss after iteration 14000: 2.303119435167614\n",
      "three-layer Loss after iteration 15000: 2.245405410866673\n",
      "three-layer Loss after iteration 16000: 2.4397566959311345\n",
      "three-layer Loss after iteration 17000: 2.2835566106522123\n",
      "three-layer Loss after iteration 18000: 2.25781052768462\n",
      "three-layer Loss after iteration 19000: 2.360811323760681\n",
      "three-layer Loss after iteration 20000: 2.5209081737715646\n",
      "three-layer Loss after iteration 21000: 2.1183156866828554\n",
      "three-layer Loss after iteration 22000: 2.1076269696212973\n",
      "three-layer Loss after iteration 23000: 2.1830845544538766\n",
      "three-layer Loss after iteration 24000: 2.0615446911105972\n",
      "three-layer Loss after iteration 25000: 1.8637608714735259\n",
      "three-layer Loss after iteration 26000: 1.55979435693045\n",
      "three-layer Loss after iteration 27000: 2.159323855724487\n",
      "three-layer Loss after iteration 28000: 3.2653652209161397\n",
      "three-layer Loss after iteration 29000: 1.2385999950014446\n",
      "three-layer Loss after iteration 30000: 1.5884225688676399\n",
      "three-layer Loss after iteration 31000: 1.3496447417881154\n",
      "three-layer Loss after iteration 32000: 1.8259729354154448\n",
      "three-layer Loss after iteration 33000: 1.4030998144013394\n",
      "three-layer Loss after iteration 34000: 1.7459623127552464\n",
      "three-layer Loss after iteration 35000: 1.5261375597477829\n",
      "three-layer Loss after iteration 36000: 1.533747344612475\n",
      "three-layer Loss after iteration 37000: 1.1277194309281964\n",
      "three-layer Loss after iteration 38000: 1.5117487983426054\n",
      "three-layer Loss after iteration 39000: 1.3811444649744897\n",
      "three-layer Loss after iteration 40000: 1.2836663294555493\n",
      "three-layer Loss after iteration 41000: 1.141680976483789\n",
      "three-layer Loss after iteration 42000: 1.7376476481699057\n",
      "three-layer Loss after iteration 43000: 1.3057372633388185\n",
      "three-layer Loss after iteration 44000: 1.2916895715361802\n",
      "three-layer Loss after iteration 45000: 1.34678685040986\n",
      "three-layer Loss after iteration 46000: 1.2334089994888933\n",
      "three-layer Loss after iteration 47000: 1.2561924658629717\n",
      "three-layer Loss after iteration 48000: 1.2075012087631187\n",
      "three-layer Loss after iteration 49000: 1.1967660521157597\n",
      "three-layer Loss after iteration 50000: 1.1786439415766452\n",
      "three-layer Loss after iteration 51000: 1.0528146813701658\n",
      "three-layer Loss after iteration 52000: 1.3093369048910806\n",
      "three-layer Loss after iteration 53000: 1.1131574157170365\n",
      "three-layer Loss after iteration 54000: 1.089038911111978\n",
      "three-layer Loss after iteration 55000: 1.083433145230913\n",
      "three-layer Loss after iteration 56000: 1.2602611755942594\n",
      "three-layer Loss after iteration 57000: 1.522817311318664\n",
      "three-layer Loss after iteration 58000: 1.2141570491635123\n",
      "three-layer Loss after iteration 59000: 1.0987381943318828\n",
      "three-layer Loss after iteration 60000: 0.7415144453839669\n",
      "three-layer Loss after iteration 61000: 0.7241835199241478\n",
      "three-layer Loss after iteration 62000: 0.7403019945834441\n",
      "three-layer Loss after iteration 63000: 0.714763370186832\n",
      "three-layer Loss after iteration 64000: 2.2812979550834185\n",
      "three-layer Loss after iteration 65000: 0.9460723857249392\n",
      "three-layer Loss after iteration 66000: 0.8336809487221917\n",
      "three-layer Loss after iteration 67000: 0.9035318475685156\n",
      "three-layer Loss after iteration 68000: 1.499917035636945\n",
      "three-layer Loss after iteration 69000: 0.915157960870896\n",
      "three-layer Loss after iteration 70000: 0.9837835964613931\n",
      "three-layer Loss after iteration 71000: 1.0248659381855454\n",
      "three-layer Loss after iteration 72000: 1.4900060792721899\n",
      "three-layer Loss after iteration 73000: 0.6960044812086954\n",
      "three-layer Loss after iteration 74000: 0.9880665916402958\n",
      "three-layer Loss after iteration 75000: 1.0018724401854782\n",
      "three-layer Loss after iteration 76000: 0.9415840220440166\n",
      "three-layer Loss after iteration 77000: 0.9539924362660506\n",
      "three-layer Loss after iteration 78000: 0.9340059581384654\n",
      "three-layer Loss after iteration 79000: 0.9259813323866821\n",
      "three-layer Loss after iteration 80000: 1.4312503755199704\n",
      "three-layer Loss after iteration 81000: 0.8386707271139211\n",
      "three-layer Loss after iteration 82000: 0.8552858418735425\n",
      "three-layer Loss after iteration 83000: 0.8707831769195887\n",
      "three-layer Loss after iteration 84000: 0.8584667220379644\n",
      "three-layer Loss after iteration 85000: 0.8513063962955458\n",
      "three-layer Loss after iteration 86000: 0.8463891399045398\n",
      "three-layer Loss after iteration 87000: 0.8414020607047362\n",
      "three-layer Loss after iteration 88000: 0.8361055332238572\n",
      "three-layer Loss after iteration 89000: 0.8316117671604903\n",
      "three-layer Loss after iteration 90000: 0.825884247175112\n",
      "three-layer Loss after iteration 91000: 0.8209928908613261\n",
      "three-layer Loss after iteration 92000: 0.816129546980413\n",
      "three-layer Loss after iteration 93000: 0.8114186107448658\n",
      "three-layer Loss after iteration 94000: 0.8058670897129528\n",
      "three-layer Loss after iteration 95000: 0.8011170068307517\n",
      "three-layer Loss after iteration 96000: 0.6641815267164035\n",
      "three-layer Loss after iteration 97000: 0.6828416103938826\n",
      "three-layer Loss after iteration 98000: 0.6962711597007943\n",
      "three-layer Loss after iteration 99000: 0.7612170145584921\n",
      "three-layer Loss after iteration 0: 1617.8504856370268\n",
      "three-layer Loss after iteration 1000: 12.658746323544165\n",
      "three-layer Loss after iteration 2000: 9.572257948944635\n",
      "three-layer Loss after iteration 3000: 7.599802709932695\n",
      "three-layer Loss after iteration 4000: 5.053586962498527\n",
      "three-layer Loss after iteration 5000: 3.9051931914982703\n",
      "three-layer Loss after iteration 6000: 4.372931196144366\n",
      "three-layer Loss after iteration 7000: 2.216928169760261\n",
      "three-layer Loss after iteration 8000: 2.594802716847551\n",
      "three-layer Loss after iteration 9000: 2.0494337453077383\n",
      "three-layer Loss after iteration 10000: 2.638478759470645\n",
      "three-layer Loss after iteration 11000: 2.107120609591382\n",
      "three-layer Loss after iteration 12000: 1.2513541634527154\n",
      "three-layer Loss after iteration 13000: 2.3504318021217196\n",
      "three-layer Loss after iteration 14000: 1.7782680446427113\n",
      "three-layer Loss after iteration 15000: 1.991363905558045\n",
      "three-layer Loss after iteration 16000: 1.2373221687263407\n",
      "three-layer Loss after iteration 17000: 1.21508574446203\n",
      "three-layer Loss after iteration 18000: 1.1677753162773474\n",
      "three-layer Loss after iteration 19000: 1.1270905427742266\n",
      "three-layer Loss after iteration 20000: 1.0942593889501446\n",
      "three-layer Loss after iteration 21000: 1.0711619415681992\n",
      "three-layer Loss after iteration 22000: 1.0389470858137315\n",
      "three-layer Loss after iteration 23000: 1.0389476019015929\n",
      "4.967412377224825e-07 1.0389470858137315 1.0389476019015929\n",
      "three-layer Loss after iteration 0: 1495.5085494804966\n",
      "three-layer Loss after iteration 1000: 9.084524186285547\n",
      "three-layer Loss after iteration 2000: 7.259454228645161\n",
      "three-layer Loss after iteration 3000: 6.648775073153049\n",
      "three-layer Loss after iteration 4000: 4.781766630132851\n",
      "three-layer Loss after iteration 5000: 4.338194924205242\n",
      "three-layer Loss after iteration 6000: 3.620947953121627\n",
      "three-layer Loss after iteration 7000: 3.3573698945594432\n",
      "three-layer Loss after iteration 8000: 3.1270046102367215\n",
      "three-layer Loss after iteration 9000: 2.9709057283694085\n",
      "three-layer Loss after iteration 10000: 2.5635010365647424\n",
      "three-layer Loss after iteration 11000: 1.8725248479089436\n",
      "three-layer Loss after iteration 12000: 2.0171074745003468\n",
      "three-layer Loss after iteration 13000: 1.617666554620499\n",
      "three-layer Loss after iteration 14000: 1.6471929500494413\n",
      "three-layer Loss after iteration 15000: 1.6846118417518563\n",
      "three-layer Loss after iteration 16000: 1.4931708009837747\n",
      "three-layer Loss after iteration 17000: 1.3294659837468974\n",
      "three-layer Loss after iteration 18000: 1.41835616235494\n",
      "three-layer Loss after iteration 19000: 1.3443444313628485\n",
      "three-layer Loss after iteration 20000: 1.386084168234282\n",
      "three-layer Loss after iteration 21000: 1.3807318446342858\n",
      "three-layer Loss after iteration 22000: 1.0380207811135882\n",
      "three-layer Loss after iteration 23000: 0.8972737874745703\n",
      "three-layer Loss after iteration 24000: 1.3971976155418477\n",
      "three-layer Loss after iteration 25000: 1.2639038093896608\n",
      "three-layer Loss after iteration 26000: 1.079565944305469\n",
      "three-layer Loss after iteration 27000: 1.3220225132514012\n",
      "three-layer Loss after iteration 28000: 1.3585851114397267\n",
      "three-layer Loss after iteration 29000: 1.2041342047486416\n",
      "three-layer Loss after iteration 30000: 1.2909527427505534\n",
      "three-layer Loss after iteration 31000: 1.1106678754846933\n",
      "three-layer Loss after iteration 32000: 1.4053168215430787\n",
      "three-layer Loss after iteration 33000: 0.9130209807455855\n",
      "three-layer Loss after iteration 34000: 0.9563592336923996\n",
      "three-layer Loss after iteration 35000: 1.3295327162883839\n",
      "three-layer Loss after iteration 36000: 1.0730707203371788\n",
      "three-layer Loss after iteration 37000: 0.8520942428850254\n",
      "three-layer Loss after iteration 38000: 1.4864158413173203\n",
      "three-layer Loss after iteration 39000: 1.0245411785137677\n",
      "three-layer Loss after iteration 40000: 0.8992175877223114\n",
      "three-layer Loss after iteration 41000: 1.3030032312821618\n",
      "three-layer Loss after iteration 42000: 1.038069894238114\n",
      "three-layer Loss after iteration 43000: 0.9650115731918526\n",
      "three-layer Loss after iteration 44000: 0.7937372150438815\n",
      "three-layer Loss after iteration 45000: 0.9068718434041919\n",
      "three-layer Loss after iteration 46000: 0.947502981277288\n",
      "three-layer Loss after iteration 47000: 1.0719684039771595\n",
      "three-layer Loss after iteration 48000: 1.1989548489216046\n",
      "three-layer Loss after iteration 49000: 1.1295147504856535\n",
      "three-layer Loss after iteration 50000: 0.7692977275344709\n",
      "three-layer Loss after iteration 51000: 0.6815445874087734\n",
      "three-layer Loss after iteration 52000: 0.7587950361899036\n",
      "three-layer Loss after iteration 53000: 0.9492066538818037\n",
      "three-layer Loss after iteration 54000: 1.1627207793347467\n",
      "three-layer Loss after iteration 55000: 0.7260078954403522\n",
      "three-layer Loss after iteration 56000: 0.6569873309612262\n",
      "three-layer Loss after iteration 57000: 0.748818593626234\n",
      "three-layer Loss after iteration 58000: 0.7552911215442047\n",
      "three-layer Loss after iteration 59000: 0.652992434072803\n",
      "three-layer Loss after iteration 60000: 0.6753467760925481\n",
      "three-layer Loss after iteration 61000: 0.7504531878138638\n",
      "three-layer Loss after iteration 62000: 0.84319048624684\n",
      "three-layer Loss after iteration 63000: 0.860019054559518\n",
      "three-layer Loss after iteration 64000: 0.7099045982172505\n",
      "three-layer Loss after iteration 65000: 0.7943591962381561\n",
      "three-layer Loss after iteration 66000: 0.9976424369892071\n",
      "three-layer Loss after iteration 67000: 0.857014283350572\n",
      "three-layer Loss after iteration 68000: 0.7337979699139686\n",
      "three-layer Loss after iteration 69000: 1.1157220396198493\n",
      "three-layer Loss after iteration 70000: 0.8771127428291414\n",
      "three-layer Loss after iteration 71000: 0.6383960268611603\n",
      "three-layer Loss after iteration 72000: 0.5972862955373217\n",
      "three-layer Loss after iteration 73000: 1.026654434031701\n",
      "three-layer Loss after iteration 74000: 0.7264025603868418\n",
      "three-layer Loss after iteration 75000: 0.5976771756770346\n",
      "three-layer Loss after iteration 76000: 1.5822428594926703\n",
      "three-layer Loss after iteration 77000: 0.5917488543179199\n",
      "three-layer Loss after iteration 78000: 0.9916139861860201\n",
      "three-layer Loss after iteration 79000: 0.6099051902048348\n",
      "three-layer Loss after iteration 80000: 1.8183223506187556\n",
      "three-layer Loss after iteration 81000: 0.5840651326333044\n",
      "three-layer Loss after iteration 82000: 0.8017452079812165\n",
      "three-layer Loss after iteration 83000: 0.631611840626303\n",
      "three-layer Loss after iteration 84000: 1.5245795498590118\n",
      "three-layer Loss after iteration 85000: 0.653919833075013\n",
      "three-layer Loss after iteration 86000: 0.6765637895673277\n",
      "three-layer Loss after iteration 87000: 0.5818202008560176\n",
      "three-layer Loss after iteration 88000: 0.6478134996614959\n",
      "three-layer Loss after iteration 89000: 0.750734081964788\n",
      "three-layer Loss after iteration 90000: 0.6113576254463694\n",
      "three-layer Loss after iteration 91000: 0.6642665890582113\n",
      "three-layer Loss after iteration 92000: 0.6262808029287805\n",
      "three-layer Loss after iteration 93000: 0.8254033750756936\n",
      "three-layer Loss after iteration 94000: 0.6735687183915998\n",
      "three-layer Loss after iteration 95000: 0.6162190776426112\n",
      "three-layer Loss after iteration 96000: 0.6805327230058864\n",
      "three-layer Loss after iteration 97000: 0.6797943501402448\n",
      "three-layer Loss after iteration 98000: 0.6093297506873463\n",
      "three-layer Loss after iteration 99000: 0.6631379394498382\n",
      "three-layer Loss after iteration 0: 1705.8505614848993\n",
      "three-layer Loss after iteration 1000: 10.603855944070101\n",
      "three-layer Loss after iteration 2000: 7.280718726006389\n",
      "three-layer Loss after iteration 3000: 6.117507288472516\n",
      "three-layer Loss after iteration 4000: 4.7013397193429185\n",
      "three-layer Loss after iteration 5000: 4.433760400554636\n",
      "three-layer Loss after iteration 6000: 3.8818072293017516\n",
      "three-layer Loss after iteration 7000: 3.043896979888807\n",
      "three-layer Loss after iteration 8000: 2.6996435250241735\n",
      "three-layer Loss after iteration 9000: 2.4458453661194146\n",
      "three-layer Loss after iteration 10000: 1.743882649907324\n",
      "three-layer Loss after iteration 11000: 1.5423508078123354\n",
      "three-layer Loss after iteration 12000: 1.6930342314992408\n",
      "three-layer Loss after iteration 13000: 1.465961650608148\n",
      "three-layer Loss after iteration 14000: 1.3185349006395335\n",
      "three-layer Loss after iteration 15000: 1.2730011217289185\n",
      "three-layer Loss after iteration 16000: 1.315584170810439\n",
      "three-layer Loss after iteration 17000: 1.2612339539333983\n",
      "three-layer Loss after iteration 18000: 1.38755036134258\n",
      "three-layer Loss after iteration 19000: 1.2468989296080037\n",
      "three-layer Loss after iteration 20000: 1.1627151075451052\n",
      "three-layer Loss after iteration 21000: 0.8420835560448384\n",
      "three-layer Loss after iteration 22000: 1.0102650027351283\n",
      "three-layer Loss after iteration 23000: 1.0092741790905888\n",
      "three-layer Loss after iteration 24000: 1.0512444819794664\n",
      "three-layer Loss after iteration 25000: 1.0758455064381862\n",
      "three-layer Loss after iteration 26000: 0.9554716624942697\n",
      "three-layer Loss after iteration 27000: 0.7441678630551771\n",
      "three-layer Loss after iteration 28000: 0.8141154660448098\n",
      "three-layer Loss after iteration 29000: 0.7001618488483624\n",
      "three-layer Loss after iteration 30000: 0.7257376616730724\n",
      "three-layer Loss after iteration 31000: 0.7471796731171864\n",
      "three-layer Loss after iteration 32000: 0.6834191371230331\n",
      "three-layer Loss after iteration 33000: 0.5067645621880014\n",
      "three-layer Loss after iteration 34000: 0.8020126737878324\n",
      "three-layer Loss after iteration 35000: 0.5426269854200152\n",
      "three-layer Loss after iteration 36000: 0.6084242866168335\n",
      "three-layer Loss after iteration 37000: 0.5843229564489241\n",
      "three-layer Loss after iteration 38000: 0.6218621628916647\n",
      "three-layer Loss after iteration 39000: 0.5904774102985294\n",
      "three-layer Loss after iteration 40000: 0.48838066524474394\n",
      "three-layer Loss after iteration 41000: 0.5394845298256485\n",
      "three-layer Loss after iteration 42000: 0.4689857186440068\n",
      "three-layer Loss after iteration 43000: 0.5389107227145962\n",
      "three-layer Loss after iteration 44000: 0.506045742167184\n",
      "three-layer Loss after iteration 45000: 0.5813002063332434\n",
      "three-layer Loss after iteration 46000: 0.7182592803021808\n",
      "three-layer Loss after iteration 47000: 0.6511804689913971\n",
      "three-layer Loss after iteration 48000: 0.5950600534334018\n",
      "three-layer Loss after iteration 49000: 0.5535720696604125\n",
      "three-layer Loss after iteration 50000: 0.47108826079713373\n",
      "three-layer Loss after iteration 51000: 0.5033369449745076\n",
      "three-layer Loss after iteration 52000: 0.4949179872031089\n",
      "three-layer Loss after iteration 53000: 0.48055185781147897\n",
      "three-layer Loss after iteration 54000: 0.46842732780218904\n",
      "three-layer Loss after iteration 55000: 0.4588131826714787\n",
      "three-layer Loss after iteration 56000: 0.4525393679498126\n",
      "three-layer Loss after iteration 57000: 0.4464503814639534\n",
      "three-layer Loss after iteration 58000: 0.4377086143081864\n",
      "three-layer Loss after iteration 59000: 0.43082629812309087\n",
      "three-layer Loss after iteration 60000: 0.4355091324767079\n",
      "three-layer Loss after iteration 61000: 0.4280089523979249\n",
      "three-layer Loss after iteration 62000: 0.416031380044546\n",
      "three-layer Loss after iteration 63000: 0.41416170186756174\n",
      "three-layer Loss after iteration 64000: 0.40635432900407137\n",
      "three-layer Loss after iteration 65000: 0.4039240409259008\n",
      "three-layer Loss after iteration 66000: 0.4024831676607894\n",
      "three-layer Loss after iteration 67000: 0.40232894896241095\n",
      "three-layer Loss after iteration 68000: 0.40032305093799186\n",
      "three-layer Loss after iteration 69000: 0.4011581128836071\n",
      "three-layer Loss after iteration 70000: 0.3996779348964109\n",
      "three-layer Loss after iteration 71000: 0.39787436428280876\n",
      "three-layer Loss after iteration 72000: 0.3963539307668959\n",
      "three-layer Loss after iteration 73000: 0.3949168922589422\n",
      "three-layer Loss after iteration 74000: 0.39286735016219304\n",
      "three-layer Loss after iteration 75000: 0.39306898554610387\n",
      "three-layer Loss after iteration 76000: 0.38391741966095577\n",
      "three-layer Loss after iteration 77000: 0.3856321275036252\n",
      "three-layer Loss after iteration 78000: 0.3752726706745923\n",
      "three-layer Loss after iteration 79000: 0.37357722121245474\n",
      "three-layer Loss after iteration 80000: 0.3733540404167253\n",
      "three-layer Loss after iteration 81000: 0.37305115013659096\n",
      "three-layer Loss after iteration 82000: 0.37304393355097937\n",
      "1.934476172757814e-05 0.37305115013659096 0.37304393355097937\n",
      "three-layer Loss after iteration 0: 1652.6068273340295\n",
      "three-layer Loss after iteration 1000: 9.656856789610346\n",
      "three-layer Loss after iteration 2000: 7.942238156264739\n",
      "three-layer Loss after iteration 3000: 6.438992470678388\n",
      "three-layer Loss after iteration 4000: 4.850433344725908\n",
      "three-layer Loss after iteration 5000: 3.2559125562303786\n",
      "three-layer Loss after iteration 6000: 2.482875995687705\n",
      "three-layer Loss after iteration 7000: 2.2668024730946668\n",
      "three-layer Loss after iteration 8000: 2.0591166898509083\n",
      "three-layer Loss after iteration 9000: 1.8711768447425774\n",
      "three-layer Loss after iteration 10000: 1.7072109362414045\n",
      "three-layer Loss after iteration 11000: 1.6564697460735922\n",
      "three-layer Loss after iteration 12000: 1.6622705164315539\n",
      "three-layer Loss after iteration 13000: 1.688105266440701\n",
      "three-layer Loss after iteration 14000: 2.006995324007455\n",
      "three-layer Loss after iteration 15000: 1.5987847575532717\n",
      "three-layer Loss after iteration 16000: 1.6882097554024167\n",
      "three-layer Loss after iteration 17000: 1.6799643575650194\n",
      "three-layer Loss after iteration 18000: 1.9276907009893927\n",
      "three-layer Loss after iteration 19000: 1.7622610634121876\n",
      "three-layer Loss after iteration 20000: 1.8233144230333733\n",
      "three-layer Loss after iteration 21000: 1.3917450402492346\n",
      "three-layer Loss after iteration 22000: 1.271538199261937\n",
      "three-layer Loss after iteration 23000: 1.0560410968264422\n",
      "three-layer Loss after iteration 24000: 1.0282271141833812\n",
      "three-layer Loss after iteration 25000: 1.278453077769268\n",
      "three-layer Loss after iteration 26000: 1.2643478203976002\n",
      "three-layer Loss after iteration 27000: 1.3823061328847481\n",
      "three-layer Loss after iteration 28000: 1.2682929740618778\n",
      "three-layer Loss after iteration 29000: 1.3212568012664767\n",
      "three-layer Loss after iteration 30000: 1.2889382508453\n",
      "three-layer Loss after iteration 31000: 0.9264682913404364\n",
      "three-layer Loss after iteration 32000: 0.7808801459400724\n",
      "three-layer Loss after iteration 33000: 1.4228962599978503\n",
      "three-layer Loss after iteration 34000: 1.5548222756593242\n",
      "three-layer Loss after iteration 35000: 1.0419478710227927\n",
      "three-layer Loss after iteration 36000: 1.2861911684177354\n",
      "three-layer Loss after iteration 37000: 0.8940881509377998\n",
      "three-layer Loss after iteration 38000: 0.4963733585147557\n",
      "three-layer Loss after iteration 39000: 0.96570044305041\n",
      "three-layer Loss after iteration 40000: 0.48128968461016486\n",
      "three-layer Loss after iteration 41000: 0.5129113287835605\n",
      "three-layer Loss after iteration 42000: 0.6884931099714131\n",
      "three-layer Loss after iteration 43000: 0.6823280381920388\n",
      "three-layer Loss after iteration 44000: 0.46194113230792483\n",
      "three-layer Loss after iteration 45000: 0.4620073955796959\n",
      "three-layer Loss after iteration 46000: 0.456737979688272\n",
      "three-layer Loss after iteration 47000: 0.4578372814536959\n",
      "three-layer Loss after iteration 48000: 0.44633377606166585\n",
      "three-layer Loss after iteration 49000: 0.4939596903603078\n",
      "three-layer Loss after iteration 50000: 0.5420955306071759\n",
      "three-layer Loss after iteration 51000: 0.5542107936051403\n",
      "three-layer Loss after iteration 52000: 0.4370713766892911\n",
      "three-layer Loss after iteration 53000: 0.4944549871163857\n",
      "three-layer Loss after iteration 54000: 0.4792380425953765\n",
      "three-layer Loss after iteration 55000: 0.4786985654373921\n",
      "three-layer Loss after iteration 56000: 0.4877413312538946\n",
      "three-layer Loss after iteration 57000: 0.48271655790716056\n",
      "three-layer Loss after iteration 58000: 0.45574066166610994\n",
      "three-layer Loss after iteration 59000: 0.6237284206658413\n",
      "three-layer Loss after iteration 60000: 0.7976846162981474\n",
      "three-layer Loss after iteration 61000: 0.5280555346777185\n",
      "three-layer Loss after iteration 62000: 0.45469818737300066\n",
      "three-layer Loss after iteration 63000: 0.43873754431548173\n",
      "three-layer Loss after iteration 64000: 0.4471525548344815\n",
      "three-layer Loss after iteration 65000: 0.5560041010742732\n",
      "three-layer Loss after iteration 66000: 0.5499591909433463\n",
      "three-layer Loss after iteration 67000: 0.5332985219284655\n",
      "three-layer Loss after iteration 68000: 0.496349538324895\n",
      "three-layer Loss after iteration 69000: 0.5083719036343266\n",
      "three-layer Loss after iteration 70000: 0.5233365618297537\n",
      "three-layer Loss after iteration 71000: 0.5216137782723518\n",
      "three-layer Loss after iteration 72000: 0.5201581218167703\n",
      "three-layer Loss after iteration 73000: 0.519747898089568\n",
      "three-layer Loss after iteration 74000: 0.5188219429430262\n",
      "three-layer Loss after iteration 75000: 0.49571221055112547\n",
      "three-layer Loss after iteration 76000: 0.44154479839996486\n",
      "three-layer Loss after iteration 77000: 0.47226419023698607\n",
      "three-layer Loss after iteration 78000: 0.4922928159709746\n",
      "three-layer Loss after iteration 79000: 0.4950696850519796\n",
      "three-layer Loss after iteration 80000: 0.46547407140104574\n",
      "three-layer Loss after iteration 81000: 0.4676937321111451\n",
      "three-layer Loss after iteration 82000: 0.475111122407491\n",
      "three-layer Loss after iteration 83000: 0.46951007757395463\n",
      "three-layer Loss after iteration 84000: 0.4664106298139268\n",
      "three-layer Loss after iteration 85000: 0.4674555711020786\n",
      "three-layer Loss after iteration 86000: 0.4649197980318276\n",
      "three-layer Loss after iteration 87000: 0.46212052444308094\n",
      "three-layer Loss after iteration 88000: 0.4614212413720487\n",
      "three-layer Loss after iteration 89000: 0.4598389913808353\n",
      "three-layer Loss after iteration 90000: 0.458201649076259\n",
      "three-layer Loss after iteration 91000: 0.4572748820329269\n",
      "three-layer Loss after iteration 92000: 0.4560165191910224\n",
      "three-layer Loss after iteration 93000: 0.45380690300997667\n",
      "three-layer Loss after iteration 94000: 0.4520355662881518\n",
      "three-layer Loss after iteration 95000: 0.45034666270859364\n",
      "three-layer Loss after iteration 96000: 0.44912458636500124\n",
      "three-layer Loss after iteration 97000: 0.44806170106410553\n",
      "three-layer Loss after iteration 98000: 0.44631959697058\n",
      "three-layer Loss after iteration 99000: 0.44503187086257695\n",
      "three-layer Loss after iteration 0: 1617.8850035893126\n",
      "three-layer Loss after iteration 1000: 8.860568799677283\n",
      "three-layer Loss after iteration 2000: 6.644150599777627\n",
      "three-layer Loss after iteration 3000: 5.317953788254772\n",
      "three-layer Loss after iteration 4000: 4.4950431663306825\n",
      "three-layer Loss after iteration 5000: 3.709247936655324\n",
      "three-layer Loss after iteration 6000: 3.14984401696811\n",
      "three-layer Loss after iteration 7000: 2.1352436897547094\n",
      "three-layer Loss after iteration 8000: 2.2544314887043706\n",
      "three-layer Loss after iteration 9000: 1.6685124328491046\n",
      "three-layer Loss after iteration 10000: 1.4969815520829586\n",
      "three-layer Loss after iteration 11000: 1.956559824046996\n",
      "three-layer Loss after iteration 12000: 1.4310358582899445\n",
      "three-layer Loss after iteration 13000: 1.6708250853756135\n",
      "three-layer Loss after iteration 14000: 1.5932480229854213\n",
      "three-layer Loss after iteration 15000: 1.5251489952868669\n",
      "three-layer Loss after iteration 16000: 1.4020781946563667\n",
      "three-layer Loss after iteration 17000: 1.5938425184691218\n",
      "three-layer Loss after iteration 18000: 1.6993018105436446\n",
      "three-layer Loss after iteration 19000: 1.2954323372024021\n",
      "three-layer Loss after iteration 20000: 1.11928715522337\n",
      "three-layer Loss after iteration 21000: 0.9900369737483136\n",
      "three-layer Loss after iteration 22000: 2.1781605576889222\n",
      "three-layer Loss after iteration 23000: 1.5879756821564732\n",
      "three-layer Loss after iteration 24000: 1.7017898017108535\n",
      "three-layer Loss after iteration 25000: 1.5113206381573887\n",
      "three-layer Loss after iteration 26000: 1.4485515025340738\n",
      "three-layer Loss after iteration 27000: 1.2649977107340826\n",
      "three-layer Loss after iteration 28000: 1.538804362373604\n",
      "three-layer Loss after iteration 29000: 1.2243597206540708\n",
      "three-layer Loss after iteration 30000: 1.1525456932950662\n",
      "three-layer Loss after iteration 31000: 1.012097070092827\n",
      "three-layer Loss after iteration 32000: 0.8758433451142335\n",
      "three-layer Loss after iteration 33000: 0.8993076929320867\n",
      "three-layer Loss after iteration 34000: 0.9837645328778191\n",
      "three-layer Loss after iteration 35000: 1.0453321627339134\n",
      "three-layer Loss after iteration 36000: 1.0594114773660748\n",
      "three-layer Loss after iteration 37000: 1.0761625788938554\n",
      "three-layer Loss after iteration 38000: 0.9216735618676305\n",
      "three-layer Loss after iteration 39000: 0.8078599431253042\n",
      "three-layer Loss after iteration 40000: 1.5922504365309478\n",
      "three-layer Loss after iteration 41000: 0.6109169555043089\n",
      "three-layer Loss after iteration 42000: 1.1289530778652674\n",
      "three-layer Loss after iteration 43000: 1.0193274779185293\n",
      "three-layer Loss after iteration 44000: 0.8002425740998235\n",
      "three-layer Loss after iteration 45000: 0.8445072904429958\n",
      "three-layer Loss after iteration 46000: 0.7559137433671185\n",
      "three-layer Loss after iteration 47000: 0.8288930320931789\n",
      "three-layer Loss after iteration 48000: 0.7999738994140914\n",
      "three-layer Loss after iteration 49000: 0.7758336414779374\n",
      "three-layer Loss after iteration 50000: 0.7370895498779674\n",
      "three-layer Loss after iteration 51000: 0.703966919515527\n",
      "three-layer Loss after iteration 52000: 0.7138098338481904\n",
      "three-layer Loss after iteration 53000: 0.5907977214226953\n",
      "three-layer Loss after iteration 54000: 0.6525833735970903\n",
      "three-layer Loss after iteration 55000: 0.6746297791374142\n",
      "three-layer Loss after iteration 56000: 0.626206741563066\n",
      "three-layer Loss after iteration 57000: 0.6652984454294241\n",
      "three-layer Loss after iteration 58000: 0.6498207462676076\n",
      "three-layer Loss after iteration 59000: 0.6457987892375591\n",
      "three-layer Loss after iteration 60000: 0.6326669117450188\n",
      "three-layer Loss after iteration 61000: 0.6148693577633889\n",
      "three-layer Loss after iteration 62000: 0.6291225293939265\n",
      "three-layer Loss after iteration 63000: 0.6178634711494874\n",
      "three-layer Loss after iteration 64000: 0.5647804155149599\n",
      "three-layer Loss after iteration 65000: 0.5937306942162743\n",
      "three-layer Loss after iteration 66000: 0.5971925772811243\n",
      "three-layer Loss after iteration 67000: 0.5978545346358982\n",
      "three-layer Loss after iteration 68000: 0.5657210050783551\n",
      "three-layer Loss after iteration 69000: 0.5638201027956669\n",
      "three-layer Loss after iteration 70000: 0.5943328794485966\n",
      "three-layer Loss after iteration 71000: 0.5566244700741232\n",
      "three-layer Loss after iteration 72000: 0.5275679515385093\n",
      "three-layer Loss after iteration 73000: 0.5113874365386806\n",
      "three-layer Loss after iteration 74000: 0.42820639870540556\n",
      "three-layer Loss after iteration 75000: 0.39445700032953596\n",
      "three-layer Loss after iteration 76000: 0.46373990368792406\n",
      "three-layer Loss after iteration 77000: 0.5004477688453307\n",
      "three-layer Loss after iteration 78000: 0.5168276440141711\n",
      "three-layer Loss after iteration 79000: 0.44192646809494446\n",
      "three-layer Loss after iteration 80000: 0.44135877635662535\n",
      "three-layer Loss after iteration 81000: 0.4367065114425129\n",
      "three-layer Loss after iteration 82000: 0.43252185241892405\n",
      "three-layer Loss after iteration 83000: 0.42825251470506637\n",
      "three-layer Loss after iteration 84000: 0.42456497421254596\n",
      "three-layer Loss after iteration 85000: 0.4181982037961542\n",
      "three-layer Loss after iteration 86000: 0.41636616423884315\n",
      "three-layer Loss after iteration 87000: 0.41376433705428306\n",
      "three-layer Loss after iteration 88000: 0.4126186418667413\n",
      "three-layer Loss after iteration 89000: 0.4107845613857929\n",
      "three-layer Loss after iteration 90000: 0.38419635993085066\n",
      "three-layer Loss after iteration 91000: 0.38813286913249045\n",
      "three-layer Loss after iteration 92000: 0.3989735129751193\n",
      "three-layer Loss after iteration 93000: 0.3941589411339104\n",
      "three-layer Loss after iteration 94000: 0.4163101696530785\n",
      "three-layer Loss after iteration 95000: 0.39821227611751575\n",
      "three-layer Loss after iteration 96000: 0.41618570463031673\n",
      "three-layer Loss after iteration 97000: 0.39665640240373795\n",
      "three-layer Loss after iteration 98000: 0.4819655171382438\n",
      "three-layer Loss after iteration 99000: 0.3980486225500918\n",
      "three-layer Loss after iteration 0: 1701.57182131676\n",
      "three-layer Loss after iteration 1000: 12.63549999306541\n",
      "three-layer Loss after iteration 2000: 8.966804644650843\n",
      "three-layer Loss after iteration 3000: 5.107363418250639\n",
      "three-layer Loss after iteration 4000: 4.960967867319951\n",
      "three-layer Loss after iteration 5000: 6.866219448857811\n",
      "three-layer Loss after iteration 6000: 3.8017431466806717\n",
      "three-layer Loss after iteration 7000: 3.9988796033460523\n",
      "three-layer Loss after iteration 8000: 2.6713524001402686\n",
      "three-layer Loss after iteration 9000: 2.4535579560119096\n",
      "three-layer Loss after iteration 10000: 2.4070515691482073\n",
      "three-layer Loss after iteration 11000: 2.5750881616760326\n",
      "three-layer Loss after iteration 12000: 2.5678178340885887\n",
      "three-layer Loss after iteration 13000: 2.3054015708078595\n",
      "three-layer Loss after iteration 14000: 2.078338493981118\n",
      "three-layer Loss after iteration 15000: 2.5832228753327544\n",
      "three-layer Loss after iteration 16000: 2.064186498267939\n",
      "three-layer Loss after iteration 17000: 1.759763678040103\n",
      "three-layer Loss after iteration 18000: 1.5836541454761184\n",
      "three-layer Loss after iteration 19000: 1.9260399552904013\n",
      "three-layer Loss after iteration 20000: 1.523493962161618\n",
      "three-layer Loss after iteration 21000: 1.3596166150197675\n",
      "three-layer Loss after iteration 22000: 1.3437233537780189\n",
      "three-layer Loss after iteration 23000: 1.3314491527264882\n",
      "three-layer Loss after iteration 24000: 1.4146587586540313\n",
      "three-layer Loss after iteration 25000: 1.3984917009735283\n",
      "three-layer Loss after iteration 26000: 1.5679875597623798\n",
      "three-layer Loss after iteration 27000: 1.464724144734021\n",
      "three-layer Loss after iteration 28000: 1.4200468149525918\n",
      "three-layer Loss after iteration 29000: 1.378596305522109\n",
      "three-layer Loss after iteration 30000: 1.3381881632178367\n",
      "three-layer Loss after iteration 31000: 1.33134201765381\n",
      "three-layer Loss after iteration 32000: 1.3025286450664288\n",
      "three-layer Loss after iteration 33000: 1.296384775183935\n",
      "three-layer Loss after iteration 34000: 1.2791972066745505\n",
      "three-layer Loss after iteration 35000: 1.2708479726337227\n",
      "three-layer Loss after iteration 36000: 1.3221130779230899\n",
      "three-layer Loss after iteration 37000: 1.2923053874952775\n",
      "three-layer Loss after iteration 38000: 1.2655229853564924\n",
      "three-layer Loss after iteration 39000: 1.2377278102631746\n",
      "three-layer Loss after iteration 40000: 1.2340979642047518\n",
      "three-layer Loss after iteration 41000: 1.224651514776168\n",
      "three-layer Loss after iteration 42000: 1.2193308254790267\n",
      "three-layer Loss after iteration 43000: 1.0548732282154403\n",
      "three-layer Loss after iteration 44000: 1.1633979602848123\n",
      "three-layer Loss after iteration 45000: 1.3314253928420046\n",
      "three-layer Loss after iteration 46000: 1.3917588666724614\n",
      "three-layer Loss after iteration 47000: 1.031365635845545\n",
      "three-layer Loss after iteration 48000: 1.6302756044173177\n",
      "three-layer Loss after iteration 49000: 1.117548739714107\n",
      "three-layer Loss after iteration 50000: 1.109036515488212\n",
      "three-layer Loss after iteration 51000: 1.1190698108786554\n",
      "three-layer Loss after iteration 52000: 1.2401517652047187\n",
      "three-layer Loss after iteration 53000: 1.0281036698921935\n",
      "three-layer Loss after iteration 54000: 1.2504417723922767\n",
      "three-layer Loss after iteration 55000: 0.9935133193318715\n",
      "three-layer Loss after iteration 56000: 0.9976880490848897\n",
      "three-layer Loss after iteration 57000: 0.9429194778349476\n",
      "three-layer Loss after iteration 58000: 1.0843672542031024\n",
      "three-layer Loss after iteration 59000: 1.2003280751483283\n",
      "three-layer Loss after iteration 60000: 1.2693010108613978\n",
      "three-layer Loss after iteration 61000: 1.206283562433419\n",
      "three-layer Loss after iteration 62000: 1.1391271052581007\n",
      "three-layer Loss after iteration 63000: 1.1203951884524177\n",
      "three-layer Loss after iteration 64000: 0.9919216063512373\n",
      "three-layer Loss after iteration 65000: 1.0778241077481312\n",
      "three-layer Loss after iteration 66000: 1.03703604329875\n",
      "three-layer Loss after iteration 67000: 1.136759391398038\n",
      "three-layer Loss after iteration 68000: 1.2144081355523169\n",
      "three-layer Loss after iteration 69000: 1.0129423098663994\n",
      "three-layer Loss after iteration 70000: 0.9756238229730642\n",
      "three-layer Loss after iteration 71000: 1.0050857033412846\n",
      "three-layer Loss after iteration 72000: 0.9920221613091337\n",
      "three-layer Loss after iteration 73000: 0.9852341579302136\n",
      "three-layer Loss after iteration 74000: 0.9799959269526033\n",
      "three-layer Loss after iteration 75000: 0.9735717175098817\n",
      "three-layer Loss after iteration 76000: 0.9711994175461435\n",
      "three-layer Loss after iteration 77000: 0.9611142599124719\n",
      "three-layer Loss after iteration 78000: 0.9572885264856544\n",
      "three-layer Loss after iteration 79000: 0.9541695541977024\n",
      "three-layer Loss after iteration 80000: 0.9500485568814244\n",
      "three-layer Loss after iteration 81000: 0.9458343075740476\n",
      "three-layer Loss after iteration 82000: 0.9411940269652854\n",
      "three-layer Loss after iteration 83000: 0.9313816840832426\n",
      "three-layer Loss after iteration 84000: 0.9356654228998917\n",
      "three-layer Loss after iteration 85000: 0.9323031879623116\n",
      "three-layer Loss after iteration 86000: 0.9279891965216687\n",
      "three-layer Loss after iteration 87000: 0.9236874176944583\n",
      "three-layer Loss after iteration 88000: 0.9204609985849685\n",
      "three-layer Loss after iteration 89000: 0.9175759091191349\n",
      "three-layer Loss after iteration 90000: 0.913772570153676\n",
      "three-layer Loss after iteration 91000: 0.9114097549989008\n",
      "three-layer Loss after iteration 92000: 0.9089146568263586\n",
      "three-layer Loss after iteration 93000: 0.9062990152375323\n",
      "three-layer Loss after iteration 94000: 0.8765404747375123\n",
      "three-layer Loss after iteration 95000: 1.0489157717538076\n",
      "three-layer Loss after iteration 96000: 1.0513378466596137\n",
      "three-layer Loss after iteration 97000: 0.9063274348714506\n",
      "three-layer Loss after iteration 98000: 0.8765060149762615\n",
      "three-layer Loss after iteration 99000: 0.9173571990983366\n",
      "three-layer Loss after iteration 0: 1677.1974586595145\n",
      "three-layer Loss after iteration 1000: 9.154311467029997\n",
      "three-layer Loss after iteration 2000: 8.821391953715127\n",
      "three-layer Loss after iteration 3000: 5.850045498787126\n",
      "three-layer Loss after iteration 4000: 4.6401736767218384\n",
      "three-layer Loss after iteration 5000: 4.039643310812671\n",
      "three-layer Loss after iteration 6000: 3.2056046121412427\n",
      "three-layer Loss after iteration 7000: 3.8081219681988148\n",
      "three-layer Loss after iteration 8000: 2.4119527477884817\n",
      "three-layer Loss after iteration 9000: 2.7689011414864098\n",
      "three-layer Loss after iteration 10000: 2.7814485702652747\n",
      "three-layer Loss after iteration 11000: 2.3416020048412713\n",
      "three-layer Loss after iteration 12000: 2.2148802963427676\n",
      "three-layer Loss after iteration 13000: 2.3026239366747423\n",
      "three-layer Loss after iteration 14000: 1.9786784875497192\n",
      "three-layer Loss after iteration 15000: 1.912930330734632\n",
      "three-layer Loss after iteration 16000: 1.8450387216927298\n",
      "three-layer Loss after iteration 17000: 1.7659246710463974\n",
      "three-layer Loss after iteration 18000: 1.620009037265866\n",
      "three-layer Loss after iteration 19000: 1.8291229702195182\n",
      "three-layer Loss after iteration 20000: 1.6200544257989922\n",
      "three-layer Loss after iteration 21000: 1.5797580615713984\n",
      "three-layer Loss after iteration 22000: 1.6282511122548649\n",
      "three-layer Loss after iteration 23000: 1.5093365185301293\n",
      "three-layer Loss after iteration 24000: 1.4607839498893933\n",
      "three-layer Loss after iteration 25000: 1.31306137260179\n",
      "three-layer Loss after iteration 26000: 1.2152574381957608\n",
      "three-layer Loss after iteration 27000: 1.2417853570915356\n",
      "three-layer Loss after iteration 28000: 1.3940423091547849\n",
      "three-layer Loss after iteration 29000: 1.1349355979870206\n",
      "three-layer Loss after iteration 30000: 1.508720486036254\n",
      "three-layer Loss after iteration 31000: 1.171093250882122\n",
      "three-layer Loss after iteration 32000: 1.231622972883161\n",
      "three-layer Loss after iteration 33000: 1.22527222625332\n",
      "three-layer Loss after iteration 34000: 1.2819931197450003\n",
      "three-layer Loss after iteration 35000: 1.2837476363789175\n",
      "three-layer Loss after iteration 36000: 1.2250842419065757\n",
      "three-layer Loss after iteration 37000: 1.1667244969882928\n",
      "three-layer Loss after iteration 38000: 1.151176176687676\n",
      "three-layer Loss after iteration 39000: 1.138561447220693\n",
      "three-layer Loss after iteration 40000: 1.0540904825794473\n",
      "three-layer Loss after iteration 41000: 1.028835135219536\n",
      "three-layer Loss after iteration 42000: 0.9823534365861994\n",
      "three-layer Loss after iteration 43000: 0.9511935282673838\n",
      "three-layer Loss after iteration 44000: 0.8945857860497687\n",
      "three-layer Loss after iteration 45000: 0.8658366342701121\n",
      "three-layer Loss after iteration 46000: 0.8013176113383597\n",
      "three-layer Loss after iteration 47000: 0.7187189991257045\n",
      "three-layer Loss after iteration 48000: 0.798165999051078\n",
      "three-layer Loss after iteration 49000: 0.767434336810463\n",
      "three-layer Loss after iteration 50000: 0.7944837771226302\n",
      "three-layer Loss after iteration 51000: 0.770018729532827\n",
      "three-layer Loss after iteration 52000: 0.7804397011138371\n",
      "three-layer Loss after iteration 53000: 0.7762308218418457\n",
      "three-layer Loss after iteration 54000: 0.9539822440326083\n",
      "three-layer Loss after iteration 55000: 0.7026606238410124\n",
      "three-layer Loss after iteration 56000: 0.7123876920111669\n",
      "three-layer Loss after iteration 57000: 0.7060716449886887\n",
      "three-layer Loss after iteration 58000: 0.6753962188912079\n",
      "three-layer Loss after iteration 59000: 0.6620713694606153\n",
      "three-layer Loss after iteration 60000: 0.6450625711106045\n",
      "three-layer Loss after iteration 61000: 0.6572633967584072\n",
      "three-layer Loss after iteration 62000: 0.6180337106359057\n",
      "three-layer Loss after iteration 63000: 0.592201105721742\n",
      "three-layer Loss after iteration 64000: 0.5885667193938336\n",
      "three-layer Loss after iteration 65000: 0.5652689014854765\n",
      "three-layer Loss after iteration 66000: 0.5660331186205398\n",
      "three-layer Loss after iteration 67000: 0.6101219961335342\n",
      "three-layer Loss after iteration 68000: 0.536865597995723\n",
      "three-layer Loss after iteration 69000: 0.4761836427464986\n",
      "three-layer Loss after iteration 70000: 0.45769765961755027\n",
      "three-layer Loss after iteration 71000: 0.6032430956614729\n",
      "three-layer Loss after iteration 72000: 0.440786800304056\n",
      "three-layer Loss after iteration 73000: 0.5077272158152695\n",
      "three-layer Loss after iteration 74000: 0.5514383613453653\n",
      "three-layer Loss after iteration 75000: 0.5176200701046463\n",
      "three-layer Loss after iteration 76000: 0.4315957902062971\n",
      "three-layer Loss after iteration 77000: 0.4125444884520837\n",
      "three-layer Loss after iteration 78000: 0.41901636482590576\n",
      "three-layer Loss after iteration 79000: 0.40287260390544727\n",
      "three-layer Loss after iteration 80000: 0.655412167773282\n",
      "three-layer Loss after iteration 81000: 0.39465751362271684\n",
      "three-layer Loss after iteration 82000: 0.3951421871230178\n",
      "three-layer Loss after iteration 83000: 0.39043470829281973\n",
      "three-layer Loss after iteration 84000: 0.3922274373587987\n",
      "three-layer Loss after iteration 85000: 0.3911430582038805\n",
      "three-layer Loss after iteration 86000: 0.3828980642987949\n",
      "three-layer Loss after iteration 87000: 0.38783559954802366\n",
      "three-layer Loss after iteration 88000: 0.3823418941916581\n",
      "three-layer Loss after iteration 89000: 0.37930947967887846\n",
      "three-layer Loss after iteration 90000: 0.37570008353887685\n",
      "three-layer Loss after iteration 91000: 0.3775432743485334\n",
      "three-layer Loss after iteration 92000: 0.3732648534524233\n",
      "three-layer Loss after iteration 93000: 0.37148733525576144\n",
      "three-layer Loss after iteration 94000: 0.37280128527784945\n",
      "three-layer Loss after iteration 95000: 0.37500297429029333\n",
      "three-layer Loss after iteration 96000: 0.3648766472815666\n",
      "three-layer Loss after iteration 97000: 0.37049876552269284\n",
      "three-layer Loss after iteration 98000: 0.3665954840063689\n",
      "three-layer Loss after iteration 99000: 0.3653297639280916\n",
      "three-layer Loss after iteration 0: 1489.8464577796078\n",
      "three-layer Loss after iteration 1000: 11.227310049676065\n",
      "three-layer Loss after iteration 2000: 7.2515285193575885\n",
      "three-layer Loss after iteration 3000: 5.680418109539494\n",
      "three-layer Loss after iteration 4000: 4.430645738630572\n",
      "three-layer Loss after iteration 5000: 3.5922641641180477\n",
      "three-layer Loss after iteration 6000: 3.109401571967602\n",
      "three-layer Loss after iteration 7000: 2.0034480979502667\n",
      "three-layer Loss after iteration 8000: 2.1323492160449584\n",
      "three-layer Loss after iteration 9000: 2.789711866824429\n",
      "three-layer Loss after iteration 10000: 1.3928513481500062\n",
      "three-layer Loss after iteration 11000: 1.0026528318863273\n",
      "three-layer Loss after iteration 12000: 1.0107861612119706\n",
      "three-layer Loss after iteration 13000: 1.1311766686750655\n",
      "three-layer Loss after iteration 14000: 0.8241401276620957\n",
      "three-layer Loss after iteration 15000: 0.8268581148874741\n",
      "three-layer Loss after iteration 16000: 0.8015781380366108\n",
      "three-layer Loss after iteration 17000: 0.8214907442870497\n",
      "three-layer Loss after iteration 18000: 0.8451707134832461\n",
      "three-layer Loss after iteration 19000: 0.8490600451339929\n",
      "three-layer Loss after iteration 20000: 0.8922100926788418\n",
      "three-layer Loss after iteration 21000: 0.8563316973259301\n",
      "three-layer Loss after iteration 22000: 0.8405014009937741\n",
      "three-layer Loss after iteration 23000: 0.866137601356502\n",
      "three-layer Loss after iteration 24000: 0.9287405484976279\n",
      "three-layer Loss after iteration 25000: 0.8694304953444428\n",
      "three-layer Loss after iteration 26000: 0.8632651693104574\n",
      "three-layer Loss after iteration 27000: 0.8049166645893355\n",
      "three-layer Loss after iteration 28000: 0.8721595235121269\n",
      "three-layer Loss after iteration 29000: 0.8486092839340394\n",
      "three-layer Loss after iteration 30000: 0.8358627449845472\n",
      "three-layer Loss after iteration 31000: 0.8450164002228381\n",
      "three-layer Loss after iteration 32000: 0.8242040904666159\n",
      "three-layer Loss after iteration 33000: 0.8163978188860994\n",
      "three-layer Loss after iteration 34000: 0.8089340347198648\n",
      "three-layer Loss after iteration 35000: 0.7841659250055764\n",
      "three-layer Loss after iteration 36000: 0.7659806288861133\n",
      "three-layer Loss after iteration 37000: 0.7482520730813216\n",
      "three-layer Loss after iteration 38000: 0.7840879638997728\n",
      "three-layer Loss after iteration 39000: 0.7535512519275479\n",
      "three-layer Loss after iteration 40000: 0.7375829464924276\n",
      "three-layer Loss after iteration 41000: 0.7290886511826057\n",
      "three-layer Loss after iteration 42000: 0.7200038158520957\n",
      "three-layer Loss after iteration 43000: 0.7151279362842277\n",
      "three-layer Loss after iteration 44000: 0.7101085287175356\n",
      "three-layer Loss after iteration 45000: 0.7051124987040688\n",
      "three-layer Loss after iteration 46000: 0.7004472839554357\n",
      "three-layer Loss after iteration 47000: 0.6963583953874984\n",
      "three-layer Loss after iteration 48000: 0.6930762466896583\n",
      "three-layer Loss after iteration 49000: 0.6909602249176985\n",
      "three-layer Loss after iteration 50000: 0.6874948590504698\n",
      "three-layer Loss after iteration 51000: 0.6809849386417528\n",
      "three-layer Loss after iteration 52000: 0.6739197229662921\n",
      "three-layer Loss after iteration 53000: 0.6675108308387748\n",
      "three-layer Loss after iteration 54000: 0.6647852897914591\n",
      "three-layer Loss after iteration 55000: 0.6630294501227821\n",
      "three-layer Loss after iteration 56000: 0.6593707151324023\n",
      "three-layer Loss after iteration 57000: 0.6577317756354665\n",
      "three-layer Loss after iteration 58000: 0.6548036945294533\n",
      "three-layer Loss after iteration 59000: 0.6536962091773284\n",
      "three-layer Loss after iteration 60000: 0.6520539043802569\n",
      "three-layer Loss after iteration 61000: 0.6496771006253973\n",
      "three-layer Loss after iteration 62000: 0.635506438933929\n",
      "three-layer Loss after iteration 63000: 0.6527404081672469\n",
      "three-layer Loss after iteration 64000: 0.6467270968465971\n",
      "three-layer Loss after iteration 65000: 0.6404712855089474\n",
      "three-layer Loss after iteration 66000: 0.6369005833795564\n",
      "three-layer Loss after iteration 67000: 0.6420174536444432\n",
      "three-layer Loss after iteration 68000: 0.6341390453044541\n",
      "three-layer Loss after iteration 69000: 0.6275106144997905\n",
      "three-layer Loss after iteration 70000: 0.6243224360815286\n",
      "three-layer Loss after iteration 71000: 0.6240807251579015\n",
      "three-layer Loss after iteration 72000: 0.6209476290319835\n",
      "three-layer Loss after iteration 73000: 0.6180852804591326\n",
      "three-layer Loss after iteration 74000: 0.6034872314046824\n",
      "three-layer Loss after iteration 75000: 0.6095527024867098\n",
      "three-layer Loss after iteration 76000: 0.6104251539955469\n",
      "three-layer Loss after iteration 77000: 0.5606603492381081\n",
      "three-layer Loss after iteration 78000: 0.6018002251617978\n",
      "three-layer Loss after iteration 79000: 0.5970541906141089\n",
      "three-layer Loss after iteration 80000: 0.5951208558874068\n",
      "three-layer Loss after iteration 81000: 0.592176473984377\n",
      "three-layer Loss after iteration 82000: 0.5954357330430674\n",
      "three-layer Loss after iteration 83000: 0.5846089719900506\n",
      "three-layer Loss after iteration 84000: 0.6035068482010804\n",
      "three-layer Loss after iteration 85000: 0.587771305483762\n",
      "three-layer Loss after iteration 86000: 0.6190062005852949\n",
      "three-layer Loss after iteration 87000: 0.5869786190959685\n",
      "three-layer Loss after iteration 88000: 0.5911322244736521\n",
      "three-layer Loss after iteration 89000: 0.589337409744585\n",
      "three-layer Loss after iteration 90000: 0.5893927748560615\n",
      "9.39446750892803e-05 0.589337409744585 0.5893927748560615\n",
      "three-layer Loss after iteration 0: 1569.803316730812\n",
      "three-layer Loss after iteration 1000: 9.688466695446785\n",
      "three-layer Loss after iteration 2000: 7.209440350339117\n",
      "three-layer Loss after iteration 3000: 6.718844438563646\n",
      "three-layer Loss after iteration 4000: 5.9525226764440005\n",
      "three-layer Loss after iteration 5000: 5.0072669369382865\n",
      "three-layer Loss after iteration 6000: 3.5982056074806303\n",
      "three-layer Loss after iteration 7000: 3.323120035424079\n",
      "three-layer Loss after iteration 8000: 3.0694135713898194\n",
      "three-layer Loss after iteration 9000: 2.3698299793809006\n",
      "three-layer Loss after iteration 10000: 1.5955318376457057\n",
      "three-layer Loss after iteration 11000: 1.1109857580194302\n",
      "three-layer Loss after iteration 12000: 1.39537168463202\n",
      "three-layer Loss after iteration 13000: 1.3473630908335583\n",
      "three-layer Loss after iteration 14000: 1.058876248366629\n",
      "three-layer Loss after iteration 15000: 1.1696585427591986\n",
      "three-layer Loss after iteration 16000: 0.9830098084131376\n",
      "three-layer Loss after iteration 17000: 0.9298590469087902\n",
      "three-layer Loss after iteration 18000: 0.9480715351958025\n",
      "three-layer Loss after iteration 19000: 0.7945903166212263\n",
      "three-layer Loss after iteration 20000: 0.7437670030612636\n",
      "three-layer Loss after iteration 21000: 0.6625685711312312\n",
      "three-layer Loss after iteration 22000: 0.835560076740477\n",
      "three-layer Loss after iteration 23000: 0.5946079329022916\n",
      "three-layer Loss after iteration 24000: 0.6393335126617831\n",
      "three-layer Loss after iteration 25000: 0.6707834563962984\n",
      "three-layer Loss after iteration 26000: 0.9626348442010606\n",
      "three-layer Loss after iteration 27000: 0.6102435590455022\n",
      "three-layer Loss after iteration 28000: 0.6940372997504329\n",
      "three-layer Loss after iteration 29000: 0.7058015345264016\n",
      "three-layer Loss after iteration 30000: 0.7045573222953236\n",
      "three-layer Loss after iteration 31000: 0.634029086706531\n",
      "three-layer Loss after iteration 32000: 0.6185381985105152\n",
      "three-layer Loss after iteration 33000: 0.5979018314799076\n",
      "three-layer Loss after iteration 34000: 0.5869602265794932\n",
      "three-layer Loss after iteration 35000: 0.5496762546123896\n",
      "three-layer Loss after iteration 36000: 0.543794304000661\n",
      "three-layer Loss after iteration 37000: 0.5386709976236094\n",
      "three-layer Loss after iteration 38000: 0.5248692297761016\n",
      "three-layer Loss after iteration 39000: 0.5362512531158665\n",
      "three-layer Loss after iteration 40000: 0.5305279497151079\n",
      "three-layer Loss after iteration 41000: 0.526408265597372\n",
      "three-layer Loss after iteration 42000: 0.5211797063234747\n",
      "three-layer Loss after iteration 43000: 0.5175001413410247\n",
      "three-layer Loss after iteration 44000: 0.512758245751079\n",
      "three-layer Loss after iteration 45000: 0.5084454135854475\n",
      "three-layer Loss after iteration 46000: 0.5076844658630122\n",
      "three-layer Loss after iteration 47000: 0.5087575512795568\n",
      "three-layer Loss after iteration 48000: 0.5070073682350983\n",
      "three-layer Loss after iteration 49000: 0.5108936190087913\n",
      "three-layer Loss after iteration 50000: 0.5111958854489292\n",
      "three-layer Loss after iteration 51000: 0.5094646943653955\n",
      "three-layer Loss after iteration 52000: 0.5126810810540785\n",
      "three-layer Loss after iteration 53000: 0.5104944935760147\n",
      "three-layer Loss after iteration 54000: 0.511494204619035\n",
      "three-layer Loss after iteration 55000: 0.5111830188443385\n",
      "three-layer Loss after iteration 56000: 0.5140601080693898\n",
      "three-layer Loss after iteration 57000: 0.5074129362673039\n",
      "three-layer Loss after iteration 58000: 0.502759334105169\n",
      "three-layer Loss after iteration 59000: 0.4981092058924945\n",
      "three-layer Loss after iteration 60000: 0.531845761327253\n",
      "three-layer Loss after iteration 61000: 0.5060563408101973\n",
      "three-layer Loss after iteration 62000: 0.49621303795842886\n",
      "three-layer Loss after iteration 63000: 0.49439771770943985\n",
      "three-layer Loss after iteration 64000: 0.5017986424397399\n",
      "three-layer Loss after iteration 65000: 0.4939048376933041\n",
      "three-layer Loss after iteration 66000: 0.49275359104505767\n",
      "three-layer Loss after iteration 67000: 0.42249731836791665\n",
      "three-layer Loss after iteration 68000: 0.5631997979636143\n",
      "three-layer Loss after iteration 69000: 0.5410666045742434\n",
      "three-layer Loss after iteration 70000: 0.5228449702257184\n",
      "three-layer Loss after iteration 71000: 0.5329818252417844\n",
      "three-layer Loss after iteration 72000: 0.5073526948728896\n",
      "three-layer Loss after iteration 73000: 0.474700706689638\n",
      "three-layer Loss after iteration 74000: 0.48119374702548473\n",
      "three-layer Loss after iteration 75000: 0.5001712414291165\n",
      "three-layer Loss after iteration 76000: 0.4852656161141932\n",
      "three-layer Loss after iteration 77000: 0.4774232701596304\n",
      "three-layer Loss after iteration 78000: 0.4878686559190033\n",
      "three-layer Loss after iteration 79000: 0.4758797270635135\n",
      "three-layer Loss after iteration 80000: 0.46836631896834746\n",
      "three-layer Loss after iteration 81000: 0.46764851406570024\n",
      "three-layer Loss after iteration 82000: 0.4663664740767155\n",
      "three-layer Loss after iteration 83000: 0.46397524955697594\n",
      "three-layer Loss after iteration 84000: 0.46104429333043523\n",
      "three-layer Loss after iteration 85000: 0.4535940548229836\n",
      "three-layer Loss after iteration 86000: 0.4584808913240239\n",
      "three-layer Loss after iteration 87000: 0.45561048588569464\n",
      "three-layer Loss after iteration 88000: 0.4519397709583989\n",
      "three-layer Loss after iteration 89000: 0.4487728163993777\n",
      "three-layer Loss after iteration 90000: 0.5734190094639898\n",
      "three-layer Loss after iteration 91000: 0.40285861482525587\n",
      "three-layer Loss after iteration 92000: 0.40502238637073107\n",
      "three-layer Loss after iteration 93000: 0.43750077940567017\n",
      "three-layer Loss after iteration 94000: 0.4598561534528392\n",
      "three-layer Loss after iteration 95000: 0.4549296138875107\n",
      "three-layer Loss after iteration 96000: 0.42646116699234743\n",
      "three-layer Loss after iteration 97000: 0.408139510904531\n",
      "three-layer Loss after iteration 98000: 0.4060068136452532\n",
      "three-layer Loss after iteration 99000: 0.41223892297951314\n",
      "three-layer Loss after iteration 0: 1731.0870055862213\n",
      "three-layer Loss after iteration 1000: 9.544437692124273\n",
      "three-layer Loss after iteration 2000: 8.563007415652391\n",
      "three-layer Loss after iteration 3000: 4.919781142161321\n",
      "three-layer Loss after iteration 4000: 3.1933330257361194\n",
      "three-layer Loss after iteration 5000: 2.1980777041866757\n",
      "three-layer Loss after iteration 6000: 2.177801758425386\n",
      "three-layer Loss after iteration 7000: 1.542786677727369\n",
      "three-layer Loss after iteration 8000: 1.4775513079740348\n",
      "three-layer Loss after iteration 9000: 1.5944180554329568\n",
      "three-layer Loss after iteration 10000: 1.4760316485233707\n",
      "three-layer Loss after iteration 11000: 1.6741678631891839\n",
      "three-layer Loss after iteration 12000: 1.3460403924761073\n",
      "three-layer Loss after iteration 13000: 1.2880383909696447\n",
      "three-layer Loss after iteration 14000: 1.162532090198438\n",
      "three-layer Loss after iteration 15000: 1.3743141587686518\n",
      "three-layer Loss after iteration 16000: 1.209797676082122\n",
      "three-layer Loss after iteration 17000: 1.3433375754211905\n",
      "three-layer Loss after iteration 18000: 1.108663254420947\n",
      "three-layer Loss after iteration 19000: 1.4011583109086925\n",
      "three-layer Loss after iteration 20000: 1.2266608463242537\n",
      "three-layer Loss after iteration 21000: 1.1790091877517899\n",
      "three-layer Loss after iteration 22000: 1.2173878298804361\n",
      "three-layer Loss after iteration 23000: 1.1257224026478116\n",
      "three-layer Loss after iteration 24000: 1.1711147015086658\n",
      "three-layer Loss after iteration 25000: 1.158979032462335\n",
      "three-layer Loss after iteration 26000: 1.1144079240268205\n",
      "three-layer Loss after iteration 27000: 1.0332801802454943\n",
      "three-layer Loss after iteration 28000: 0.9721713406992081\n",
      "three-layer Loss after iteration 29000: 1.0613838806048375\n",
      "three-layer Loss after iteration 30000: 1.065120767762922\n",
      "three-layer Loss after iteration 31000: 1.0572502279977787\n",
      "three-layer Loss after iteration 32000: 1.01881819705394\n",
      "three-layer Loss after iteration 33000: 1.0176739634218843\n",
      "three-layer Loss after iteration 34000: 1.1016371821573345\n",
      "three-layer Loss after iteration 35000: 0.9659639963542325\n",
      "three-layer Loss after iteration 36000: 0.9841618342633862\n",
      "three-layer Loss after iteration 37000: 1.0261774348023278\n",
      "three-layer Loss after iteration 38000: 0.8439399452378796\n",
      "three-layer Loss after iteration 39000: 0.8876569332271634\n",
      "three-layer Loss after iteration 40000: 0.9009183873648928\n",
      "three-layer Loss after iteration 41000: 0.8391094887259397\n",
      "three-layer Loss after iteration 42000: 0.8805015218209951\n",
      "three-layer Loss after iteration 43000: 0.7927053918305424\n",
      "three-layer Loss after iteration 44000: 0.7270082432689072\n",
      "three-layer Loss after iteration 45000: 0.7318992526640492\n",
      "three-layer Loss after iteration 46000: 1.0951784130983706\n",
      "three-layer Loss after iteration 47000: 0.7165499740218365\n",
      "three-layer Loss after iteration 48000: 1.1805161965934634\n",
      "three-layer Loss after iteration 49000: 0.7847407292377817\n",
      "three-layer Loss after iteration 50000: 0.8263807058411766\n",
      "three-layer Loss after iteration 51000: 0.934334186818922\n",
      "three-layer Loss after iteration 52000: 0.869234477428823\n",
      "three-layer Loss after iteration 53000: 0.8254442455043649\n",
      "three-layer Loss after iteration 54000: 0.8406603908986252\n",
      "three-layer Loss after iteration 55000: 0.8571769455084127\n",
      "three-layer Loss after iteration 56000: 0.8453968563758876\n",
      "three-layer Loss after iteration 57000: 0.8372772566648357\n",
      "three-layer Loss after iteration 58000: 0.8286996321634132\n",
      "three-layer Loss after iteration 59000: 0.8178032342187357\n",
      "three-layer Loss after iteration 60000: 0.8098926971160353\n",
      "three-layer Loss after iteration 61000: 0.8044621628074666\n",
      "three-layer Loss after iteration 62000: 0.7933394911007959\n",
      "three-layer Loss after iteration 63000: 0.7850629367915917\n",
      "three-layer Loss after iteration 64000: 0.787030353025686\n",
      "three-layer Loss after iteration 65000: 0.7890863506660839\n",
      "three-layer Loss after iteration 66000: 0.7478296327605016\n",
      "three-layer Loss after iteration 67000: 0.7516454662131459\n",
      "three-layer Loss after iteration 68000: 0.7615744565696697\n",
      "three-layer Loss after iteration 69000: 0.7717211148548024\n",
      "three-layer Loss after iteration 70000: 0.7585711759628031\n",
      "three-layer Loss after iteration 71000: 0.72605629730999\n",
      "three-layer Loss after iteration 72000: 0.72071031198862\n",
      "three-layer Loss after iteration 73000: 0.7376032309075817\n",
      "three-layer Loss after iteration 74000: 0.7014216765882291\n",
      "three-layer Loss after iteration 75000: 0.7135430195645095\n",
      "three-layer Loss after iteration 76000: 0.7028177146119922\n",
      "three-layer Loss after iteration 77000: 0.7041848633761834\n",
      "three-layer Loss after iteration 78000: 0.7018882734656077\n",
      "three-layer Loss after iteration 79000: 0.7245358928688747\n",
      "three-layer Loss after iteration 80000: 0.7156724848544898\n",
      "three-layer Loss after iteration 81000: 0.746580913129681\n",
      "three-layer Loss after iteration 82000: 0.7229251283900964\n",
      "three-layer Loss after iteration 83000: 0.7063427485687086\n",
      "three-layer Loss after iteration 84000: 0.6953731688055955\n",
      "three-layer Loss after iteration 85000: 0.685321919950795\n",
      "three-layer Loss after iteration 86000: 0.6971495539449423\n",
      "three-layer Loss after iteration 87000: 0.6623522472851808\n",
      "three-layer Loss after iteration 88000: 0.6680103502901081\n",
      "three-layer Loss after iteration 89000: 0.6505221968740283\n",
      "three-layer Loss after iteration 90000: 0.6530621732968436\n",
      "three-layer Loss after iteration 91000: 0.6443458126913887\n",
      "three-layer Loss after iteration 92000: 0.6504098318036893\n",
      "three-layer Loss after iteration 93000: 0.6328051586244577\n",
      "three-layer Loss after iteration 94000: 0.593994254907894\n",
      "three-layer Loss after iteration 95000: 0.5597854731136999\n",
      "three-layer Loss after iteration 96000: 0.5857556463254774\n",
      "three-layer Loss after iteration 97000: 0.6367378017146335\n",
      "three-layer Loss after iteration 98000: 0.5513095171362876\n",
      "three-layer Loss after iteration 99000: 0.5238446535273219\n",
      "three-layer Loss after iteration 0: 1582.38355919691\n",
      "three-layer Loss after iteration 1000: 11.367000451821816\n",
      "three-layer Loss after iteration 2000: 6.446328606743146\n",
      "three-layer Loss after iteration 3000: 5.59253091843557\n",
      "three-layer Loss after iteration 4000: 6.032730314326266\n",
      "three-layer Loss after iteration 5000: 4.147420200040197\n",
      "three-layer Loss after iteration 6000: 3.7361423429522205\n",
      "three-layer Loss after iteration 7000: 3.912430623730677\n",
      "three-layer Loss after iteration 8000: 3.302314927817086\n",
      "three-layer Loss after iteration 9000: 2.9401525647502487\n",
      "three-layer Loss after iteration 10000: 2.745842226218564\n",
      "three-layer Loss after iteration 11000: 2.3971758412594757\n",
      "three-layer Loss after iteration 12000: 3.167086367953266\n",
      "three-layer Loss after iteration 13000: 2.848698113682511\n",
      "three-layer Loss after iteration 14000: 2.465185766982752\n",
      "three-layer Loss after iteration 15000: 2.3123541150557605\n",
      "three-layer Loss after iteration 16000: 2.1346053426029035\n",
      "three-layer Loss after iteration 17000: 2.0645024803394674\n",
      "three-layer Loss after iteration 18000: 2.185551990720867\n",
      "three-layer Loss after iteration 19000: 2.0632494268113617\n",
      "three-layer Loss after iteration 20000: 1.4493360024844983\n",
      "three-layer Loss after iteration 21000: 1.378681226440653\n",
      "three-layer Loss after iteration 22000: 2.0163987687275697\n",
      "three-layer Loss after iteration 23000: 1.886228869057261\n",
      "three-layer Loss after iteration 24000: 1.6081287277771528\n",
      "three-layer Loss after iteration 25000: 1.57413453302715\n",
      "three-layer Loss after iteration 26000: 1.1701316066040794\n",
      "three-layer Loss after iteration 27000: 1.4755391679014243\n",
      "three-layer Loss after iteration 28000: 2.1989141730861634\n",
      "three-layer Loss after iteration 29000: 1.3175498917140849\n",
      "three-layer Loss after iteration 30000: 1.6197009469065213\n",
      "three-layer Loss after iteration 31000: 1.3424538496669662\n",
      "three-layer Loss after iteration 32000: 1.2235127857925203\n",
      "three-layer Loss after iteration 33000: 1.1786141352147843\n",
      "three-layer Loss after iteration 34000: 1.2196036815006432\n",
      "three-layer Loss after iteration 35000: 1.160644105966404\n",
      "three-layer Loss after iteration 36000: 1.106445497181901\n",
      "three-layer Loss after iteration 37000: 1.1413336202224986\n",
      "three-layer Loss after iteration 38000: 1.0751848937116257\n",
      "three-layer Loss after iteration 39000: 1.0652181302500203\n",
      "three-layer Loss after iteration 40000: 1.000883300518203\n",
      "three-layer Loss after iteration 41000: 1.0253356015578547\n",
      "three-layer Loss after iteration 42000: 0.9692700678519082\n",
      "three-layer Loss after iteration 43000: 0.9704616991342807\n",
      "three-layer Loss after iteration 44000: 0.9679913640614809\n",
      "three-layer Loss after iteration 45000: 0.9538841921536505\n",
      "three-layer Loss after iteration 46000: 0.9573866660865714\n",
      "three-layer Loss after iteration 47000: 0.9476990660689277\n",
      "three-layer Loss after iteration 48000: 0.9986408587016093\n",
      "three-layer Loss after iteration 49000: 0.9710158549563458\n",
      "three-layer Loss after iteration 50000: 0.9479778620617315\n",
      "three-layer Loss after iteration 51000: 1.1113252142036176\n",
      "three-layer Loss after iteration 52000: 0.9894089004630021\n",
      "three-layer Loss after iteration 53000: 0.9042616691079493\n",
      "three-layer Loss after iteration 54000: 0.9046827048323679\n",
      "three-layer Loss after iteration 55000: 0.8987023503713957\n",
      "three-layer Loss after iteration 56000: 1.0208592804647934\n",
      "three-layer Loss after iteration 57000: 0.9842043127247262\n",
      "three-layer Loss after iteration 58000: 0.8949814251347041\n",
      "three-layer Loss after iteration 59000: 1.0027257000938383\n",
      "three-layer Loss after iteration 60000: 0.9010292879609524\n",
      "three-layer Loss after iteration 61000: 0.9145023241114242\n",
      "three-layer Loss after iteration 62000: 0.9329781806854281\n",
      "three-layer Loss after iteration 63000: 0.9533646132085555\n",
      "three-layer Loss after iteration 64000: 0.9014985330402497\n",
      "three-layer Loss after iteration 65000: 0.925670614842104\n",
      "three-layer Loss after iteration 66000: 0.9159517645567757\n",
      "three-layer Loss after iteration 67000: 0.9669308983375783\n",
      "three-layer Loss after iteration 68000: 0.8889482036763044\n",
      "three-layer Loss after iteration 69000: 0.9632243898892936\n",
      "three-layer Loss after iteration 70000: 0.8995527909752711\n",
      "three-layer Loss after iteration 71000: 0.9318420738910407\n",
      "three-layer Loss after iteration 72000: 0.8995098955267484\n",
      "three-layer Loss after iteration 73000: 0.9139060948830704\n",
      "three-layer Loss after iteration 74000: 0.9394821011223997\n",
      "three-layer Loss after iteration 75000: 0.9181070698000446\n",
      "three-layer Loss after iteration 76000: 0.8917663267599538\n",
      "three-layer Loss after iteration 77000: 0.889911206287843\n",
      "three-layer Loss after iteration 78000: 0.9281477138855415\n",
      "three-layer Loss after iteration 79000: 0.9309583851999818\n",
      "three-layer Loss after iteration 80000: 0.8954209483940837\n",
      "three-layer Loss after iteration 81000: 0.878943861151745\n",
      "three-layer Loss after iteration 82000: 0.8805961427167804\n",
      "three-layer Loss after iteration 83000: 0.9063091534863356\n",
      "three-layer Loss after iteration 84000: 0.8471791469148449\n",
      "three-layer Loss after iteration 85000: 0.8846166384107437\n",
      "three-layer Loss after iteration 86000: 0.9392551432170015\n",
      "three-layer Loss after iteration 87000: 0.8554400741317655\n",
      "three-layer Loss after iteration 88000: 0.878934476836911\n",
      "three-layer Loss after iteration 89000: 0.8728062970790315\n",
      "three-layer Loss after iteration 90000: 0.9503400077394873\n",
      "three-layer Loss after iteration 91000: 0.8692384813969576\n",
      "three-layer Loss after iteration 92000: 0.850127406717525\n",
      "three-layer Loss after iteration 93000: 0.866771208301102\n",
      "three-layer Loss after iteration 94000: 0.8845393216556369\n",
      "three-layer Loss after iteration 95000: 0.8627332247026286\n",
      "three-layer Loss after iteration 96000: 0.8544955364698291\n",
      "three-layer Loss after iteration 97000: 0.8733994640830454\n",
      "three-layer Loss after iteration 98000: 0.8746707594537149\n",
      "three-layer Loss after iteration 99000: 0.9634407790636769\n",
      "three-layer Loss after iteration 0: 1691.9106528212833\n",
      "three-layer Loss after iteration 1000: 10.694715096560838\n",
      "three-layer Loss after iteration 2000: 7.116073908518069\n",
      "three-layer Loss after iteration 3000: 6.177384732354344\n",
      "three-layer Loss after iteration 4000: 4.619504806100483\n",
      "three-layer Loss after iteration 5000: 4.328214402194048\n",
      "three-layer Loss after iteration 6000: 4.062251905705139\n",
      "three-layer Loss after iteration 7000: 3.232760674013863\n",
      "three-layer Loss after iteration 8000: 2.875011270595628\n",
      "three-layer Loss after iteration 9000: 3.0634952031780482\n",
      "three-layer Loss after iteration 10000: 2.2773512609171136\n",
      "three-layer Loss after iteration 11000: 2.4299732770880116\n",
      "three-layer Loss after iteration 12000: 1.9269427416715537\n",
      "three-layer Loss after iteration 13000: 1.5782343773054874\n",
      "three-layer Loss after iteration 14000: 2.1506499347587584\n",
      "three-layer Loss after iteration 15000: 1.7761126014649218\n",
      "three-layer Loss after iteration 16000: 1.7773261157240967\n",
      "three-layer Loss after iteration 17000: 1.4074091614750184\n",
      "three-layer Loss after iteration 18000: 1.924575764015671\n",
      "three-layer Loss after iteration 19000: 1.7078162164974007\n",
      "three-layer Loss after iteration 20000: 1.3867040216930686\n",
      "three-layer Loss after iteration 21000: 1.0397179180608092\n",
      "three-layer Loss after iteration 22000: 0.86721934694433\n",
      "three-layer Loss after iteration 23000: 1.1802173496784765\n",
      "three-layer Loss after iteration 24000: 1.2149033231789637\n",
      "three-layer Loss after iteration 25000: 1.0373084151461616\n",
      "three-layer Loss after iteration 26000: 1.6800403479441792\n",
      "three-layer Loss after iteration 27000: 0.990870037448994\n",
      "three-layer Loss after iteration 28000: 1.295423967452557\n",
      "three-layer Loss after iteration 29000: 1.0671290974858985\n",
      "three-layer Loss after iteration 30000: 1.1003626238974231\n",
      "three-layer Loss after iteration 31000: 0.9911450938996628\n",
      "three-layer Loss after iteration 32000: 1.3718271856376218\n",
      "three-layer Loss after iteration 33000: 0.9054147688576912\n",
      "three-layer Loss after iteration 34000: 0.7216063475734373\n",
      "three-layer Loss after iteration 35000: 0.9798066521284192\n",
      "three-layer Loss after iteration 36000: 0.8525122705792649\n",
      "three-layer Loss after iteration 37000: 0.9351041843179635\n",
      "three-layer Loss after iteration 38000: 0.6135592224037355\n",
      "three-layer Loss after iteration 39000: 0.724278162022182\n",
      "three-layer Loss after iteration 40000: 0.9530456288251608\n",
      "three-layer Loss after iteration 41000: 0.9461890010661262\n",
      "three-layer Loss after iteration 42000: 0.8540095296357231\n",
      "three-layer Loss after iteration 43000: 1.2598949648461408\n",
      "three-layer Loss after iteration 44000: 0.9130138390086872\n",
      "three-layer Loss after iteration 45000: 0.8820094292062147\n",
      "three-layer Loss after iteration 46000: 0.8208528954875831\n",
      "three-layer Loss after iteration 47000: 0.8702503684981673\n",
      "three-layer Loss after iteration 48000: 0.6497977619515865\n",
      "three-layer Loss after iteration 49000: 0.8866334543895935\n",
      "three-layer Loss after iteration 50000: 0.5423529420340586\n",
      "three-layer Loss after iteration 51000: 0.8137541635305785\n",
      "three-layer Loss after iteration 52000: 0.9661535885884288\n",
      "three-layer Loss after iteration 53000: 1.243571014377977\n",
      "three-layer Loss after iteration 54000: 0.5497292250012911\n",
      "three-layer Loss after iteration 55000: 0.5440068676369526\n",
      "three-layer Loss after iteration 56000: 1.3044425600130625\n",
      "three-layer Loss after iteration 57000: 0.5934949118152144\n",
      "three-layer Loss after iteration 58000: 0.6501583465003361\n",
      "three-layer Loss after iteration 59000: 0.4298203839953809\n",
      "three-layer Loss after iteration 60000: 1.2412546459928995\n",
      "three-layer Loss after iteration 61000: 0.8162679266314214\n",
      "three-layer Loss after iteration 62000: 0.603419693468345\n",
      "three-layer Loss after iteration 63000: 0.47800476457466246\n",
      "three-layer Loss after iteration 64000: 0.5810693590902646\n",
      "three-layer Loss after iteration 65000: 0.5398910760960035\n",
      "three-layer Loss after iteration 66000: 0.8569852091131002\n",
      "three-layer Loss after iteration 67000: 1.0291211496129646\n",
      "three-layer Loss after iteration 68000: 0.7750083238069957\n",
      "three-layer Loss after iteration 69000: 1.1129308630016648\n",
      "three-layer Loss after iteration 70000: 0.7965561077978336\n",
      "three-layer Loss after iteration 71000: 0.6294805360118036\n",
      "three-layer Loss after iteration 72000: 0.4559501031859384\n",
      "three-layer Loss after iteration 73000: 0.905335993634604\n",
      "three-layer Loss after iteration 74000: 0.7781739328695693\n",
      "three-layer Loss after iteration 75000: 0.576091530147045\n",
      "three-layer Loss after iteration 76000: 0.6180782921773107\n",
      "three-layer Loss after iteration 77000: 0.6275445647895094\n",
      "three-layer Loss after iteration 78000: 0.6296026058712377\n",
      "three-layer Loss after iteration 79000: 0.626446279145109\n",
      "three-layer Loss after iteration 80000: 0.620749751996044\n",
      "three-layer Loss after iteration 81000: 0.6145867304803101\n",
      "three-layer Loss after iteration 82000: 0.6079821509745214\n",
      "three-layer Loss after iteration 83000: 0.6024215300563608\n",
      "three-layer Loss after iteration 84000: 0.5986267075619197\n",
      "three-layer Loss after iteration 85000: 0.5884597832641055\n",
      "three-layer Loss after iteration 86000: 0.5850242905911217\n",
      "three-layer Loss after iteration 87000: 0.5691588493637361\n",
      "three-layer Loss after iteration 88000: 0.49338360938664805\n",
      "three-layer Loss after iteration 89000: 0.438987180221608\n",
      "three-layer Loss after iteration 90000: 0.49897131428391983\n",
      "three-layer Loss after iteration 91000: 0.5979076365228527\n",
      "three-layer Loss after iteration 92000: 0.6265467649489934\n",
      "three-layer Loss after iteration 93000: 0.5139549402962966\n",
      "three-layer Loss after iteration 94000: 0.43861088012437743\n",
      "three-layer Loss after iteration 95000: 0.41645246116968665\n",
      "three-layer Loss after iteration 96000: 0.42624289456507414\n",
      "three-layer Loss after iteration 97000: 0.6749216254652897\n",
      "three-layer Loss after iteration 98000: 0.672191515907558\n",
      "three-layer Loss after iteration 99000: 0.4310452184772874\n",
      "three-layer Loss after iteration 0: 1653.3790964688205\n",
      "three-layer Loss after iteration 1000: 13.88460662085286\n",
      "three-layer Loss after iteration 2000: 7.562909015044363\n",
      "three-layer Loss after iteration 3000: 6.666456365477028\n",
      "three-layer Loss after iteration 4000: 17.05173419581856\n",
      "three-layer Loss after iteration 5000: 4.50755588314949\n",
      "three-layer Loss after iteration 6000: 2.943737857912141\n",
      "three-layer Loss after iteration 7000: 2.4776682946294684\n",
      "three-layer Loss after iteration 8000: 2.1985194291570003\n",
      "three-layer Loss after iteration 9000: 2.017934932800954\n",
      "three-layer Loss after iteration 10000: 1.7762226503014102\n",
      "three-layer Loss after iteration 11000: 1.4909538306734305\n",
      "three-layer Loss after iteration 12000: 1.7880730793961939\n",
      "three-layer Loss after iteration 13000: 1.5566106258807626\n",
      "three-layer Loss after iteration 14000: 1.464929890737291\n",
      "three-layer Loss after iteration 15000: 1.3886474338969215\n",
      "three-layer Loss after iteration 16000: 1.470460815235331\n",
      "three-layer Loss after iteration 17000: 1.2760769520096935\n",
      "three-layer Loss after iteration 18000: 1.2979256228610478\n",
      "three-layer Loss after iteration 19000: 1.0968569405562096\n",
      "three-layer Loss after iteration 20000: 1.0026208319291467\n",
      "three-layer Loss after iteration 21000: 1.1194792440079755\n",
      "three-layer Loss after iteration 22000: 1.185223508359359\n",
      "three-layer Loss after iteration 23000: 0.8836187553889377\n",
      "three-layer Loss after iteration 24000: 1.0725177979313985\n",
      "three-layer Loss after iteration 25000: 1.4518588362552207\n",
      "three-layer Loss after iteration 26000: 0.8334890516481662\n",
      "three-layer Loss after iteration 27000: 0.8742330608181718\n",
      "three-layer Loss after iteration 28000: 1.0564804438840638\n",
      "three-layer Loss after iteration 29000: 0.8915009361131764\n",
      "three-layer Loss after iteration 30000: 0.8784643147040436\n",
      "three-layer Loss after iteration 31000: 0.8711364207207561\n",
      "three-layer Loss after iteration 32000: 0.8303122348384285\n",
      "three-layer Loss after iteration 33000: 0.7951371931902756\n",
      "three-layer Loss after iteration 34000: 0.7637994650382319\n",
      "three-layer Loss after iteration 35000: 0.7771908829161975\n",
      "three-layer Loss after iteration 36000: 0.9802899112083294\n",
      "three-layer Loss after iteration 37000: 0.6795549640443854\n",
      "three-layer Loss after iteration 38000: 0.936582205548724\n",
      "three-layer Loss after iteration 39000: 0.6298318235042368\n",
      "three-layer Loss after iteration 40000: 0.6698918099078515\n",
      "three-layer Loss after iteration 41000: 0.6542035363513625\n",
      "three-layer Loss after iteration 42000: 0.544787797766065\n",
      "three-layer Loss after iteration 43000: 0.6585236046268218\n",
      "three-layer Loss after iteration 44000: 0.6663045507672777\n",
      "three-layer Loss after iteration 45000: 0.6323060583367754\n",
      "three-layer Loss after iteration 46000: 0.6363809844005046\n",
      "three-layer Loss after iteration 47000: 0.6276686006277277\n",
      "three-layer Loss after iteration 48000: 0.618719237645866\n",
      "three-layer Loss after iteration 49000: 0.6051968106974406\n",
      "three-layer Loss after iteration 50000: 0.6008802680833145\n",
      "three-layer Loss after iteration 51000: 0.5954123826126341\n",
      "three-layer Loss after iteration 52000: 0.5875261890376922\n",
      "three-layer Loss after iteration 53000: 0.5790143455993232\n",
      "three-layer Loss after iteration 54000: 0.5705248373839407\n",
      "three-layer Loss after iteration 55000: 0.5627018609624252\n",
      "three-layer Loss after iteration 56000: 0.5632831780775168\n",
      "three-layer Loss after iteration 57000: 0.5295343934965008\n",
      "three-layer Loss after iteration 58000: 0.5839806782021295\n",
      "three-layer Loss after iteration 59000: 0.5347702625194918\n",
      "three-layer Loss after iteration 60000: 0.5328520594650078\n",
      "three-layer Loss after iteration 61000: 0.5306552285199932\n",
      "three-layer Loss after iteration 62000: 0.5234976611567287\n",
      "three-layer Loss after iteration 63000: 0.4305716828973314\n",
      "three-layer Loss after iteration 64000: 0.4366101016706112\n",
      "three-layer Loss after iteration 65000: 0.43442112371080094\n",
      "three-layer Loss after iteration 66000: 0.6549674168969574\n",
      "three-layer Loss after iteration 67000: 0.45983170820326663\n",
      "three-layer Loss after iteration 68000: 0.46608963016387267\n",
      "three-layer Loss after iteration 69000: 0.483500329716054\n",
      "three-layer Loss after iteration 70000: 0.4970293467776125\n",
      "three-layer Loss after iteration 71000: 0.4398005818048048\n",
      "three-layer Loss after iteration 72000: 0.4765102222369824\n",
      "three-layer Loss after iteration 73000: 0.4816931599532973\n",
      "three-layer Loss after iteration 74000: 0.5100955711846769\n",
      "three-layer Loss after iteration 75000: 0.48194186288054514\n",
      "three-layer Loss after iteration 76000: 0.4817774098185957\n",
      "three-layer Loss after iteration 77000: 0.47719931183563064\n",
      "three-layer Loss after iteration 78000: 0.47078911688811254\n",
      "three-layer Loss after iteration 79000: 0.46742042304373804\n",
      "three-layer Loss after iteration 80000: 0.38996596574866077\n",
      "three-layer Loss after iteration 81000: 0.4130716768394028\n",
      "three-layer Loss after iteration 82000: 0.46403658127755093\n",
      "three-layer Loss after iteration 83000: 0.4698394928733263\n",
      "three-layer Loss after iteration 84000: 0.46051161888400455\n",
      "three-layer Loss after iteration 85000: 0.4491280124962216\n",
      "three-layer Loss after iteration 86000: 0.4384426322222463\n",
      "three-layer Loss after iteration 87000: 0.4318402008236362\n",
      "three-layer Loss after iteration 88000: 0.4283362693215153\n",
      "three-layer Loss after iteration 89000: 0.42743589643705365\n",
      "three-layer Loss after iteration 90000: 0.42149052309011154\n",
      "three-layer Loss after iteration 91000: 0.42339260008488877\n",
      "three-layer Loss after iteration 92000: 0.4199767030211375\n",
      "three-layer Loss after iteration 93000: 0.4187087185857331\n",
      "three-layer Loss after iteration 94000: 0.41666589335781856\n",
      "three-layer Loss after iteration 95000: 0.41336285012866414\n",
      "three-layer Loss after iteration 96000: 0.4114563578941155\n",
      "three-layer Loss after iteration 97000: 0.41273578862280114\n",
      "three-layer Loss after iteration 98000: 0.4118906172353901\n",
      "three-layer Loss after iteration 99000: 0.41113460277570324\n",
      "three-layer Loss after iteration 0: 1632.5711971482685\n",
      "three-layer Loss after iteration 1000: 8.129149945666066\n",
      "three-layer Loss after iteration 2000: 5.964712042298643\n",
      "three-layer Loss after iteration 3000: 5.1722882526937015\n",
      "three-layer Loss after iteration 4000: 3.2002112351905128\n",
      "three-layer Loss after iteration 5000: 3.6556300485670303\n",
      "three-layer Loss after iteration 6000: 2.6903594663244337\n",
      "three-layer Loss after iteration 7000: 3.0201873952015448\n",
      "three-layer Loss after iteration 8000: 1.9442771749441505\n",
      "three-layer Loss after iteration 9000: 2.3646943176394544\n",
      "three-layer Loss after iteration 10000: 1.7825409731671353\n",
      "three-layer Loss after iteration 11000: 1.9730581909635687\n",
      "three-layer Loss after iteration 12000: 1.389775493840087\n",
      "three-layer Loss after iteration 13000: 1.1364065448300698\n",
      "three-layer Loss after iteration 14000: 0.8788168397983229\n",
      "three-layer Loss after iteration 15000: 1.5731519414197546\n",
      "three-layer Loss after iteration 16000: 1.2744954820247458\n",
      "three-layer Loss after iteration 17000: 1.112370096862213\n",
      "three-layer Loss after iteration 18000: 1.1485658808305648\n",
      "three-layer Loss after iteration 19000: 0.9008694494166242\n",
      "three-layer Loss after iteration 20000: 0.9153298479556549\n",
      "three-layer Loss after iteration 21000: 0.8544095334738581\n",
      "three-layer Loss after iteration 22000: 0.849374858686519\n",
      "three-layer Loss after iteration 23000: 0.8093026809704887\n",
      "three-layer Loss after iteration 24000: 0.7693949022247979\n",
      "three-layer Loss after iteration 25000: 0.732118184445049\n",
      "three-layer Loss after iteration 26000: 0.7086783928562475\n",
      "three-layer Loss after iteration 27000: 0.6990051018273844\n",
      "three-layer Loss after iteration 28000: 0.6799478430951819\n",
      "three-layer Loss after iteration 29000: 0.6612924923139961\n",
      "three-layer Loss after iteration 30000: 0.6507793881735028\n",
      "three-layer Loss after iteration 31000: 0.6414112650246312\n",
      "three-layer Loss after iteration 32000: 0.6982884437743683\n",
      "three-layer Loss after iteration 33000: 0.6325229389212321\n",
      "three-layer Loss after iteration 34000: 0.6283383389457647\n",
      "three-layer Loss after iteration 35000: 0.6750787307814045\n",
      "three-layer Loss after iteration 36000: 0.6082131263580491\n",
      "three-layer Loss after iteration 37000: 0.5889238883774095\n",
      "three-layer Loss after iteration 38000: 0.582116348014326\n",
      "three-layer Loss after iteration 39000: 0.5714952354174185\n",
      "three-layer Loss after iteration 40000: 0.5595781034499658\n",
      "three-layer Loss after iteration 41000: 0.5551701669696737\n",
      "three-layer Loss after iteration 42000: 0.5707264101898821\n",
      "three-layer Loss after iteration 43000: 0.5614019557856983\n",
      "three-layer Loss after iteration 44000: 0.5516474361723906\n",
      "three-layer Loss after iteration 45000: 0.5445694894415752\n",
      "three-layer Loss after iteration 46000: 0.5272994422447344\n",
      "three-layer Loss after iteration 47000: 0.5753750873805599\n",
      "three-layer Loss after iteration 48000: 0.5450101844970352\n",
      "three-layer Loss after iteration 49000: 0.5527855252045181\n",
      "three-layer Loss after iteration 50000: 0.5141152716256795\n",
      "three-layer Loss after iteration 51000: 0.5063338626322551\n",
      "three-layer Loss after iteration 52000: 0.5031528121922125\n",
      "three-layer Loss after iteration 53000: 0.5158818262996676\n",
      "three-layer Loss after iteration 54000: 0.5099245509624214\n",
      "three-layer Loss after iteration 55000: 0.5121092752363757\n",
      "three-layer Loss after iteration 56000: 0.5124400245587049\n",
      "three-layer Loss after iteration 57000: 0.5084744573719622\n",
      "three-layer Loss after iteration 58000: 0.5085090808492284\n",
      "6.809285454610351e-05 0.5084744573719622 0.5085090808492284\n",
      "three-layer Loss after iteration 0: 1350.9717395258235\n",
      "three-layer Loss after iteration 1000: 9.998863515301647\n",
      "three-layer Loss after iteration 2000: 7.3437600216689525\n",
      "three-layer Loss after iteration 3000: 7.667051106703404\n",
      "three-layer Loss after iteration 4000: 4.363671258644466\n",
      "three-layer Loss after iteration 5000: 3.7281445763089214\n",
      "three-layer Loss after iteration 6000: 2.7354076680449997\n",
      "three-layer Loss after iteration 7000: 2.2190867860468764\n",
      "three-layer Loss after iteration 8000: 1.8169410457004274\n",
      "three-layer Loss after iteration 9000: 1.1958022120585658\n",
      "three-layer Loss after iteration 10000: 1.716583433439678\n",
      "three-layer Loss after iteration 11000: 1.486332215804619\n",
      "three-layer Loss after iteration 12000: 1.325108354968109\n",
      "three-layer Loss after iteration 13000: 1.2732669847793734\n",
      "three-layer Loss after iteration 14000: 1.2858889876246347\n",
      "three-layer Loss after iteration 15000: 0.9065767329176966\n",
      "three-layer Loss after iteration 16000: 1.188426648077061\n",
      "three-layer Loss after iteration 17000: 1.1857667265925196\n",
      "three-layer Loss after iteration 18000: 1.1262324031552735\n",
      "three-layer Loss after iteration 19000: 0.9774318799345088\n",
      "three-layer Loss after iteration 20000: 1.0267310658276534\n",
      "three-layer Loss after iteration 21000: 1.0371618590799292\n",
      "three-layer Loss after iteration 22000: 1.0534493376162675\n",
      "three-layer Loss after iteration 23000: 1.011035155775183\n",
      "three-layer Loss after iteration 24000: 0.9887718168624098\n",
      "three-layer Loss after iteration 25000: 0.907267748760339\n",
      "three-layer Loss after iteration 26000: 0.9469311923332036\n",
      "three-layer Loss after iteration 27000: 0.963696433007036\n",
      "three-layer Loss after iteration 28000: 0.9703261582062792\n",
      "three-layer Loss after iteration 29000: 0.8982421507925549\n",
      "three-layer Loss after iteration 30000: 0.8868540716693537\n",
      "three-layer Loss after iteration 31000: 0.6521306699579832\n",
      "three-layer Loss after iteration 32000: 0.7029046229299097\n",
      "three-layer Loss after iteration 33000: 0.9410143893680427\n",
      "three-layer Loss after iteration 34000: 0.7482808160967198\n",
      "three-layer Loss after iteration 35000: 0.7812351735325713\n",
      "three-layer Loss after iteration 36000: 0.6211219941911926\n",
      "three-layer Loss after iteration 37000: 0.6280017091454375\n",
      "three-layer Loss after iteration 38000: 0.9961910753928297\n",
      "three-layer Loss after iteration 39000: 0.8030387725178071\n",
      "three-layer Loss after iteration 40000: 0.7627087297121241\n",
      "three-layer Loss after iteration 41000: 0.7871528228150129\n",
      "three-layer Loss after iteration 42000: 0.8696225417809316\n",
      "three-layer Loss after iteration 43000: 0.7854205548119497\n",
      "three-layer Loss after iteration 44000: 0.7601336174293455\n",
      "three-layer Loss after iteration 45000: 0.5036547060007087\n",
      "three-layer Loss after iteration 46000: 0.7913164682887749\n",
      "three-layer Loss after iteration 47000: 0.9740709245371661\n",
      "three-layer Loss after iteration 48000: 0.6815797692082841\n",
      "three-layer Loss after iteration 49000: 0.7081241379928851\n",
      "three-layer Loss after iteration 50000: 0.6719610536422121\n",
      "three-layer Loss after iteration 51000: 0.5754539775482269\n",
      "three-layer Loss after iteration 52000: 0.5545074163949256\n",
      "three-layer Loss after iteration 53000: 0.5630908566772355\n",
      "three-layer Loss after iteration 54000: 0.573834373286354\n",
      "three-layer Loss after iteration 55000: 0.6310271123423937\n",
      "three-layer Loss after iteration 56000: 0.6322921425082316\n",
      "three-layer Loss after iteration 57000: 0.6327914919516601\n",
      "three-layer Loss after iteration 58000: 0.6102541896466929\n",
      "three-layer Loss after iteration 59000: 0.6031656461709746\n",
      "three-layer Loss after iteration 60000: 0.5972180265039078\n",
      "three-layer Loss after iteration 61000: 0.586922114501118\n",
      "three-layer Loss after iteration 62000: 0.553719757851768\n",
      "three-layer Loss after iteration 63000: 0.5365854196325563\n",
      "three-layer Loss after iteration 64000: 0.5440352624232987\n",
      "three-layer Loss after iteration 65000: 0.5448504056925537\n",
      "three-layer Loss after iteration 66000: 0.5021782481500889\n",
      "three-layer Loss after iteration 67000: 0.4870272106175091\n",
      "three-layer Loss after iteration 68000: 0.48684923322178253\n",
      "three-layer Loss after iteration 69000: 0.530427301433286\n",
      "three-layer Loss after iteration 70000: 0.5191265126377772\n",
      "three-layer Loss after iteration 71000: 0.4973367198208085\n",
      "three-layer Loss after iteration 72000: 0.5302868720854329\n",
      "three-layer Loss after iteration 73000: 0.5030538638525223\n",
      "three-layer Loss after iteration 74000: 0.5072051933344884\n",
      "three-layer Loss after iteration 75000: 0.520132632591812\n",
      "three-layer Loss after iteration 76000: 0.4965381763528823\n",
      "three-layer Loss after iteration 77000: 0.5098897055363643\n",
      "three-layer Loss after iteration 78000: 0.497197190257463\n",
      "three-layer Loss after iteration 79000: 0.44444631007599955\n",
      "three-layer Loss after iteration 80000: 0.4964171251864967\n",
      "three-layer Loss after iteration 81000: 0.5397083437697996\n",
      "three-layer Loss after iteration 82000: 0.5708225723837622\n",
      "three-layer Loss after iteration 83000: 0.5836356038676018\n",
      "three-layer Loss after iteration 84000: 0.5250001229165655\n",
      "three-layer Loss after iteration 85000: 0.5439019376848708\n",
      "three-layer Loss after iteration 86000: 0.5839967048185358\n",
      "three-layer Loss after iteration 87000: 0.5525334160826967\n",
      "three-layer Loss after iteration 88000: 0.4933313623196278\n",
      "three-layer Loss after iteration 89000: 0.5068619013906683\n",
      "three-layer Loss after iteration 90000: 0.546781081360756\n",
      "three-layer Loss after iteration 91000: 0.5496469090905729\n",
      "three-layer Loss after iteration 92000: 0.546151414059402\n",
      "three-layer Loss after iteration 93000: 0.4829778316887708\n",
      "three-layer Loss after iteration 94000: 0.5215415555038048\n",
      "three-layer Loss after iteration 95000: 0.5134839528752372\n",
      "three-layer Loss after iteration 96000: 0.49224340206100414\n",
      "three-layer Loss after iteration 97000: 0.49493526299926327\n",
      "three-layer Loss after iteration 98000: 0.49177704749213264\n",
      "three-layer Loss after iteration 99000: 0.49514046285507207\n",
      "three-layer Loss after iteration 0: 1053.7090560540735\n",
      "three-layer Loss after iteration 1000: 10.34584625785531\n",
      "three-layer Loss after iteration 2000: 7.50132409378065\n",
      "three-layer Loss after iteration 3000: 7.061150842103\n",
      "three-layer Loss after iteration 4000: 6.193912916926175\n",
      "three-layer Loss after iteration 5000: 4.376952213943106\n",
      "three-layer Loss after iteration 6000: 4.004766582766866\n",
      "three-layer Loss after iteration 7000: 3.3540430083795028\n",
      "three-layer Loss after iteration 8000: 3.1784672870714994\n",
      "three-layer Loss after iteration 9000: 2.4551882241546235\n",
      "three-layer Loss after iteration 10000: 2.330995329258319\n",
      "three-layer Loss after iteration 11000: 2.2096718568484643\n",
      "three-layer Loss after iteration 12000: 2.0923566938944695\n",
      "three-layer Loss after iteration 13000: 2.0341751020576906\n",
      "three-layer Loss after iteration 14000: 2.231490573956413\n",
      "three-layer Loss after iteration 15000: 1.774776463372682\n",
      "three-layer Loss after iteration 16000: 1.7862540943996055\n",
      "three-layer Loss after iteration 17000: 1.99848117146405\n",
      "three-layer Loss after iteration 18000: 1.9220744076886618\n",
      "three-layer Loss after iteration 19000: 1.6114687642223637\n",
      "three-layer Loss after iteration 20000: 1.5340059901056016\n",
      "three-layer Loss after iteration 21000: 1.8934083627643699\n",
      "three-layer Loss after iteration 22000: 1.590514680538728\n",
      "three-layer Loss after iteration 23000: 1.5101361252636438\n",
      "three-layer Loss after iteration 24000: 1.4135438781142031\n",
      "three-layer Loss after iteration 25000: 1.619085563956721\n",
      "three-layer Loss after iteration 26000: 1.4925445572009064\n",
      "three-layer Loss after iteration 27000: 1.1629034673683554\n",
      "three-layer Loss after iteration 28000: 1.2560207960261687\n",
      "three-layer Loss after iteration 29000: 1.4142449209539927\n",
      "three-layer Loss after iteration 30000: 1.8174799367525467\n",
      "three-layer Loss after iteration 31000: 1.349545610498683\n",
      "three-layer Loss after iteration 32000: 1.4215841177946715\n",
      "three-layer Loss after iteration 33000: 1.1094847315026757\n",
      "three-layer Loss after iteration 34000: 1.5455480102118602\n",
      "three-layer Loss after iteration 35000: 1.315522140801988\n",
      "three-layer Loss after iteration 36000: 1.430076929218347\n",
      "three-layer Loss after iteration 37000: 1.1397392455087756\n",
      "three-layer Loss after iteration 38000: 1.156638707134451\n",
      "three-layer Loss after iteration 39000: 1.098360462255293\n",
      "three-layer Loss after iteration 40000: 1.204190542137243\n",
      "three-layer Loss after iteration 41000: 1.1334130360378625\n",
      "three-layer Loss after iteration 42000: 1.1685000648838226\n",
      "three-layer Loss after iteration 43000: 1.183433849696096\n",
      "three-layer Loss after iteration 44000: 1.1306211550644234\n",
      "three-layer Loss after iteration 45000: 0.7757276279722373\n",
      "three-layer Loss after iteration 46000: 1.1378412352685534\n",
      "three-layer Loss after iteration 47000: 0.775844607827275\n",
      "three-layer Loss after iteration 48000: 0.8531356664363742\n",
      "three-layer Loss after iteration 49000: 1.1691970566897218\n",
      "three-layer Loss after iteration 50000: 0.9539413717201831\n",
      "three-layer Loss after iteration 51000: 0.8339906901896198\n",
      "three-layer Loss after iteration 52000: 0.9463933646184827\n",
      "three-layer Loss after iteration 53000: 1.0583184355802202\n",
      "three-layer Loss after iteration 54000: 1.0022842436918045\n",
      "three-layer Loss after iteration 55000: 0.896620051429583\n",
      "three-layer Loss after iteration 56000: 0.8258498410168716\n",
      "three-layer Loss after iteration 57000: 1.0436692828467649\n",
      "three-layer Loss after iteration 58000: 1.1427764116840982\n",
      "three-layer Loss after iteration 59000: 0.82217978153895\n",
      "three-layer Loss after iteration 60000: 0.6667808576647538\n",
      "three-layer Loss after iteration 61000: 1.1070157818300779\n",
      "three-layer Loss after iteration 62000: 1.0195275542053757\n",
      "three-layer Loss after iteration 63000: 0.7438772478024369\n",
      "three-layer Loss after iteration 64000: 0.8471200409595941\n",
      "three-layer Loss after iteration 65000: 0.6925649056147609\n",
      "three-layer Loss after iteration 66000: 0.7698237509107166\n",
      "three-layer Loss after iteration 67000: 0.7151586735441491\n",
      "three-layer Loss after iteration 68000: 0.8859036043596877\n",
      "three-layer Loss after iteration 69000: 0.9069027646213539\n",
      "three-layer Loss after iteration 70000: 0.9105570416197879\n",
      "three-layer Loss after iteration 71000: 0.8265536719927504\n",
      "three-layer Loss after iteration 72000: 0.845117862917488\n",
      "three-layer Loss after iteration 73000: 0.8406345217376736\n",
      "three-layer Loss after iteration 74000: 0.7143906693972392\n",
      "three-layer Loss after iteration 75000: 0.680519226750617\n",
      "three-layer Loss after iteration 76000: 0.6090889593655701\n",
      "three-layer Loss after iteration 77000: 0.9223643023647148\n",
      "three-layer Loss after iteration 78000: 0.8890976418746711\n",
      "three-layer Loss after iteration 79000: 0.8818462787901071\n",
      "three-layer Loss after iteration 80000: 0.8981988096373812\n",
      "three-layer Loss after iteration 81000: 0.6656059899859663\n",
      "three-layer Loss after iteration 82000: 0.7942512697372895\n",
      "three-layer Loss after iteration 83000: 0.6639457335346154\n",
      "three-layer Loss after iteration 84000: 0.7446719050055044\n",
      "three-layer Loss after iteration 85000: 0.7621863683674259\n",
      "three-layer Loss after iteration 86000: 0.7662638489074349\n",
      "three-layer Loss after iteration 87000: 0.7592973230809859\n",
      "three-layer Loss after iteration 88000: 0.7467388942412274\n",
      "three-layer Loss after iteration 89000: 0.7421042021304559\n",
      "three-layer Loss after iteration 90000: 0.7371254407975135\n",
      "three-layer Loss after iteration 91000: 0.7313523309880007\n",
      "three-layer Loss after iteration 92000: 0.7263571215553534\n",
      "three-layer Loss after iteration 93000: 0.7228609070235484\n",
      "three-layer Loss after iteration 94000: 0.719746318276561\n",
      "three-layer Loss after iteration 95000: 0.7171477787111324\n",
      "three-layer Loss after iteration 96000: 0.6588840951587064\n",
      "three-layer Loss after iteration 97000: 0.7091370447976789\n",
      "three-layer Loss after iteration 98000: 0.7582395865717897\n",
      "three-layer Loss after iteration 99000: 0.7447189960418145\n",
      "three-layer Loss after iteration 0: 1742.4394878878588\n",
      "three-layer Loss after iteration 1000: 11.966670229392188\n",
      "three-layer Loss after iteration 2000: 8.399304918525878\n",
      "three-layer Loss after iteration 3000: 6.918886893175845\n",
      "three-layer Loss after iteration 4000: 6.578967358778365\n",
      "three-layer Loss after iteration 5000: 5.938720905652877\n",
      "three-layer Loss after iteration 6000: 4.952597473800029\n",
      "three-layer Loss after iteration 7000: 4.262139438740603\n",
      "three-layer Loss after iteration 8000: 4.094356519679382\n",
      "three-layer Loss after iteration 9000: 3.9410347553867684\n",
      "three-layer Loss after iteration 10000: 3.6180386727449103\n",
      "three-layer Loss after iteration 11000: 3.293240793070521\n",
      "three-layer Loss after iteration 12000: 3.0320718215923117\n",
      "three-layer Loss after iteration 13000: 3.00029233908089\n",
      "three-layer Loss after iteration 14000: 2.7911905241858106\n",
      "three-layer Loss after iteration 15000: 2.4378646884206057\n",
      "three-layer Loss after iteration 16000: 2.251787235918452\n",
      "three-layer Loss after iteration 17000: 2.1987461557995784\n",
      "three-layer Loss after iteration 18000: 2.046207123894136\n",
      "three-layer Loss after iteration 19000: 1.9773523183281274\n",
      "three-layer Loss after iteration 20000: 1.8560236108640487\n",
      "three-layer Loss after iteration 21000: 2.083839522441678\n",
      "three-layer Loss after iteration 22000: 1.8433347341849766\n",
      "three-layer Loss after iteration 23000: 1.9829506495958078\n",
      "three-layer Loss after iteration 24000: 2.0322079299009754\n",
      "three-layer Loss after iteration 25000: 1.8795447633508289\n",
      "three-layer Loss after iteration 26000: 2.086602471752441\n",
      "three-layer Loss after iteration 27000: 1.8478532219473887\n",
      "three-layer Loss after iteration 28000: 1.6519242391055022\n",
      "three-layer Loss after iteration 29000: 1.5438069363114513\n",
      "three-layer Loss after iteration 30000: 1.3772523118710998\n",
      "three-layer Loss after iteration 31000: 1.3532247950873242\n",
      "three-layer Loss after iteration 32000: 1.2904453606084336\n",
      "three-layer Loss after iteration 33000: 1.2909764840630233\n",
      "three-layer Loss after iteration 34000: 1.1843343126819996\n",
      "three-layer Loss after iteration 35000: 1.095522561335034\n",
      "three-layer Loss after iteration 36000: 1.083816036150598\n",
      "three-layer Loss after iteration 37000: 0.9958135974264357\n",
      "three-layer Loss after iteration 38000: 1.005031634092596\n",
      "three-layer Loss after iteration 39000: 1.0860272411861676\n",
      "three-layer Loss after iteration 40000: 0.9135851046183993\n",
      "three-layer Loss after iteration 41000: 0.8826842216100615\n",
      "three-layer Loss after iteration 42000: 0.8549529441698639\n",
      "three-layer Loss after iteration 43000: 0.8339856885435168\n",
      "three-layer Loss after iteration 44000: 0.8162126862808344\n",
      "three-layer Loss after iteration 45000: 0.7960711796033219\n",
      "three-layer Loss after iteration 46000: 0.7919594121274908\n",
      "three-layer Loss after iteration 47000: 0.7804126612938553\n",
      "three-layer Loss after iteration 48000: 0.7725973920493802\n",
      "three-layer Loss after iteration 49000: 0.7658031216989523\n",
      "three-layer Loss after iteration 50000: 0.7581353096076521\n",
      "three-layer Loss after iteration 51000: 0.7517836427802446\n",
      "three-layer Loss after iteration 52000: 0.7469738890818279\n",
      "three-layer Loss after iteration 53000: 0.7419214684286711\n",
      "three-layer Loss after iteration 54000: 0.7391391733610017\n",
      "three-layer Loss after iteration 55000: 0.7355759422909764\n",
      "three-layer Loss after iteration 56000: 0.7326812254309054\n",
      "three-layer Loss after iteration 57000: 0.7304388195183401\n",
      "three-layer Loss after iteration 58000: 0.7272865306754401\n",
      "three-layer Loss after iteration 59000: 0.7237119620873563\n",
      "three-layer Loss after iteration 60000: 0.7214969042648838\n",
      "three-layer Loss after iteration 61000: 0.7194490526075104\n",
      "three-layer Loss after iteration 62000: 0.7179748392686881\n",
      "three-layer Loss after iteration 63000: 0.7164689507069678\n",
      "three-layer Loss after iteration 64000: 0.8654929981461087\n",
      "three-layer Loss after iteration 65000: 0.6973373711147814\n",
      "three-layer Loss after iteration 66000: 0.7304788826807896\n",
      "three-layer Loss after iteration 67000: 0.7125261757389391\n",
      "three-layer Loss after iteration 68000: 0.7525138658741418\n",
      "three-layer Loss after iteration 69000: 0.7242837830647735\n",
      "three-layer Loss after iteration 70000: 0.7096002342854925\n",
      "three-layer Loss after iteration 71000: 0.7065860067526316\n",
      "three-layer Loss after iteration 72000: 0.7027439675667233\n",
      "three-layer Loss after iteration 73000: 0.7003159552087695\n",
      "three-layer Loss after iteration 74000: 0.6987825161281435\n",
      "three-layer Loss after iteration 75000: 0.6969535215413096\n",
      "three-layer Loss after iteration 76000: 0.6953183718004581\n",
      "three-layer Loss after iteration 77000: 0.6940112595904607\n",
      "three-layer Loss after iteration 78000: 0.692569941424703\n",
      "three-layer Loss after iteration 79000: 0.6912721200977553\n",
      "three-layer Loss after iteration 80000: 0.6896955063543581\n",
      "three-layer Loss after iteration 81000: 0.6888134921093088\n",
      "three-layer Loss after iteration 82000: 0.6877476901475877\n",
      "three-layer Loss after iteration 83000: 0.68706139801063\n",
      "three-layer Loss after iteration 84000: 0.6859958212891466\n",
      "three-layer Loss after iteration 85000: 0.6852569486230767\n",
      "three-layer Loss after iteration 86000: 0.6843848553973428\n",
      "three-layer Loss after iteration 87000: 0.6836652419047446\n",
      "three-layer Loss after iteration 88000: 0.683051309756382\n",
      "three-layer Loss after iteration 89000: 0.6823431221727709\n",
      "three-layer Loss after iteration 90000: 0.6818265515225903\n",
      "three-layer Loss after iteration 91000: 0.6811041706267011\n",
      "three-layer Loss after iteration 92000: 0.6806183778321337\n",
      "three-layer Loss after iteration 93000: 0.6798999921968251\n",
      "three-layer Loss after iteration 94000: 0.6795800447371302\n",
      "three-layer Loss after iteration 95000: 0.6789409942791766\n",
      "three-layer Loss after iteration 96000: 0.678510240825848\n",
      "three-layer Loss after iteration 97000: 0.6780017874321634\n",
      "three-layer Loss after iteration 98000: 0.677660752678981\n",
      "three-layer Loss after iteration 99000: 0.677044474504531\n",
      "three-layer Loss after iteration 0: 1503.3963859748778\n",
      "three-layer Loss after iteration 1000: 25.063472812571774\n",
      "three-layer Loss after iteration 2000: 13.919059413292812\n",
      "three-layer Loss after iteration 3000: 8.604312186070262\n",
      "three-layer Loss after iteration 4000: 7.522719402476115\n",
      "three-layer Loss after iteration 5000: 5.4047011113629795\n",
      "three-layer Loss after iteration 6000: 4.889437723240427\n",
      "three-layer Loss after iteration 7000: 4.373122968857698\n",
      "three-layer Loss after iteration 8000: 3.7091016379695096\n",
      "three-layer Loss after iteration 9000: 3.882454432470587\n",
      "three-layer Loss after iteration 10000: 3.9246479813349793\n",
      "three-layer Loss after iteration 11000: 3.2272015149096647\n",
      "three-layer Loss after iteration 12000: 3.2607156395912367\n",
      "three-layer Loss after iteration 13000: 2.8895125290379773\n",
      "three-layer Loss after iteration 14000: 2.3537948804239264\n",
      "three-layer Loss after iteration 15000: 2.0863755569100975\n",
      "three-layer Loss after iteration 16000: 2.0147181323255072\n",
      "three-layer Loss after iteration 17000: 1.8332612448235126\n",
      "three-layer Loss after iteration 18000: 2.2416404603460824\n",
      "three-layer Loss after iteration 19000: 1.9913048254202264\n",
      "three-layer Loss after iteration 20000: 1.5467305832851423\n",
      "three-layer Loss after iteration 21000: 0.9849727900447354\n",
      "three-layer Loss after iteration 22000: 1.841643405076855\n",
      "three-layer Loss after iteration 23000: 1.0578497098915862\n",
      "three-layer Loss after iteration 24000: 1.0890863677750025\n",
      "three-layer Loss after iteration 25000: 2.0018015379975496\n",
      "three-layer Loss after iteration 26000: 1.320385143394751\n",
      "three-layer Loss after iteration 27000: 0.9695130217935912\n",
      "three-layer Loss after iteration 28000: 1.4415045955926926\n",
      "three-layer Loss after iteration 29000: 1.1197704100205064\n",
      "three-layer Loss after iteration 30000: 2.289124156468185\n",
      "three-layer Loss after iteration 31000: 1.1831677411939652\n",
      "three-layer Loss after iteration 32000: 1.0312211367299617\n",
      "three-layer Loss after iteration 33000: 0.8698905094766173\n",
      "three-layer Loss after iteration 34000: 1.1221553902612453\n",
      "three-layer Loss after iteration 35000: 0.8289219391322109\n",
      "three-layer Loss after iteration 36000: 1.4438791176802337\n",
      "three-layer Loss after iteration 37000: 1.944025937755712\n",
      "three-layer Loss after iteration 38000: 0.9722188372594125\n",
      "three-layer Loss after iteration 39000: 1.1412209220249108\n",
      "three-layer Loss after iteration 40000: 1.2480595818095317\n",
      "three-layer Loss after iteration 41000: 0.795454546069344\n",
      "three-layer Loss after iteration 42000: 0.9005581097667986\n",
      "three-layer Loss after iteration 43000: 0.8631889552452379\n",
      "three-layer Loss after iteration 44000: 0.8015516179869073\n",
      "three-layer Loss after iteration 45000: 1.1207310052785928\n",
      "three-layer Loss after iteration 46000: 0.8077059548865276\n",
      "three-layer Loss after iteration 47000: 0.7978843728678784\n",
      "three-layer Loss after iteration 48000: 0.8450170962721402\n",
      "three-layer Loss after iteration 49000: 0.9003489613655307\n",
      "three-layer Loss after iteration 50000: 1.2694689367064769\n",
      "three-layer Loss after iteration 51000: 0.808806963396681\n",
      "three-layer Loss after iteration 52000: 0.869277221381901\n",
      "three-layer Loss after iteration 53000: 0.8845130656047826\n",
      "three-layer Loss after iteration 54000: 0.9896648252607482\n",
      "three-layer Loss after iteration 55000: 1.3052511139940168\n",
      "three-layer Loss after iteration 56000: 0.929991476936515\n",
      "three-layer Loss after iteration 57000: 0.9097789630405436\n",
      "three-layer Loss after iteration 58000: 1.0835319787422066\n",
      "three-layer Loss after iteration 59000: 0.9089108604301535\n",
      "three-layer Loss after iteration 60000: 0.8935031484414094\n",
      "three-layer Loss after iteration 61000: 0.8346636002868141\n",
      "three-layer Loss after iteration 62000: 0.8192668342725195\n",
      "three-layer Loss after iteration 63000: 0.81481475161664\n",
      "three-layer Loss after iteration 64000: 1.1268688584478754\n",
      "three-layer Loss after iteration 65000: 1.120704954370148\n",
      "three-layer Loss after iteration 66000: 1.029301077798432\n",
      "three-layer Loss after iteration 67000: 0.7980696739391859\n",
      "three-layer Loss after iteration 68000: 0.7450158712025513\n",
      "three-layer Loss after iteration 69000: 0.7524191816507212\n",
      "three-layer Loss after iteration 70000: 1.000456398356502\n",
      "three-layer Loss after iteration 71000: 0.7828919353245121\n",
      "three-layer Loss after iteration 72000: 0.762729954161769\n",
      "three-layer Loss after iteration 73000: 1.9026889412402939\n",
      "three-layer Loss after iteration 74000: 0.6916828128990151\n",
      "three-layer Loss after iteration 75000: 0.7243141946791292\n",
      "three-layer Loss after iteration 76000: 1.7897487286332716\n",
      "three-layer Loss after iteration 77000: 0.737939913795139\n",
      "three-layer Loss after iteration 78000: 0.7841646935027521\n",
      "three-layer Loss after iteration 79000: 1.1908003718936375\n",
      "three-layer Loss after iteration 80000: 0.7182369435084355\n",
      "three-layer Loss after iteration 81000: 0.7839723088093362\n",
      "three-layer Loss after iteration 82000: 0.8051096719369135\n",
      "three-layer Loss after iteration 83000: 0.7165629299610183\n",
      "three-layer Loss after iteration 84000: 1.0695683883872484\n",
      "three-layer Loss after iteration 85000: 0.7656720438600696\n",
      "three-layer Loss after iteration 86000: 0.8930957620159811\n",
      "three-layer Loss after iteration 87000: 0.7454370511890944\n",
      "three-layer Loss after iteration 88000: 0.9879539323069033\n",
      "three-layer Loss after iteration 89000: 0.7337885018400455\n",
      "three-layer Loss after iteration 90000: 1.2664426817298287\n",
      "three-layer Loss after iteration 91000: 1.0844494063620802\n",
      "three-layer Loss after iteration 92000: 0.8356742468694438\n",
      "three-layer Loss after iteration 93000: 0.7779628776132379\n",
      "three-layer Loss after iteration 94000: 0.6901955306412881\n",
      "three-layer Loss after iteration 95000: 0.7050059186874225\n",
      "three-layer Loss after iteration 96000: 0.7482863929625083\n",
      "three-layer Loss after iteration 97000: 0.760041077326957\n",
      "three-layer Loss after iteration 98000: 0.8423310711119141\n",
      "three-layer Loss after iteration 99000: 0.8360724748220941\n",
      "three-layer Loss after iteration 0: 1690.6102183556607\n",
      "three-layer Loss after iteration 1000: 10.518057119310846\n",
      "three-layer Loss after iteration 2000: 6.89276030248644\n",
      "three-layer Loss after iteration 3000: 3.6786894123909493\n",
      "three-layer Loss after iteration 4000: 4.267147464897593\n",
      "three-layer Loss after iteration 5000: 2.9548979300909215\n",
      "three-layer Loss after iteration 6000: 2.2182545328475145\n",
      "three-layer Loss after iteration 7000: 1.9153263851067353\n",
      "three-layer Loss after iteration 8000: 1.363447271673564\n",
      "three-layer Loss after iteration 9000: 1.497929511710127\n",
      "three-layer Loss after iteration 10000: 1.4284961030583758\n",
      "three-layer Loss after iteration 11000: 1.5602701030865256\n",
      "three-layer Loss after iteration 12000: 1.4080678885414155\n",
      "three-layer Loss after iteration 13000: 1.3388558112080742\n",
      "three-layer Loss after iteration 14000: 1.4384234737404795\n",
      "three-layer Loss after iteration 15000: 1.4860674659357382\n",
      "three-layer Loss after iteration 16000: 1.847707772690074\n",
      "three-layer Loss after iteration 17000: 1.140851872514287\n",
      "three-layer Loss after iteration 18000: 1.3115117628151973\n",
      "three-layer Loss after iteration 19000: 1.1784273031978572\n",
      "three-layer Loss after iteration 20000: 1.0099585126861086\n",
      "three-layer Loss after iteration 21000: 0.9710621721376969\n",
      "three-layer Loss after iteration 22000: 0.9735273891968773\n",
      "three-layer Loss after iteration 23000: 0.9367414814193207\n",
      "three-layer Loss after iteration 24000: 0.9684939219161631\n",
      "three-layer Loss after iteration 25000: 0.8639248473134881\n",
      "three-layer Loss after iteration 26000: 0.9416857162920768\n",
      "three-layer Loss after iteration 27000: 0.869599202848214\n",
      "three-layer Loss after iteration 28000: 0.8936280560053683\n",
      "three-layer Loss after iteration 29000: 0.7139028236749052\n",
      "three-layer Loss after iteration 30000: 0.7500528466223055\n",
      "three-layer Loss after iteration 31000: 0.7724728132849926\n",
      "three-layer Loss after iteration 32000: 0.8124036438484302\n",
      "three-layer Loss after iteration 33000: 0.6810618557304765\n",
      "three-layer Loss after iteration 34000: 0.5833199006291857\n",
      "three-layer Loss after iteration 35000: 0.6263815031912602\n",
      "three-layer Loss after iteration 36000: 0.5846241976118808\n",
      "three-layer Loss after iteration 37000: 0.7284242341327233\n",
      "three-layer Loss after iteration 38000: 0.7156071840795488\n",
      "three-layer Loss after iteration 39000: 0.7085860978024132\n",
      "three-layer Loss after iteration 40000: 0.754509232813029\n",
      "three-layer Loss after iteration 41000: 0.5663980156864574\n",
      "three-layer Loss after iteration 42000: 0.5275096329400046\n",
      "three-layer Loss after iteration 43000: 0.48547353802304694\n",
      "three-layer Loss after iteration 44000: 0.47665316894531257\n",
      "three-layer Loss after iteration 45000: 0.4713657792271884\n",
      "three-layer Loss after iteration 46000: 0.47450705337869864\n",
      "three-layer Loss after iteration 47000: 0.5051980007385538\n",
      "three-layer Loss after iteration 48000: 0.5688923131017787\n",
      "three-layer Loss after iteration 49000: 0.5783421856060823\n",
      "three-layer Loss after iteration 50000: 0.5478838800450759\n",
      "three-layer Loss after iteration 51000: 0.5476222200041165\n",
      "three-layer Loss after iteration 52000: 0.5417015484835139\n",
      "three-layer Loss after iteration 53000: 0.5377146343584139\n",
      "three-layer Loss after iteration 54000: 0.5707972677314147\n",
      "three-layer Loss after iteration 55000: 0.5696269395307942\n",
      "three-layer Loss after iteration 56000: 0.5084544147311968\n",
      "three-layer Loss after iteration 57000: 0.505745808199372\n",
      "three-layer Loss after iteration 58000: 0.5019620908506143\n",
      "three-layer Loss after iteration 59000: 0.48501143551389986\n",
      "three-layer Loss after iteration 60000: 0.48897860483313116\n",
      "three-layer Loss after iteration 61000: 0.4817118956680066\n",
      "three-layer Loss after iteration 62000: 0.4805823157663036\n",
      "three-layer Loss after iteration 63000: 0.4783212225653665\n",
      "three-layer Loss after iteration 64000: 0.4735935482186489\n",
      "three-layer Loss after iteration 65000: 0.46948652902338645\n",
      "three-layer Loss after iteration 66000: 0.466027763670133\n",
      "three-layer Loss after iteration 67000: 0.4625813551119797\n",
      "three-layer Loss after iteration 68000: 0.45406676664563445\n",
      "three-layer Loss after iteration 69000: 0.45244926836600613\n",
      "three-layer Loss after iteration 70000: 0.4547616964893857\n",
      "three-layer Loss after iteration 71000: 0.45086941282276705\n",
      "three-layer Loss after iteration 72000: 0.4522837743071723\n",
      "three-layer Loss after iteration 73000: 0.4505592693694526\n",
      "three-layer Loss after iteration 74000: 0.45276542579290485\n",
      "three-layer Loss after iteration 75000: 0.4592990477041725\n",
      "three-layer Loss after iteration 76000: 0.4608195551200706\n",
      "three-layer Loss after iteration 77000: 0.4585263672486263\n",
      "three-layer Loss after iteration 78000: 0.4577462570793513\n",
      "three-layer Loss after iteration 79000: 0.456274191233599\n",
      "three-layer Loss after iteration 80000: 0.4545645718707534\n",
      "three-layer Loss after iteration 81000: 0.44868840199082854\n",
      "three-layer Loss after iteration 82000: 0.452599508830439\n",
      "three-layer Loss after iteration 83000: 0.45520377669656453\n",
      "three-layer Loss after iteration 84000: 0.47261109636165316\n",
      "three-layer Loss after iteration 85000: 0.5043463320954412\n",
      "three-layer Loss after iteration 86000: 0.4882800516267053\n",
      "three-layer Loss after iteration 87000: 0.47120336523779005\n",
      "three-layer Loss after iteration 88000: 0.46514214799180964\n",
      "three-layer Loss after iteration 89000: 0.46188051215987\n",
      "three-layer Loss after iteration 90000: 0.4592459356337544\n",
      "three-layer Loss after iteration 91000: 0.4575567497791789\n",
      "three-layer Loss after iteration 92000: 0.45620956786224837\n",
      "three-layer Loss after iteration 93000: 0.5197985981380933\n",
      "three-layer Loss after iteration 94000: 0.4877898183353405\n",
      "three-layer Loss after iteration 95000: 0.4747365758578517\n",
      "three-layer Loss after iteration 96000: 0.4691122121281519\n",
      "three-layer Loss after iteration 97000: 0.46056564326051624\n",
      "three-layer Loss after iteration 98000: 0.4461828363176886\n",
      "three-layer Loss after iteration 99000: 0.42381173745424267\n",
      "{'8': {'losses': [3.2960968311154177, 2.686495056837814, 2.4458781013588773, 4.544646024419433, 1.8736233570154375, 3.7150274328964783, 3.755064348507962, 3.323821306313002, 2.3656330077323857, 2.8821372454077623, 1.8249821216792543, 4.544452063669773, 2.502430270529242, 3.6063009618568547, 3.4578563644686366, 1.7114213029144902, 9.024377381672672, 2.8911891463878923, 3.720615318719514, 2.89793472933845], 'iterations': [100000, 79001, 100000, 73001, 100000, 37001, 100000, 54001, 80001, 37001, 63001, 68001, 100000, 100000, 52001, 100000, 84001, 80001, 67001, 100000]}, '16': {'losses': [0.8505529278327828, 0.8270121257547873, 0.38741576049150755, 1.1059465033382587, 0.5655733964191837, 1.2031323535257308, 0.34851881778852445, 0.49317906542681655, 1.0161271003154295, 0.11947003747853555, 0.6312674566872153, 0.41668216531978486, 0.33536437726154544, 0.19081193877421462, 0.2036937898266589, 0.434113104549623, 0.27704362986726544, 0.4756775760892486, 0.4038232597900414, 0.552467015010619], 'iterations': [100000, 85001, 100000, 100000, 100000, 100000, 100000, 100000, 67001, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000]}} {'4+4': {'losses': [10.993200418489621, 12.518692864415698, 2.6389228918846745, 2.352087583440563, 4.493967609616839, 7.811905752583267, 10.416046573266362, 11.86792313481236, 6.411681078446581, 3.813007071587286, 3.999027370875976, 5.105173453430588, 9.157078659370166, 8.68953208689747, 5.595165772763855, 6.822550680637237, 238.86521823364492, 5.82268233259982, 4.28674406448485, 2.323141385983939], 'iterations': [69001, 93001, 45001, 100000, 100000, 90001, 100000, 64001, 77001, 100000, 100000, 100000, 100000, 100000, 100000, 82001, 18001, 100000, 100000, 91001]}, '8+8': {'losses': [0.7612170145584921, 1.0389476019015929, 0.6631379394498382, 0.37304393355097937, 0.44503187086257695, 0.3980486225500918, 0.9173571990983366, 0.3653297639280916, 0.5893927748560615, 0.41223892297951314, 0.5238446535273219, 0.9634407790636769, 0.4310452184772874, 0.41113460277570324, 0.5085090808492284, 0.49514046285507207, 0.7447189960418145, 0.677044474504531, 0.8360724748220941, 0.42381173745424267], 'iterations': [100000, 23001, 100000, 82001, 100000, 100000, 100000, 100000, 90001, 100000, 100000, 100000, 100000, 100000, 58001, 100000, 100000, 100000, 100000, 100000]}}\n",
      "--- 971.6080002784729 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "numDataOne = 15\n",
    "numData = numDataOne*numDataOne\n",
    "# create data for regression\n",
    "xs=np.linspace(-8,8,numDataOne)\n",
    "ys=np.linspace(-8,8,numDataOne)\n",
    "counter=0\n",
    "\n",
    "X = np.empty((0, 2))\n",
    "y = np.empty((0, 1))\n",
    "\n",
    "for r in np.arange(0,numDataOne):\n",
    "    for c in np.arange(0,numDataOne):\n",
    "        X = np.vstack((X, [xs[r], ys[c]]))\n",
    "        y = np.vstack((y, xs[r]**2 + ys[c]**2 + 1))\n",
    "\n",
    "# training set size\n",
    "num_examples = len(X)\n",
    "# input layer dimensionality\n",
    "nn_input_dim = 2\n",
    "# output layer dimensionality\n",
    "nn_output_dim = 1\n",
    "# learning rate for gradient descent\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Parameters\n",
    "num_runs = 20\n",
    "tolerance = 0.0001\n",
    "max_iterations = 100000\n",
    "\n",
    "# Data structures to store results\n",
    "results_old = {\"8\": {\"losses\": [], \"iterations\": []}}\n",
    "results_new = {\"4+4\": {\"losses\": [], \"iterations\": []}}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# Test the old (2-layer) network with 8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 8, 1)\n",
    "    model_old, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"8\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"8\"][\"iterations\"].append(iterations)\n",
    "\n",
    "# Test the new (3-layer) network with 4+4 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [4,4], 1)\n",
    "    model_new, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"4+4\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"4+4\"][\"iterations\"].append(iterations)\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"16\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"8+8\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 16, 1)\n",
    "    model_old, losses, iterations = old_train(model_old, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_old[\"16\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 8+8 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [8, 8], 1)\n",
    "    model_new, losses, iterations = train(model_new, X, y, num_passes=max_iterations, learning_rate=learning_rate, tolerance=tolerance)\n",
    "    results_new[\"8+8\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"8+8\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "\n",
    "'''\n",
    "# Update data structures to store results for the next configurations\n",
    "results_old[\"32\"] = {\"losses\": [], \"iterations\": []}\n",
    "results_new[\"16+16\"] = {\"losses\": [], \"iterations\": []}\n",
    "\n",
    "# Test the old (2-layer) network with 32 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_old = old_create_model(X, 32, 1)\n",
    "    _, losses, iterations = old_train(model_old, X, y, \n",
    "                                      num_passes=max_iterations, \n",
    "                                      learning_rate=learning_rate, \n",
    "                                      tolerance=tolerance)\n",
    "    results_old[\"32\"][\"losses\"].append(losses[-1])\n",
    "    results_old[\"32\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "# Test the new (3-layer) network with 16+16 neurons\n",
    "for _ in range(num_runs):\n",
    "    model_new = create_model(X, [16, 16], 1)\n",
    "    _, losses, iterations = train(model_new, X, y, \n",
    "                                  num_passes=max_iterations, \n",
    "                                  learning_rate=learning_rate, \n",
    "                                  tolerance=tolerance)\n",
    "    results_new[\"16+16\"][\"losses\"].append(losses[-1])\n",
    "    results_new[\"16+16\"][\"iterations\"].append(iterations)\n",
    "    # print(\"Done with run {}\".format(_))\n",
    "\n",
    "print(results_old, results_new)\n",
    "'''\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:31:30.644922500Z",
     "start_time": "2023-10-26T07:15:19.022962400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3534991186420675, 0.5418936200773886] [18.199187450961603, 0.5989254062053273]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8': 3.3534991186420675, '16': 0.5418936200773886} {'4+4': 18.199187450961603, '8+8': 0.5989254062053273}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8': 78700.6, '16': 97600.1} {'4+4': 86450.45, '8+8': 92650.2}\n"
     ]
    }
   ],
   "source": [
    "# Calculate average losses for old and new models for different configurations\n",
    "avg_losses_old = {config: np.mean(data['losses']) for config, data in results_old.items()}\n",
    "avg_losses_new = {config: np.mean(data['losses']) for config, data in results_new.items()}\n",
    "\n",
    "# Bar chart for average loss values\n",
    "labels = [f\"{old_key} & {new_key}\" for old_key, new_key in zip(avg_losses_old.keys(), avg_losses_new.keys())]\n",
    "\n",
    "\n",
    "old_vals = list(avg_losses_old.values())\n",
    "new_vals = list(avg_losses_new.values())\n",
    "bar_width = 0.35\n",
    "\n",
    "\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "print(old_vals, new_vals)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(r1[:len(old_vals)], old_vals, color='b', width=bar_width, edgecolor='grey', label='2-layer Model')\n",
    "plt.bar(r2[:len(new_vals)], new_vals, color='r', width=bar_width, edgecolor='grey', label='3-layer Model')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Average Losses for 2-layer vs 3-layer Models', fontweight='bold')\n",
    "plt.xlabel('Model Configuration', fontweight='bold')\n",
    "plt.ylabel('Average Loss', fontweight='bold')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xticks(r1 + 0.5 * bar_width, labels)  # This ensures labels are centered between the old and new model bars\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(avg_losses_old, avg_losses_new)\n",
    "\n",
    "# Calculate average iterations for old and new models for different configurations\n",
    "avg_iterations_old = {config: np.mean(data[\"iterations\"]) for config, data in results_old.items()}\n",
    "avg_iterations_new = {config: np.mean(data[\"iterations\"]) for config, data in results_new.items()}\n",
    "\n",
    "# Bar chart for average iteration values\n",
    "old_it_vals = list(avg_iterations_old.values())\n",
    "new_it_vals = list(avg_iterations_new.values())\n",
    "\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(r1[:len(old_it_vals)], old_it_vals, color='b', width=bar_width, edgecolor='grey', label='2-layer Model')\n",
    "plt.bar(r2[:len(new_it_vals)], new_it_vals, color='r', width=bar_width, edgecolor='grey', label='3-layer Model')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Average Iterations for Convergence for 2-layer vs 3-layer Models', fontweight='bold')\n",
    "plt.xlabel('Model Configuration', fontweight='bold')\n",
    "plt.ylabel('Average Iterations', fontweight='bold')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xticks([r + 0.5*bar_width for r in range(len(labels))], labels)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(avg_iterations_old, avg_iterations_new)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:46:33.743130400Z",
     "start_time": "2023-10-26T07:46:33.181583600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion\n",
    "\n",
    "### \n",
    "<p>\n",
    "When we compare the 2-layer model with the 3-layer model, it's evident that the 2-layer model consistently demonstrates superior performance in terms of loss. \n",
    "\n",
    "This essentially means that, on average, the 2-layer model's predictions are closer to the actual values than those of the 3-layer model. \n",
    "\n",
    "Such a result can be counterintuitive because one might often assume that adding more layers to a neural network, thereby increasing its depth, would inherently lead to improved performance. \n",
    "\n",
    "However, in some scenarios, simpler models with fewer layers can capture the underlying patterns in the data more effectively without being susceptible to overfitting. \n",
    "\n",
    "\n",
    "\n",
    "When we shift our focus from loss values to the number of iterations required for convergence, the distinction between the two models becomes less clear-cut. \n",
    "\n",
    "The iterations to convergence it gives us an idea about the efficiency and speed of the learning process. \n",
    "\n",
    "In this case, the difference between the iterations to converge varies only on the number of neurons and not the number of layers.\n",
    "\n",
    "Specifically, as the number of neurons in the model increases, there is a corresponding increase in the number of iterations required for convergence. \n",
    "\n",
    "This seems to suggests that while models with a greater number of neurons have a higher capacity to learn complex patterns and nuances in data, this enhanced capacity comes with the trade-off of potentially longer training times. \n",
    "\n",
    "This observation underscores an essential principle in neural network design: increasing model complexity doesn't always lead to faster or better learning. \n",
    "\n",
    "Instead, it might introduce challenges in terms of training efficiency and risk of overfitting.\n",
    "\n",
    "In conclusion, the exploration of these models underscores the multifaceted nature of machine learning model evaluation. \n",
    "\n",
    "While the 2-layer model exhibits better loss values, the intricacies of model convergence, especially in relation to model complexity, emphasize the importance of a holistic approach to model assessment and selection.\n",
    "\n",
    "\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus: arbitrary number of layers (20 points):\n",
    "Change the functions such that they can accept an arbitrary number of layers, but\n",
    "keep the overall call-logic and training loops the same - do NOT use classes! For this,\n",
    "you will need to play around with the dictionaries in create_model, forward,\n",
    "backprop.\n"
   ],
   "metadata": {
    "id": "t8NWou_MVkxi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create a multi-layer neural network\n",
    "def create_multilayer_model(X, layer_sizes):\n",
    "    model = {}\n",
    "    # using ReLU as the default activation function\n",
    "    model['activation_function'] = 'relu'  \n",
    "\n",
    "    # Create weights and biases for each layer based on layer_sizes\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        model[f'W{i+1}'] = np.random.randn(layer_sizes[i], layer_sizes[i+1]) / np.sqrt(layer_sizes[i])\n",
    "        model[f'b{i+1}'] = np.zeros((1, layer_sizes[i+1]))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define the forward pass given a model and data\n",
    "def feed_forward_multilayer(model, x):\n",
    "    # get activation function\n",
    "    act_func = activation_functions.get(model['activation_function'])\n",
    "\n",
    "    z = {}\n",
    "    a = {}\n",
    "    a[0] = x  # the input layer\n",
    "\n",
    "    # Compute activations and outputs for each layer\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "    for i in range(1, num_layers+1):\n",
    "        z[i] = a[i-1].dot(model[f'W{i}']) + model[f'b{i}']\n",
    "        a[i] = act_func(z[i])\n",
    "\n",
    "    return z, a\n",
    "\n",
    "# define the regression loss\n",
    "def calculate_loss_multilayer(model, X, y):\n",
    "    z, a = feed_forward_multilayer(model, X)\n",
    "    out = a[len(a) - 1]\n",
    "    \n",
    "    # calculate MSE loss\n",
    "    loss = 0.5 * np.sum((out - y) ** 2)\n",
    "    # data_loss = np.mean((y - output) ** 2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# back-propagation for the multi-layer network\n",
    "def backprop_multilayer(X, y, model, z, a):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # get activation function derivate\n",
    "    act_func_derivative = activation_derivatives.get(model['activation_function'])\n",
    "    num_layers = len(model) // 2  # for W and b\n",
    "\n",
    "    # Initialize the gradients\n",
    "    dW = {}\n",
    "    db = {}\n",
    "    delta = {}\n",
    "\n",
    "    # Compute the error for the last layer\n",
    "    delta[num_layers] = a[num_layers] - y\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    for i in reversed(range(1, num_layers+1)):\n",
    "        dW[i] = a[i-1].T.dot(delta[i]) / m\n",
    "        db[i] = np.sum(delta[i], axis=0, keepdims=True) / m\n",
    "        \n",
    "        if i > 1:  # Skip delta computation for the input layer\n",
    "            delta[i-1] = delta[i].dot(model[f'W{i}'].T) * act_func_derivative(a[i-1])\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "# training loop\n",
    "def train_multilayer(model, X, y, num_passes=100000, learning_rate=0.001, tolerance=0.0001):\n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "    \n",
    "    # iteration counter\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        # get predictions\n",
    "        z, a = feed_forward_multilayer(model, X)\n",
    "        \n",
    "        # feed this into backprop\n",
    "        dW, db = backprop_multilayer(X, y, model, z, a)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for key in dW:\n",
    "            model[f'W{key}'] -= learning_rate * dW[key]\n",
    "            model[f'b{key}'] -= learning_rate * db[key]\n",
    "\n",
    "        # calculate loss\n",
    "        loss = calculate_loss_multilayer(model, X, y)\n",
    "\n",
    "        # print loss per 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            losses.append(loss)\n",
    "            print(f\"Loss after iteration {i}: {loss}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "\n",
    "        i += 1\n",
    "        if i >= num_passes:\n",
    "            done = True\n",
    "\n",
    "    return model, losses, i"
   ],
   "metadata": {
    "id": "bk4EXE1lVkg1",
    "ExecuteTime": {
     "end_time": "2023-10-26T07:54:54.401915700Z",
     "start_time": "2023-10-26T07:54:54.384223400Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 215320.92616788292\n",
      "Loss after iteration 1000: 24781.628623278644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jws\\AppData\\Local\\Temp\\ipykernel_20816\\4132343458.py:101: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if np.abs((previous_loss - loss) / previous_loss) < tolerance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 2000: 15036.6445922948\n",
      "Loss after iteration 3000: 12505.222484740549\n",
      "Loss after iteration 4000: 9071.066537862675\n",
      "Loss after iteration 5000: 8353.688396404947\n",
      "Loss after iteration 6000: 8022.15818954586\n",
      "Loss after iteration 7000: 7486.8033804344195\n",
      "Loss after iteration 8000: 7300.194265458081\n",
      "Loss after iteration 9000: 7146.612527933595\n",
      "Loss after iteration 0: 279476.3213218919\n",
      "Loss after iteration 1000: 105564.54864534386\n",
      "Loss after iteration 2000: 105564.54810495625\n",
      "Loss after iteration 0: 93314.28999370235\n",
      "Loss after iteration 1000: 23854.149942046926\n",
      "Loss after iteration 2000: 11948.252000386288\n",
      "Loss after iteration 3000: 7206.511238483894\n",
      "Loss after iteration 4000: 5822.632912972147\n",
      "Loss after iteration 5000: 5189.13728966589\n",
      "Loss after iteration 6000: 4620.346557541847\n",
      "Loss after iteration 7000: 4303.566142865066\n",
      "Loss after iteration 8000: 4269.70158132828\n",
      "Loss after iteration 9000: 4305.2852740478265\n",
      "Loss after iteration 0: 97923.96709973906\n",
      "Loss after iteration 1000: 105564.54866873231\n",
      "Loss after iteration 2000: 105564.54810495625\n",
      "--- 3.541996955871582 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# architectures in lists of layer sizes\n",
    "architectures = [[2, 8, 1], [2, 4, 4, 1], [2, 16, 1], [2, 8, 8, 1]]\n",
    "\n",
    "# re-run the model training process with the provided architectures\n",
    "results = {}\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "# create and train multiple models with different layer sizes\n",
    "for arch in architectures:\n",
    "    model = create_multilayer_model(X, arch)\n",
    "    trained_model, losses, iterations = train_multilayer(model, X, y, num_passes=10000, learning_rate=0.01, tolerance=0.00001)\n",
    "    results[str(arch)] = losses\n",
    "    \n",
    "    #iterations\n",
    "    results[str(arch)+'iter'] = iterations\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:54:58.856123Z",
     "start_time": "2023-10-26T07:54:55.308825700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['[2, 8, 1]', '[2, 8, 1]iter', '[2, 4, 4, 1]', '[2, 4, 4, 1]iter', '[2, 16, 1]', '[2, 16, 1]iter', '[2, 8, 8, 1]', '[2, 8, 8, 1]iter'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAKyCAYAAAAjLAa+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1gUVxfA4d8uIE1QUSNFsYMKFrBhFwv2XmLvXWOvMdbYjb0Fo2KvscTYYxQ7Vuwae0HBKHaQuvP9QZjPFRAsuIjnfR4f2blTzszu7O7Ze+eMRlEUBSGEEEIIIYQQQiQLraEDEEIIIYQQQgghUjNJvIUQQgghhBBCiGQkibcQQgghhBBCCJGMJPEWQgghhBBCCCGSkSTeQgghhBBCCCFEMpLEWwghhBBCCCGESEaSeAshhBBCCCGEEMlIEm8hhBBCCCGEECIZSeIthBAi1VMU5ZvY5odI6fEJIb4seU8QInlJ4i3EBxowYADOzs4sWbLE0KF8UcePH8fZ2Znjx48bOhSDaN26Na1bt050Pp1OR8WKFXF2dubixYtfILKUady4cQwYMEB93LhxY5YvX/7J6506dSolSpSgSJEibNmyJd55KlWqxNChQ9XHp0+fpkuXLp+87Q+xYcMGJk+erD7etGkTzs7OBAQEfNE44hMREcGECRP4888/P9s6b9++zejRo6lSpQqFChWiYsWK9O/fn6tXr362bYgP5+fnR7Vq1XB1daVTp06fff2tW7fG2dk5wX9Nmzb9rNv71PPo5cuXzJ07lzp16uDm5kapUqVo27Yt+/bt+6xxvu2ff/6hfv36uLq6UrNmzRT1XhArvveEoUOHUqlSJQNGJUTqY2zoAIT4mrx69Yq9e/fi5OTEunXraN++PRqNxtBhiRTkyJEjPHnyhFy5crF27VrGjRtn6JAM4vTp0zRu3BiAN2/ecOXKFcaMGfNJ67x27RqLFi2iadOm1KtXj1y5ciVpuQ0bNnDz5s1P2vaHWrBgASVKlFAfV6xYkXXr1vHdd9990Tji8++//7Js2TImTpz4Wda3Z88eBg8eTN68eenevTtZs2YlKCiIZcuW0bRpUxYsWECZMmU+y7bEh5kyZQo6nY6FCxeSMWPGZNlGgQIFGDVqVLxtlpaWybLNj3Hz5k06d+6MTqejTZs25MuXj9DQUP7880+6d+9Onz596NGjx2ff7rx583j48CHz5s3DxsYGBweHFPNeECu+94QePXrQpk0bA0YlROojibcQH2Dbtm0ADB8+nLZt2+Ln50epUqUMHJVISTZt2oSbmxvlypVjwYIFDB06lLRp0xo6rC/q9evX/PPPPxQtWhSAs2fPYmpqSr58+T5pvc+fPwegVq1aFCtW7FPD/KJsbGywsbExdBif3b179xgyZAjlypVj5syZGBkZqW1eXl40b96cIUOGsG/fPtKkSWPASL9Nz58/p3jx4pQuXTrZtpE2bVqKFCmSbOv/HCIjI+nbty8mJiasXr1a70eIKlWqMGLECGbNmkWlSpU++X3qXc+ePcPJyYkKFSqo076G9wJHR0dDhyBEqiNDzYX4ABs3bqRUqVJ4eHiQPXt21q5dq7Z16NCBhg0bxlmmR48e1K1bV3186tQpWrVqReHChSlRogRDhgzh6dOnavumTZsoUKAAGzZsoEyZMpQoUYIbN24QHR3NwoULqV27NoUKFaJIkSI0a9YMPz8/ve35+vrSsGFDChUqRLVq1di2bRtVq1Zlzpw56jzPnz9n5MiRlC5dmoIFC9K0aVOOHTv2WY7Rq1evmDhxIlWqVKFgwYLUrl2b33//XW+eixcv0rZtW4oWLYqbmxvt2rXj7NmzavvTp08ZMGAAZcqUoWDBgtSrVy/BYcWxknJ85syZQ9WqVfH19aVOnTq4urpSrVq1OOt++PAhvXr1omjRopQpUwYfH58k7fuLFy/Yu3cvnp6e1K5dmzdv3vDHH3+o7eHh4RQtWlRvCDJAVFQUHh4eer3jGzZsoFatWri6ulKxYkXmzJlDdHS02j506FDatm3LqFGjcHd3p2bNmkRHR/P06VPGjBmDp6cnrq6ulChRgp49e8YZ1rh48WIqV65MoUKFaNasGfv27YtzKcG1a9fo2rUr7u7uuLu707NnT+7fv5/g/g8dOhRnZ2eKFi1KdHQ09erVw9nZmXbt2hESEkKBAgXee6nCjh07aNiwIW5ubpQpU4aRI0fy4sULIOa5ix3q37Zt2yQPgRw6dCibN2/mwYMHODs7s2nTJiDmuZgyZQoVKlTA1dWVOnXqsGPHDr1lK1WqxIQJE2jbti2FChVi+PDhAFy9epVevXrh4eGBi4sL5cqVY9y4cYSFhanLPXjwgM2bN6tDSuMbXnrkyBFatGhB0aJFKVmyJAMGDCAwMFBtj30vOHfuHN9//z0FCxbE09OTxYsX68W5bds26tatS6FChfDw8GDgwIE8evQo3uMREBBA5cqVARg2bJjecUwsnvisWLGCiIgIfvrpJ72kG8Dc3JwhQ4bQqFEj9XmE9z/PkPh5+iXOo9evXzNy5EhKlSqFm5sb/fr1Y+nSpTg7O+ttc+/evTRs2JCCBQtSpkwZxo0bR2hoaJL3Jda///7LkCFD1O21atUKf39/tT2217pq1arqOlasWJHg8xIQEICzszMPHjxgy5Yteuf2hQsX6NixIyVLlsTd3Z1u3bpx/fp1ddnYy4rWrl2Lp6cn7u7uHDlyJMFtJVVYWBjTpk3Dy8sLV1dX3N3dad++PVeuXNGb78CBAzRr1owiRYpQtmxZRo4cycuXL/XmOXfuHM2aNaNgwYJUrFiRRYsWvXfbBw4c4Nq1a/Tp0yfenv/evXvTqlUroqKi1GlJPU7Hjh2jQ4cOFC5cmDJlyjB16lT1Nebs7MyJEyc4efKk+v4T33vB5s2bqVmzJgULFqRu3bocO3aMAgUKqO9Xc+bMifPai11/7Gd77HPu4+ND9erVKVy4MBs3bgRiXqctWrTAzc0NV1dXqlevzqpVq9Tl4ntPeHeoeXR0NKtWraJOnTrq5SS//PIL4eHh6jxDhw6lXbt2bNy4Ub3EoV69ehw8ePC9z48Q3wxFCJEk165dU5ycnJSdO3cqiqIo8+bNU1xcXJTHjx8riqIomzdvVpycnJQ7d+6oy7x48UJxcXFRFi1apCiKopw4cUJxcXFROnbsqOzbt0/ZvHmzUrFiRaVWrVrKmzdvFEVRlI0bNypOTk5K9erVlf379yubNm1SdDqdMmnSJKVw4cLK8uXLlePHjytbt25VqlWrppQoUUIJDQ1VFEVRjh07puTPn1/p0aOH4uvrqyxbtkxxd3dXXFxclNmzZyuKoihhYWFK3bp1ldKlSyvr169XfH19lR9++EEpUKCAcvTo0QT338/PT3FyclL8/PwSnOfNmzdK7dq1lVKlSilr1qxRDh48qIwcOVJxcnJSFixYoCiKorx69UopWbKk0qdPH+XIkSPK/v37laZNmyru7u7Ky5cvFUVRlA4dOij16tVT/vrrL+XYsWPK0KFDFScnJ+XYsWMJbjspx2f27NlK4cKFFU9PT2X9+vXKkSNHlA4dOihOTk7KjRs3FEVRlJCQEMXT01OpWrWqsn37dmXnzp1KjRo1FBcXF6VVq1YJbl9RFGXFihVK/vz5lX///VdRFEVp27atUqdOHb15hg0bplSoUEHR6XTqNF9fX8XJyUk5f/68oiiK8uuvvyrOzs7Kzz//rBw6dEhZuHChUrBgQWXYsGHqMkOGDFEKFCigdO7cWTl69Kiyd+9eRafTKY0bN1aqVq2qbNu2TfHz81OWLVumuLm5KR06dFCXnTNnjpIvXz5l6tSpyqFDh5QJEyYoBQsW1Ht+b926pbi5uSmNGjVS9uzZo+zYsUOpU6eOUqZMGeXJkyfx7v/du3cVf39/5aefflIaN26s+Pv7K/7+/sr333+vDBs2TPH391devXoV77Lz5s1TnJ2dlTFjxigHDx5UVq1apZQoUUKpU6eO8ubNGyUwMFBZuXKl4uTkpKxcuVK5dOlSgs+Dp6enMmTIEDWmzp07K2XKlFH8/f2V4OBgRafTKR07dlTc3NwUHx8f5eDBg8qIESMUJycnZfPmzXrrKVCggHqczpw5ozx69Ehxd3dXOnTooOzfv185cuSIMnHiRMXJyUnx9vZWFEVRLl26pJQpU0bp3Lmz4u/vr4SHh6vn9f379xVF+f/7Rf/+/RVfX19l8+bNiqenp1KuXDn1+G7cuFFxdnZWKlasqCxdulQ5evSo0r9/f8XJyUk5ePCgoiiKcurUKSV//vzKnDlzFD8/P2XLli1KmTJllJYtW8Z7bMLDw5U9e/YoTk5OyowZM9TjmJR44lOtWjWlcePGCba/K7HnWVGSdp4m53mkKIrSunVrpVixYsqqVauU/fv3K507d1ZcXV0VJycnddmtW7cqTk5OyoABA5QDBw4oq1evVooXL660bdtWjSsp+/L69WulUqVKSoUKFZSNGzcqhw8fVjp06KAUKVJEuX37tqIoijJixAj1ffzQoUPK9OnTlXz58ilz585N8Hn29/fXex2+evVKOXbsmOLi4qJ06NBB2bt3r7J9+3albt26iru7uxpP7Ht9mTJllJ07dyqbN29WQkJC4t1Oq1atlJYtWyqRkZHx/nv7+fnhhx+UUqVKKRs2bFCOHz+urF+/XilTpoxSo0YNdb59+/Ypzs7OSo8ePZT9+/crmzdvVkqVKqW+f8WeR8WKFVOWL1+uHD16VOnbt6/i5OSk7Nu3L8HX3YgRI5T8+fMrr1+/TnCet33IcSpdurQyd+5c5ejRo8qECRMUJycnZc2aNYqiKIq/v79Sv359pX79+ur7T0LvBcOHD1cOHjyozJ07VylSpIji5OSkbNy4UVGUmNfR26+9WE5OTupn+/379xUnJyfFzc1N+f3335Vdu3YpgYGByv79+xUnJydl3LhxytGjR5V9+/YpnTp1UpycnJSzZ88m+J4wZMgQxdPTU93Wjz/+qLi4uCgzZ85UDh8+rCxcuFApXLiw0qFDB/X5GzJkiFK0aFGlRo0ayrZt2xRfX1+lQYMGSqFChZTnz58n6dgLkZpJ4i1EEk2cOFEpUaKEEh4eriiKojx8+FDJly+fmlCGhIQoRYoU0fsitGHDBiVfvnxKUFCQoiiK8v333yu1a9dWoqKi1Hlu3bql5M+fX1m5cqWiKP//YrFlyxa97ffv319ZunSp3rTdu3crTk5Oir+/v6IoitKiRQulbt26el92tm3bpvfhvG7dOvUDN5ZOp1NatmypNGzYMMH9T0rivWrVKsXJyUk5c+aM3vQff/xRKViwoPLs2TPF399fcXJyUk6fPq223717V5kyZYoSGBioKIqiuLq6qsdVURQlOjpamTRpkt4y70rK8Yn98vL2DwwPHjxQnJyclMWLFyuKoigrV65UnJ2dlevXr6vzPHz4MEmJd4MGDZSuXbuqj//44484+xp7HE+ePKlOGzRokFK9enVFURTl5cuXSqFChZSRI0fqrXv9+vWKk5OTcu3aNUVRYr7gODk5qcdMURQlKChIad26td66FUVRfv75Z8XV1VVRlJjXaaFChZSff/5Zb57YxDP2+e3fv79SunRpvUT52bNnStGiRZVJkya99zj07NlTmTJlivq4dOnSyl9//ZXg/M+fP1dcXV2VESNG6E0/efKkmmgrStJeg4qin3grStwvkIcPH1acnJyU7du36y03cOBApUyZMkpkZKS6nipVqujNc+jQIaVly5ZxfkCoXbu23o8b78bw9pft6OhopUyZMnrzK0rMeeDi4qJMnjxZb5n169er84SHhysFCxZUxo4dqyiKonh7eytubm7q+5KixCSgc+bM0XsfeFvsF/TYL/VJjSc+hQsXVvr27Ztg+9uS+jwn5TxNzvPo6NGjipOTk7J79251WnR0tFKjRg01+dHpdEr58uWVjh076q0/dtn9+/cneV9WrFihODs7K5cvX1bnCQ0NVby8vJT169crt27dUpydndUfdmLNmDFDKViwoPL06dP4D7gS93XYuHFjpWbNmnqfQS9evFBKlCih9O7dW+/Yzps3L8H1xmrVqpXi5OSU4L/YH6rDw8OVDh06xDnnlixZojg5Oak/VjZo0ECpX7++3mt3+/btipeXl/L48WP1nFi9erXesXJxcVEmTJiQYJydO3dWSpcunej+xPqQ4zRjxgy9ZStVqqT3OdCqVSu9z453E++KFSvqza8oMef1xybeP/74o948v/32m95rQFFi3svf/rHw3fcERdF/37x+/bre/LG2bNmiODk5Kb6+vuoyTk5Oyt27d9V5Tpw4oTg5OSm7du2KE78Q3xoZai5EEkRGRrJ161aqVKlCWFgYL1++xNLSkqJFi7J+/Xp0Oh0WFhZUqVJFb7jq9u3bKVWqFFmyZOHNmzecO3eOChUqoCgKUVFRREVFkS1bNnLnzh1nKF/+/Pn1Hk+bNo22bdvy9OlTTp06xcaNG9m6dSsQU5E0IiICf39/vLy89Aq+Va9eHWPj/5dzOHbsGJkzZ8bFxUWNITo6Gk9PTy5evKg35PNDnThxAgcHB9zc3PSm161bl/DwcM6dO0fevHmxsbGhW7dujBw5kr/++otMmTIxaNAgbG1tAShZsiRz5syhd+/ebNiwgSdPnjBkyBDc3d0T3HZix+dtb1+PGLvN2OGhp06dwtHRkTx58qjz2NnZJXoN49WrV7l06RJeXl68fPmSly9f4uHhgYWFBevWrVPnK1GiBPb29mzfvh2IGTa7d+9e6tWrB4C/vz9hYWFUqlRJfX6ioqLUIX9vv07Sp0+vxg+QJUsWli9fTtGiRQkICODIkSOsWLGCM2fOqMfg7NmzhIWFUb16db34a9eurffYz8+PEiVKYGZmpsaQNm1aihUrxtGjR+M9BjqdjqioKM6dO4erqytRUVHcu3ePJ0+eULBgQaKiouK9Xc3Zs2eJiIiIE0OxYsVwcHDgxIkT7z32H+rYsWNoNBoqVKgQ5xg/fvxYbzjpu+dh2bJlWblyJaampty4cYO///6bBQsW8PTp0zivs4Tcvn2bx48fx9lfR0dH3Nzc4uzv2+dTmjRpsLGxUV+vxYsX582bN9SuXZtp06Zx6tQpypYtS69evZJc+PFD43mbkZGR3tDt9/nQ5/l952lynkd+fn6YmJhQpUoVdZpWq6VmzZrq41u3bhEUFBRn/cWLFydt2rRx3s/fty+nT58ma9aseq81c3Nzdu/eTZMmTfDz80NRlHj3JTw8nNOnTyd4zN8WGhrKhQsXqFGjht5lAdbW1nh6esY5/u++9hPi4uLC77//Hu+/2BooadKkYfHixdSsWZNHjx7h5+fH2rVr2b9/PxDzHh0WFsbly5epUqWK3mu3Zs2a7N69m0yZMqnT3q7xYG5uTqZMmeIMR3/bh7xOP/Q4vft5Z2trq3e5wfvcvXuXhw8fxnk/rlWrVpKWj8+7z1unTp2YNGkSISEhXLx4kR07duDt7Q3E/WxMSOw+vxtXrVq1MDIy0ruEyMbGRu/68NjX+5s3bz58Z4RIZaS4mhBJ4OvrS3BwsPpl4l2HDh2iQoUK1KtXj61bt3L16lUyZcrE8ePHmTBhAhBzGxOdTsdvv/3Gb7/9Fmcdpqameo8tLCz0Hl+4cIExY8Zw4cIFzM3NyZMnD/b29kDMvTefP39OdHR0nOvXjIyMSJ8+vfr4+fPnPH78GBcXl3j39fHjx6RLly7xgxKPFy9ekDlz5jjTY78wxf5gsWrVKhYsWMDOnTtZt24dZmZm1KtXj59++ok0adIwY8YMfv31V3bu3Mnu3bvRarWULl2asWPH4uDgEO+2Ezs+bzM3N1f/1mq1evO8ePGCDBkyxFl/5syZefLkSYL7Hvu6GDZsGMOGDdNr27lzJz/++CPp0qVDo9FQp04dNmzYwE8//cT+/fsJDQ2lTp06wP8LiCV0+6t///1X/Tu+isFbt25l+vTpBAYGkj59evLnz4+ZmZnaHltP4N3iPu++bp4/f86OHTviXPcc37KxfvzxRzZv3gxA37599drKly8PwPLlyylZsqReW+yPPW9/sY6VKVMmXr16Fe/2Ptbz589RFCXBH3L+/fdf9cvru+ehTqdj+vTprFq1itDQUOzs7ChUqFCc8zex7UPC+3v58mW9aW8/fxDzmo19vbq5ubFw4UKWLl2Kj48PCxcuJFOmTHTr1i1Jt7/7mHjeZm9vz8OHDxNsj4yM5MWLF2TKlOmDn+f3nafJeR49e/aM9OnTq9uM9fY5Erv+MWPGxFut/+31J7Yvz58/f2/F8beLCsYnoev53/Xq1SsURUny8X/3tZ8QS0tLChYsmOh8hw4dYsKECdy6dQtLS0vy5cunbkNRFF68eIGiKEmqvv728QT9cyI+Dg4O+Pr6EhISkmCl9aCgIGxtbT/4OL3v/ExM7Pvxu/sc37aT6t3n7enTp4waNYq9e/ei0WjInj27+sNFUuOMPXff/Xw3NjYmQ4YMesfk3ecm9kcUnU73YTsiRCokibcQSbBx40ayZcvG+PHj9aYrikKvXr1Yu3YtFSpUoFSpUmTOnJmdO3eSOXNmTE1N8fLyAmK+nGg0Gtq1axfvF6h3P6ze9vr1azp16oSzszPbt28nV65caLVaDhw4wO7du4GYD24TE5M4yaFOp1O/uAFYWVmRI0cOfvnll3i3lTVr1iQdk/ikS5eOu3fvxpn++PFjADWhzZUrl1qA5vz58/zxxx+sWbMGR0dHOnXqhJWVFYMGDWLQoEHcunWLv//+m/nz5zNmzBgWLlwYZ/1JOT5JlSFDhnj34e1j+K6IiAj+/PNPvLy8aNWqlV5bQECAmpC2a9cOgHr16uHt7c3x48fZsWMHxYsXV39QsLa2BuCXX34hR44ccbb1vi9kp06dYsiQIbRu3ZqOHTuSJUsWIOaWQrG9YrG9D8HBwXq343q7wB/EvE5Kly5N+/bt42zn7REUb+vVqxe5c+dmyZIl6vO0dOlSnjx5wsCBAwHImTNnnOVif+iJvQ3b2x4/fky2bNkS3OePYWVlhYWFRYL3Fc+ePXuCy8YmuWPGjMHLywsrKysA9dZpSRH7Q1h8P+Q8fvw43h9+3qdcuXKUK1eON2/e4Ofnx/Llyxk3bhyFCxemUKFCyRpP2bJlWbZsGY8fP473R7cDBw7Qs2dP5s6d+9mf5+Q6j7JkycKzZ8/Q6XR6yXdwcLD6d+z6Bw8erHfbuFgf8uOllZVVvPd0PnPmDOnSpVO3tWzZsniTxtgfGJOyHY1Gk+Dz/PYPtJ/bvXv36NmzJ1WqVMHb25ts2bKh0WhYtWoVhw4dAmKqo2s0mjjvReHh4fj5+VG4cOGP3n7ZsmVZsWIFhw4ditO7DDHvf5UrV6ZFixb069fvix2nt9+P3/bu49jkNTo6Wu2FDwkJSdI2Bg4cyK1bt1i6dClubm6kSZOGN2/esH79+iTHGft6fvz4sd6P35GRkTx79uyD37OE+FbJUHMhEvH48WMOHTpErVq1KFmypN4/Dw8PqlevzoEDB3j06BFGRkbUqVOH/fv3s2vXLqpUqaL++pw2bVoKFCjArVu3KFiwoPovb968zJkz573Vnm/dusXz589p06YNefLkUb8MxlYK1el0GBkZ4e7uzt9//6237L59+/QqtZYoUYLAwEAyZsyoF8eRI0dYtGhRnMrEH6J48eI8ePBArxovxPTCmpiYUKhQIXbt2oWHhwePHz/GyMgINzc3Ro8ejbW1NQ8fPuTBgwdUqFCBXbt2ATFJeufOnSldunSCPWtJOT5J5eHhQUBAABcuXFCnPX36VK/q+rv27dvH8+fPadasWZzXSKNGjciRI4fecPPcuXPj4uLC9u3bOXDggF7V+8KFC2NiYsKjR4/0nh9jY2OmT58e7xf0WP7+/uh0On744Qc16Y6OjlaHhut0OvLly4eVlRV//fWX3rJ79uzRexxbTT9//vxqDK6urixdujTOsrGyZs1KeHi43jLPnz/H3d1dfRzfrdUKFy5MmjRp1Nv1xTp16hQPHz587yUGSfFuz2WJEiUIDQ1FURS9Y3zt2jXmzZund7686/Tp0+TJk4dGjRqpSfejR4+4du2a3uvs3W2+LWfOnGTOnDnO/t6/f5+zZ89+0P5OnjyZRo0aoSgK5ubmeHp6MmTIEIAEz5d3z/FPiadly5aYmJgwfvz4OEN5Q0NDmT17NhkyZKB8+fKf/XlOrvOoRIkSREVFsW/fPnWaoijs3btXfZwrVy4yZsxIQECA3vqzZMnCtGnT3jtK4F3FihXj/v37epc4hIeH88MPP/D777+rvZPPnj3T29bTp0+ZNWvWe38UfJuFhQWurq7s3LlT77l69eoVvr6+6u3/ksPFixcJDw+nS5cuODo6qolkbNKtKAqWlpbkz59fHX4e6+DBg3Tp0iXOKIIPUbZsWZycnJgxYwbPnj2L0z5t2jSioqKoU6fOFz1Otra2ODo6Jvp+HPu+GRQUpE5L6iUGp0+fxsvLi5IlS6q39Hv3szGxz/3YH5diL+2ItX37dqKjo5P1tSNEaiI93kIkYsuWLURFRSU4zK9+/fps2LCB9evX88MPP1CvXj2WLFmCVquNM6S8f//+dOnShQEDBlC3bl2io6NZsmQJ586do0ePHgnGkDNnTtKmTcuvv/6KsbExxsbG7N69Wx3eHHvtVO/evWndujW9e/emcePGPHz4kFmzZgH//8W8YcOGrFy5kvbt29OtWzfs7Ow4evQov/32G61atcLExOS9x2P37t1xbv8C0KRJExo2bMjq1avp2bMnvXv3JmvWrOzbt4+NGzfSq1cvrK2tcXd3R6fT0bNnT7p06YKlpSU7d+7k1atXeHl54eDggK2tLePGjeP169c4Ojpy8eJFDhw4QNeuXT/p+CRFvXr1WL58Ob169aJfv36kTZuWBQsWvDd537hxIxkzZsTDwyPe9rp16zJ79myOHz+uDrOuV68ekydPxtjYWK8HJkOGDHTq1IlZs2bx+vVrSpYsyaNHj5g1axYajea995iN7d0cO3asegunVatWcfXqVSAmEUqbNi2dOnVi9uzZmJubU6JECU6cOMGaNWuA/yeMPXr0oFmzZnTt2pXmzZtjamrKunXr2Lt3L7Nnz04whn/++Ucvxn/++Yfvv/8+wfkhpse1S5cuzJs3DxMTEzw9PQkICGDWrFnkyZOHBg0avHf5xFhbW/PkyRMOHDhA/vz5qVChAsWLF6dHjx706NGD3Llzc/78eWbPnk25cuXee4/dQoUKMX/+fBYuXEiRIkW4e/cu3t7eRERE6L3OrK2tuXz5MidOnIjT66zVaunfvz/Dhg1T3wuePXum9grHN8ogIR4eHvj4+DB06FDq1q1LZGQkixYtIn369Am+HmN/MDh27Bi5c+emcOHCHx1P1qxZGT16NMOHD6dly5Y0a9YMOzs77t27h4+PD/fv32fx4sWYmppiamr62Z/n5DiPihcvTpkyZRg+fDhPnjzB3t6e33//nX/++Ud9HzUyMqJfv36MHDkSIyMjPD09efnyJfPnz+fRo0cJXsoTn4YNG7JixQq6d+9O7969yZAhA8uXLycyMpIWLVqQLVs26taty4gRI3jw4AGurq7cvn2bGTNmkDVr1nh79BMyYMAAOnbsSJcuXWjRogWRkZEsXLiQiIgIevbsmeT1vO3169fv/WGyYMGCuLi4YGxszNSpU+nQoQMRERFs2rQJX19f4P/Xu/fu3Zvu3bvTv39/6tevz5MnT5g+fTpVqlTBycmJixcvflSMxsbGTJkyhQ4dOtCoUSPatGlDvnz5ePr0KZs2beLQoUMMGDBAPVeT4zjFR6PR0Lt3bwYOHMioUaOoWrUqV69eZd68ecD/348rVKjAxIkTGTlyJB07diQwMJB58+YlOGz+bYUKFeLPP//ExcUFW1tbzpw5w8KFC9FoNOp7VnzvCW+LPT9nz57NmzdvKF68OFeuXGHu3LmULFmScuXKfbZjIkRqJom3EInYtGkTefPmxcnJKd72okWLkjVrVjZs2ECPHj3Ily8fTk5OPHv2TC0sE6ts2bIsXryYuXPn0rt3b0xMTHBxccHHx+e9xbusrKyYP38+U6ZMoU+fPmrPwMqVK+ncuTOnTp2iUqVKFCtWjDlz5jBr1ix69OiBg4MDI0aMoF+/fuoHtIWFBatWrWLatGlMnTqVV69e4eDgwIABA+jQoUOixyP23p/vql69Ora2tqxYsYJp06apX3hz5crF+PHj1aG43333HYsWLWLWrFkMHz6cN2/eqL3+sYnC3LlzmT59OrNmzeLZs2fY2dnRq1evBK/XTOrxSYo0adKwbNkyJkyYwPjx49FoNDRt2pRs2bLFGf4HMb2dR44coVmzZgn2GtSrV485c+awdu1aNfGuXbs2U6ZMwdPTU/3SE6tv375kzpyZ1atXs2jRItKlS0epUqXo379/nHnfVrJkSUaOHImPjw+7du0iU6ZMlCxZkrlz59KzZ09Onz5NhQoV6Nq1K4qisG7dOhYvXkzhwoUZOHAgEydOVEdo5MuXj1WrVjFjxgwGDx6Moig4OTkxb9489Z6v8bl69SpVq1YFYkYKPH78+L1JTqwffviBTJkysXLlStatW0f69OmpXr06ffv2TfK1pglp2LChOuS5d+/edOnShYULFzJr1iy8vb0JDg4mS5YstG/fPtEv1V27duXZs2csX76cefPmYWdnR7169dBoNHh7e/Py5Uusra3p0KEDEyZMoGPHjvHeB75hw4ZYWlri7e1Nz549SZs2LeXKlaN///7xDtlOSIUKFfjll19YsmSJWlCtaNGiLF++PMEhsWnTpqV9+/asW7eOAwcOcOTIkU+Kp0GDBmTPnp1ly5Yxc+ZMgoODyZw5M+7u7syZM4fcuXOr837u5zk5ziOAGTNmMGnSJLUntHLlyjRv3lzv/ttNmjTB0tKSRYsWsW7dOiwsLHB3d+eXX375oGHzadOmZeXKlUyZMoWff/4ZnU5HkSJFWL58ubqeiRMn4u3tzdq1awkKCiJjxozUrFmTvn37ftAopVKlSuHj48Ps2bPp378/adKkoVixYkyePJm8efMmeT1vu3z58nt/XDt58iTZs2dn2rRpzJ07l+7du5MuXTqKFCnCihUraN26NadOncLZ2RlPT09+/fVX9T3LxsaGOnXq8MMPP3xUbG/Lnz8/v//+Oz4+PqxZs4ZHjx5hYWGBs7MzixYt0ksek+M4JaROnTqEhoayePFiNm7cSN68eRk+fDjDhw9Xz4mcOXMyefJkFixYQJcuXcidOzc///wzP//8c6LrnzRpkt68OXLkYMyYMWzdupVTp04B8b8nvGv8+PFkz56djRs38ttvv/Hdd9/Rpk0bevTo8d4RPkKI/9MoSa2sIIRI8f7++29sbW31eluuX79O7dq1mT9//nsTJvFtiIqKYtu2bZQsWRI7Ozt1+qpVqxg3bhzHjx9XrykV4lv04MEDzp49S+XKlfUKZ/Xu3Zv79++rBQSF+By2bdtGgQIF9Ooe+Pr60rVrV/74448k/XAphPg6SI+3EKnI4cOH2bFjBwMHDiRnzpw8evSIBQsWkCtXLsqWLWvo8EQKYGxszG+//cayZcvo3r07GTJk4Nq1a8ycOZP69etL0i2+eVqtlqFDh1K5cmUaN26MkZERhw4dYs+ePUycONHQ4YlUZuvWrcyYMYO+fftiZ2fH3bt3mT17NiVKlJCkW4hURnq8hUhFwsLCmDVrFrt37+bff/8lffr0lCtXjgEDBnzS7UlE6nL//n2mT5/O8ePHefnyJfb29tStW5euXbsmeo2/EN8CPz8/5s2bx5UrV4iKiiJ37ty0b98+zj3IhfhUz549Y9q0aRw8eJCnT5+SKVMmqlWrRu/evZN0DbcQ4ushibcQQgghhBBCCJGMpBqCEEIIIYQQQgiRjCTxFkIIIYQQQgghkpEk3kIIIYQQQgghRDKSxFsIIYQQQgghhEhGkngLIYQQQgghhBDJSO7jnYyePn2F1Iz/eBoN2NhYyXEU4guQ802IL0vOOSG+LDnnPo/Y4yg+nCTeyUhRQKczdBRfL40m5n+dDnmDFCKZyfkmxJcl55wQX5acc5+HVsZLfzQ5dEIIIYQQQgghRDKSxFsIIYQQQgghhEhGkngLIYQQQgghhBDJSK7xFkIIIYQQqY6iKERFRRo6DJFCaDQQFhZGZGSEXOP9HkZGxmjlQu5kIYm3EEIIIYRIVaKiIgkODkJRpMqt+L+nT7XopPJxoszN02JtbYMmtiKd+Cwk8RZCCCGEEKmGoii8ePEUrVZLunSZ0Wik907EMDLSEB0t3d0JURSFiIhwXr9+BkC6dBkNHFHqIom3EEIIIYRINXS6aCIjw0iXLhNp0pgZOhyRghgba4mKkh7v90mTxhSA16+fYWWVQYadf0ZyJIUQQgghRKoRO5TYyEj6l4T4GLHJd3R0lIEjSV0k8RZCCCGEEKmOXJ8qxMeRcyd5SOIthBBCCCGEEEIkIxmDI4QQQgghUj2tVoNW++V68nQ6BZ0uaYW8xo8fzc6d2xJsnz37V9zdi32u0PSULVssWdf/oZo0qYtOp+P33/9MNT2vKe0YC8OQxFsIIYQQQqRqWq2GdOksMTb+colcVJTCixchSUq++/QZSLduvQD4+++/WLt2Jb/9tkxtt7ZOl2xxpiQXL54nLCyM8PBw/P1PS6IqUhVJvIUQQgghRKqm1WowNtbQsiVcuZL828ufH1atiulhT0rinTZtWtKmTav+rdVqyZgxU3KHmeLs3bubwoWLEBUVxc6d2yTxFqmKJN5CCCGEEOKbcOUK+PsbOooP8+LFc+rU8WLp0tXkypWHqKgoqlevSPPmrenYsSsAo0cPx97egS5denDx4nnmzZvF9ev/kCGDDS1btqF+/cZJ2tbjx/8ya9YvnDp1kvDwMHLmzEXfvoMoVKgIkyeP4+nTYCZPnqHOP2PGFF6/fsWIET/z6FEQ06dP5tSpE2TIYEPNmnVo27YjRkZG7NjxJ3/+uZn06W04c+YkAwYMxcurht62dTod+/fvpVWr9piamjJnzgz69x+Cubk5d+/eoWXLxqxf/wf29g4A3L9/jxYtGvH773+SJYstW7ZsZNWqZTx//gxn5/z06zeY3LnzANC4cR0qVarK7t07sLGxYcmSVRw5cpDFi725e/cOadKkoWTJ0gwZ8hMWFhYA7Nmzk0WLfiU4+AnlylVEURQcHbPTsWNXFEVh2bLFbN78O+HhYRQq5Eb//kOwtbX9qOf4yJFDLF78K3fu3MHe3p7OnbtToUIlAK5fv8a0aZO4fv0frKysqVevIe3bdwbg9OmTzJkzg3v37pAxY+b/nutGHxWDSH5SXE0IIYQQQogUKl269Dg758Pf/zQAV65cIjw8nPPnzwGgKAqnT5+kZMnS3Llzm969u1OkiDtLlqykQ4cuzJ07kwMH9idpW2PHjiA6Woe3tw9Llqwic+bvmDZtEgBVqlTj5MnjhIS8BmISZV/ffVSuXA1FURg+fDAZMtjg47OKH38cxV9/7WLFCh913RcunCdnzlx4ey+lRIlScbZ95swpgoODKVOmHGXKlCM8PAxf378ByJ49B3nyOOnth6/v37i6FiJLFlsOHz6Ij89C+vYdxJIlqyhc2I3evbvy8uVLdf6//trFrFnz+PHH0Tx8+ICffhpCgwZNWLXqd8aOncTp0yfYunUTAOfOnWXixLG0aNGGJUtWYW5uzr59f6nr2rhxHXv27GTUqHF4ey/FxsaG/v17EhX14bffOn36JMOHD6J69VosXbqa2rXrMXLkMK5ejRmaMW7cKPLmdWbFivUMHTqCVauWcezYYaKjoxkxYiienpVZtep3OnfuxvTpk7l9+9YHxyC+DEm8hRBCCCGESMGKF/dQE++zZ/3x8CjN5csXiY6O5saN60RGRuDi4sqff27GycmZrl174uiYgxo1atOo0fesXr080W0oikK5chXp128Q2bPnIGfOXDRs2FRN5NzcimJlZc2RI4cAOHfOn8jISEqU8OD06ZMEBQUyePBwHB1z4O5ejJ49+7J+/Rp1/RqNhrZtO5AjR07Sp08fZ/t79+4md+682Ns7kDFjJlxcCrJr13a1vXJlLw4c2Kc+3r//bypXrgrA6tXLad26PWXKlCNbNkc6d+5Olix27NmzQ53fy6sGefLkJW9eJ3Q6HX37DqJu3QbY2dlTooQHRYuWUPd18+YNVKpUlfr1G5E9ew4GDBhK5szfqetavXoFPXr0wd29GNmz52DQoB95+fIlfn5HEz3O79q4cT0VK1amadMWODpmp1mzVlSsWIk1a1YAEBT0kHTp0mFra4eHR2lmzpyPk1M+QkJe8/LlC2xsMmJnZ4+XVw1mzpz/TV6i8LWQoeZCCCGEEEKkYCVLlmLr1s0oisK5c2eoVasuly9f5Pr1a/j7n6ZYsRIYGxtz584dChRw0Vu2YMFC/PHHRgCqVi2nTi9UyI1p02arjzUaDQ0aNGbv3t1cvHieu3fv8M8/V9HpdABotVoqVarK/v178fKqwb59e6lQwRNjY2Pu3r3Ny5cvqFatgro+nU5HeHg4L148ByBDBhtMTc3i3b/IyEgOHNhP48bfq9MqVPBk3rxZBAUFYWtrS5UqXvz223yePHlMZGQkN29ex9OzCgB3795m/vw5eHvPU5ePiIjg/v176mM7Ozv172zZHDExScOyZYu5desmd+7c4vbtW1SrVhOAmzevU69eQ3V+Y2Nj8uUrAEBoaCj//vuIUaOGodX+vw8zPDxcb3tJdffuberV0x8e7upamO3btwLQunV7vL3n8ccfmyhduizVqtVUk+v69RszefI4li5dRJky5ahVqx7W1tYfHIP4MiTxFkIIIYQQIgVzcSlIREQEN25c58KFc/z44ygKFizMhQvnOH36hHo9cJo0aeIsGx2tIzo6Jnn28VmtTjc1NdWbT6fT0a9fT169ekXlylUpU6Y8kZGRDB8+SJ2nSpVq/PBDV0JCXnPw4D5GjPj5v21E4+iYg0mTpsXZvqVl2gRji3X8+FFevXrJsmWLWb58CRDTA68oCrt3b6dt247Y2dmTL18BDhzYT0REBIULu6kJaHR0NL1796dYsRLvbNtS/TtNmv/v7/Xr1+jRoxNly5anSBF3mjVrqdc7b2RkjPJOTTzlvwnR0dEA/PzzZBwds+vN8zFJb3zHRaeLRqeL2U6rVu2oVKkqBw/u58iRQ/Tp053Bg4dTp059Bg4cSsOGTTh0yJdDhw7wxx+bmDRpOqVKlfngOETyk6HmQgghhBBCpGDGxsYULVqMTZs2kCFDRmxsMlKokBunT5/g7NkzlCwZc820o2N2Ll26qLfspUvn1QQxa9Zs6r+3h04D3Llzi7NnzzBz5nzatOlA6dJlCQ5+Avw/6XRxcSVz5sysWrUcRYkZfg6QLVt2Hj0KIn36DOr6AwMfsHixd5Luxb137x6yZ8/B0qWr8fFZhY/PKpYuXU2RIu56w82rVPHi2LHDHDrkS+XKXur0bNmy8/jxv3r7t3z5Ei5duhDv9nbv3kGRIm6MGjWOBg0akz+/CwEB99T9zJkzF//88//y9zFD+q8BYGVlRYYMNjx9+kTdVpYstsyfP5t79+4muq/vinnO9OO8ePECjo7ZCQ8PZ+bMXzAxMaFZs1bMmeNN3boN8PXdR3DwE6ZNm0zWrNlo27YjixYtp2jREhw5cvCDYxBfhiTeQgghhBBCpHDFi3uwa9c2ChUqDEDhwm4cOXIIOzt7vvsuCwANGjTh+vVreHvP4969u+zcuY1NmzbQsGGTRNefNq0VWq2Wv//eTVBQIPv372XJEm8gZth2rMqVvVi7dhWenpUxMjICoEQJD2xtbRk7dgQ3b97g3Dl/pkyZgJmZmTpPQsLCwjhy5CC1atUjV648ev8aNmzK/fv3uHjxPACVKlXl3Dl/rl69QsWKldR1xPZY79q1nQcPApg/fzb79v1F9uw5491munTpuHnzBpcvX+TevbvMmTODK1cuExkZs5+NGjXl77/3sG3bFu7du8Ps2dMIDHyo/ojw/fctWLhwAYcPH+T+/XtMmvQzFy6cw9ExR4L7eeXKJfz8jur9CwsLo2nTlvj6/s369Wu4f/8e69at4uDB/TRo0ARTU1POnz/LjBlTuXfvDlevXubcOX+cnJyxtk7HwYP7mD17Og8eBHD27Blu3LhG3rzOiTzThhEeHo6rqyu+vr7qtNu3b1OlShUsLS0pUKAAe/bs0Vtm7969uLq6YmFhQaVKlbh1S79w3MyZM3FwcMDKyoqOHTsSGhqqtoWFhdGxY0fSp0+PnZ0d06bFHY3xpclQc5EyRUdjcvwohL7AxCIdESVLQyJv3EKIjyTnmxDiG5E//9e7nZIlSzF9eiSFChUBwNk5H6amppQsWVqdx9bWlilTZjB//izWrl1Jliy29OrVj1q16ia6/u++y8KAAUNZunQR3t7zyJYtO336DGTcuFFcv/4Prq6FgJjEe/nyJXo9zkZGRkyaNJ2ZM6fSpUtbzM0t8PSsQq9efRLd7uHDB4iMjKRGjVpx2sqXr0jGjBnZuXMbrq6FyJQpM87O+TE1NSNduvTqfJUre/H06VMWLfqVp0+fkjNnLiZPnkG2bI7xbrNx42Zcu/YPffv2JE2aNBQp4kb79p3Zu3c3AK6uhejffwhLlvzGixfP8fSsgqtrIUxMTABo3rw1oaGhTJ06npCQEPLlK8D06XPeO9R8wYI5caatXbsZFxdXRowYy5IlC1mwYDaOjtkZO3YiRYsWB2Ds2IlMnz6ZTp3aYmRkRKVKVWjXriMmJiZMmjSdWbOm0bZtMywsLKlVqy516tRP9Jh/aWFhYbRo0YJLly6p0xRFoX79+hQsWJBTp06xZcsWGjRowJUrV3B0dOTevXvUr1+fMWPGUL16dcaOHUv9+vU5d+4cGo2GjRs3Mnr0aFauXEmWLFlo164dgwcPZu7cuQAMGjSIU6dOsW/fPu7evUvbtm3Jnj07jRsn7dZ6yUGjKO9ewSA+l+DgV/xXj0J8gDTbtpL2p8EYPXyoTou2t+f1uClE1E78g0MIkXRyvglhGBoNZMpkxZMnr+JcSyo+TWRkBMHBgWTMaIeJScz1s1qthnTpLDE2TnzY8+cSFaXw4kUIOl3qeoJPnvRj8uTxbNiwNUnDyFMSY2MtUVGJfzm/fPkiadOm1evBbtWqKS1atKZmzTrJGGHKEN85FEurhYwZrZK8rsuXL9OiRQsUReH8+fPs37+fihUrsm/fPurWrcujR4/Ua/GrVKlC2bJlGT16NCNHjuTgwYNqD3loaCi2trZs3bqVihUrUr58eSpVqsTo0aMBOHz4MF5eXjx58gRFUciUKRM7d+6kYsWKAIwbN469e/fq9bh/adLjLVKUNNu2Yt2xNe9+C9EGBmLdsTUvF6+QZECIz0TONyHEt0Kni0mCtdovlyjqdEqqSrqfPHnC+fNnWbFiCbVr1/vqku4PcfHiBTZuXMdPP40hY8ZM7N27m3//faReSy+S7sCBA3h6ejJ+/Hi9Ynd+fn64u7vrTStbtizHjh1T28uXL6+2WVhY4O7uzrFjxyhXrhwnT55Uk24ADw8PIiIiOHfuHIqiEBkZSenSpfXWPX78eHQ6nV41+i9JEm+RckRHk/anwaAovPtWrlEUFI2GtD8O4plHKRkGK8Snio4m7Y+D3n++/TSEpzVqyfkmhEgVUlsi/KW9fv2KiRPH4uLiSrNmrQwdTrJq2LAJgYEPGT58MK9fvyZvXid++WWW3CP7I3Tv3j3e6YGBgdjb2+tNy5IlCwEBAYm2P3/+nLCwML12Y2NjMmbMSEBAAFqtlkyZMulVjM+SJQthYWEEBweTOXPmz7V7H0QS72Sk0cT8E0ljcvyo3nDXd2kUBaOgQDIVyP0FoxLi26RRFIwePiDN8aNElimX+AJCiA8S+/1Avid8fhqNBq1Wi0ajkeP7GeXMmZO9e7/+itlJeU2YmBjTt+8A+vYdkPwBpUDvO4diH798+VJvuqmpaZxb1L1PaGhonPlNTU0JDw9PtD22iFpC7YqixNsGqOs3BEm8k5GNTdKvfxBA6AtDRyCEeEe60BeQSd7LhEguH3KtpEg6OzsbQ4cgxFctsXMoa9asvHr1Sn08atQovaHfiTEzMyM4OFhvWnh4OBYWFmr7u0lyeHg46dOnx8zMTH0c3/LR0dHxtgHq+g1BEu9k9PSpFFf7ECYW6UiXhPlebPyTyDJlkz0eIVIzkyOHSdco8QIxLyzSEfnkVaLzCWFoWq0G63RmGBvJV5vkFBUdxcsXYSl6yHZkZCRPnwZhY2OrVqEWAsDISEt0tHw5T8z7ziGtNqZzMXZIeKwP6e0GcHBw0KtyDhAUFISdnZ3aHhQUFKe9SJEiZMyYETMzM4KCgsiXLx8AUVFRBAcHY2dnh6IoPHnyhKioKIyNjdVlzc3NSZ8+/QfF+TnJp1MyUpQ4NYvEe0SULE20vT3awEA08Rw4RaNBZ2dPROmyoJVrToX4FBGlyybtfCtZGuR9THwFNBoNxkbGtNzUkiuPrxg6nFQpf+b8rGq4Co1GQ0q+KY6iKOh0OhRFke9hIg55TSTufedQ7OP33TotKTw8PJg0aRJv3rzB3NwciKlMXrZsWbX98OHD6vyhoaH4+/szevRotFotxYsX5/Dhw2rV8mPHjmFiYkLhwjH3uTcxMcHPz09d3+HDhylevLjBCquBJN4iJTEy4vW4KVh3bI2i0eglA8p/F5S8HjdZCj0J8Tm873z7738538TX6MrjK/gH+Rs6DCGEEO9RoUIFsmXLRvv27RkxYgR//vknJ06cwMfHB4AOHTowdepUJk2aRJ06dRg7diw5c+ZUE+0ePXrQtWtXXF1dcXBwoHv37nTu3FkdSt62bVu6deuGj48PDx484JdfflHXbSiGS/mFiEdE7bq8XLwC3X/DTGLp7Ozl1kZCfGYJnW8a4E2nbnK+CSGEECJZGBkZ8ccffxAYGEjRokVZuXIlmzdvxtHREYAcOXKwadMmfHx8KF68OMHBwWzZskW9jV2zZs0YNmwYXbt2pWrVqpQsWZIpU6ao658+fTpFixbF09OTnj17MmbMGBo2bGiQfY2lUVLyWKGvXHCwXOP90aKjSXP8KOlCX/DCIl3McFfpeRMiebx1vr35cwfma1YS4VGaF1t3GToyIZLM2FhLhgyWuHu7S493MnGzdeNM1zM8exZCVFTK/YITGRlBcHAgGTPaYWKSJvEFxDfD2Fibol+7KcX7ziGtVopCfiwZai5SJiOjmFsYZbKKKewkPw8JkXzeOt/euLpjtn4NafyOYvTPVaKd8xk6OiGE+Cy0Wg1a7Ze7v9iH3Dd8/PjR7Ny5LcH22bN/VduHDx/9OcL7YIGBD2nSpC4bNmzFzs4+8QWS2Zs3b6hTpypOTvmYP3+RocP5LFLaMRaflyTeQgghVDo7eyKqVsd013bMVi4l5OdJhg5JCCE+mVarIV36L1v1Pio6ihfPk1YBvk+fgXTr1guAv//+i7VrV/Lbb8vUdmvrdO9NzL9Fhw8fIGPGTFy4cI4HDwJwcMhq6JCEeK8UkXiHh4dTtGhR5s6dq14wf+/ePbp164avry/29vZMmDCBpk2bqsusWbOGn376icDAQKpVq8Zvv/1GpkyZgJhKfMOGDWPx4sVER0fTqVMnJk2apFaxCw4OpkuXLuzZs4dMmTLx888/06pVK3Xd/v7+dOvWjQsXLuDi4sKvv/5K0aJFv9wBEUIIAwpr2z4m8V63mpAfR8F/1UaFEOJrpdV+2ar3sRXgtVpNkhLvtGnTkjZtWvVvrVZLxoyZkjvMr9revbspV64iJ08eZ9eu7XTs2NXQIQnxXgZPvMPCwmjRooXefdyioqKoVasWuXLlwt/fH19fX1q1akWBAgVwdXXlxIkTdOzYkV9//ZUiRYrQu3dv2rVrx7ZtMb8ETp8+ndWrV7N582YiIyNp1aoV3333HQMHDgSgXbt2vHnzhmPHjnH8+HE6deqEk5MTJUqUICQkhJo1a9KyZUuWLl3Kr7/+Sq1atbh58yaWlpYGOUZCCPElRVSsTHTWbBgF3Md02x+EN2lm6JCEEOKz+Nqr3oeEhDBq1DAOHz5IunTp6dbtB7y8qgPQuHEdKlWqyu7d27GxyciSJau4ffsmM2ZM5dKli2TJkoUmTZrTsGETdX0HDuznt9/mExj4kFy5ctOjRx/c3JLW2XT79i3mzJnOhQvniY6OIl++AgwePJwcOXLSt2+P//4fpM4/eHA/8uZ1onPn7ty6dSPBuBYv9ubGjWu8fPmSW7duMmHC1DgxvXz5khMn/KhTpz4mJibs2rWDDh26oNFo8PM7yvDhg9i+/W/MzMwAOHHCj59+GsKOHX9hZGTCsmWL2bz5d8LDwyhUyI3+/Ydga2sLQNmyxWjXrhObN2/A1bUQkyfP4M8/t7BmzQoePnyApaUllSp50bfvQIz+qz+0bt0q1qxZSWhoKDVr1ubmzRvUqFGbmjXrEBERwfz5s/nrr50AlCxZmr59B2Jtne5jXgLs2PEnq1YtIzAwkJw5c/HDD/0oUsQdgNOnTzJnzgzu3btDxoyZadmyDfXrNwLg77/3sGjRrzx6FIS9vQNduvSkfPmKHxWD+DgGrWp++fJlPDw8uHnzpt70HTt2cP/+fVasWIGzszNdu3alZs2aHD16FIC5c+fStGlT2rRpQ6FChVixYgU7duzg9u3bAMyaNYuxY8dStmxZPD09mTx5MnPnzgXg5s2bbNu2jUWLFuHq6krHjh1p1aoV8+fPB2DdunWYm5szdepU8ufPz8yZM7GysmLDhg1f8MgIIYQBGRkR1qotAObLDXvrDSGEEP938OB+nJ3zs3z5OipX9mLSpLG8fv1abf/rr11Mnz6PH38cTUREOAMH9qFQoSIsW7aGnj37snTpInbt2g7A9evXGD9+NG3adGTZsrV4edVk4MDeBATcTzQOnU7HkCH9sLOzZ+nS1SxYsITo6GgWLJgNQJUq1ThwYL96v/fXr19z8qQflSt7ER4e9t64AA4dOkDVqtWYPXsBBQq4xHMc9qHVailWrCTlylUgMPAB587F/KBSrFgJzM3N8fM7os7v6/s3ZcuWx8zMjI0b17Fnz05GjRqHt/dSbGxs6N+/J1FRUer8R44cZMGCxXTr9gP+/qeZOXMqXbv2ZM2aTQwcOIzt2//g8OEDAOzZs5PFixfSu/cAfv11CYGBDzl79oy6Lm/veVy9epmpU2cxe7Y3r1+/ZsSIoYk/2fHYseNPZsyYQqtW7Vi6dBXFipVg0KA+PH78L9HR0YwYMRRPz8qsWvU7nTt3Y/r0ydy+fYtnz57y888jad26PatXb6RmzbqMHj2cly9ffFQc4uMYNPE+cOAAnp6eHDt2TG+6r68vlStX1rsx+5YtW+jSpQsAfn5+lC9fXm3Lli0bjo6O+Pn58fDhQ+7fv6/XXrZsWe7evUtgYCDHjx8nW7Zs5MiRQ689NobYG63HlqrXaDSUKVMmToxCCJGahbVojWJkhMnxYxhdTf5hmUIIIRLn6lqIFi3a4OCQlbZtOxIREcHdu3fUdi+vGuTOnYe8eZ34669dpE+fgc6du5MtmyNly5anTZv2rF+/BoC1a1dQp059vLyqkzVrNpo0aYaHR2k2b/490TjCw8OpX78RvXr1w8EhK87O+ahRoza3b98CoEKFSjx//owLF84BcOiQL9myOZIrV+5E4wKwsclI/fqNyZvXGVNTszjb/+uvPRQvXhIzMzPy53fhu++yqNfAGxsbU6FCJXx99wEQHR3NoUMHqFSpKgCrV6+gR48+uLsXI3v2HAwa9CMvX77Ez++ouv569Rri6JiDnDlzYW5uwdChI6hQoRJ2dvZ4elYhb15ndV83bdpA06bNqVSpCrly5Wb48DGYmpoCMSN7N21az6BBP1KggCu5c+dhxIix+Puf5ubNG0l5yvX8/vtaGjduRo0atXF0zEH37j+QK1ceNm5cT0jIa16+fIGNTUbs7Ozx8qrBzJnzyZgxE48f/0tUVBSZM3+Hra0dzZu3YtKkaaRJY/rBMYiPZ9Ch5t27d493+q1bt8iRIwdDhw5lxYoVZMqUiTFjxlC/fn0AAgMDsbfXr/SXJUsWAgICCAwMBNBrz5IlC4DantCyset2cXGJ037x4sWP31EhhPjK6GztiPCqgenObTFF1sZNNnRIQgjxzXNwcFD/jr0mPCIiXJ1mZ2en/n3nzh1u3rxO1arl1GnR0Tp1ePSdO3e4dWsvW7duUtsjIyMpUaIUQUFBtG79/yHpXl41aNWqnfrY3Nyc+vUbs2vXdq5evcy9e3f4559/sLGxAcDKygoPj9Ls3/83hQoVYd++v6hc2StJcQHY2v5/P94VHPyEs2dPM3jwcCCmk6x8+Yrs2LGNfv0GY2ZmRuXKXgwbNoDIyEguXDhHZGQkJUuWIjQ0lH//fcSoUcPU2k8Q80PC/fv33tr+/3OFfPnyY2pqyuLF3ty+fZObN28QEHCfEiU8ALh587resbG2tsbRMTsADx8GEBkZSbdu7fX2QafTcf/+XXLnzpPgfsbnzp07tG/fWW+aq2tB7t69jbV1OurXb8zkyeNYunQRZcqUo1atelhbW2NlZUXp0mXp168njo7ZKVu2AnXq1FeH4osvw+DXeMfn9evXLF26lO+//54///yT/fv307hxY/z8/ChWrBihoaHqL0mxTE1NCQ8PJzQ0VH38dhugtie0LJBoe3zCw8PjtFtbW6PRgObL3bUi1Yk9dnIMhUh+8Z1vYW3bxyTe69YQ+tNoKbImhFCl5M/mlBzbp9JqjeJMix3ODej1YEZHR1O0aHH69x8S77qio6Np2bIt1avX0ptuampKhgw2+PisVqdZWloSFhamPg4NDaVz5zakS5eesmXLU6VKNe7du8OaNSvVeapUqca8ebPo0KELp06dUK/3TiyumP1I+P7r+/btJTo6milTxjNlynj1GOh0Og4e3I+XVw2KFHHH3NyCkyePc/z4UcqXr4iJiQlv3kQA8PPPk9XkONbbI23f3v7x48cYNmwg1avXxMOjNO3bd2HatP/f8SPmBwP9Anqxz0l0dDQA8+cvwtzcQm+e2B8pPkR8xyU6Wkd0dMy9yQcOHErDhk04dMiXQ4cO8Mcfm5g0aTqlSpVhypSZXL58kcOHD3Lw4H42b/6d+fN/I29e5wS3F18uk5rPr+SWIhNvY2NjMmbMyIIFC9Bqtbi7u3Po0CEWLlxIsWLFMDMzi5PohoeHY2Fhof5yEx4ervc3oLYntCyQaHt8Jk6cyJgxY9TH9vb2PHjwABsbubn855AxoxxHIb4UvfOtcT3Inh3t3btk8t0NrVsbLjAhRIqRIUPKLjYbFhbG06dajIw0GBvH9GoaGRnm6sqP2W7svcZjY48Vexnku9ONjLTqNK32//ucI0cODh8+QLZsWdXe5J07t3PlymX69x9E9uzZCQp6SI4c/09A58yZiaNjdurVa6A3HeDhw4fq9s6fP8OTJ09YtWo9xsYx6cSpU8cBRd1+hQoVmTx5HOvWrSRPnrxkz+6YpLi0Wg0ajSbOfsbat28PxYqVoF+/gXrThwzpz65d26lZsxagpXLlKvj5HeHIkUP8+OMIjI21WFlZkSGDDc+fP1UvS42MjOSnn4bSqlUbbGwKxzmm27ZtoU6dugwaNAyIKQL98GEAxYsXx9hYS86cubl+/SoVK3oCEBLymgcPAtBqNTg6OmJkZMTr1y/Jnz8/AE+fPmX8+DH07TsAa2v977ixr5e3t/+27Nmzc+XKJTw9K6nTLl++SJEibrx48ZQlS36jT58BODl1pmPHzvTt25OjRw+RLVtWtm7dQu/e/ShUqBDdu/ekefPGnDx5XI3rbTqdBq1WS4YMltIr/hmlyMTbzs4OjUajNwTE2dmZ8+fPAzHDbIKCgvSWCQoKws7OTh2CExQUpF7HHTtvbHtCyya27oQMGzaM/v37x5n+9OkrdLqk7LGIj0YTkwQEB79CSfxOHEKIT5DQ+Wbeog2WE38mcu58XtSob7D4hEiMkZE2xSeEqcWzZyFqD1tKFBkZgU6nIzpaISrKsHFGR+s+OIbY24+9u1xsL+q709/ehk73/32uWrU6ixZ5M3HiOJo3b83DhwFMnz6VZs1aEhWlo0mTFvTs2Qln5wKULl2WI0cOsnbtKmbNWhBvzLHPeXS0jrRprXnzJpT9+/eRL18BTp06we+/r8PSMq26rLFxGsqWrcDq1Svp3Ll7kuPS6RQUJf7nLjDwIRcunOfnnyeRPXsuvba6dRvy669zCQwMInPm7/D09KJfv56YmppSuHBRoqJ0GBtr+f77Fvz66zysrdOTPXsOli5dxPnz53BwyK5u8+1jamVlzfnz5/jnn2toNBpWrlzKkydPCAuLICpKR6NGTfnll4nkypWX7NlzsmjRr4SGhqIoYGpqTp069Zk8eSKDB/9Ihgw2zJkzg0ePAvnuO7t4n0uA06dPceuWfo94yZKlaNq0JZMmjcXRMQcFCriyfftWbty4xvDho7GwsMLXdx/R0QrNm7fi8eN/uXbtGuXKeWJubsmmTRuwsLDEy6sGt2/fIjDwIXnyOCXwXMeMIHj2LAQTk0i9Nq0W6Vz8SCky8fbw8GDcuHFER0erv4RduXJFTaQ9PDw4fPgw7dq1A+D+/fvcv38fDw8P7O3tcXR05PDhw+r8hw8fxtHRETs7Ozw8PLh79y4BAQFkzZpVbffw8FDXPWnSJBRFQaPRoCgKR44cYfjw4QnGa2pqGmd4OoCiIAnjZyDHUYgv593zLax5KyymTMDkhB/aK1eIzhf3l3EhxLcnJX8uvy+2/Jm/zHvYl9rO+1hYWPLLL7OZPXsa7du3wNo6HY0aNaV165jrjV1dCzJixFiWLFnI/PmzcHDIyqhR49VbU72Pq2sh2rXrxLRpk4mIiCB37jz07z+ESZN+5vHjf8mc+TsAKlWqyl9/7aJy5apJjut99u7dQ/r06SlbtkKctpo167Jo0a/s2rWD1q3b4epakPTp01OyZCm1Vx6gefPWhIaGMnXqeEJCQsiXrwDTp8/RG2r+tg4dujJhwmi6dm2HpWVaSpUqQ/36jbl+/R8gZkh9QMB9pk6dSEREBHXrNsDW1k7dZq9e/Zg7dyY//TSEqKgoihRxY+rUWXrXtL9r/PjRcab5+vpRuXJVnj4NZtGiX3n6NJg8eZyYPn0u2bPnAGDSpOnMmjWNtm2bYWFhSa1adalTpz5arZbx46eyYMEcli/3IUOGDHTt2ku9Tj0h8X0HT8nnfkqnUZSUcfg0Gg379++nYsWKvHwZMxyjdu3aDBo0iD179tCnTx+OHz+Ou7s7x44do2LFisyfP5/ixYvTp08frKys2Lp1KwCTJk1izpw5rFq1CoCWLVsyYMAAtVe6evXqhIeHM2vWLE6ePEmvXr04cOAAJUqU4OXLl+TJk4fmzZvTtWtXvL29Wb9+PTdu3Pjg+3gHB0uP96fQaCBTJiuePJEebyGS2/vON+t2LTHd8SehnbsRMn6KYQIUIhHGxjE93u7e7l/1fZpTMjdbN850PcOzZyEG70l+n8jICIKDA8mY0Q4Tk5hrYrVaDenSm2Fs9OX6nKKio3jxPEztwf4Wbd26mT17djJ37kJDhwLEvE987teuv/9p7O0dyJIl5j7gUVFR1K5dhQkTfsHdvdhn3daXEt85FEurlctAP1aK7PG2trbmr7/+onv37ri6upI9e3bWrVuHu3vML3ClSpXC29ubkSNH8vTpU7y8vPjtt9/U5QcNGsS///5LgwYNMDY2pmPHjvTr109tX758OZ06daJkyZLY2dmxZMkSSpQooW5727ZtdOvWjYULF1KoUCF27NjxwUm3EEKkFm/atMd0x5+YrV9LyE9jpMiaEOKro9MpvHgepl4//aW2+a0m3QEB97l69TLLli2mS5cehg4nWR065MuFC+cZNGgYFhaWbNiwBgsLS1xcCho6NJHCpJge79RIerw/jfR4C/HlvPd80+mwKVEYo3t3eTnnV8K/b2GQGIV4H+nxTn5fc4+3+LJOnvRj2LCBlCtXkREjxurVbTKk5OjxDg0NYdq0yfj5HSE8PBxX10L06TOQnDlzJb5wCiU93skjRfZ4CyGESEG0WsJatcVywljMl/tI4i2EEOK9ihf3YO/ew4YO44uwsLBkxIixhg5DfAVSxs9PQgghUrSw5q1QjI0xOXkcoyuXDR2OEEIIIcRXRRJvIYQQidJlsSWiWk0AzFb4GDgaIYQQQoiviyTeQgghkuRNm5hbvZitXwuhoQaORgghhBDi6yGJtxBCiCSJrOBJtGMOtC9fYLp1s6HDEUIIIYT4akjiLYQQImm0Wt60bguA+XIZbi6EEEIIkVSSeAshhEiysGb/FVk7dQKjy5cMHY4QQgghxFdBEm8hhBBJpmTJQkT1WgCYS5E1IYT4bHr16sLixd7q43379vLs2dNk297b61+82Jtevbok27YScv36P1y4cO6T1nH06GF++KEr1apVoHbtKgwbNpDbt299pgiF+Hwk8RZCCPFBYousmW5YJ0XWhBCpV3Q0JkcOYbppAyZHDkF09BfbdFBQICNHDiUsLOyLrL9589ZMmDA1Wbb1Pj/+OIj79+999PLr169h5MihlC5djoULlzFjxnzMzMzo2bMz9+7d/YyRCvHpJPEWQgjxQSLLVyQ6uxRZE0KkXmm2bcWmqAvpG9TCultH0jeohU1RF9Js2/pFtq8oyhddv4WFBdbW6ZJ1m0mJ40M8eBDAggWzGTToR5o3b0X27DnIm9eJESPG4uDggI/Pb58xUiE+nSTeQgghPoxWy5vW7QAwX7bEsLEIIcRnlmbbVqw7tkb78KHedG1gINYdW3+R5LtJk7rq/zt2/AnAgQP7adWqCZUrl6Fz5zb4+59W5+/VqwszZkyhSZN6NGxYi9DQEM6fP0v37h2pXLkMVaqUZeDA3jx58iTe9b871PzixfN0796RKlXK0qRJXbZs+V1tGz9+NHPmTGfkyGFUrlyGhg1rsWvXdrX99OmTtGvXgkqVStOkST22bNkY7z726tWFoKBAJkwYw/jxowG4c+c2/fv/gJdXBerXr4GPz2/odLp4l9+7dzfW1umoWrW63nStVsvw4WPo3Lm7Ou3IkUN06NCSChVK0apVEw4c2KcXx7Jli+nfvxeVKpWhWbOGHD9+DIAFC+bEGYLv7T2PPn16APDq1St+/nkEXl4VqFevOjNmTCE8PGYUwZkzp2jcuA6//DKRatUqsHLlUgDWrVtF/fo18PKqwMyZU/nhh67qcxwREcHMmb9Qq1ZlatWqzNixI3j58gUAgYEPKVu2GAcO7KNp03pUqlSawYP7qu0Afn5H6dChJZUrl6Ft2+acOnVCbXvf60d8GZJ4CyGE+GBh37eMKbJ2+iRGly4aOhwhhHg/RYGQkMT/vXxJ2h8HgaKgeWcVmv96Z9MOHwwvXya+rk/ozf3tt2Xq/5UrV+X69WuMHz+aNm06smzZWry8ajJwYG8CAu6ry+zY8ScjR45lwoRf0OkUBg/uS4kSHqxYsZ7p0+cSEBDAypU+8a7/bXfu3KZ37+4UKeLOkiUr6dChC3PnzuTAgf3qPBs3rsfZOR/Ll6+jQoVKTJ06gdevXxMdHc2IEUPx9KzMqlW/07lzN6ZPnxzvNdcTJkzlu++y0Lv3APr0Gcjz58/p2bMTmTJlYuHCpQwYMISNG9exYcOaeI/RjRvXcXbOj1YbN53JkSMn9vYOQMwPAcOHD6J69VqsWLGW2rXrMXLkMK5evaLOv3z5EqpUqcaKFevIm9eJyZPHodPpqFLFi/Pnz+pda+/r+zdVqngBMGnSWF6/fs2CBYuZOPEXrly5zPTpU9R5g4ICiYiIYPHilVSpUp09e3ayePFCevcewK+/LiEw8CFnz55R5/f2nsfVq5eZOnUWs2d78/r1a0aMGKq3b8uX+zB69HjmzFnIlSuXWbNmJQC3bt1kyJB+lC/vydKla6hSpRrDhg0gOPhJkl4/IvkZGzoAIYQQXx8lSxYiatTG9M8tmK/w4fWkaYYOSQgh4qcopK/thcnJ45+8Ko2iYBT4kMx5siY6b2QJD57/uRs076bwiUufPoP6v6mpGWvXrqBOnfp4ecX07jZp0oyzZ0+zefPv/PBDPwBKly5LwYKFAQgOfkLbtp1o1qwlGo0Ge3sHKlasxJUrl+Jd/9v+/HMzTk7OdO3aEwBHxxzcuXOb1auXU6GCJwB58jjRsmXM7SU7derKhg1ruH37Jtmz5+DlyxfY2GTEzs4eOzt7MmXKTMaMmeLso7V1OrRaLWnTpiVt2rRs2LAWU1MzBg8ejrGxMTly5CQ4+Ak+Pr/x/fct4yz/+vUrMmSwSfRYbty4nooVK9O0aQuMjbU0a9aKK1cusWbNCsaMmQBAqVJlqVmzDgBt23akXbvmPH0aTN68zmTL5sjBg77Uq9eQmzdvEBj4kAoVPHnwIIBDhw6wY8c+0qZNC8CQIT/Rvn0Lfvihv7r9li3bkjVrNgBGj95A06bNqVSpCgDDh4+hYcOaAISFhbFp03oWLVpB7tx5ABgxYiy1alXm5s0bWFhYANCxY1cKFHAFwMurOlevXgZg+/Y/KFiwMO3adQKgdet2hIW94fXr10l6/YjkJ4m3EEKIj/KmTXtM/9yC6YZ1vB4xFiwtDR2SEELE7yOS35Tkzp073Lq1l61bN6nTIiMjKVGilPrY1tZe/TtjxkzUqFGbdetWcf36Ne7cuc2NG9fUxDyxbRUo4KI3rWDBQvzxx/+HjMcmkgCWljFJZ1RUFNbW6ahfvzGTJ49j6dJFlClTjlq16mFtbZ3odu/evY2zc36Mjf+fnri6FiY4OJhXr15hZWWlN7+1dTpevXqZpPXWq9dIb5qra2G2b///JQPZsjm+tT+W6v4AVKpUlQMH9lOvXkN8ff+mePGSWFun48KF8+h0Oho0qKG3bp1Op9eTbGtrp/598+Z1WrVq99Y+WOPomB2Ahw8DiIyMpFu39nHWd//+XZyd8wP6x97CwlKN8969/88TK3a4fVJePyL5SeIthBDio0SWq0B09hwY3b2D6dbNhDdvZeiQhBAiLo0mpuc5CXdhMPE7SvrmjRKd7/majUR6lH7/TBYWny3hj46OpmXLtlT/73aOsUxNTdW/06RJo/79+PG/dOrUGmfn/BQrVpK6dRtw9OhhLl26kOi23l7P/7evIzr6/9dam5iYxJkntlDawIFDadiwCYcO+XLo0AH++GMTkyZNp1SpMh+8XZ0uWu//tzk752fdupUoioLmneP8999/cfz4UX78cVSC6317nW8n++/uT8wQdB9evXrFgQP7aN68NRDznKRNm5ZFi1bEWTZz5sxc+u8yrLefIyMjI0D/EoTY7UT/VzV//vxFmJtb6M1jY2PDixcx13K/e+xjl49vH2Il5fUjkp9c4y2EEOLjaLW8aR3zy7z5cimyJoRIwTSamFE5ifyLrFiJaHt7lAQSZkWjIdregciKlRJf3yck3e8mko6O2QkMfEDWrNnUf1u3bsLP72i8yx88uB8rq3RMmTKTpk2bU7iwGw8fPkhw/e9u69I7tTsuXTqv9sy+T3DwE6ZNm0zWrNlo27YjixYtp2jREhw5cjDR/XR0zM4//1xRe3ABLl68QPr0GeKtuF6pUhVevnzJX3/t1pseHR3N2rUrefPmzVv7o/+Dw8WLF5K0PwDZs+cgR45cbNmykYCA+5QvX1Fd7+vXr9FoNOpzEh4ezrx5s4iIiIx3XTlz5uKff66qj0NCXhMQEACAg0NWjIyMePHihbo+S0tLZs+eztOnid/PPWtWR27cuK43rVu3Duzdu/uDXz8ieUjiLYQQ4qOFNYstsnYKo4uJ96QIIUSKZmTE63ExxbHeTb5jH78eNxmMjJI1DDMzcwBu3LhGaGgoTZu2YO/ePWzYsJYHDwJYv34169at1hsi/TZr63Q8ehTEqVMnePAggJUrl3LgwD4iIiLiXf/bGjRowvXr1/D2nse9e3fZuXMbmzZtoGHDJonGbW2djoMH9zF79nQePAjg7Nkz3Lhxjbx5nRPYTzPu3r3Dy5cv8PKqQWRkJFOmjOfOndscOuTLkiXeNGjQON4fCmxt7WjfvjOTJv3MunWruH//HpcvX+Snnwbz4EEA3br1AqBp05b4+v7N+vVruHfvHuvWreLgwf00aJD4/sSqXNmL5cuXULJkaXVofY4cOSlZsjRjxvzElSuX+Oefq4wfP5o3b0LjDIuP1ajR92zYsIYDB/Zx585tJk78mTdvQtFoNFhYWFKnTn1++WUSZ86c4vbtW/z88ygePLiPnZ19vOt7W/36jTh/3p+1a1cSEHCfFSt8uH37JkWKuH/w60ckD0m8hRBCfDTlu+8I/68gjfkKHwNHI4QQny6idl1eLl6Bzs5Ob7rOzp6Xi1cQUbtusseQPn16qlWrwciRw9i2bQuurgUZMWIsmzdvoFWrJmzduplRo8ZTpIh7vMtXqlSVatVq8NNPQ+jUqQ1nzpyiV6++3L17m4iIiDjrf5utrS1Tpszg+PGjtG3bjGXLFtOrVz9q1Up8v01MTJg0aTo3blyjbdtmjBw5jFq16lKnTv1452/QoAmbNq1n0qRxWFhYMm3abB48CKBDh5bMmDGVJk2a07595wS316ZNBwYP/pG//tpNx46tGTKkP1qtll9/XYyDQ0wBPBcXV0aMGMuWLb/TsmUTduz4k7FjJ1K0aPFE9ydWlSpevHkTqlYzjzVixFjs7Ozp06cHffv2wNExu1qwLf71VKNZs1ZMnTqRLl3aYWtrh62tnTpMvFevfhQrVoKffhpC167tMTY2YurUWf8NUX8/B4esjBs3he3bt9Kmzffs3/83kyfPIFOmzB/8+hHJQ6N8yp3rxXsFB78igVsPiiTQaCBTJiuePHn1KXfkEEIkwaecbyYHfUnfuC46K2uCz/8jRdaEQRgba8mQwRJ3b3f8g/wNHU6q5GbrxpmuZ3j2LISoqJT7BScyMoLg4EAyZrTDxCTu9b1JFh2Nid9RtI+C0GWxjbmmO5l7ukXyMjbWGvS16+9/Gnt7B7JksQViCrjVrl2FCRN+wd29mMHietf7ziGtFjJmjL9HX7yfFFcTQgjxSSLLlic6R06M7tzG7I9NhLVobeiQhBDi0xkZEVmmnKGjEKnIoUO+XLhwnkGDhmFhYcmGDWuwsLDExaWgoUMTX4AMNRdCCPFp3iqyZiZF1oQQQoh4derUDUfH7PTr15N27Zpz9+4dpk2bI9XFvxHS4y2EEOKThTVrieWknzE5cxqjC+eJLljI0CEJIYQQKYqFhSUjRow1dBjCQKTHWwghxCdTMmeWImtCCCGEEAmQxFsIIcRnEdYmZri56e/r4fVrA0cjhBBCCJFySOIthBDis4gsU46onLnQvn6F2R+bDB2OEOIbJzfuEeLjKErKvWvB10yu8RZCCPF5aLWEtW5P2rEjMFu+hLCWbQwdkRDiG2RkZAxoeP36BWnTpkOj0Rg6JJFC6HQaoqPlB5mEKIpCdHQUr149R6PRYmxsYuiQUhVJvIUQQnw2Yd+3wHLiWEz8z2B84RxRBQsbOiQhxDdGq9WSIUNmnj17zNOnbwwdjkhBtFotOp305iYmTRozrK1t5Eerz0wSbyGEEJ+Nkjkz4bXqYLZlE2bLl/J66gxDhySE+AaZmprz3XdZiY6OMnQoIoXQaCBDBkuePQtBrkJImFarRas1kqQ7GUjiLYQQ4rMKa9MBsy2bMN24ntejfoa0aQ0dkhDiGxSTQKQxdBgihdBowMzMDBOTSEm8hUFIcTUhhBCfVWSZckTlyh1TZG3LRkOHI4QQQghhcJJ4CyGE+Lw0GsJax9xazGz5EgMHI4QQQghheJJ4CyGE+OzCvm+BkiYNJmf9MT5/1tDhCCGEEEIYlCTeQgghPjslUybCa9UBwGz5UsMGI4QQQghhYJJ4CyGESBZhbToAYLpxPZrXrwwcjRBCCCGE4UjiLYQQIllEli5LVO48aENeY7pZiqwJIYQQ4tslibcQQojkoVdkzcfAwQghhBBCGI4k3kIIIZKNWmTtnD/G5/wNHY4QQgghhEFI4i2EECLZKBkzEl67LiBF1oQQQgjx7ZLEWwghRLJSi6xt2iBF1oQQQgjxTZLEWwghRLKKLFWGqDx5Y4qsbfrd0OEIIYQQQnxxkngLIYRIXlJkTQghhBDfOEm8hRBCJLuwps1jiqydP4vx2TOGDkcIIYQQ4ouSxFsIIUSyiymyVg8AsxVLDRuMEEIIIcQXJom3EEKILyKsbUyRNbONG9C8emngaIQQQgiR3P79918aN25M+vTpyZMnD0uXLlXbbt++TZUqVbC0tKRAgQLs2bNHb9m9e/fi6uqKhYUFlSpV4tatW3rtM2fOxMHBASsrKzp27EhoaOiX2KWPJom3EEKILyLSozRReZ3QhIZIkTUhhBAilVMUhQYNGhAQEMD+/fuZOXMm/fv3Z9OmTSiKQv369bG1teXUqVO0bt2aBg0acO/ePQDu3btH/fr1ad++PSdPniRz5szUr18fRVEA2LhxI6NHj8bb25t9+/bh5+fH4MGDDbm7iZLEWwghxJeh0RDWuh3wX5G1/z48hRBCCJH6nD59mqNHj7J69Wrc3NyoXbs2Q4YMYerUqezfv5+bN2/i7e1N/vz5GTZsGKVKlWLJkiUALFq0iGLFijFgwABcXFzw8fHhzp07HDhwAIBZs2bRt29fateuTfHixfH29mbJkiUputdbEm8hhBBfTFjT5iimpphcOCdF1oQQQohU7NatW2TOnJlcuXKp0woVKsSpU6c4dOgQ7u7uWFpaqm1ly5bl2LFjAPj5+VG+fHm1zcLCAnd3d44dO0Z0dDQnT57Ua/fw8CAiIoJz5859gT37OJJ4CyGE+GIUGymyJoQQQnwLsmTJwvPnz/V6oe/fv09UVBRBQUHY29vHmT8gIACAwMDABNufP39OWFiYXruxsTEZM2ZUl0+JjA0dQGqm0cT8Ex8n9tjJMRQi+X3J8y2sbQfMNq7HbNPvhI4dj2JlnfwbFd8MSxNLrNJYGTqMVMnS5P89U/LZLL428r3y84g9fi9f6hdJNTU1xdTUVG9ayZIlsbe354cffmD27NkEBgYyffp0AMLCwuLMb2pqSnh4OAChoaEJtscm8u9bPiWSxDsZ2djIB//nkDGjHEchvpQvcr7V9oL8+dFcuULG3X9Ct27Jv03xzTjU4ZChQ0j1MmSwTHwmIVIo+V75eWTNmpVXr16pj0eNGsXo0aP15jEzM2PDhg00bdoUa2trvvvuOwYPHkz//v3RarW8efNGb/7w8HAsLCzUZd9NosPDw0mfPj1mZmbq44SWT4kk8U5GT5++QqczdBRfL40m5s0xOPiV1GASIpl96fPNrEUb0o4YRtS8BTxv1EK6IMQnMzLSkiGDJeWWlOPco5R7jd/XrHCWwhzqcIhnz0KIjpYvOOLrIt8rPw+tNqZz8d0h3e/2PscqXrw4t2/fJigoiEyZMrFnzx4yZcpE7ty549w+LCgoCDs7OwAcHBwICgqK016kSBEyZsyImZkZQUFB5MuXD4CoqCiCg4PV5VMiSbyTkaJI0d7PQY6jEF/Olzrfwpo2x3LcaIwvnsfozGmi3Isl/0bFNyEkMoRXEa8Sn1F8sJDIEPVv+VwWXyv5XvlpYo+dtXXil4k9ffqUunXr8scff2BrawvA9u3bqVixIh4eHkyaNIk3b95gbm4OwOHDhylbtiwQUyzt8OHD6rpCQ0Px9/dn9OjRaLVaihcvzuHDh6lYsSIAx44dw8TEhMKFC3/Gvf28pLiaEEKIL07JYEN4nfqAFFkTQgghUiMbGxtev37N4MGDuXXrFosWLWLJkiUMHjyYChUqkC1bNtq3b8+lS5eYNGkSJ06coGPHjgB06NCBI0eOMGnSJC5dukT79u3JmTOnmmj36NGDqVOnsmXLFk6ePEn37t3p3Llzih5qLom3EEIIg3jTpgMAZpt/R/PyhYGjEUIIIcTntm7dOm7evEnBggWZOXMmGzZsoHjx4hgZGfHHH38QGBhI0aJFWblyJZs3b8bR0RGAHDlysGnTJnx8fChevDjBwcFs2bIFzX+XpjVr1oxhw4bRtWtXqlatSsmSJZkyZYohdzVRGkWRwRbJJThYrvH+FBoNZMpkxZMnci2OEMnNIOebopChXAmMr/3Dq8nTCWvf6QttWKRGxsYx13i7e7vjH+Rv6HBSJTdbN850PcOzZyFERckXHPF1ke+Vn4dWKwXqPpb0eAshhDAMjYawNu0BMF/uIxfdCSGEECLVksRbCCGEwYQ1aYZiaorxpQsYnzll6HCEEEIIIZKFJN5CCCEMRslgQ3jdBoAUWRNCCCFE6iWJtxBCCINSi6xt2ShF1oQQQgiRKqWIxDs8PBxXV1d8fX3jtL148QIHBweWLl2qN33NmjXkzp0bCwsLGjRowJMnT9Q2RVEYOnQomTNnxsbGhsGDB6N7q8pZcHAwjRo1wsrKipw5c7Jy5Uq9dfv7+1OyZEksLCwoXrw4p0+f/qz7K4QQ4v+iSpQkyjkfmtBQTH9fb+hwhBBCCCE+O4Mn3mFhYTRv3pxLly7F2z5kyBAePnyoNy32Hm+jRo3Cz8+PZ8+e0a5dO7V9+vTprF69ms2bN7Nx40ZWrVrF9OnT1fZ27drx4sULjh07xk8//USnTp04ceIEACEhIdSsWZNy5cpx+vRpSpcuTa1atQgJCfn8Oy+EEEKKrAkhhBAi1TNo4n358mU8PDy4efNmvO2HDx/m77//xtbWVm/63Llzadq0KW3atKFQoUKsWLGCHTt2cPv2bQBmzZrF2LFjKVu2LJ6enkyePJm5c+cCcPPmTbZt28aiRYtwdXWlY8eOtGrVivnz5wMx95ozNzdn6tSp5M+fn5kzZ2JlZcWGDRuS8UgIIcS3LaxJMxQzM4wvX8T49ElDhyOEEEII8VkZNPE+cOAAnp6eHDt2LE5beHg4nTt3Zt68eZiamuq1+fn5Ub58efVxtmzZcHR0xM/Pj4cPH3L//n299rJly3L37l0CAwM5fvw42bJlI0eOHHrtsTH4+flRtmxZ9ebsGo2GMmXKxBujEEKIz0NJn0GKrAkhhBAi1TI25Ma7d++eYNuECRNwc3PDy8srTltgYCD29vZ607JkyUJAQACBgYEAeu1ZsmQBUNsTWjZ23S4uLnHaL168mGCs4eHhhIeH602ztrZGo4H/8nfxEWKPnRxDIZJfSjjfwtp2wGz9Gsy2bCT05wko6dIbLhghxHvJZ7P42qSEz7nUQI7fxzNo4p2Qy5cv8+uvv3L+/Pl420NDQ+P0gpuamhIeHk5oaKj6+O02QG1PaNnE1p2QiRMnMmbMGPWxvb09Dx48wMbGKrFdFUmQMaMcRyG+FIOebzUqg4sLmkuXyLhrK/TsabhYhBAJypDB0tAhCPHR5HulMJQUl3grikLnzp0ZO3as2lP9LjMzsziJcHh4OBYWFpiZmamP3/4bUNsTWjaxdSdk2LBh9O/fP870p09f8VYxdfGBNJqYN8fg4FdSa0mIZJZSzjezlm1J++NgouYv4HnT1vLTukgyIyOtJIRfyLNnIURHyxcc8XVJKZ9zXzutFulc/EgpLvG+d+8eR48e5dy5cwwYMACI6YXu1q0b69atY+fOnTg4OBAUFKS3XFBQEHZ2djg4OKiPY6/jjp03tj2hZYFE2+Njamoap5ccYgrzyon96eQ4CvHlGPp8C2v8PZZjR2J8+RJGJ08QVbyk4YIRQiRIPpfF18rQn3NfOzl2H8/gtxN7l4ODA9evX+fs2bPqP3t7e8aOHcuiRYsA8PDw4PDhw+oy9+/f5/79+3h4eGBvb4+jo6Ne++HDh3F0dMTOzg4PDw/u3r2rXtMd2+7h4aGu++jRoyj/vaoUReHIkSNquxBCiOSjpM9AeL2GAJhLkTUhhBBCpBIpLvE2NjYmT548ev+MjY357rvv1N7s7t27s2LFChYvXsz58+dp06YNtWvXJmfOnGr7kCFD8PX1xdfXl6FDh9KnTx8AcuXKRbVq1WjdujXnz59n8eLFrF69mp7/XUvYuHFjnj9/Tt++fbl8+TJ9+/YlJCSEpk2bGuaACCHEN+bNf/f0Nv1jE5oXzw0bjBBCCCHEZ5DiEu+kKFWqFN7e3owZM4bSpUuTIUMGfHx81PZBgwbx/fff06BBA5o0aULr1q3p16+f2r58+XKsrKwoWbIk48ePZ8mSJZQoUQKIqUa+bds2Dh06RNGiRfHz82PHjh1YWsp1Y0II8SVEFStBVP4CaN68wfT3dYYORwghhBDik2kURUbqJ5fgYCmu9ik0GsiUyYonT6QIhhDJLaWdb2aLvbEaNoio/AV45ntMiqyJRBkbxxRXc/d2xz/I39DhpEputm6c6XqGZ89CiIqSLzji65LSPue+VlqtVIb/WF9lj7cQQojULbzx9yjm5hhfuYzxyROGDkcIIYQQ4pNI4i2EECLFUdKlf6vImk8icwshhBBCpGySeAshhEiR9IqsPX9m4GiEEEIIIT6eJN5CCCFSpKiixYnK74ImLAyzDWsNHY4QQgghxEeTxFsIIUTKpNGovd5mK5Yi1XCEEEII8bWSxFsIIUSKFd64aUyRtatXMD5x3NDhCCGEEEJ8FEm8hRBCpFhKuvSE1W8ESJE1IYQQQny9JPEWQgiRooXFFlnbulmKrAkhhBDiqySJtxBCiBQtyr0YUQVcY4qsrV9j6HCEEEIIIT6YJN5CCCFSNimyJoQQQoivnCTeQgghUrzwxk1RLCww/ucqxsf9DB2OEEIIIcQHkcRbCCFEiqdYp5Mia0IIIYT4akniLYQQ4qugV2Tt2VMDRyOEEEIIkXSSeAshhPgqRLkVJcqlIJrwcCmyJoQQQoiviiTeQgghvg5SZE0IIYQQXylJvIUQQnw1whs1iSmydu0fTI4fM3Q4QgghhBBJIom3EEKIr4ZinY6wBo0BMFsuRdaEEEII8XWQxFsIIcRXRS2y9ucWNE+DDRyNEEIIIUTiJPEWQgjxVYkq4k6kayEpsiaEEEKIr4Yk3kIIIb4uGo3a6y1F1oQQQgjxNZDEWwghxFcnpsiaJcbXr2Hid9TQ4QghhBBCvJck3kIIIb46ipU1YQ2lyJoQQgghvg6SeAshhPgqqUXWtv0hRdaEEEIIkaJJ4i2EEOKrFFXYjciChWOKrK2TImtCCCGESLkk8RZCCPF10iuy5iNF1oQQQgiRYkniLYQQ4qsV3rBxTJG1G9cxOXbE0OEIIYQQQsRLEm8hhBBfLcXKmrBGTQApsiaEEEKIlEsSbyGEEF81vSJrwVJkTQghhBApjyTeQgghvmpRhd2ILFQETUQEZutWGzocIYQQQog4JPEWQgjx1ZMia0IIIYRIySTxFkII8dULb9gYnWVajG/ewOToYUOHI4QQQgihRxJvIYQQXz0lrRXhDf8rsrZCiqwJIYQQImWRxFsIIUSqENY2tsjaVimyJoQQQogURRJvIYQQqUJUoSJEFnaLKbK2dpWhwxFCCCGEUEniLYQQItWQImtCCCGESIkk8RZCCJFqhDdoFFNk7dZNTI4cMnQ4QgghhBCAJN5CCCFSESWtFeGNmgJSZE0IIYQQKYck3kIIIVIVvSJrT54YOBohhBBCCEm8hRBCpDJRBQsTWcQNTWSkFFkTQgghRIogibcQQohUJ6xNB0CKrAkhhBAiZZDEWwghRKoTVr8RurRWGN++hcnhg4YORwghhBDfOEm8hRBCpD5p00qRNSGEEMLA7t+/T+3atbG2tiZHjhzMnDlTbfP396dkyZJYWFhQvHhxTp8+rbfsmjVryJ07NxYWFjRo0IAnb9VtURSFoUOHkjlzZmxsbBg8eDA6ne5L7dZHkcRbCCFEqvTmv3t6m27/E83jxwaORgghhPj2NG3alLRp03L69GlmzZrF8OHD2bx5MyEhIdSsWZNy5cpx+vRpSpcuTa1atQgJCQHgxIkTdOzYkVGjRuHn58ezZ89o166dut7p06ezevVqNm/ezMaNG1m1ahXTp0830F4mjUZR5OK35BIc/IoU/sNLiqbRQKZMVjx58kou0RQimaXW8y19tYqY+J/h9YixvPmhr6HDEcnI2FhLhgyWuHu74x/kb+hwUiU3WzfOdD3Ds2chREXJFxzxdUmtn3NfmlYLGTNaJWneZ8+eYWNjw4ULF3B1dQWgUaNG2NnZ4e7uzrhx47h58yYajQZFUXBycmL48OG0a9eONm3aoNVqWbp0KRDTc549e3Zu3rxJzpw5cXR0ZOzYsWoyvnLlSn766Sfu3LmTDHv9eUiPtxBCiFRLLbK2cinyS6gQQgjx5Zibm2NhYYGPjw+RkZH8888/HDlyBDc3N/z8/ChbtiwajQYAjUZDmTJlOHbsGAB+fn6UL19eXVe2bNlwdHTEz8+Phw8fcv/+fb32smXLcvfuXQIDA7/sTn4ASbyFEEKkWmH1GkqRNSGEEMIAzMzMmDdvHt7e3pibm5MvXz5q1KhBx44dCQwMxN7eXm/+LFmyEBAQAPDe9tjk+u32LFmyAKjLp0TGhg4gNdNoYv6JjxN77OQYCpH8Uu35ZpWW8CZNMfdZjPmKpURVqGjoiEQyszSxxCpN0oZBig9jaWKp/p3q3itEqpdqP+e+sNjj9/LlS73ppqammJqaxpn/ypUr1KlThwEDBnDx4kV++OEHqlSpQmhoaJz5TU1NCQ8PB3hve2hoqPr47TZAXT4lksQ7GdnYyAf/55DU60iEEJ8uVZ5vfX4An8WY7vgTU90b+O47Q0ckktGhDocMHUKqlyGDZeIzCZFCpcrPOQPImjUrr169Uh+PGjWK0aNH683z999/s2jRIgICAjA3N6dYsWI8ePCAcePGkStXrjhJcnh4OBYWFkBMb3lC7WZmZurjt/8G1OVTIkm8k9HTp1Jc7VNoNDFvjsHBUgRDiOSWqs83h1ykcy+KyZnThMz9lTe9+xk6IpEMjIxiiquVW1KOc4/OGTqcVKlwlsIc6nCIZ89CiI6WLzji65KqP+e+IK02pnPx3SHd8fV2nz59mrx582Jubq5Oc3NzY/z48ZQrV46goCC9+YOCgrCzswPAwcEhwXYHBwf1cY4cOdS/AXX5lEgS72SkKMiJ/RnIcRTiy0mt51tYmw6YnDmN2YqlhPbsE/PNQaRKIZEhvIp4lfiM4oOFRIaof6fG9wnxbUitn3NfSuyxs7a2TnRee3t7bty4QUREBGnSpAHg6tWr5MyZEw8PDyZNmoSiKGpV8yNHjjB8+HAAPDw8OHz4sFq1/P79+9y/fx8PDw/s7e1xdHTk8OHDauJ9+PBhHB0dU3TiLd88hBBCpHph9Rqis7LG6M5tTA4dMHQ4QgghRKpXp04dTExM6NSpE9euXePPP/9kwoQJ9O7dm8aNG/P8+XP69u3L5cuX6du3LyEhITRt2hSA7t27s2LFChYvXsz58+dp06YNtWvXJmfOnGr7kCFD8PX1xdfXl6FDh9KnTx9D7m6iJPEWQgiR+llaEt445sPcbMVSw8YihBBCfAPSpUvH33//TWBgIMWLF6dfv3789NNPdOnSBWtra7Zt28ahQ4coWrQofn5+7NixA0vLmBoSpUqVwtvbmzFjxlC6dGkyZMiAj4+Puu5Bgwbx/fff06BBA5o0aULr1q3p1y9lX0qmURQZbJFcgoPlGu9PodFApkxWPHki1+IIkdy+hfPN6OIFbCqVQTE2JvjsVRQpspaqGBvHXOPt7u2Of5C/ocNJldxs3TjT9QzPnoUQFSVfcMTX5Vv4nPsStFopUPexpMdbCCHENyHatSCRRYuhiYrCbO1KQ4cjhBBCiG+IJN5CCCG+GW/adADAfMVSZEiSEEIIIb4USbyFEEJ8M8LrNogpsnb3DiYHfQ0djhBCCCG+EZJ4CyGE+HZYWhLe5Hvgv15vIYQQQogvQBJvIYQQ35TY4eZpdm5D8+iRgaMRQgghxLdAEm8hhBDflOgCLkQWLS5F1oQQQgjxxaSIxDs8PBxXV1d8fX3VaX5+fpQuXZq0adPi7OzMokWL9JbZu3cvrq6uWFhYUKlSJW7duqXXPnPmTBwcHLCysqJjx46EhoaqbWFhYXTs2JH06dNjZ2fHtGnT9Ja9ffs2VapUwdLSkgIFCrBnz57Pv9NCCCEM5k3b2CJry6TImhBCCCGSncET77CwMJo3b86lS5fUaUFBQdSoUYOKFSvi7+/PmDFj+OGHH9i+fTsA9+7do379+rRv356TJ0+SOXNm6tevT+wtyTdu3Mjo0aPx9vZm3759+Pn5MXjwYHX9gwYN4tSpU+zbt4/58+czZswYfv/9dwAURaF+/frY2tpy6tQpWrduTYMGDbh3794XPCpCCCGSU3jdBuis02F07w4mB/YbOhwhhBBCpHIGTbwvX76Mh4cHN2/e1Ju+ZcsWbG1tmTBhAnnz5qVZs2a0adOG1atXA7Bo0SKKFSvGgAEDcHFxwcfHhzt37nDgwAEAZs2aRd++falduzbFixfH29ubJUuWEBoaSkhICIsWLWLWrFm4u7vToEEDBg8ezNy5cwHYv38/N2/exNvbm/z58zNs2DBKlSrFkiVLvuzBEUIIkXwsLKTImhBCCCG+GIMm3gcOHMDT05Njx47pTa9evTo+Pj5x5n/x4gUQMwy9fPny6nQLCwvc3d05duwY0dHRnDx5Uq/dw8ODiIgIzp07x7lz54iMjKR06dJqe9myZTl+/Dg6nQ4/Pz/c3d2xtLTUa383RiGEEF+3N63bA5Bm13YpsiaEEEKIZGVsyI1379493uk5cuQgR44c6uN///2XtWvXMnr0aAACAwOxt7fXWyZLliwEBATw/PlzwsLC9NqNjY3JmDEjAQEBaLVaMmXKRJo0afSWDQsLIzg4+L3rTkh4eDjh4eF606ytrdFoQKN57yEQ7xF77OQYCpH8vsXzTefiQmTxEpicPIH5mhW86TfQ0CEJ8dX4lt4rROrwLX7OJQc5fh/PoIl3Urx584ZGjRpha2tL165dAQgNDcXU1FRvPlNTU8LDw9Uiagm1K4oSbxugLp/QsgmZOHEiY8aMUR/b29vz4MEDbGysPnBvRXwyZpTjKMSX8s2dbz17QLsTWK5ejuXPo0Br8NInQqR4GTJYJj6TECnUN/c5J1KMFJ14v379mnr16nHt2jUOHz6MhYUFAGZmZnES4fDwcNKnT4+ZmZn6+N12CwsLoqOj422DmCHrZmZmBAcHx7tsQoYNG0b//v3jTH/69JUUy/0EGk3Mm2Nw8Cv+q5snhEgm3+z55lkdG+t0aO/c4cXvfxBZqYqhIxIfychIKwnhF/LsWQjR0fIFR3xdvtnPuc9Mq0U6Fz9Sik28X758SY0aNbhx4wb79u0jb968apuDgwNBQUF68wcFBVGkSBEyZsyImZkZQUFB5MuXD4CoqCiCg4Oxs7NDURSePHlCVFQUxsbG6rLm5uakT58eBwcHvQrrse12dnYJxmpqahqnlxxAUZAT+zOQ4yjEl/PNnW/mFoQ1bYbFIm/Mli8lwlMSbyGS4pt6nxCpyjf3OfeZybH7eClyTJ1Op6Nhw4bcunWLAwcO4OLiotfu4eHB4cOH1cehoaH4+/vj4eGBVqulePHieu3Hjh3DxMSEwoULU6RIEUxMTPDz81PbDx8+TPHixdFqtXh4eHDmzBnevHmj1+7h4ZGMeyyEEMJQwt4qsqZ9FJTI3EIIIYQQHy5FJt6LFy9m//79LFq0iPTp0xMUFERQUBBPnz4FoEOHDhw5coRJkyZx6dIl2rdvT86cOalYsSIAPXr0YOrUqWzZsoWTJ0/SvXt3OnfujIWFBRYWFrRt25Zu3bpx8uRJtmzZwi+//EKfPn0AqFChAtmyZaN9+/ZcunSJSZMmceLECTp27GiowyGEECIZRecvQGTxkmiiozFbvcLQ4QghhBAiFUqRiffGjRvR6XTUrl0bOzs79V/Dhg2BmKrnmzZtwsfHh+LFixMcHMyWLVvQ/Fdmr1mzZgwbNoyuXbtStWpVSpYsyZQpU9T1T58+naJFi+Lp6UnPnj0ZM2aMum4jIyP++OMPAgMDKVq0KCtXrmTz5s04Ojp++QMhhBDii3jTJqbX22zlMqQ4hxBCCCE+N42iyEj95BIcLMXVPoVGA5kyWfHkiRTBECK5ffPn25s3ZCzkjPbFc56v3UhkpaqGjkh8IGPjmOJq7t7u+Af5GzqcVMnN1o0zXc/w7FkIUVHyBUd8Xb75z7nPRKuVyvAfK0X2eAshhBBflLk5YU2bxfy5fKlhYxFCCCFEqiOJtxBCCMFbRdZ270AbFGjgaIQQQgiRmkjiLYQQQgDR+fITWcJDiqwJIYQQ4rOTxFsIIYT4j1pkbdVyiI42cDRCCCGESC0k8RZCCCH+E16nPrr06TG6f480vn8bOhwhhBBCpBKSeAshhBCxzM0Ja9ocADMpsiaEEEKIz0QSbyGEEOItapG1PTulyJoQQgghPgtJvIUQQoi3RDvnI7JkqZgia6uWGzocIYQQQqQCkngLIYQQ75Aia0IIIYT4nCTxFkIIId6hFlkLuE+a/XsNHY4QQgghvnKSeAshhBDvMjMj7PsWMX9KkTUhhBBCfCJJvIUQQoh4qEXW/tqFNvChgaMRQgghxNdMEm8hhBAiHtFOzkR4lJYia0IIIYT4ZJJ4CyGEEAkIkyJrQgghhPgMJPEWQgghEhBeux66DBkwehBAmn1/GTocIYQQQnylJPEWQgghEmJmRljT/4qsrVhq2FiEEEII8dWSxFsIIYR4j9jh5mn27EL78IGBoxFCCCHE10gSbyGEEOI9ovM6EVGqDBqdToqsCSGEEOKjSOIthBBCJEKKrAkhhBDiU0jiLYQQQiQivFZddDY2GD18QJq/9xg6HCGEEEJ8ZSTxFkIIIRIjRdaEEEII8Qkk8RZCCCGSQC2y9j/27jwuqrL94/hnBpRNUERFQHHNLXcl0dDUFsstybVMQ8nMLLVFxazcskzL9MlfhSmYW5a5PZnZppaYuKKZS4srGJjiDjJs8/sDmUdyZVhmgO/79eLlnHOfc59rBu+ZubjPuc7332I8GWfjaERERKQoUeItIiJyBzJq30VqmyAVWRMREZFcU+ItIiJyhyxF1pYugvR0G0cjIiIiRYUSbxERkTuUs8ja97YOR0RERIoIJd4iIiJ3ysmJlL79AXBeFGnjYERERKSoUOItIiKSCykDQwAo/cN3KrImIiIid0SJt4iISC5k1LqL1HvbZhVZW/yprcMRERGRIkCJt4iISC6pyJqIiIjkhhJvERGRXDJ17kamlxcO8X9T+ofvbB2OiIiI2Dkl3iIiIrmlImsiIiKSC0q8RURErJAy4CkASv/4Pca4WBtHIyIiIvZMibeIiIgVMmrdRWpQOxVZExERkdtS4i0iImIlFVkTERGRO6HEW0RExEqmzt3IrFABh4R4Sn//ra3DERERETulxFtERMRapUuryJqIiEgxc+TIEUaPHk2PHj2Ij48nIiKCqKioPPWpxFtERCQPchRZiz1h42hEREQkL37++WcaN27M0aNHWb9+PVeuXOHQoUN07NiRlStXWt2vEm8REZE8yKhZm9S292Ewm3FeoiJrIiIiRdmYMWOYNm0aX375JaVKlQJg+vTpTJ8+nTfeeMPqfpV4i4iI5JGlyNoSFVkTEREpyvbt20fnzp2vW9+9e3cOHz5sdb9KvEVERPLI9EjXrCJrpxIo/d16W4cjIiJicwsWLMBgMFz3YzRmpaAxMTG0atUKV1dXAgIC2LVrV479P/vsM2rVqoWrqyvBwcGcOXPG0mY2mwkLC6NixYqUL1+eMWPGkJmZmS9xV69enR07dly3/uuvv6Z69epW96vEW0REJK9Klyal35OAiqyJiIgA9O3bl/j4eMvPiRMnqF27NiNHjiQpKYnOnTvTtm1bdu3aRZs2bejSpQtJSUkAbN++ndDQUCZMmEB0dDTnzp0jJCTE0vfMmTNZunQpq1atYsWKFSxZsoSZM2fmS9xvvvkmzzzzDKNHjyY9PZ2FCxfy1FNP8fLLLzNp0iSr+1XiLSIikg+uPHm1yNqGHzCeOG7jaERERGzLxcWFypUrW34WL16M2Wxm2rRpfP7557i4uDBjxgzq16/PrFmzcHd3Z/ny5QDMmTOHPn36MHDgQBo3bsyiRYtYt24dR48eBWD27NlMnjyZoKAgOnTowDvvvMOcOXPyJe7g4GB+/vlnTp06RcOGDVmzZg0mk4nNmzfTp08fq/tV4i0iIpIPMmvWIrVtexVZExER+ZezZ8/yzjvvMG3aNJycnIiOjiYoKAiDwQCAwWDg3nvvZevWrQBER0fTrl07y/5Vq1bF39+f6Oho/v77b2JjY3O0BwUFcfz4ceLj4/MlXh8fH1577TV27NhBTEwMjz32GNWqVctTn0q8RURE8smVp64WWVu6GNLSbByNiIiIffjoo4/w9fWlV69eAMTHx+Pr65tjG29vb+Li4m7bnp1cX9vu7e0NYNk/LzZs2EDt2rVZsmSJZd3s2bOpX78+W7ZssbpfxzxHJjdlMGT9iHWyXzu9hiIFT+Mtf6Q90oXMihVxOJWA0/frSe3SzdYhlThupdxwL+1u6zCKJbdSbpbHeq+Qokafc/kj+/W7ePFijvVOTk44OTndcB+z2cy8efMYM2aMZV1ycvJ12zs5OWEymW7bnpycbFm+tg2w7J8XL7/8MuPHj2fs2LGWdVu2bOHtt99m1KhRNyy8dieUeBeg8uX1wZ8fvLz0OooUFo23fDB4MLzzDh7LFsFTT9g6mhJn8+DNtg6h2PP0dLv9RiJ2Sp9z+aNKlSpcunTJsjxhwgQmTpx4w2137txJXFwc/fr1s6xzdna+Lkk2mUy4urrett3Z2dmyfO1jwLJ/Xvzxxx+Wmflr9enThylTpljdrxLvAnT27CXyqap9iWQwZL05JiZewmy2dTQixZvGW/4x9nyc8u+8g/nbbzm3+zcy/fN2TZjcGQcHI56ebrSNaMveU3ttHU6x1MS7CZsHb+bcuSQyMvQFR4oWfc7lD6Mxa3Lx36d032y2G2D9+vW0a9cOT09Pyzo/Pz8SEhJybJeQkICPj89t2/38/CzL2bf3yt42e/+8qFevHl988QXjxo3Lsf6rr76iVq1aVverxLsAmc1oYOcDvY4ihUfjLe8yqtcktV0HSv+8EadFn5L86hu2DqlESUpL4lLqpdtvKLmWlJZkeaz3CSmq9DmXN9mvnYeHxx3vs23bNu69994c6wIDA5k2bRpmsxmDwYDZbGbLli2MHz/e0h4VFWW5hVhsbCyxsbEEBgbi6+uLv78/UVFRlsQ7KioKf3//fEm8p06dSvfu3fn+++9p0aIFAHv37mXz5s2sWLHC6n5VXE1ERCSf/a/I2iIVWRMRkRLtt99+o0GDBjnW9erVi/PnzzNq1CgOHDjAqFGjSEpKstyua9iwYSxatIj58+fz66+/MnDgQLp27UqNGjUs7WPHjmXTpk1s2rSJsLAwRo4cmS/xPvzww8TExNCsWTMOHjzIX3/9RdOmTdm/fz+dO3e2ul/NeIuIiOSz1Ie7kFmxEg7/nKL0t9+Q2rW7rUMSERGxiVOnTuU4zRyyZszXrl3Ls88+y9y5c2ncuDHr1q3DzS2rhkTr1q0JDw/njTfe4OzZszz00EN88sknlv1Hjx7NP//8Q3BwMI6OjoSGhvLiiy/mW8x333037733Xr71B2Awm3WyRUFJTNQ13nlhMECFCu6cOaNrcUQKmsZb/nN7cyKu/5lJaof7ufD5KluHU+w5OmZd4908vDkxCTG2DqdYala5GbuH7ubcuSTS0/UFR4oWfc7lD6Ox+BeoO3/+PO+99x47duwgLS2Nf6fLGzZssKpfzXiLiIgUgCtPPoXrf2ZSatMGjMePkVmtuq1DEhERkdsYMGAAO3bsoH///rm6lv12lHiLiIgUgMzqNUi9rwOlf9qIy+JPSRo/wdYhiYjkK6PRgNFYtG6M7eBQdEpcZWaayczU9Hxh++GHH/j5558JCAjI136VeIuIiBSQKwMHU/qnjTgvXUTSmFehVClbhyQiki+MRgNlyznj6FC00omidB/69Ix0LpxPUfJdyPz8/DAa8/8PNEVrpIiIiBQhqQ93JrNiJYyn/6H0+nWkdnvU1iGJiOQLo9GAo4Mj/Vf25+Dpg7YOp9ipX7E+Sx5bgtFoUOJdyGbMmMGwYcOYPHkytWvXpnTp0jna/f39repXibeIiEhBKVWKlCcG4Dr7PVwWRSrxFpFi5+DpgypoKMVKz549ASy3DjMYsi6nyL7neEZGhlX9KvEWEREpQFeefArX2e9RetMGjMeOklm9hq1DEhERkZs4evRogfRbdKoLiIiIFEGZ1aqT2r4jAC6LP7VxNCIiInIr1apVo1q1aly+fJndu3dToUIFMjIy8Pf3p1q1alb3axeJt8lkomHDhmzatMmy7ujRozzwwAO4ubnRoEEDvvvuuxz7/PDDDzRs2BBXV1c6duzIkSNHcrTPmjULPz8/3N3dCQ0NJTk52dKWkpJCaGgo5cqVw8fH57qbo9/u2CIiIrlxZeBgAJyXLoLUVBtHIyIiIjdz7tw5HnjgAZo0aULv3r05deoUo0aNomHDhhw/ftzqfm2eeKekpPD444+zf/9+yzqz2UyPHj2oXLkyO3fuZMCAAQQHB3PixAkATpw4QY8ePRg0aBA7duygYsWK9OjRw3Jz8xUrVjBx4kTCw8PZsGED0dHRjBkzxtL/6NGj2blzJxs2bODDDz9k0qRJfPnll3d0bBERkdxK7fQIGZW8MZ45Telv19k6HBEREbmJESNG4ObmxpkzZ3BxcQFg/vz5VK1alREjRljdr00T7wMHDhAYGMjhw4dzrN+4cSOHDx8mPDyc+vXrM27cOFq3bk1ERAQA8+bNo2XLlrz88svcfffdREZGcuzYMX766ScAZs+ezahRo+jatSsBAQGEh4cTERFBcnIySUlJzJs3j9mzZ9O8eXOCg4MZM2YMc+bMuaNji4iI5NrVImsALgsjbRyMiIiI3Mz69et56623KFeunGVdxYoVmTlzpiXftIZNE++ffvqJDh06sHXr1hzro6Ojad68OW5u/7vPXlBQkGW76Oho2rVrZ2lzdXWlefPmbN26lYyMDHbs2JGjPTAwkNTUVPbu3cvevXtJS0ujTZs2Ofretm0bmZmZtz22iIiINVL6D8RsMFD6p40Yjx65/Q4iIiJiEykpKdetO336NKVKlbK6T5sm3sOGDeP999/H1dU1x/r4+Hh8fX1zrPP29iYuLu627efPnyclJSVHu6OjI15eXsTFxREfH0+FChVy3I/N29ublJQUEhMTb3tsERERa2RWq06aiqyJiIjYtSeeeIKRI0eyf/9+DAYDSUlJbNy4kWeeeYa+ffta3a9d3k4sOTkZJyenHOucnJwwmUy3bc8uonazdrPZfMM2wLL/rY59IyaT6bp2Dw8PDAa4ets3sUL2a6fXUKTgabwVjpSnBlN64484L1tMcth4uOaPwCJFid4rRAqPPY03e4qloMyYMYNx48bRokULUlNTadKkCQ4ODgwZMoQZM2ZY3a9dJt7Ozs4kJibmWGcymSwz487OztcluiaTiXLlyuHs7GxZvtH+GRkZN2yDrFPWb3fsG3n77beZNGmSZdnX15eTJ09Svrz7nTxduQ0vL72OIoVF462APdEbxr2CMT6eCls2QO/eto5IJNc8Pd1uv5GI5AuNt8JXunRp3nvvPd58802OHDlCeno6tWrVokyZMnnq1y4Tbz8/vxxVzgESEhLw8fGxtCckJFzX3rRpU7y8vHB2diYhIYF69eoBkJ6eTmJiIj4+PpjNZs6cOUN6ejqOjo6WfV1cXChXrtxtj30j48aN46WXXrpu/dmzl8jMzP3zlywGQ1YSkJh4iasF60WkgGi8FR7Xfv1xff9dUv/vIy52eNjW4RQbDg5GfUEtJOfOJZGRoS84JZ3GXOGwt/FmNFLsJxcXLlx43bq9e/diMBgoXbo0Pj4+BAYG5rh0+U7YZeIdGBjItGnTuHLliqWEe1RUFEFBQZb2qKgoy/bJycnExMQwceJEjEYjAQEBREVF0b59ewC2bt1KqVKlaNKkCQClSpUiOjra0l9UVBQBAQEYjcbbHvtGnJycrjs9HcBsRl9g84FeR5HCo/FW8K70fwqXWe9R+qeNGA4fJrNmLVuHJJJrep8QKTz2NN7sKZaCsmDBAn7++WecnZ2pW7cuZrOZv/76i6SkJKpVq8a5c+coW7Ys69evt0z03gmb38f7Ru677z6qVq3KoEGD2L9/P9OmTWP79u2EhoYCMHjwYLZs2cK0adPYv38/gwYNokaNGpZE+7nnnmPGjBmsXr2aHTt2MGzYMIYMGYKrqyuurq489dRTPPvss+zYsYPVq1fz7rvvMnLkyDs6toiISF5k+lcjrcP9gIqsiYiI2JtGjRrRpUsX4uLi2LVrF7t37yYuLo7HHnuMXr16cebMGbp162bJH++UXSbeDg4OrFmzhvj4eFq0aMHixYtZtWoV/v7+AFSvXp2VK1cSGRlJQEAAiYmJrF69GsPVq/379evHuHHjGDp0KA8++CCtWrVi+vTplv5nzpxJixYt6NChA8OHD2fSpEk89thjd3RsERGRvLoycDAAzssWQ2qqjaMRERGRbJ9++inTpk3LcR9vDw8PpkyZwty5c3FwcGDkyJH88ssvuerXbk41N//rvIXatWvf8gbljzzyCI888shN28PCwggLC7thm6urK59++imffnrjmYbbHVtERCQvUh96mAzvyjicSsDpm7WYHn3M1iGJiIgIUKZMGQ4ePEj9+vVzrD948KDl8uLLly9bLku+U3aTeIuIiJQYjo6k9B+A28wZOC9coMRbRETETrz88ssMHjyYffv20bJlS8xmM7t27WLWrFmMHj2auLg4nn32WTp37pyrfpV4i4iI2EBK/6dwff9dSm/ehPGIiqyJiIjYgxdffJFKlSrx4Ycf8u677+Lo6Mjdd9/Nxx9/TN++ffn5559p06YNU6ZMyVW/+ZJ4nz59mgoVKliusRYREZFby6zqT2rHB3D68XtcFi0gaULuPsBFREQk/82YMYPHH3+c/v3737C9Xbt2tGvXLtf95rq42t9//02/fv3Ys2cPKSkp3HfffVSuXJnq1auzd+/eXAcgIiJSUqVcW2TNZLJxNCIiIjJ16lRSC6Dwaa4T72HDhnH69Gm8vLxYsGAB+/bt45dffqF79+688MIL+R6giIhIcZX6YCcyKvtgTEzE6Zu1tg5HRESkxHviiSeYOnUqf/75Z74m4Lk+1XzDhg3s2rWLqlWrsmrVKh599FFatWpFpUqVuPvuu/MtMBERkWLP0ZGUJwbgNnM6zosWYOrR09YRiYiIlGjffPMNx48fZ8GCBTdsz8jIsKrfXCfezs7OXLlyhXPnzrFp0yaWLl0KwNGjRylfvrxVQYiIiJRUKU8+hev7Myi9+SccjvxFRs3atg5JRESkxLpZwp1XuU68e/ToQd++fXFxccHT05MuXbrwxRdfMHLkSEJCQgogRBERkeIrs0pVUu9/EKcfvsN54QKSJr5p65BERERKrPvuuw+AS5cu8ddff9GgQQNMJhMeHh556jfX13h/9NFHDB06lPvuu48NGzbg7OyMyWRi/PjxvPXWW3kKRkREpCSyFFn7fImKrImIiNiQyWRiyJAheHp6EhAQwMmTJwkJCeHhhx/m3LlzVveb68Tb0dGRF198kVmzZtGgQQNSUlKoV68eAwYM0O3ERERErJD6wENk+PhmFVlb95WtwxERESmxRo8ezf79+4mJicHFxQWASZMmcebMGUaMGGF1v7lOvA8cOEBgYCC//PIL58+fp1mzZrRq1YoqVaqwceNGqwMREREpsa4WWQNwXrTAtrGIiIiUYCtXruQ///kPjRo1sqxr1KgRc+fO5ZtvvrG631wn3sOHD6dmzZrUqVOH+fPnc/78eeLj43n11Vd5+eWXrQ5ERESkJEvpPxCz0UjpqJ9xOPynrcMREREpkS5duoSrq+t16zMzM0lPT7e631wn3tu2bWPq1KlUqFCB1atX89hjj+Ht7c0TTzzBoUOHrA5ERESkJMsusgbgvHCBbYMREREpobp378748eO5dOkSAAaDgaNHj/LCCy/QpUsXq/vNdeJdrlw5EhISiI2NZevWrXTt2hWAmJgYvL29rQ5ERESkpFORNREREduaM2cORqMRT09PkpKSaNGiBbVr18bT05MPPvjA6n5zfTuxkJAQunfvjpOTEzVq1OChhx7i448/5pVXXmHKlClWByIiIlLSpd7/IBm+fjj8fRKnr/+L6bHetg5JRESkRClbtiwrVqzg8OHDHDp0iPT0dOrWrUu9evXy1G+uE++33nqLgIAAjh8/zuOPP46DgwP+/v4sW7bMMvstIiIiVrhaZM3t3Wk4L1qgxFtERKSQPfTQQzz++OMEBwfn6dTyf8v1qeaAJYht27axatUqqlevrqRbREQkH1iKrG3ZjMNfKrImIiJSmFq2bMk777xD5cqV6datG0uWLOHy5ct57jfXiff58+cJDg6mXr16DBo0iEGDBtGoUSM6duzIhQsX8hyQiIhISZbpV4XUBx4CwHlhpI2jERERKVneeustDh06xI4dO2jRogXvvPMOlSpVolevXnz55ZdW95vrxHvEiBHExcVx4MABEhMTOX/+PPv27ePy5cu89NJLVgciIiIiWVIGDgLA+YulkJJi42hERERKnkaNGjFx4kQ2b97MhAkT+O677+jbt6/V/eU68f7vf//LRx99RN26dS3rGjRowJw5c1i9erXVgYiIiEiW1PsfIsPXD+PZszh9/V9bhyMiIlKinDlzhnnz5tG5c2e8vb35/PPPGT9+PEeOHLG6z1wn3s7OzhiN1+9mNBrJyMiwOhARERG5ysGBlP4DAZ1uLiIiUpjat2+Pj48PM2fOpFWrVuzdu5fdu3czduxYqlWrZnW/uU68u3fvznPPPcfhw4ct6/78888831BcRERE/sdSZG3rFhz+/MPW4YiIiJQIrVu3ZufOnRw4cIAJEybkONM7L3KdeE+fPh1nZ2fq1KmDl5cXXl5e1KtXj/Lly/Of//wnX4ISEREp6TJ9/Uh9sBOgWW8REZHC8vbbb9OkSRMSEhKIjY3lxIkTOX6slev7eJcrV45Nmzaxb98+Dh48iLOzM3Xr1s23vwSIiIhIlpSBg3D69hucv1hK0vgJ4Oxs65BERESKte+//54hQ4YQGxubY73ZbMZgMFh9eXWuE+9sjRo1olGjRpblffv2MWPGDBYuXGhtlyIiInKN1I4PkuFXBYeTcTitXYOpl/XVVEVEROT2nn/+eVq1asVXX32Fh4dHvvWb61PNb+bvv/9myZIl+dWdiIiIqMiaiIhIoYqNjWXatGk0atSIatWqXfdjrXxLvEVERCT/WYqsRf+Cwx+/2zocERGRYq1du3ZERUXle79Wn2ouIiIiBS/Tx5fUhx7Gaf06nBdFkjRlmq1DEhERKbbatWvHsGHDWLt2LXfddRelS5fO0f7GG29Y1a8SbxERETuXMnBQVuL9+VKSxk9UkTUREZEC8v333xMQEMA///zDP//8k6PNYDAUbOLdoUMHDAbDLbdJTEy0KgARERG5tdQOD5BRpSoOcbE4fbUaU+9+tg5JRESkWNq4cWOB9HtHiXf79u3vqLPHHnssL7GIiIjIjVwtsub2zlRcFkYq8RYRESlAMTExvPvuuxw8eJCMjAzq1q3L8OHDue+++6zu844S7wkTJlh9ABEREcm7lCcG4PruNEpt24rD74fIqFvP1iGJiIgUO6tWraJv37707NmTQYMGkZGRwdatW3nwwQdZvnw5jz76qFX9qqq5iIhIEZDp40vqgw8D4LxItxYTEREpCK+//jrvvPMOn332GS+88AKjRo3i888/55133snThLQSbxERkSIi5alBADh/8RlcuWLjaERERIqfI0eO0K1bt+vWd+vWjd9/t/62nkq8RUREiojU9veTUdUf4/nzOH212tbhiIiIFDv169fnm2++uW79unXrqF69utX96nZiIiIiRUV2kbVpb2YVWevzuK0jEhERKVYmTZpEz5492bZtG61atQIgOjqaL7/8kkWLFlndr1Uz3kuWLKFly5aUK1eOI0eOMGrUKKZNm2Z1ECIiInJnUp4YgNnBgVLbo3E4dNDW4YiIiBQrXbt25ZtvvuHKlSt89NFHREZGkpmZyebNm+nTp4/V/eY68f7oo48YPXo0ISEhpKamAtCyZUtmzJjBpEmTrA5EREREbi+zsg+pDz0CqMiaiIhIQejYsSMrVqzgwIED7N69m88++4x77rknT33mOvH+z3/+wyeffMLzzz+Pg4MDAE8++SSLFi1i3rx5eQpGREREbu+KpcjaMhVZExERu2UymRg+fDienp54e3vz6quvYjabgax7Zbdq1QpXV1cCAgLYtWtXjn0/++wzatWqhaurK8HBwZw5c8bSZjabCQsLo2LFipQvX54xY8aQmZmZp1j/+usvBgwYwPHjx3OsHzx4MI8//jjHjh3LU/+5TryPHz9O/fr1r1tfq1YtEhMT8xSMiIiI3F5adpG1C+dx+u8qW4cjIiJyQyNHjuT777/n22+/ZenSpXzyySfMnTuXpKQkOnfuTNu2bdm1axdt2rShS5cuJCUlAbB9+3ZCQ0OZMGEC0dHRnDt3jpCQEEu/M2fOZOnSpaxatYoVK1awZMkSZs6caXWcBw8eJDAwkD///JOUlJQcbY888ghHjx4lICCAP//80+pj5DrxDgwMZOHChZZlg8GA2Wzm3XffzfP0u4iIiNwBo5GUJ58CwGWhTjcXERH7c/bsWebPn88nn3zCPffcw/3338/LL7/Mtm3b+Pzzz3FxcWHGjBnUr1+fWbNm4e7uzvLlywGYM2cOffr0YeDAgTRu3JhFixaxbt06jh49CsDs2bOZPHkyQUFBdOjQgXfeeYc5c+ZYHev48eN5+OGH2bp1K3Xr1s3R1rt3b3755RfuvfdeXn31VauPYdWp5hEREQQEBJCSksJzzz1H7dq1Wbt2LbNmzbI6EBEREblzliJrO7bhcPCArcMRERHJISoqirJly3LfffdZ1oWFhREREUF0dDRBQUEYDAYgazL33nvvZevWrUBWFfF27dpZ9qtatSr+/v5ER0fz999/Exsbm6M9KCiI48ePEx8fb3WsY8eOtcTzb0ajkVdffZWoqCir+gcrEu+GDRvyxx9/8OyzzzJq1Cjq1avH6NGj+fPPP2natKnVgYiIiMidy/SuTGqnzoCKrImIiP05cuQI1atXZ+HChdSrV4+aNWsyZcoUMjMziY+Px9fXN8f23t7exMXFAdyyPTu5vrbd29sbwLJ/bqWmpuLk5HTLbcqXL8+VPNRVseo+3s7OzoSGhlp90JLCYMj6Eetkv3Z6DUUKnsZb0ZTy1CCc1n2F8/LPSX59Eri62jokm3Mr5YZ7aXdbh1EsuZVyszzWe4Vk05grGPY63rJjuXjxYo71Tk5O1yWuly9f5s8//yQ8PJzIyEji4+MZOnQorq6uJCcnX7e9k5MTJpMJ4JbtycnJluVr2wDL/rnVvHlz1q1bR506dW66zdq1a687DT03cp14G43Gm07Bly5dGh8fH/r06cOUKVMoVaqU1YEVB+XL600oP3h56XUUKSwab0VMr0ehenWMx45RYeN6eOopW0dkc5sHb7Z1CMWep6fb7TeSEkNjrmDZ63irUqUKly5dsixPmDCBiRMn5tjG0dGRixcvsnTpUqpVqwbAiRMn+PDDD7nrrruuS5JNJhOuV/+A7OzsfNN2Z2dny/K1jwHL/rn18ssv07dvX7y9vXn88ceva1+2bBnjx49n7ty5VvUPViTeH330EZMmTWLixIm0bt0as9nMzp07mTBhAoMHD6ZRo0ZMmjQJs9nMO++8Y3VgxcHZs5fIY1X7Es1gyEoCEhMvcfWuAyJSQDTeii6XJwbi9tZk0v7vQy50eczW4diMg4MRT0832ka0Ze+pvbYOp1hq4t2EzYM3c+5cEhkZ+oJT0mnMFSx7HW9GY9bk4r9P6b7Rado+Pj44Oztbkm6AunXrEhsbS/v27UlISMixfUJCAj4+PgD4+fndtN3Pz8+yXL16dcvj7GNao0uXLkydOpVBgwYxevRoWrRoQdmyZTl37hy7d+/m7NmzvPHGG/Tv39+q/sGKxHvGjBlERETw8MMPW9Y1btyYqlWrMnz4cCZNmoSfnx89e/Ys8Ym32Yy+wOYDvY4ihUfjrei58vgAXKe/Rakd2zHu309Gg7ttHZJNJaUlcSn10u03lFxLSkuyPNb7hGTTmCsY9jresmPx8PC47baBgYGkpKTwxx9/WE7hPnjwINWrVycwMJBp06ZhNpstd8nasmUL48ePt+wbFRVluYVYbGwssbGxBAYG4uvri7+/P1FRUZbEOyoqCn9/f6sTb8i69Vn37t1ZunQpv/76K6dOncLLy4uXXnqJ3r174+/vb3XfYEXiferUKapUqXLd+sqVK3Py5Ekg6y8N/z7vX0RERPKf2dub1E6dcfr6v7gsiuTy2+/aOiQRERHq1q1Lly5dCAkJ4aOPPiIhIYFp06bx2muv0atXL8LCwhg1ahRDhw4lPDycpKQk+vTpA8CwYcNo3749rVu3JiAggJEjR9K1a1dq1KhhaR87dqwlLw0LC+Pll1/Oc8w1atSwJP/5LddVzR988EGGDx/O8ePHLeuOHz/OyJEjuf/++8nIyCAiIoJGjRrla6AiIiJyY1cGDgLAafnncLXojIiIiK0tWbKE2rVrExQUxMCBA3n++ed54YUX8PDwYO3atWzevJkWLVoQHR3NunXrcHPLuqa9devWhIeHM2nSJNq0aYOnpyeRkf+7g8fo0aPp27cvwcHB9O7dmwEDBvDiiy/a6mneEYPZnLuTF86ePUvfvn358ccf8fLywmw2c+7cOTp16kRERAQ7duxg8ODBrFmzhjZt2hRU3EVCYqKu8c4LgwEqVHDnzBldcypS0DTeirjMTMrf0xSHE8e4+J+PMPWz/hq0osrRMet60+bhzYlJiLF1OMVSs8rN2D10N+fOJZGeri84JZ3GXMGy1/FmNKoQq7Vyfap5+fLl+f777/njjz/Yt28fjo6ONGjQgLvuuguABx54gH/++eemlc9FREQknxmNXBnwFGWmTsLl04gSmXiLiIjYs1yfag6Qnp6Om5sbAQEBNGvWjNKlS/PHH3/w+eef4+LioqRbRESkkKX0exKzoyOldu3AYf9vtg5HRESkyLt48SLnzp3Ll75ynXivWbPGUkmuRo0a1KhRg5o1a1K/fv18uaBdREREcs/s7U3qw10AcFkUeZutRURE5GZmz56Nn58fnp6eVKhQgcqVKzN58uQ89ZnrxDssLIzg4GAOHDiAp6cnv/zyC1999RXVq1fnzTffzFMwIiIiYj0VWRMREcmbKVOmMHXqVN544w327NnDrl27eOONN5gzZw7Tpk2zut9cX+N95MgR1q5dS61atWjRogUJCQk8+uijODg48Morr1jutSYiIiKFK61dezKqVcfh+DGc1qzE9PiTtg5JRESkSJk7dy7z58+nW7dulnVNmzbFz8+PESNGEBYWZlW/uZ7xLleuHMlX/4per1499uzZY3l89OhRq4IQERGRfGA0cmVACAAuCyNsG4uIiEgRdPHiRerUqXPd+rp163L69Gmr+8114t2lSxeee+45Dhw4QPv27Vm0aBG7d+8mPDwcX19fqwMRERGRvPtfkbWdOPy2z9bhiIiIFClt2rTh3XffJfOa+0JnZGTw7rvvcs8991jdb65PNZ89ezYjR45k586dDBgwgBUrVhAQEECZMmVYvHix1YGIiIhI3pkrVSL1ka44fbUal0WRXH5npq1DEhERKTJmzpxJu3bt+P7772nRogUAu3btwmQysX79eqv7zfWM99q1a5kxYwYDBw7EYDCwePFizp8/z5kzZ3KcBy8iIiK2YSmy9uUXkJRk42hERESKjvr163Po0CFeeuklvL29qVatGuPGjePPP/+kSZMmVveb68T7ueee48yZMznWubu7U6pUKauDuJnY2Fi6du2Kh4cH1atXZ9asWZa2mJgYWrVqhaurKwEBAezatSvHvp999hm1atXC1dWV4ODgHDGbzWbCwsKoWLEi5cuXZ8yYMTlOJUhMTKRnz564u7tTo0YNzeSLiEiRktb2PjKq18B46SLOa1baOhwREZEiY/DgwZQuXZoRI0bw4YcfMnPmTIYOHUpaWhq9evWyut9cn2reoUMHli5dyquvvoqTk5PVB74Tffr0oVq1auzatYsDBw7wxBNPUK1aNR566CE6d+5M//79WbBgAR9//DFdunTh8OHDuLm5sX37dkJDQ/n4449p2rQpI0aMICQkhLVr1wJZpw8sXbqUVatWkZaWxpNPPkmlSpV45ZVXAAgJCeHKlSts3bqVbdu28fTTT1OnTp08ndMvIiJSaIxGrjwZQpk3J+C8MIKUJwbYOiIRERG7tXXrVv78808APv30U5o3b46Hh0eObQ4dOsR3331n9TEMZrPZnJsdgoKC+OWXXzAajVSqVAlnZ+cc7UeOHLE6mGudO3eO8uXLs2/fPho2bAhAz5498fHxoXnz5rz55pscPnwYg8GA2WymTp06jB8/npCQEAYOHIjRaGTBggVA1sx5tWrVOHz4MDVq1MDf35/Jkydbbn22ePFiXnvtNY4dO8bhw4epXbs2R48epXr16gA8/fTTpKenW/q7U4mJl7hmIl1yyWCAChXcOXPmErn7XyoiuaXxVvwYTp/Gq2k9DGlpnP0xioxGjW0dUoFydDTi6elG8/DmxCTE2DqcYqlZ5WbsHrqbc+eSSE/XF5ySTmOuYNnreDMawcvL3dZh5Lu9e/cSHByM2Wzm+PHjVKlSBQcHB0u7wWDAzc2N5557jmHDhll1jFzPeA8ZMoQhQ4ZYdbDccHFxwdXVlcjISKZNm8aRI0fYsmULU6dOJTo6mqCgIAwGA5D1Qtx7771s3bqVkJAQoqOjc9xfrWrVqvj7+xMdHY2TkxOxsbG0a9fO0h4UFMTx48eJj49n27ZtVK1a1ZJ0Z7e//fbbBf6cRURE8ou5YkVMj3TF+b+rsoqsTX/f1iGJiIjYpSZNmlgmkDt06MDKlSvx9PTM12PkOvF+6qmnLI/PnTtH2bJlMRgMliQ4vzg7O/N///d/PP/888yePZuMjAxCQkIIDQ1l9erV3H333Tm29/b25rfffgMgPj7+ulubeXt7ExcXR3x8PECOdm9vbwBL+832vRmTyYTJZMqxzsPDA4MhaxZJrJP92uk1FCl4Gm/Fk+mpQTj/dxVOX35B0sQ3wc3N1iFJMaH3CpHCY0/jzZ5iKSgbN24skH5znXibzWbeeust3n//fc6fP88ff/zBG2+8QZkyZZg9e3a+Xvd98OBBunXrxssvv8xvv/3GCy+8wAMPPEBycvJ1x3FycrIkv7dqT05Otixf2wZY2m/V9428/fbbTJo0ybLs6+vLyZMnKV+++J2GYQvF8XQWEXul8VbM9OgCtWtj/OsvKvzwNYSG2joiKQY8PfUHHJHCovFWfOQ68Z4yZQqfffYZCxYsoG/fvkDWLPjQoUMZPXo0//nPf/IlsB9//JF58+YRFxeHi4sLLVu25OTJk7z55pvUrFnzukTYZDLh6uoKZM2W36w9+5p0k8mU4zFgab9V3zcybtw4XnrppevWnz2ra7zzwmDISgISE3XNqUhB03grvlyeGIjb5DdI+78PufBoH1uHU2AcHIz6glpIzp1LIiNDX3BKOo25wmFv481oRJOLVsp14r1gwQIWLFhAu3btMBqz7kb24IMP8umnn9K7d+98S7x37drFXXfdhYuLi2Vds2bNmDp1Km3btiUhISHH9gkJCfj4+ADg5+d303Y/Pz/LcvZ13NnbZrffqu8bcXJyuuFMv9mMvsDmA72OIoVH4634udK3P65vT6FUzG4cft1LeiPr70Eqkk3vEyKFx57Gmz3FUtTk+j7ep06duu4aaABPT08uX76cL0FB1unaf/31F6mpqZZ1hw4dokaNGgQGBvLLL7+QXZDdbDazZcsWAgMDAQgMDCQqKsqyX2xsLLGxsQQGBuLr64u/v3+O9qioKPz9/fHx8SEwMJDjx4/nuKY7KirK0reIiEhRYq5YEVPnbgA4L1xg22BERESKgEOHDnHhwgUAvv32W4YPH878+fPz1GeuE+/777+fGTNmWJYNBgOXLl3i1VdfpUOHDnkK5lrdunWjVKlSPP300/zxxx989dVXvPXWW4wYMYJevXpx/vx5Ro0axYEDBxg1ahRJSUn06ZN1Ct2wYcNYtGgR8+fP59dff2XgwIF07dqVGjVqWNrHjh3Lpk2b2LRpE2FhYYwcORKAmjVr0qlTJwYMGMCvv/7K/PnzWbp0KcOHD8+35yYiIlKYUgYOAsBpxReQj38kFxERKW7mzp1Lo0aN2LNnDzExMXTv3p0jR47w2muv8cYbb1jdb64T7w8//JCYmBgqV67MlStX6N69O1WqVOHYsWN88MEHVgfyb2XLluXHH38kPj6egIAAXnzxRV577TWeeeYZPDw8WLt2LZs3b6ZFixZER0ezbt063K5Wa23dujXh4eFMmjSJNm3a4OnpSWRkpKXv0aNH07dvX4KDg+nduzcDBgzgxRdftLQvXLgQd3d3WrVqxdSpU4mIiOCee+7Jt+cmIiJSmNKC2pFeoybGy5dwXr3C1uGIiIjYrenTp7Nw4ULuu+8+IiIiaNq0Kd988w2ff/458+bNs7pfg9ls3Zn6P/74I4cOHSI9PZ26devy0EMPWa75liyJiSqulhcGA1So4M6ZMyr2JFLQNN6KP5c5sykz+XXSmjbj/Hc/2TqcfOfomFXoqXl4c2ISYmwdTrHUrHIzdg/dzblzSaSn6wtOSacxV7DsdbwZjcX/DiguLi788ccfVK1alerVqzN06FDGjRvHkSNHaNKkCZcuXbKq31wXV3vmmWd4/PHH6dixI/fff79VBxUREZHCldKvP25vT6bUnhgcf91DeuOmtg5JRETE7tSrV48lS5ZQqVIlTpw4QY8ePUhLS+O9996jSRPrC5Tmeor68uXL9OjRA19fX1544QW2bNli9cFFRESkcJgrVMDURUXWREREbuW9997j3Xff5emnn+a5556jfv36vPjii6xatYrZs2db3W+uE++lS5dy+vRp5s6dy+XLl+nevTv+/v688sor7Ny50+pAREREpGClDBwMqMiaiIjIzXTs2JF//vmHxMRE5syZA8Drr7/O8ePHadGihdX95vpUc4DSpUvTrVs3unXrRmpqKjNnzuStt97i/fffJyMjw+pgREREpOCk3duW9Jq1cDxyGOdVX5IyIMTWIYmIiNidpKQkfv/9d9LS0vh3SbR27dpZ1adViXdGRgYbN25k5cqVrF69mvT0dPr370+/fv2sCkJEREQKgcFAyoBBlJn0Gs4LI5V4i4iI/MvixYt59tlnSU5Ovq7NYDBYPdGc61PNQ0JC8Pb2pnfv3ly5coXIyEji4+P56KOPqFOnjlVBiIiISOFI6dcfc+nSlNobg+NeVSIWERG51quvvsqQIUO4cOECmZmZOX7ycnZ3rhNvk8nE/PnzOXXqFJGRkbRv357ly5fzyCOP4O/vb3UgIiIiUvDMXl4qsiYiInITiYmJjBw5Enf3/L1tWq4T788++4xHH32UHTt28Mwzz1C5cmX69+9PbGwss2bNytfgREREJP9ZiqytXI7hsnX3IxURESmOunXrxooVK/K931xd4338+HEWLlzIwoULOXLkCOXKlePixYssW7aM3r1753twIiIikv/S2gSRXqs2jof/wmnll6QMHGTrkEREROyCn58f48eP54svvuCuu+6idOnSOdojIiKs6veOZrwjIyPp0KEDNWvWZO7cuTz00EN89913nDp1CqPRyN13323VwUVERMQGrhZZA3BeGGnjYEREROzH2bNnefzxx2nQoAGlSpXCbDbn+LHWHc14h4aGUrt2bRYuXEj//v2tPpiIiIjYh5S+T+D21iRK/boHx70xpDdpZuuQREREbC4ysmD+IH1HM94RERHUrFmTkJAQKlWqxKBBg/jvf/9LSkpKgQQlIiIiBcvs5YWpa3dAs94iIiLXWrNmDffeey/ly5enbNmy3HPPPSxcuDBPfd5R4h0SEsL69ev5+++/mTBhAocPHyY4OJgKFSqQmZnJpk2bSEtLy1MgIiIiUriyi6w5r1CRNREREYDw8HD69+9Pu3bt+PTTT/n0009p3749w4cPZ968eVb3m6uq5hUrVmT48OH8/PPPHD9+nAkTJtC0aVOef/55fH19eemll6wORERERApXWut7Sa99F4bkJJxWLLd1OCIiIjY3ffp0PvzwQ95++226detGjx49mD59OnPmzGHGjBlW95vr24llq1KlCqNHj2bXrl38/vvvPP/886xfv97qQERERKSQ/bvIWh6KxoiIiBQHp06donXr1tetb9OmDSdOnLC6X6sT72vdddddTJgwgQMHDuRHdyIiIlJIUvo+jrl0aUrt24vj3hhbhyMiImJTzZo1u+H13AsWLKBBgwZW95ur+3iLiIhI8WIu74Wp66M4r1yO88JILjdtbuuQREREbGb69Oncf//9bNy4kVatWgEQHR3Nnj17WLt2rdX95suMt4iIiBRdKU9dLbK28ksMly7aOBoRERHbad26Nbt27eKee+7h4MGDHD16lHbt2nHo0CE6dOhgdb+a8RYRESnh0gLbkH5XHRz//AOnFctJCQm1dUgiIiI2U79+fWbOnJmvfSrxFhERKekMBlIGhFDmjVdxXhiZNQNuMNg6KhERkULRsWNHVq5cSbly5ejQoQOGW3wGbtiwwapjKPEWERERUvo+gdvUSZT67Vcc9+wmvVkLW4ckIiJSKO677z5Kly4NQPv27QvkGEq8RUREBLNneUzdeuD85edZRdaUeIuISAkxYcIEy+MaNWrQt29fnJyccmyTlJTE/PnzrT6GiquJiIgIACkDr97Te5WKrImISMlx5swZTpw4wYkTJxg0aBD79++3LGf/bNiwgbFjx1p9DM14i4iICABprVqTXqcujn/8jtOXX5Ay6GlbhyQiIlLgNm3aRJ8+fSzXdgcEBORoN5vNADz55JNWH0OJt4iIiGTJLrL2+jhcFkZmVTdXkTURESnmevXqxbFjx8jMzKRmzZps376dihUrWtoNBgNubm54eXlZfQwl3iIiImKR0udx3N6ciOP+fTjG7CK9eUtbhyQiIlLg/P39AcjMzLzpNmlpaZQqVcqq/pV4i4iIiIXZszym7sE4L1+WVWRNibeIiJQgp06d4u2332b//v1kZGQAWaeam0wmDh48yLlz56zqV8XVREREJIcrA64WWVu9AsPFCzaORkREpPAMHjyY9evXExAQQFRUFK1ataJixYps376dSZMmWd2vEm8RERHJIb1VIOl162FITsbpyy9sHY6IiEih+emnn4iMjOStt96iSZMmdO3alS+++IKpU6fyzTffWN2vEm8RERHJ6WqRNQCXhZFwtZqriIhIcWc2m/Hz8wOgQYMG7N69G4A+ffqwY8cOq/tV4i0iIiLXSenzOGZnZxwP/Ibj7p22DkdERKRQNG/enEWLFgHQtGlTvv/+ewCOHj1qua2YNVRcTURERK5jLueZVWTti8+yiqy1CLj9TiIiIkXctGnT6Nq1K66urgwcOJAZM2bQqFEjTpw4oft4i4iISP67MmBQVuK9egVJU97G7FHW1iGJiIgUqKZNm3L8+HGuXLmCl5cXO3fuZNWqVXh5edGnTx+r+9Wp5iIiInJD6fe0Ir1efQxXruC0/HNbhyMiIlLg7r77bg4fPoy3tzcAvr6+DB8+nH79+mE0Wp8+K/EWERGRG1ORNRERKWEcHBxITU3N9351qrmIiIjcVErvfrhNmYDjwf047tpBest7bB2SiIhIgenSpQsPPvggXbt2pXr16jg7O+dof+ONN6zqVzPeIiIiclPmcp6YHn0MuDrrLSIicodWrVqFwWDI8dOrVy8AYmJiaNWqFa6urgQEBLBr164c+3722WfUqlULV1dXgoODOXPmjKXNbDYTFhZGxYoVKV++PGPGjCEzMzNfYt63bx8tWrQgPj6erVu3snHjRsvPpk2brO5XM94iIiJyS1cGDML586U4rVnJ5SlvYy5bztYhiYhIEXDgwAG6devG3LlzLeucnZ1JSkqic+fO9O/fnwULFvDxxx/TpUsXDh8+jJubG9u3byc0NJSPP/6Ypk2bMmLECEJCQli7di0AM2fOZOnSpaxatYq0tDSefPJJKlWqxCuvvJLnmDdu3JjnPm5EM94iIiJyS+kB95Bev0FWkbUvVWRNRETuzMGDB2nYsCGVK1e2/JQrV47PP/8cFxcXZsyYQf369Zk1axbu7u4sX74cgDlz5tCnTx8GDhxI48aNWbRoEevWrePo0aMAzJ49m8mTJxMUFESHDh145513mDNnTr7FfeTIEUaPHk2PHj2Ij48nIiKCLVu25KlPJd4iIiJyawYDV1RkTUREcunAgQPUqVPnuvXR0dEEBQVhMBgAMBgM3HvvvWzdutXS3q5dO8v2VatWxd/fn+joaP7++29iY2NztAcFBXH8+HHi4+PzHPPPP/9M48aNOXr0KOvXr+fKlSscOnSIDh06sHLlSqv7VeItIiIit2Xq3Q+ziwuOBw/guHO7rcMRERE7Zzab+f333/n222+pU6cOtWrVIiwsjNTUVOLj4/H19c2xvbe3N3FxcQC3bM9Orq9tz771V/b+eTFmzBimTZvGl19+SalSpQCYPn0606dPt7qwGuga7wJlMGT9iHWyXzu9hiIFT+NNbqtcOUyPPobzsiW4LIzk8j2tbB3RDbmVcsO9tLutwyiW3Eq5WR7rvUKyacwVDHsdb9mxXLx4Mcd6JycnnJyccqw7ceIEycnJODk58cUXX3D06FFGjBjBlStXLOv/3YfJZAK4ZXtycrJl+do2wLJ/Xuzbt4/OnTtft7579+6MGzfO6n6VeBeg8uX1JpQfvLz0OooUFo03uaURw2HZEpzXrMT5ozng6WnriK6zefBmW4dQ7Hl6ut1+IykxNOYKlr2OtypVqnDp0iXL8oQJE5g4cWKObapVq0ZiYiKenp4YDAaaNm1KZmYmTz75JO3bt78uSTaZTLi6ugJZBdhu1p59ey+TyZTjMWDZPy+qV6/Ojh07qFmzZo71X3/9NdWrV7e6XyXeBejs2UvkU1X7EslgyEoCEhMv6XJCkQKm8SZ3pPbdlGtwN44H9nP543mkDHnW1hFZODgY8fR0o21EW/ae2mvrcIqlJt5N2Dx4M+fOJZGRoS84JZ3GXMGy1/FmNGZNLv77lO5/z05nK1++fI7l+vXrk5KSQuXKlUlISMjRlpCQgI+PDwB+fn43bffz87MsZyfC2dtm758Xb775JiEhIezcuZP09HQWLlzI0aNHWbZsGYsWLbK6XyXeBchsVv2Z/KDXUaTwaLzJrWUVWXMfNxrnhZFcCR1qX+dAAklpSVxKvXT7DSXXktKSLI/1PiHZNOYKhr2Ot+xYPDw8brvtt99+yxNPPEFsbKxlJnrPnj14eXnRtm1bpk2bhtlsxmAwYDab2bJlC+PHjwcgMDCQqKgoQkJCAIiNjSU2NpbAwEB8fX3x9/cnKirKknhHRUXh7++fL4l3cHAwNWvW5L333qNhw4asWbOGunXr8vPPP9OqlfWXWam4moiIiNwxU6++WUXWDh3EcYeKrImIyI21adMGFxcXnn76aX7//Xe++eYbRo8ezZgxY+jVqxfnz59n1KhRHDhwgFGjRpGUlESfPn0AGDZsGIsWLWL+/Pn8+uuvDBw4kK5du1KjRg1L+9ixY9m0aRObNm0iLCyMkSNH5kvcCxcupF69eixcuJAdO3YQExPDsmXLaNiwIf/5z3+s7lcz3iIiInLHzGXLkdKjJy6fLcZlYQSX7LTImoiI2Ja7uzvffvsto0aNomXLlri7uzN06FBGjx6NwWBg7dq1PPvss8ydO5fGjRuzbt063Nyyrmlv3bo14eHhvPHGG5w9e5aHHnqITz75xNL36NGj+eeffwgODsbR0ZHQ0FBefPFFq2M9c+aMpWjboEGDaNiwIRUqVMixzd69exk7diwjRoyw6hgGs9meTl4oXhITdY13XhgMUKGCO2fO6JpTkYKm8Sa54bhzO56dH8Ds7Ezir79jLmf7ImuOjlnXmzYPb05MQoytwymWmlVuxu6huzl3Lon0dH3BKek05gqWvY43o7F4FmL98ssv6dOnj+W+4tmuPRUe4Mknn2ThwoVWHUMz3iIiIpIr6S0CSG/QEMcDv+G8fBlXhgyzdUgiIiJW69WrF8eOHSMzM5OaNWuyfft2KlasaGk3GAy4ubnh5eVl9TGUeIuIiEjuGLKLrL2SVWTt6WftrsiaiIhIbvj7+wOQWUCnLCvxFhERkVwz9e5LmSlv4Pj7IRy3byO9VaCtQxIREbHK5MmT73jbN954w6pjKPEWERGRXDN7lM0qsrZ0UVaRNSXeIiJSRG3cuPGOtjMYDEq8RUREpHClDAjBZekinP67istvTsPsWd7WIYmIiOTanSbeeaH7eIuIiIhV0pu3JP3uRhhMJpyXL7N1OCIiInZLibeIiIhY52qRNQDnhZHoXnQiIiI3psRbRERErGbq1QezqyuOf/yO47ZoW4cjIiJil5R4i4iIiNXMHmVJCe4FgMvCCBtHIyIiYp+UeIuIiEiepFw93dzpq9UYzp21bTAiIiJ2yK4Tb5PJxPDhw/H09MTb25tXX30V89Xrx2JiYmjVqhWurq4EBASwa9euHPt+9tln1KpVC1dXV4KDgzlz5oylzWw2ExYWRsWKFSlfvjxjxozJcaP0xMREevbsibu7OzVq1GDx4sWF84RFRESKoPRmLUhr2DiryNoXn9k6HBEREbtj14n3yJEj+f777/n2229ZunQpn3zyCXPnziUpKYnOnTvTtm1bdu3aRZs2bejSpQtJSUkAbN++ndDQUCZMmEB0dDTnzp0jJCTE0u/MmTNZunQpq1atYsWKFSxZsoSZM2da2kNCQrhw4QJbt27ltdde4+mnn2b79u2F/fRFRESKBoPBMuutImsiIiLXM5jN9vnpePbsWby9vfnhhx+47777AJg2bRp//PEHQUFBvPnmmxw+fBiDwYDZbKZOnTqMHz+ekJAQBg4ciNFoZMGCBQDExsZSrVo1Dh8+TI0aNfD392fy5MmWZHzx4sW89tprHDt2jMOHD1O7dm2OHj1K9erVAXj66adJT0+39HenEhMvcc1EuuSSwQAVKrhz5swlfYcTKWAab5JXhksX8WpUF0NyEuf/u560wDaFenxHRyOenm40D29OTEJMoR67pGhWuRm7h+7m3Lkk0tP1Baek05grWPY63oxG8PJyt3UYRZLdznhHRUVRtmxZS9INEBYWRkREBNHR0QQFBWEwGAAwGAzce++9bN26FYDo6GjatWtn2a9q1ar4+/sTHR3N33//TWxsbI72oKAgjh8/Tnx8PNu2baNq1aqWpDu7PbtvERERuZ7Z3YOUx7KKrDl/qiJrIiIi17LbxPvIkSNUr16dhQsXUq9ePWrWrMmUKVPIzMwkPj4eX1/fHNt7e3sTFxcHcMv2+Ph4gBzt3t7eAJb2W/UtIiIiN2YpsrZ2DYazibYNRkRExI442jqAm7l8+TJ//vkn4eHhREZGEh8fz9ChQ3F1dSU5ORknJ6cc2zs5OWEymQBu2Z6cnGxZvrYNsLTfqu8bMZlM17V7eHhgMGSdvinWyX7t9BqKFDyNN8kPGc2ak96oCY779uK8/DNSnn3e1iFJAdF7hUjhsafxZk+xFDV2m3g7Ojpy8eJFli5dSrVq1QA4ceIEH374IXfdddd1ia7JZMLV1RUAZ2fnm7Y7Oztblq99DFjab9X3jbz99ttMmjTJsuzr68vJkycpX17XP+QHXUciUng03iTPnnsWhg2jzOJPKTM+TN/SiiFPTzdbhyBSYmi8FR92m3j7+Pjg7OxsSboB6tatS2xsLO3btychISHH9gkJCfj4+ADg5+d303Y/Pz/LcvZ13NnbZrffqu8bGTduHC+99NJ168+eVXG1vDAYspKAxEQVexIpaBpvkl8MnbpR3vUVDL//zvmvviW9zb2FclwHB6O+oBaSc+eSyMjQF5ySTmOucNjbeDMa0eSilew28Q4MDCQlJYU//viDOnXqAHDw4EGqV69OYGAg06ZNw2w2W6qab9myhfHjx1v2jYqKslQtj42NJTY2lsDAQHx9ffH39ycqKsqSeEdFReHv74+Pjw+BgYEcP36cuLg4qlSpYmkPDAy8aaxOTk7XnZ4OWXdT0RfYvNPrKFJ4NN4kr8xlPEjp2RuXRQtw/jSCS60LJ/GWwqX3CZHCY0/jzZ5iKWrstrha3bp16dKlCyEhIezdu5dvv/2WadOmMWzYMHr16sX58+cZNWoUBw4cYNSoUSQlJdGnTx8Ahg0bxqJFi5g/fz6//vorAwcOpGvXrtSoUcPSPnbsWDZt2sSmTZsICwtj5MiRANSsWZNOnToxYMAAfv31V+bPn8/SpUsZPny4zV4LERGRokRF1kRERHKy28QbYMmSJdSuXZugoCAGDhzI888/zwsvvICHhwdr165l8+bNtGjRgujoaNatW4ebW9bpLq1btyY8PJxJkybRpk0bPD09iYyMtPQ7evRo+vbtS3BwML1792bAgAG8+OKLlvaFCxfi7u5Oq1atmDp1KhEREdxzzz2F/vxFRESKovSmzUlr3BRDairOn39m63BERERszmA264SBgpKYqGu888JggAoV3DlzRtecihQ0jTfJb86fRuA+ehTpte/i3JadBV5kzdEx63rT5uHNiUmIKdBjlVTNKjdj99DdnDuXRHq6vuCUdBpzBctex5vRqEKs1rLrGW8REREpmkw9e5PpVgbHv/6k1NYttg5HRETEppR4i4iISL4zl3HH9FhvAJwXRtg4GhEREdtS4i0iIiIFImVgCABOa/+LIVFF1kREpORS4i0iIiIFIr1JM9KaNLtaZG2prcMRERGxGSXeIiIiUmCyby3mvChSN4AVEZESS4m3iIiIFBjTY72yiqwd/otSv0TZOhwRERGbUOItIiIiBcZcxh1Tzz6AiqyJiEjJpcRbRERECpSlyNrXX2E4c8a2wYiIiNiAEm8REREpUOmNm5LWVEXWRESk5FLiLSIiIgUuZcAgQEXWRESkZFLiLSIiIgUuJbgXmWXccTxymFJbNts6HBERkUKlxFtEREQKXpkyKrImIiIllhJvERERKRQqsiYiIiWVEm8REREpFOmNmpDWrDmGtDScly2xdTgiIiKFRom3iIiIFJocRdYyM20cjYiISOFQ4i0iIiKFJqVHz6wia0ePqMiaiIiUGEq8RUREpPCUKYOpV3aRtUgbByMiIlI4lHiLiIhIobpy9XRzp3VfYTh92sbRiIiIFDwl3iIiIlKoMho1Jq15CxVZExGREkOJt4iIiBS67CJrLiqyJiIiJYASbxERESl0KT16kunugcOxo5SK+tnW4YiIiBQoJd4iIiJS+NzcVGRNRERKDCXeIiIiYhM5iqz984+NoxERESk4SrxFRETEJjIaNiKtRUsM6ekqsiYiIsWaEm8RERGxGRVZExGRkkCJt4iIiNhMyqOPZRVZO36MUpt/snU4IiIiBUKJt4iIiNiOmxum3n0BcFGRNRERKaaUeIuIiIhNZRdZK/3NWhVZExGRYkmJt4iIiNhUxt0NSWsRcLXI2mJbhyMiIpLvlHiLiIiIzV0ZmF1kbYGKrImISLGjxFtERERszvToY2R6lM0qsvbzJluHIyIikq+UeIuIiIjtubqqyJqIiBRbSrxFRETELliKrK3/GsOpUzaORkREJP8o8RYRERG7kNHgbtJa3qMiayIiUuwo8RYRERG78b8ia5+qyJqIiBQbSrxFRETEbpi6B2cVWTtxjFI/bbR1OCIikk+6dOlCSEiIZTkmJoZWrVrh6upKQEAAu3btyrH9Z599Rq1atXB1dSU4OJgzZ85Y2sxmM2FhYVSsWJHy5cszZswYMu38j7VKvEVERMR+uLqS0qcfoCJrIiLFxbJly1i3bp1lOSkpic6dO9O2bVt27dpFmzZt6NKlC0lJSQBs376d0NBQJkyYQHR0NOfOncuRtM+cOZOlS5eyatUqVqxYwZIlS5g5c2ZhP61cUeItIiIidiXlmiJrxlMJNo5GRETy4uzZs4wePZqAgADLus8//xwXFxdmzJhB/fr1mTVrFu7u7ixfvhyAOXPm0KdPHwYOHEjjxo1ZtGgR69at4+jRowDMnj2byZMnExQURIcOHXjnnXeYM2eOTZ7fnVLiLSIiInYlo34D0gJaYcjIwPkzFVkTESnKXnnlFQYMGECDBg0s66KjowkKCsJgMABgMBi499572bp1q6W9Xbt2lu2rVq2Kv78/0dHR/P3338TGxuZoDwoK4vjx48THxxfSs8o9R1sHUJwZDFk/Yp3s106voUjB03gTe5MyMIRSO7bhvPhTrox8CYx3NlfgVsoN99LuBRxdyeRWys3yWO8Vkk1jrmDY63jLjuXixYs51js5OeHk5HTd9hs2bODnn39m3759DBs2zLI+Pj6eu+++O8e23t7e/Pbbb5Z2X1/f69rj4uIsyfW17d7e3gDExcXh4+Nj5bMrWEq8C1D58noTyg9eXnodRQqLxpvYjdCn4PVxOJw4ToWYaOjU6Y522zx4cwEHJp6ebrffSEoMjbmCZa/jrUqVKly6dMmyPGHCBCZOnJhjm5SUFIYOHcr//d//4eLikqMtOTn5ukTdyckJk8l02/bk5GTL8rVtgGV/e6TEuwCdPXtJd0LJA4MhKwlITLyE2WzraESKN403sUduvfvh8snHmD74Py61aHPLbR0cjHh6utE2oi17T+0tpAhLlibeTdg8eDPnziWRkaEvOCWdxlzBstfxZjRmTS7GxcXlWH+j2e5JkybRsmVLOt3gD6fOzs7XJckmkwlXV9fbtjs7O1uWr30MWPa3R0q8C5DZjL7A5gO9jiKFR+NN7MmVAYNw+eRjSq9fhyE+nszKtz99MCktiUupl267neReUlqS5bHeJySbxlzBsNfxlh2Lh4fHbbddtmwZCQkJlClTBvhfcvzll1/yxBNPkJCQs3hmQkKC5TRxPz+/m7b7+flZlqtXr255DNjtaeag4moiIiJipzLq1SftnkAVWRMRKYI2bdrEvn372LNnD3v27KF79+50796dPXv2EBgYyC+//IL5aiZvNpvZsmULgYGBAAQGBhIVFWXpKzY2ltjYWAIDA/H19cXf3z9He1RUFP7+/nadeGvGW0REROzWlQEhlNoejfPiT0ke8RI4ONg6JBERuQPVqlXLsezunlVHpnbt2lSqVImwsDBGjRrF0KFDCQ8PJykpiT59+gAwbNgw2rdvT+vWrQkICGDkyJF07dqVGjVqWNrHjh1LlSpVAAgLC+Pll18uxGeXe5rxFhEREbtl6h5MZrlyOMSeoNRPG2wdjoiI5AMPDw/Wrl3L5s2badGiBdHR0axbtw43t6xicq1btyY8PJxJkybRpk0bPD09iYyMtOw/evRo+vbtS3BwML1792bAgAG8+OKLtno6d0Qz3iIiImK/XFxI6fM4rnM/wuXTSNI6PmjriERExAoLFizIsXzPPfewe/fum24fEhJCSEjIDdscHByYOXMmM2fOzMcIC5ZmvEVERMSupQwYBEDp777BmBBv42hERERyT4m3iIiI2LWMuvVIa9U6q8ja0kW2DkdERCTXlHiLiIiI3bsyIAQA58WfQkaGbYMRERHJJSXeIiIiYvdM3XpkFVmLi6X0ph9tHY6IiEiuKPEWERER++fiQkrfJwBw/jTyNhuLiIjYFyXeIiIiUiRYiqx9vx5j/N82jkZEROTOKfEWERGRIiGjTl1SA9uoyJqIiBQ5SrxFRESkyEhRkTURESmClHiLiIhIkWHq1oNMT08cTsZReuMPtg5HRETkjijxFhERkaLD2ZmUPleLrC1UkTURESkalHiLiIhIkZJ9unnp79Zj/PukbYMRERG5A0q8RUREpEjJqFOX1Nb3YsjMVJE1EREpEopM4t2lSxdCQkIsyzExMbRq1QpXV1cCAgLYtWtXju0/++wzatWqhaurK8HBwZw5c8bSZjabCQsLo2LFipQvX54xY8aQmZlpaU9MTKRnz564u7tTo0YNFi9eXODPT0RERO6cpcjakoUqsiYiInavSCTey5YtY926dZblpKQkOnfuTNu2bdm1axdt2rShS5cuJCUlAbB9+3ZCQ0OZMGEC0dHRnDt3LkfSPnPmTJYuXcqqVatYsWIFS5YsYebMmZb2kJAQLly4wNatW3nttdd4+umn2b59e6E9XxEREbk1U9dHySxfPqvI2obvbR2OiIjILdl94n327FlGjx5NQECAZd3nn3+Oi4sLM2bMoH79+syaNQt3d3eWL18OwJw5c+jTpw8DBw6kcePGLFq0iHXr1nH06FEAZs+ezeTJkwkKCqJDhw688847zJkzB4DDhw+zdu1a5s2bR8OGDQkNDeXJJ5/kww8/LPwnLyIiIjemImsiIlKE2H3i/corrzBgwAAaNGhgWRcdHU1QUBAGgwEAg8HAvffey9atWy3t7dq1s2xftWpV/P39iY6O5u+//yY2NjZHe1BQEMePHyc+Pp5t27ZRtWpVqlevnqM9u28RERGxD5Yia99/iyEuzrbBiIiI3IKjrQO4lQ0bNvDzzz+zb98+hg0bZlkfHx/P3XffnWNbb29vfvvtN0u7r6/vde1xcXHEx8cD5Gj39vYGsLTfbN+bMZlMmEymHOs8PDwwGODq3wbECtmvnV5DkYKn8SZFUWadOqS1CaLUL1E4LVkI06baOqQSQ+8VIoXHnsabPcVS1Nht4p2SksLQoUP5v//7P1xcXHK0JScn4+TklGOdk5OTJfm9VXtycrJl+do2wNJ+q75v5O2332bSpEmWZV9fX06ePEn58u53+nTlFry89DqKFBaNNylyhg+DX6JwWbIQ3px0++0lzzw93WwdgkiJofFWfNht4j1p0iRatmxJp06drmtzdna+LhE2mUy4urrett3Z2dmyfO1jwNJ+q75vZNy4cbz00kvXrT979hLXFEuXXDIYspKAxMRLmM22jkakeNN4kyLrvocoX748xrg4WL/e1tGUCOfOJZGRoS84JZ2Dg1FJYSGwt/FmNKLJRSvZbeK9bNkyEhISKFOmDPC/5PjLL7/kiSeeICEhIcf2CQkJ+Pj4AODn53fTdj8/P8ty9nXc2dtmt9+q7xtxcnK6bpYcwGxGX2DzgV5HkcKj8SZFTmknUvr2x/WjDyA8HLraOqCSQe8TIoXHnsabPcVS1NhtcbVNmzaxb98+9uzZw549e+jevTvdu3dnz549BAYG8ssvv2C++ps3m81s2bKFwMBAAAIDA4mKirL0FRsbS2xsLIGBgfj6+uLv75+jPSoqCn9/f3x8fAgMDOT48eM5rumOioqy9C0iIiL2JbvIGuvW4X021aaxiIiI3IjdznhXq1Ytx7K7e9YpDbVr16ZSpUqEhYUxatQohg4dSnh4OElJSfTp0weAYcOG0b59e1q3bk1AQAAjR46ka9eu1KhRw9I+duxYqlSpAkBYWBgvv/wyADVr1qRTp04MGDCA2bNns2PHDpYuXcpPP/1UWE9dREREciGj9l2kBbWlVNRmemxJZH3A7fcREREpTHY7430rHh4erF27ls2bN9OiRQuio6NZt24dbm5Z15m0bt2a8PBwJk2aRJs2bfD09CQy8n/3+Bw9ejR9+/YlODiY3r17M2DAAF588UVL+8KFC3F3d6dVq1ZMnTqViIgI7rnnnkJ/niIiInJnTE8NAqDHlkQcMmwcjIiIyL8YzGadqV9QEhNVXC0vDAaoUMGdM2dU7EmkoGm8SVHnmJGGZ6O6cOYM3R6HtXVtHVHx06xyM3YP3c25c0mkp+sLTknn6JhVXK15eHNiEmJsHU6xY6/jzWjUHVCsVSRnvEVERERycHKCkBAAhu60bSgiIiL/psRbREREiochQwB45C+oet62oYiIiFxLibeIiIgUD3XqsKNuGRzMEKozX0VExI4o8RYREZFiY0VQBQCe3o2KrImIiN1Q4i0iIiLFxsamZTntCn6XoPOfto5GREQkixJvERERKTbSShmJbJr1eOgum4YiIiJiocRbREREipVPWmT9+8if4H/epqGIiIgASrxFRESkmPnLC36skfUlJ3S3raMRERFR4i0iIiLF0Nyrs96hMSqyJiIitqfEW0RERIqdVfXgn6tF1rqoyJqIiNiYEm8REREpdtIcIbJZ1uOhO20bi4iIiBJvERERKZY+aZ7178N/QbVzto1FRERKNiXeIiIiUiwd9oIfsousxdg6GhERKcmUeIuIiEixZSmythscVWRNRERsRIm3iIiIFFur68EpN/C9DF3+sHU0IiJSUinxFhERkWIrzREim2Y9HrrLpqGIiEgJpsRbREREirV5V4usdVKRNRERsREl3iIiIlKsHfaC72tmfel5eretoxERkZJIibeIiIgUe5YiazEqsiYiIoVPibeIiIgUe2vqZhVZ87kMXVVkTURECpkSbxERESn20hwholnW46E7bRuLiIiUPEq8RUREpETILrL20GGoriJrIiJSiBxtHYAUHqPRgNFosHUYuebgUHT+PpSZaSYz02zrMERE5AaOlIfvasJDR7KKrL12v60jEhGRkkKJdwlhNBooW9YNR8eil3h7errZOoQ7lp5u5sKFJCXfIiJ2am6LrMR7cAxMbA/pDraOSERESgIl3iWE0WjA0dFA//5w8KCtoyme6teHJUuyzipQ4i0iYp/W1IOEq0XWuv0OqxrYOiIRESkJlHiXMAcPQkyMraMQERGxjXSHrCJrr0bB0F1KvEVEpHAUnYtnRURERPJBdpG1ToehxlnbxiIiIiWDEm8REREpUY6Wh29rZT1+erdtYxERkZJBibeIiIiUOHNbZP07OAYcM2wbi4iIFH9KvEVERKTE+W9diC8DlZOg+++2jkZERIo7Jd4iIiJS4mQXWQMYutO2sYiISPGnxFtERERKpHnNIZOs+3rXVJE1EREpQEq8RUREpEQ65gnfqciaiIgUAiXeIiIiUmJdW2StVLptYxERKW7++usvOnXqRJkyZfD392fGjBmWtqNHj/LAAw/g5uZGgwYN+O6773Ls+8MPP9CwYUNcXV3p2LEjR44cydE+a9Ys/Pz8cHd3JzQ0lOTk5EJ5TtZS4i0iIiIl1ldXi6x5q8iaiEi+yszMpEuXLlSsWJGYmBg+/vhj3nzzTZYuXYrZbKZHjx5UrlyZnTt3MmDAAIKDgzlx4gQAJ06coEePHgwaNIgdO3ZQsWJFevTogdlsBmDFihVMnDiR8PBwNmzYQHR0NGPGjLHl070tJd4iIiJSYqU7wPzsImu7bBuLiEhxcurUKZo2bcpHH33EXXfdRefOnbn//vuJiopi48aNHD58mPDwcOrXr8+4ceNo3bo1ERERAMybN4+WLVvy8ssvc/fddxMZGcmxY8f46aefAJg9ezajRo2ia9euBAQEEB4eTkREhF3PeivxFhERkRItu8jag0egVqKtoxERKR58fHz4/PPPcXd3x2w2s2XLFn7++Wfat29PdHQ0zZs3x83NzbJ9UFAQW7duBSA6Opp27dpZ2lxdXWnevDlbt24lIyODHTt25GgPDAwkNTWVvXv3Ft4TzCVHWwdQnBkMWT/2xM0N3N1tHUXxdM37ht393kVuJ/v/rP7vSlHnVsoN99K5+6A76w0/1knmwT8yeG5vaSY+7FRA0RVtbqX+90Gn9wrJZs2Yk9uz1/GWHcvFixdzrHdycsLJ6ebvndWrV+fEiRN07dqVnj17MmrUKHx9fXNs4+3tTVxcHADx8fE3bT9//jwpKSk52h0dHfHy8rLsb4+UeBeg8uXt701o82ZbR1D8eXq63X4jETvl5WV/71siubF5sJUfdPVXQ3AwLx0qx0vfx0Lp0vkaV3Gizzm5ltVjTu6IvY63KlWqcOnSJcvyhAkTmDhx4k23X7FiBQkJCQwbNowXX3yR5OTk6xJ1JycnTCYTwC3bs08nv9X+9kiJdwE6e/YSmZm2jiKLg4MRT0832rYFOz4Do0hr0iTrDxvnziWRkWEnv3iRO2QwZCXdiYmXuFq3RKRIsXzORbRl76ncf9A5ZpjZ727A559/GBhSltWNShVAlEVbE+8mbB68WZ9zAuR9zMmt2et4MxqzJhf/PbN8q9lugJYtWwKQkpJC//79GTx4MElJSTm2MZlMuLq6AuDs7HxdEm0ymShXrhzOzs6W5Zvtb4+UeBcgsxm7+wKblATX/HFK8tG17x329nsXuVP2+L4lkhtJaUlcSrXug25eM3j9ZxgYncKiuin5HFnRl5T2vw86vU9ItryMObk5ex1v2bF4eHjcdttTp06xdetWevToYVnXoEEDUlNT8fHx4eDBgzm2T0hIwMfHBwA/Pz8SEhKua2/atCleXl44OzuTkJBAvXr1AEhPTycxMdGyvz1ScTURERER/ldk7YGjKrImIpJXR48e5bHHHuPkyZOWdbt27aJixYoEBQWxe/durly5YmmLiooiMDAQyCqWFhUVZWlLTk4mJiaGwMBAjEYjAQEBOdq3bt1KqVKlaNKkSSE8M+so8RYREREBTpSD9bWzHg/ZbdNQRESKvICAAFq0aMHgwYM5cOAA69atY/To0YwfP5777ruPqlWrMmjQIPbv38+0adPYvn07oaGhAAwePJgtW7Ywbdo09u/fz6BBg6hRowbt27cH4LnnnmPGjBmsXr2aHTt2MGzYMIYMGWLXp5or8RYRERG5KjzrMkQGxUCpdNvGIiJSlDk4OLBmzRrc3Nxo3bo1Tz/9NCNGjGDEiBGWtvj4eFq0aMHixYtZtWoV/v7+QFYV9JUrVxIZGUlAQACJiYmsXr0aw9Wy6v369WPcuHEMHTqUBx98kFatWjF9+nRbPt3b0jXeIiIiIld9fRecdAe/S9DjECxvaOuIRESKLl9fX1auXHnDttq1a/PTTz/ddN9HHnmERx555KbtYWFhhIWF5TnGwqIZbxEREZGrMhxgfrOsx0N32TYWEREpPpR4i4iIiFxj/tUia/cfhdoqsiYiIvlAibeIiIjINU6Ug2/uyno8RLPeIiKSD5R4i4iIiPxLeIusfwftgdIqsiYiInmkxFtERETkX9bdBXHuUDE5q8iaiIhIXijxFhEREfmXDIesa70Bhu60bSwiIlL0KfEWERERuYH5zSDDAB2PwV1nbB2NiIgUZUq8RURERG4gthx8Uzvr8ZDdNg1FRESKOCXeIiIiIjcR3jLr35A9KrImIiLWU+ItIiIichPf1P5fkbXgg7aORkREiiol3iIiIiI3keEA87KLrOme3iIiYiUl3iIiIiK3ML95VpG1DsegjoqsiYiIFZR4i4iIiNxCXNms+3oDDNGst4iIWEGJt4iIiMhthLfI+jdkDzil2TQUEREpguw68T558iS9evWifPny+Pn58dJLL5GSkgLA0aNHeeCBB3Bzc6NBgwZ89913Ofb94YcfaNiwIa6urnTs2JEjR47kaJ81axZ+fn64u7sTGhpKcnKypS0lJYXQ0FDKlSuHj48P7733XsE/WREREbFb62tDrAdUuALBh2wdjYiIFDV2m3ibzWZ69epFcnIymzdvZtmyZXz11Ve8/vrrmM1mevToQeXKldm5cycDBgwgODiYEydOAHDixAl69OjBoEGD2LFjBxUrVqRHjx6YzWYAVqxYwcSJEwkPD2fDhg1ER0czZswYy7FHjx7Nzp072bBhAx9++CGTJk3iyy+/tMnrICIiIraXo8jaTtvGIiIiRY+jrQO4md9//53o6GgSEhLw9vYGYPLkybzyyis88sgjHD58mF9++QU3Nzfq16/Pjz/+SEREBBMnTmTevHm0bNmSl19+GYDIyEgqV67MTz/9RPv27Zk9ezajRo2ia9euAISHh/PQQw8xffp0zGYz8+bN45tvvqF58+Y0b96c/fv3M2fOHHr16mWz10NERERsa34zeOMnaH8c6p6G3yvaOiIRESkq7HbGu3Llyqxfv96SdGe7cOEC0dHRNG/eHDc3N8v6oKAgtm7dCkB0dDTt2rWztLm6utK8eXO2bt1KRkYGO3bsyNEeGBhIamoqe/fuZe/evaSlpdGmTZscfW/bto3MzMyCeroiIiJi506Wha+zi6zttm0sIiJStNjtjHe5cuXo1KmTZTkzM5M5c+Zw//33Ex8fj6+vb47tvb29iYuLA7hl+/nz50lJScnR7ujoiJeXF3FxcRiNRipUqEDp0qVz7JuSkkJiYiIVK17/522TyYTJZMqxzsPDA4MBDAbrXwMpuvR7l6Im+/+s/u+K3Fp4S+j+R1aRtfEdwVTK1hHZht4rRAqPPY03e4qlqLHbxPvfxowZw+7du9mxYwfvv/8+Tk5OOdqdnJwsyW9ycvJN27OLqN2s3Ww237ANuC65zvb2228zadIky7Kvry8nT56kfHl3K56pFHWenm6330jETnl56X1L5FbW14YTHuB/ER47CJ81tnVEhU+fcyKFR+Ot+CgSiffYsWOZNWsWn3/+OQ0bNsTZ2ZnExMQc25hMJlxdXQFwdna+Lkk2mUyUK1cOZ2dny/KN9s/IyLhhG2Dp/9/GjRvHSy+9dN36s2cvYS9npzs4GDVwC8m5c0lkZNjJL17kDhkMWUl3YuIlrtahFClSCutzLtOYVWRt8iYYuqtkJt76nBPQd8vCYm/jzWhEk4tWsvvE+4UXXuCjjz5i8eLF9OzZEwA/Pz/279+fY7uEhAR8fHws7QkJCde1N23aFC8vL5ydnUlISKBevXoApKenk5iYiI+PD2azmTNnzpCeno6jo6NlXxcXF8qVK3fDGJ2cnK6bJQcwm9EX2BJKv3cpqvS+JXJ7Ec1gwk9w33GodxoOlcAia3qfECk89jTe7CmWosZui6sBTJo0iY8//phly5bRr18/y/rAwEB2797NlStXLOuioqIIDAy0tEdFRVnakpOTiYmJITAwEKPRSEBAQI72rVu3UqpUKZo0aULTpk0pVaoU0dHROfoOCAjAaLTrl0tEREQKwcmysLZO1uMhu2wbi4iIFA12m0kePHiQKVOmEBYWRlBQEAkJCZaf++67j6pVqzJo0CD279/PtGnT2L59O6GhoQAMHjyYLVu2MG3aNPbv38+gQYOoUaMG7du3B+C5555jxowZrF69mh07djBs2DCGDBmCq6srrq6uPPXUUzz77LPs2LGD1atX8+677zJy5EgbvhoiIiJiT8JbZP371F5wSrNtLCIiYv/sNvFes2YNGRkZvPnmm/j4+OT4cXBwYM2aNcTHx9OiRQsWL17MqlWr8Pf3B6B69eqsXLmSyMhIAgICSExMZPXq1RiuluHr168f48aNY+jQoTz44IO0atWK6dOnW449c+ZMWrRoQYcOHRg+fDiTJk3iscces8nrICIiIvbn29pwvCx4XYGeB20djYiI2DuD2awz9QtKYqL9FFdzdMwqgNG8OcTE2Dqa4qlZM9i9O6sIRnq6nfziRe6QwQAVKrhz5oyKq0nRZPmcC29OTELhfNC99hNM2Qg/+8N9gwvlkDbVrHIzdg/drc85AWwz5koSex1vRqPugGItu53xFhEREbFnEc0g3QDtTkD9f2wdjYiI2DMl3iIiIiJW+NvjmiJru20bi4iI2Dcl3iIiIiJWCm+Z9e9Te8BZRdZEROQmlHiLiIiIWOm7WllF1sqnQM8Dto5GRETslRJvEREREStlGuGT5lmPh+qe3iIichOOtg5ARKS4MhoNGI0GW4eRKw4ORevvsZmZZjIzVYZdbCuiGUzcBG1PQIN/4EAlW0ckIiL2Rom3iEgBMBoNlC3rhqNj0Uq8PT3dbB1CrqSnm7lwIUnJt9hUvAd8VReCD8GQXfDiI7aOSERE7I0SbxGRAmA0GnB0NNC/Pxw8aOtoiqf69WHJkqyzCpR4i62Ft8hKvJ/aC+MegJRSto5IRETsiRJvEZECdPAgxMTYOgoRKWjf14JjZaH6Beh1ABY3sXVEIiJiT4rWxXwiIiIidijTCJ+0yHo8dKdtYxEREfujxFtEREQkH0Q2hXQDBMVmFVkTERHJpsRbREREJB/Ee8B/62Y9fka3FhMRkWso8RYRERHJJ+Ets/4duBec02wbi4iI2A8l3iIiIiL55PuacLQceKZA7/22jkZEROyFEm8RERGRfGI2wifNsx4P1enmIiJylRJvERERkXwU2QzSjHBvLNx9ytbRiIiIPVDiLSIiIpKPEtxVZE1ERHJS4i0iIiKSz8Kv3tN74F5wSbVtLCIiYntKvEVERETy2Q814Ug5KGeC3gdsHY2IiNiaEm8RERGRfGY2widXZ72H7rRtLCIiYntKvEVEREQKQGTTrCJrbeKgoYqsiYiUaEq8RURERArAKXdYoyJrIiKCEm8RERGRAhPeMuvfASqyJiJSoinxFhERESkgP9aAw55ZRdb67Ld1NCIiYitKvEVEREQKiNkInzTPejxUp5uLiJRYSrxFRERECtCCpllF1lrHQaMEW0cjIiK2oMRbREREpACdcofV9bIeq8iaiEjJpMRbREREpICFX72n94BfwVVF1kREShwl3iIiIiIFbMPVImtlVWRNRKREUuItIiIiUsDMRph7ddZ76E7bxiIiIoVPibeIiIhIIcgushZ4EhqryJqISImixFtERESkEPxTBlZdLbI2cSP02wf3HQVjpm3jEinujJlZY01jrvCdPHmSXr16Ub58efz8/HjppZdISUkB4OjRozzwwAO4ubnRoEEDvvvuuxz7/vDDDzRs2BBXV1c6duzIkSNHcrTPmjULPz8/3N3dCQ0NJTk5udCelzWUeIuIiIgUkv0Vs/4N/h0+WwGbPoVjsyD4gE3DEim2gg9kjbFNn2rMFTaz2UyvXr1ITk5m8+bNLFu2jK+++orXX38ds9lMjx49qFy5Mjt37mTAgAEEBwdz4sQJAE6cOEGPHj0YNGgQO3bsoGLFivTo0QOz2QzAihUrmDhxIuHh4WzYsIHo6GjGjBljy6d7W0q8RURERApB8AGY8BOY/7Xe7yJ8+YUSAZH8Fnwga2z5Xcy5XmOucPz+++9ER0cTGRnJ3XffTdu2bZk8eTJLly5l48aNHD58mPDwcOrXr8+4ceNo3bo1ERERAMybN4+WLVvy8ssvc/fddxMZGcmxY8f46aefAJg9ezajRo2ia9euBAQEEB4eTkREhF3PeivxFhERESlgxkyYvT7rseHfbVf/nbVep8CK5Jdrx9y/Ex6NucJRuXJl1q9fj7e3d471Fy5cIDo6mubNm+Pm5mZZHxQUxNatWwGIjo6mXbt2ljZXV1eaN2/O1q1bycjIYMeOHTnaAwMDSU1NZe/evQX8rKznaOsAijODIevHnri5gbu7raMonq5537C737vYjsZcwdGYkxtxK+WGe2n7G3RBR9KpevHKTduNgP9F+Ps9MJWyz//QpY0H4K1qeGT+e85eSiyjgXWXT5GaaX//Z53SzHgn3bw9e8x1+tuFqJr2lxK5lfrfh5w9fcZlx3LxYs7TCJycnHBycsqxrly5cnTq1MmynJmZyZw5c7j//vuJj4/H19c3x/be3t7ExcUB3LL9/PnzpKSk5Gh3dHTEy8vLsr89sr//ZcVI+fL298G/ebOtIyj+PD3dbr+RlBgacwVPY06utXmwnQ66zz6DeU/cdrOsRMFeE1sTnD2Bg63DELtS2dYB5NG6jvPh8cdtHcZN2etnXJUqVbh06ZJlecKECUycOPGW+4wZM4bdu3ezY8cO3n///esSdScnJ0wmEwDJyck3bc8+nfxW+9sjJd4F6OzZS2TayekrDg5GPD3daNsW7PgMjCKtSZOsJOvcuSQyMuzkFy82ozFX8DTm5FqWMRfRlr2n7G/QBR1JZ90dbDfyUSf2+NlnanuX113M6z6PixevkGkvX3DEZoxGIx4eLjz936f5M/FPW4dznaYnM5i95vZJWOcNoUQdG1oIEeVOE+8mbB682e4+44zGrMnFf88s/zsJ/rexY8cya9YsPv/8cxo2bIizszOJiYk5tjGZTLi6ugLg7Ox8XRJtMpkoV64czs7OluWb7W+PlHgXILM568eeJCXBNX+cknyUdM3pTPb2exfb0ZgrOBpzciNJaUlcSrW/QfetL8R6ZBV1ulGBnUwgzgPmNDGRaacVeC5VNkLLlmScSyI93X4SAbENR0cjeLqxe5eRGAf7K2gVVQFe2Xj7Mfet7xUyUws7uttLSvvfh5w9fcZlx+Lh4XHH+7zwwgt89NFHLF68mJ49ewLg5+fH/v37c2yXkJCAj4+PpT0hIeG69qZNm+Ll5YWzszMJCQnUq5d1j8b09HQSExMt+9sjO31rFxERESk+Mo0w8uGrj//ddvXfUQ9jt0m3SFGjMWcfJk2axMcff8yyZcvo16+fZX1gYCC7d+/mypX/1b6IiooiMDDQ0h4VFWVpS05OJiYmhsDAQIxGIwEBATnat27dSqlSpWjSpEkhPCvr6L+aiIiISCFY1QB69YGT/5ooivPIWr+qgW3iEimuNOZs6+DBg0yZMoWwsDCCgoJISEiw/Nx3331UrVqVQYMGsX//fqZNm8b27dsJDQ0FYPDgwWzZsoVp06axf/9+Bg0aRI0aNWjfvj0Azz33HDNmzGD16tXs2LGDYcOGMWTIEJ1qLiIiIiJZX/TX1IO2x8HnMsSXgc3VNOsmUlA05mxnzZo1ZGRk8Oabb/Lmm2/maDObzaxZs4bQ0FBatGhB7dq1WbVqFf7+/gBUr16dlStXMmrUKCZPnkybNm1YvXo1hqtl1fv168exY8cYOnQoJpOJnj17Mn369EJ/jrmhxFtERESkEGUa4acato5CpOTQmLONsLAwwsLCbtpeu3Ztfvrpp5u2P/LIIzzyyCNW929v9LceERERERERkQKkxFtERERERESkACnxFhERERERESlASrxFRERERERECpASbxEREREREZECpMRbREREREREpAAp8RYREREREREpQEq8RURERERERAqQEm8RERERERGRAqTEW0RERERERKQAKfEWERERERERKUBKvEVEREREREQKkBJvERERERERkQKkxFtERERERESkACnxFhERERERESlASrxvIiUlhdDQUMqVK4ePjw/vvfeerUMSERERERGRIsjR1gHYq9GjR7Nz5042bNjA8ePHeeqpp6hWrRq9evWydWgiIiIiIiJShCjxvoGkpCTmzZvHN998Q/PmzWnevDn79+9nzpw5SrxFREREREQkV3Sq+Q3s3buXtLQ02rRpY1kXFBTEtm3byMzMtGFkIiIiIiIiUtQo8b6B+Ph4KlSoQOnSpS3rvL29SUlJITEx0YaRiYiIiIiISFGjU81vIDk5GScnpxzrspdNJtN125tMpuvWe3h4YDCAg0PBxWmNNm2gYkVbR1E83XXX/x7b2+9dbEdjruBozMmNtKnahoquGnQF4S6v/w06jTnJpjFXMDTeih+D2Ww22zoIe7N8+XJeeOEFEhISLOsOHjxIgwYNSExMpHz58jm2nzhxIpMmTbIsN2vWjN27dxdavCIiIiIiImK/lHjfwC+//EK7du1ISUnB0THrpICNGzfSpUsXLl++jNGY8wz9G814Ozk5XTdrLiIiIiIiIiWPrvG+gaZNm1KqVCmio6Mt66KioggICLgu6YasJNvDwyPHj5JuERERERERAV3jfUOurq489dRTPPvss0RGRnLy5EneffddIiMjbR2aiIiIiIiIFDE61fwmkpOTGTZsGCtWrKBs2bKMHj2aUaNG2TosERERERERKWKUeIuIiIiIiIgUIF3jLSIiIiIiIlKAlHiLiIiIiIiIFCAl3iIiIiIiIiIFSIm35FndunWpW7cuoaGhAJw6dYoRI0Zwzz330LZtW95+++3r7nN+K5999hn3338/zZs3JzQ0lNjY2FzHdP78edq0aUNcXBwAp0+ftsQZFhaW6/5EbCm/x1i2//73vwwYMOC69UuWLKF9+/Y0b96cESNGcP78+Vz3vXPnTu6//37L8tdff215HitXrsx1fyIFqaDG2N69e6lfv77lsyg30tPTefTRR/nggw9yvS/Aa6+9lmPf0NBQy/MUsTV7+u74xx9/8OSTT9KsWTM6derE2rVrc/18QGNObk+Jt+SLDz74gJkzZ2I2mxkxYgRXrlxhyZIlvP/++2zcuJFZs2bdUT+bN29mxowZvPbaa6xYsQJXV1eGDx+eq1guXLjAs88+S2JiomWdl5cXUVFRPPLII7nqS8Re5NcYyxYdHc0bb7xx3fp169Yxffp0xo0bx7Jly4iPj2fy5Mm56vv3339n5MiRXFu784EHHiAqKorKlSvnqi+RwpLfYywtLY3XXnuNzMxMq+KJiIjg0KFDVu37ySefsHz58hzrZs6caXUSL1IQ7OG7Y2pqKs8++ywNGjRgzZo1DBkyhLCwMPbt25er56IxJ3dCibfki7Jly1K2bFmOHDnCnj17ePvtt7nrrrto2bIlI0aMuOO/Hv70008EBQXRoUMHatSowfPPP8/vv//O2bNn72j/nTt38thjj5GcnJxjvdFopGLFijg7O+f6uYnYg/waYwBz5sxhyJAhVK1a9bq2Tz75hCFDhtCpUyfq1KnDmDFj+OOPP8jIyLijvpctW0a/fv3w8vLKsd7JyYmKFSvi4OBwx3GKFKb8HGMA8+bNo0yZMlbFcvz4cRYuXEjt2rVztd/ly5cZMWIEn3zyCT4+Pjnasp+fiL2wh++Of/31FydPnmTkyJH4+/vTq1cv6tSpw/bt2+/o2BpzkhtKvCVfVaxYkXnz5lGhQoUc6y9fvnxH+5crV44dO3Zw+PBh0tPTWb16NX5+fnf8xhUVFUXPnj31F0YptvI6xgC2bNnC/Pnzeeihh67r48CBAzz44IOWdQEBAaxdu/aOE+aff/6Zd955h5CQkDuOR8Se5McYO3r0KEuWLLH60qY33niDF154gfLly+dqv7i4OEwmEytXrrzhH9ZE7JEtvztmb7N8+XIyMzOJiYnhyJEjNGjQ4I6OrTEnueFo6wCkePHw8KBt27aW5czMTBYvXkxgYOAd7T9gwAC2bt1K586dcXBwwMXFhSVLltzxl/5Ro0YBWHU9nUhRkNcxBlnXwgFs27Ytx/rsa+LOnj1Lv379iIuL495772X8+PF4eHjcUd8ffvghgK7jliIrr2PMbDZbEud/n/lxJ1asWIHJZKJPnz65nmWvV68e4eHhuT6miC3Z8rujn58fL730Eu+++y7Tp08nIyODF154gdatW9/RsTXmJDc04y0FasaMGRw4cIAXX3zxjrb/559/MJlMvPvuuyxbtoyAgABGjx5tVVEbkZIgt2PsVpKSkgCYPHkyQ4YMYfbs2fz555+MGTMmz32LFFW5HWNffvklaWlp9OnTJ9fHSkxMZObMmUyePBmDwZDr/UWKg8L87piWlsaRI0fo27cvy5cvZ9y4cXzyySfX/WFaJD9oxlsKzIwZM/j00095//33qVOnzh3tM2HCBB566CG6desGwHvvvUf79u358ccf6dy5c0GGK1LkWDPGbsXRMesj4ZlnnrFUJJ86dSo9evTg1KlTeHt75/kYIkVJbsfY6dOnef/991mwYIFVifPUqVN57LHH8mU8ixRFhf3dcfXq1fz222+sXbsWg8HA3XffzV9//cUnn3xCq1at8vx8RK6lGW8pEFOmTCEyMpIZM2bQqVOnO95v//791KtXz7Ls5uZGtWrVOHnyZEGEKVJkWTvGbqVixYoA1KxZ07KuRo0aACQkJOTLMUSKCmvGWFRUFOfOnaNv3740a9aMrl27AtC1a1c+/vjj2+7/9ddfs3DhQpo1a0azZs3YuXMn4eHhdOnSJU/PRaQosMV3x/3791OnTp0cfyirX78+f//9d+6CF7kDmvGWfDdnzhyWLVvGzJkzefjhh3O1b6VKlTh8+DDt2rUDsm7zEBcXR5UqVQoiVJEiKS9j7FZ8fX2pVKkShw4dokmTJgAcPnwYg8GAr69vvh1HxN5ZO8YefPBBmjdvblk+deoUAwYMYO7cuXc0e/fdd9/lWH7llVdo0qQJgwYNuvPgRYogW313rFSpErt27cqx7ujRo/reKQVCibfkq8OHD/Phhx/yzDPP0KJFC06fPm1py55NO336NO7u7je8tVfv3r35+OOPqV69OtWqVSM8PBw3Nzc6duwIwKVLl8jIyKBcuXKF8nxE7E1ex9itGAwGQkJC+M9//kOVKlXw8vJi4sSJPPDAA5a+z549i5OTE25ubvn3pETsSF7GWJkyZXLcQiy7uJOvr6/lc+tWn2PVqlXLsezs7EzZsmXx8/MDshKKCxcuUL58ed2aT4oNW3537NatG3PnzmXGjBn07duX3bt388UXX/B///d/gMac5C+dai756scffyQjI4OPPvqIoKCgHD/ZgoKCWLdu3Q33Dw0NJTQ0lDfffJPevXuTmJjIggULcHJyArKuf3vhhRcK5bmI2KO8jrHbGTx4MP3792fMmDE8/vjj+Pv78/bbb1vae/XqRURERJ6fh4i9KugxlpfPsZiYGIKCgoiPj7dqfxF7ZMvvjlWrViUiIoJdu3bx6KOP8sknnzB16lRLlXWNOclXZpE8qlOnjjk6OvqOt//yyy/Na9eutepYJpPJPHToUKv2NZvN5rFjx5rHjh1r9f4itlCYY+x2tm7dag4PD7d6/w4dOphXrFiRjxGJ5F1R+hwbM2aM+Z9//rFq3+joaHOdOnWsPrZIftGYk5JIM96SLy5cuMCFCxduu11mZiarVq2yulJkREQEDz30UK73y8zM5PTp06SkpFh1XBFbK6wxdjuLFi2iQ4cOud7PZDJx+vRpMjIyCiAqkbyz988xgBMnTnD27FnL6be5cafPT6SwaMxJSWMwm81mWwchRVvdunWBrNOA5s+ff9vt09LSKFWqlFXHsnbf06dPW05ZCg4OZtq0aVYdX8QWCnOMFVTfX3/9NS+99BIAb7/9No899lh+hyZitaLwOQZgNpvJyMiw3PovN0JDQ4mKigLg999/t+r4IvlFY05KIiXeIiIiIiIiIgVIp5qLiIiIiIiIFCAl3iIiIiIiIiIFSIm3iIiIiIiISAFS4i0iIiIiIiJSgJR4i4iIiIiIiBQgJd4iIiJ34MKFC0ybNo2OHTvSpEkTHnnkERYsWEBmZmae+zabzbz++us0bdqU+++/nw8++IABAwbkQ9TWxbJkyRLLclhYGGFhYTaJRUREpLjQ7cRERERu49y5c/Tt25dKlSoxfPhwqlSpwr59oAf8twAABs1JREFU+5gyZQqdO3fm9ddfz1P/Bw8epEePHsydO5e6devi7u5OWloa5cqVy58nkAvbt29nwIABlvvOXrp0CQB3d/dCj0VERKS4yP3d4EVEREqY9957j9KlSzN//nycnJwAqFq1Ks7Ozjz33HM8+eST1KhRw+r+s5Pbdu3aYTAY8iVma/377/FKuEVERPJOp5qLiIjcQmpqKl9//TX9+/e3JN3ZOnTowIIFC/Dz8+PChQu8/vrrtGnThhYtWjB69GguXLgAwLZt2+jYsSNLly6lbdu2NG3alNGjR5Oamsq2bdssp5XXq1ePDz744LpTzaOioujWrRuNGzfm6aefZsqUKZbTv290KnjdunXZtm0bAB07dmTGjBkEBQXRo0cPzGYzP/74Iz169KBRo0a0bNmSl156iaSkJOLi4hg4cGCOPv7d/8aNGwkODqZx48Z07tyZ7777ztI2YMAAPvroI0JDQ2ncuDGdOnVi8+bN+fWrEBERKbKUeIuIiNzCiRMnSE5OplGjRte1GQwGAgMDKV26NM8//zwHDx7k448/JjIyksOHD+dIWP/55x++/fZb5s2bxwcffMB3333H6tWradasGR988AGQlWAPHjw4xzFiY2MZNmwYjzzyCKtXr6ZRo0Y5rsG+E1999RXz589n2rRpxMbGMnLkSJ544gm++eYbZs2axS+//MIXX3yBj49PjliaNWuWo5+tW7fywgsv8Oijj7JmzRp69+7Niy++yG+//WbZ5uOPP6ZLly6sXbuWevXq8frrr+fLdfAiIiJFmU41FxERuYWLFy8Ctz7l+tChQ2zfvp3169dbTjmfMWMGnTt35siRIwCkpaXx2muvcdddd1G3bl3atm3Lvn376NOnD2XLlgWgYsWK1/W9fPlyGjduzHPPPQfAyJEj+eWXX3L1HLp3707dunUBOHbsGK+99hp9+vQBoEqVKrRp04Y///wTBweHW8ayZMkSOnXqREhICAA1atTg119/JSIigpkzZwJw33338dhjjwEwbNgwHn30UU6fPo23t3euYhYRESlOlHiLiIjcQnaBs+zTxm/kyJEjeHh45LjOu1atWpQtW5YjR45YkvZq1apZ2suUKUN6evptj//7779fN9vetGnTW8bzb35+fpbH1atXp3Tp0nz00Uf8+eef/Pnnn/z11188+uijt+3n8OHD9OvXL8e6Zs2asWLFihz9ZytTpgzAHT1PEZH/b+9+Qtn/AziOv34x5aBcuGiyMgc1HOwg5YLUTG5iRdOk0HZSc1YKpaRETnbQ0trBUWrlsFZSY/7EWo2Jm4vmQOb3O/z6LsMPtRb5PR+39+f9/nw+7/fx9Xm/P+838Jux1BwAgA9UV1errKxMJycn79aPjY2ppKTk3bpMJqNMJpMtv273lYNFioqK3rR7WX69Gdt7Ifflv+lnZ2fq7u5WIpFQc3OzZmZmZLPZPu3H6+f88fz8nLOU3GAwvGnDASoAgP87gjcAAB8oLi6WzWbTxsaGHh8fc+pCoZBCoZBqamp0d3eXXVYuSYlEQul0Oq/dziXJbDa/Cf0vywaDQff399ny1dXVh8/b2tqS1WrVwsKCHA6HGhoadHl5mQ3HH+2qbjKZdHh4mHMtGo3mPUYAAH47gjcAAJ9wu91Kp9NyuVza29tTKpVSIBDQ1NSUhoaGVFtbq7a2Nnm9XsViMcViMXm9XlmtVtXV1eX17r6+Ph0cHGhtbU3JZFKrq6va39/PBmSLxaJwOKxIJKJ4PK7p6el3Z53/KC8v1/n5uWKxmJLJpGZnZ3V0dJT9qFBaWipJOj4+1sPDQ869TqdT29vb8vl8uri40Pr6unZ2djQwMJDXGAEA+O0I3gAAfKKiokJ+v19Go1GTk5Oy2+3y+XzyeDzZncvn5uZkNBrldDrlcrlkNpu1vLyc97urqqq0tLSkYDConp4eRaNRtbe3Z8N1b2+vurq6ND4+rpGREdntdlVWVv7n8wYHB9XU1CSn0ymHw6GbmxtNTEzo9PRU0r/HiLW2tqq/v1+7u7s59zY2Nmp+fl5+v192u13BYFCLi4tqaWnJe5wAAPxmf/3Nj1cAAPxY8XhcT09Pqq+vz14bHR2VxWKR2+3+xp4BAICvYsYbAIAfLJVKaXh4WOFwWNfX1woEAopEIurs7PzurgEAgC9ixhsAgB9uZWVFm5ubur29lclkksfjUUdHx3d3CwAAfBHBGwAAAACAAmKpOQAAAAAABUTwBgAAAACggAjeAAAAAAAUEMEbAAAAAIACIngDAAAAAFBABG8AAAAAAAqI4A0AAAAAQAERvAEAAAAAKCCCNwAAAAAABfQPf/1x4FiK8asAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting data for plotting\n",
    "print(results.keys())\n",
    "\n",
    "configs_two_layer = ['[2, 8, 1]', '[2, 16, 1]']\n",
    "configs_three_layer = ['[2, 4, 4, 1]', '[2, 8, 8, 1]']\n",
    "\n",
    "# Compute Average losses\n",
    "average_losses_two_layer = [np.mean(results[config]) for config in configs_two_layer]\n",
    "average_losses_three_layer = [np.mean(results[config]) for config in configs_three_layer]\n",
    "\n",
    "iterations_of_convergence_two_layer = [np.mean(results[config+'iter']) for config in configs_two_layer]\n",
    "iterations_of_convergence_three_layer = [np.mean(results[config+'iter']) for config in configs_three_layer]\n",
    "\n",
    "# Create the combined bar + line graph\n",
    "fig, ax1 = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Combine the results\n",
    "configs_combined = configs_two_layer + configs_three_layer\n",
    "median_losses_combined = average_losses_two_layer + average_losses_three_layer\n",
    "iterations_of_convergence_combined = iterations_of_convergence_two_layer + iterations_of_convergence_three_layer\n",
    "\n",
    "# Bar graph for Average Loss\n",
    "bars = ax1.bar(configs_combined, median_losses_combined,\n",
    "               color=['blue', 'blue', 'green', 'green'],\n",
    "               width=0.4, align='center')\n",
    "ax1.set_title('Average Loss and Average # of Iterations to Convergence for Each Configuration')\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Average Loss', color='black')\n",
    "ax1.tick_params('y', colors='black')\n",
    "\n",
    "# Line graph for Iterations using twin axes\n",
    "ax2 = ax1.twinx()\n",
    "line, = ax2.plot(configs_combined, iterations_of_convergence_combined, color='red', marker='o', linestyle='-')\n",
    "ax2.set_ylabel('Iterations to Convergence', color='black')\n",
    "ax2.tick_params('y', colors='black')\n",
    "\n",
    "# Legend\n",
    "ax1.legend([bars[0], bars[2], line], [\"Two-layer Average Loss\", \"Three-layer Average Loss\", \"Iterations to Convergence\"], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(iterations_of_convergence_combined)\n",
    "# print(median_losses_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:54:59.289277900Z",
     "start_time": "2023-10-26T07:54:58.859124300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part2 Pytorch version (20 points):\n",
    "\n",
    "<p>\n",
    "Add all code to the same threelayer.ipynb.\n",
    "Given that everything is easier with pytorch, adapt the code from class to solve the\n",
    "exact same regression problem with three layers and the same number of\n",
    "parameters. Use the ‘“nn” layers. Visualize the network architecture as well.\n",
    "Test the network 20 times with ADAM optimizer and 20 times with SGD optimizer,\n",
    "using a suitably high number of iterations. Record, plot, and compare the loss\n",
    "evaluation of the two optimizer runs. What can you say about the optimizers?\n",
    "</p>"
   ],
   "metadata": {
    "id": "xJhRwYl8YCuL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "Yf_A3FKBYCNW",
    "ExecuteTime": {
     "end_time": "2023-10-26T07:55:08.128802300Z",
     "start_time": "2023-10-26T07:55:02.587240800Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def train_model_pytorch(model, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "    # Choose the optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Convert data to torch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Print every 1000 epochs\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T07:55:08.145799600Z",
     "start_time": "2023-10-26T07:55:08.124580600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Loss: 192.4335\n",
      "Epoch [2000/10000], Loss: 96.9576\n",
      "Epoch [3000/10000], Loss: 39.4108\n",
      "Epoch [4000/10000], Loss: 21.6837\n",
      "Epoch [5000/10000], Loss: 13.1703\n",
      "Epoch [6000/10000], Loss: 9.3041\n",
      "Epoch [7000/10000], Loss: 6.8145\n",
      "Epoch [8000/10000], Loss: 4.8742\n",
      "Epoch [9000/10000], Loss: 3.8784\n",
      "Epoch [10000/10000], Loss: 3.4342\n",
      "Epoch [1000/10000], Loss: 146.3619\n",
      "Epoch [2000/10000], Loss: 44.6248\n",
      "Epoch [3000/10000], Loss: 17.7828\n",
      "Epoch [4000/10000], Loss: 13.1166\n",
      "Epoch [5000/10000], Loss: 13.1765\n",
      "Epoch [6000/10000], Loss: 11.6305\n",
      "Epoch [7000/10000], Loss: 9.4988\n",
      "Epoch [8000/10000], Loss: 9.6435\n",
      "Epoch [9000/10000], Loss: 9.5128\n",
      "Epoch [10000/10000], Loss: 8.5256\n",
      "difference of optimizers: 5.091455459594727\n",
      "Done with run 0\n",
      "Epoch [1000/10000], Loss: 199.2941\n",
      "Epoch [2000/10000], Loss: 101.8423\n",
      "Epoch [3000/10000], Loss: 39.9074\n",
      "Epoch [4000/10000], Loss: 23.8457\n",
      "Epoch [5000/10000], Loss: 20.1718\n",
      "Epoch [6000/10000], Loss: 17.8953\n",
      "Epoch [7000/10000], Loss: 13.6703\n",
      "Epoch [8000/10000], Loss: 5.8991\n",
      "Epoch [9000/10000], Loss: 3.8416\n",
      "Epoch [10000/10000], Loss: 2.8841\n",
      "Epoch [1000/10000], Loss: 78.5127\n",
      "Epoch [2000/10000], Loss: 35.0078\n",
      "Epoch [3000/10000], Loss: 26.2989\n",
      "Epoch [4000/10000], Loss: 17.9420\n",
      "Epoch [5000/10000], Loss: 14.9377\n",
      "Epoch [6000/10000], Loss: 13.0467\n",
      "Epoch [7000/10000], Loss: 11.5620\n",
      "Epoch [8000/10000], Loss: 11.9573\n",
      "Epoch [9000/10000], Loss: 9.1091\n",
      "Epoch [10000/10000], Loss: 9.5447\n",
      "difference of optimizers: 6.660538911819458\n",
      "Done with run 1\n",
      "Epoch [1000/10000], Loss: 202.7274\n",
      "Epoch [2000/10000], Loss: 91.9189\n",
      "Epoch [3000/10000], Loss: 40.1055\n",
      "Epoch [4000/10000], Loss: 16.4754\n",
      "Epoch [5000/10000], Loss: 11.2504\n",
      "Epoch [6000/10000], Loss: 7.3485\n",
      "Epoch [7000/10000], Loss: 4.6173\n",
      "Epoch [8000/10000], Loss: 3.4909\n",
      "Epoch [9000/10000], Loss: 2.6518\n",
      "Epoch [10000/10000], Loss: 2.1517\n",
      "Epoch [1000/10000], Loss: 128.1484\n",
      "Epoch [2000/10000], Loss: 67.8543\n",
      "Epoch [3000/10000], Loss: 44.0844\n",
      "Epoch [4000/10000], Loss: 31.0380\n",
      "Epoch [5000/10000], Loss: 26.3892\n",
      "Epoch [6000/10000], Loss: 22.6049\n",
      "Epoch [7000/10000], Loss: 21.0226\n",
      "Epoch [8000/10000], Loss: 25.0231\n",
      "Epoch [9000/10000], Loss: 21.2013\n",
      "Epoch [10000/10000], Loss: 19.6253\n",
      "difference of optimizers: 17.473631381988525\n",
      "Done with run 2\n",
      "Epoch [1000/10000], Loss: 214.3473\n",
      "Epoch [2000/10000], Loss: 84.8801\n",
      "Epoch [3000/10000], Loss: 37.4294\n",
      "Epoch [4000/10000], Loss: 25.7617\n",
      "Epoch [5000/10000], Loss: 21.6248\n",
      "Epoch [6000/10000], Loss: 20.3745\n",
      "Epoch [7000/10000], Loss: 18.4487\n",
      "Epoch [8000/10000], Loss: 12.1889\n",
      "Epoch [9000/10000], Loss: 4.0643\n",
      "Epoch [10000/10000], Loss: 3.2656\n",
      "Epoch [1000/10000], Loss: 68.9854\n",
      "Epoch [2000/10000], Loss: 29.4777\n",
      "Epoch [3000/10000], Loss: 22.3196\n",
      "Epoch [4000/10000], Loss: 18.5763\n",
      "Epoch [5000/10000], Loss: 15.0880\n",
      "Epoch [6000/10000], Loss: 13.5783\n",
      "Epoch [7000/10000], Loss: 12.6778\n",
      "Epoch [8000/10000], Loss: 12.6003\n",
      "Epoch [9000/10000], Loss: 11.6270\n",
      "Epoch [10000/10000], Loss: 12.3722\n",
      "difference of optimizers: 9.10653281211853\n",
      "Done with run 3\n",
      "Epoch [1000/10000], Loss: 204.4762\n",
      "Epoch [2000/10000], Loss: 106.6905\n",
      "Epoch [3000/10000], Loss: 45.1490\n",
      "Epoch [4000/10000], Loss: 19.1367\n",
      "Epoch [5000/10000], Loss: 10.7837\n",
      "Epoch [6000/10000], Loss: 6.3432\n",
      "Epoch [7000/10000], Loss: 4.6503\n",
      "Epoch [8000/10000], Loss: 3.7342\n",
      "Epoch [9000/10000], Loss: 3.2036\n",
      "Epoch [10000/10000], Loss: 2.7718\n",
      "Epoch [1000/10000], Loss: 107.7772\n",
      "Epoch [2000/10000], Loss: 49.6895\n",
      "Epoch [3000/10000], Loss: 27.3351\n",
      "Epoch [4000/10000], Loss: 18.0698\n",
      "Epoch [5000/10000], Loss: 14.0163\n",
      "Epoch [6000/10000], Loss: 11.9430\n",
      "Epoch [7000/10000], Loss: 11.6601\n",
      "Epoch [8000/10000], Loss: 12.1815\n",
      "Epoch [9000/10000], Loss: 8.6207\n",
      "Epoch [10000/10000], Loss: 7.8353\n",
      "difference of optimizers: 5.063469886779785\n",
      "Done with run 4\n",
      "Epoch [1000/10000], Loss: 185.5588\n",
      "Epoch [2000/10000], Loss: 95.0035\n",
      "Epoch [3000/10000], Loss: 40.4948\n",
      "Epoch [4000/10000], Loss: 28.0629\n",
      "Epoch [5000/10000], Loss: 18.9056\n",
      "Epoch [6000/10000], Loss: 9.8130\n",
      "Epoch [7000/10000], Loss: 5.6750\n",
      "Epoch [8000/10000], Loss: 4.0288\n",
      "Epoch [9000/10000], Loss: 3.1938\n",
      "Epoch [10000/10000], Loss: 2.6045\n",
      "Epoch [1000/10000], Loss: 67.7074\n",
      "Epoch [2000/10000], Loss: 32.8602\n",
      "Epoch [3000/10000], Loss: 24.2077\n",
      "Epoch [4000/10000], Loss: 21.2390\n",
      "Epoch [5000/10000], Loss: 18.0529\n",
      "Epoch [6000/10000], Loss: 16.6745\n",
      "Epoch [7000/10000], Loss: 14.8981\n",
      "Epoch [8000/10000], Loss: 14.7048\n",
      "Epoch [9000/10000], Loss: 12.7699\n",
      "Epoch [10000/10000], Loss: 12.4008\n",
      "difference of optimizers: 9.79629111289978\n",
      "Done with run 5\n",
      "Epoch [1000/10000], Loss: 237.9526\n",
      "Epoch [2000/10000], Loss: 115.1778\n",
      "Epoch [3000/10000], Loss: 37.9173\n",
      "Epoch [4000/10000], Loss: 24.8287\n",
      "Epoch [5000/10000], Loss: 20.3915\n",
      "Epoch [6000/10000], Loss: 15.1652\n",
      "Epoch [7000/10000], Loss: 5.7996\n",
      "Epoch [8000/10000], Loss: 3.5457\n",
      "Epoch [9000/10000], Loss: 2.7146\n",
      "Epoch [10000/10000], Loss: 2.1300\n",
      "Epoch [1000/10000], Loss: 127.6914\n",
      "Epoch [2000/10000], Loss: 38.7702\n",
      "Epoch [3000/10000], Loss: 23.3600\n",
      "Epoch [4000/10000], Loss: 22.2972\n",
      "Epoch [5000/10000], Loss: 19.0814\n",
      "Epoch [6000/10000], Loss: 15.7386\n",
      "Epoch [7000/10000], Loss: 14.4818\n",
      "Epoch [8000/10000], Loss: 13.2149\n",
      "Epoch [9000/10000], Loss: 12.4443\n",
      "Epoch [10000/10000], Loss: 11.9361\n",
      "difference of optimizers: 9.806105613708496\n",
      "Done with run 6\n",
      "Epoch [1000/10000], Loss: 190.8555\n",
      "Epoch [2000/10000], Loss: 96.3237\n",
      "Epoch [3000/10000], Loss: 36.5410\n",
      "Epoch [4000/10000], Loss: 20.4884\n",
      "Epoch [5000/10000], Loss: 15.2200\n",
      "Epoch [6000/10000], Loss: 11.6930\n",
      "Epoch [7000/10000], Loss: 9.0189\n",
      "Epoch [8000/10000], Loss: 5.5832\n",
      "Epoch [9000/10000], Loss: 4.5425\n",
      "Epoch [10000/10000], Loss: 3.9329\n",
      "Epoch [1000/10000], Loss: 53.5398\n",
      "Epoch [2000/10000], Loss: 27.1279\n",
      "Epoch [3000/10000], Loss: 20.4950\n",
      "Epoch [4000/10000], Loss: 15.3998\n",
      "Epoch [5000/10000], Loss: 12.9365\n",
      "Epoch [6000/10000], Loss: 11.4015\n",
      "Epoch [7000/10000], Loss: 10.0102\n",
      "Epoch [8000/10000], Loss: 8.7400\n",
      "Epoch [9000/10000], Loss: 8.4096\n",
      "Epoch [10000/10000], Loss: 7.7126\n",
      "difference of optimizers: 3.7796308994293213\n",
      "Done with run 7\n",
      "Epoch [1000/10000], Loss: 243.1889\n",
      "Epoch [2000/10000], Loss: 142.0649\n",
      "Epoch [3000/10000], Loss: 69.8999\n",
      "Epoch [4000/10000], Loss: 19.8931\n",
      "Epoch [5000/10000], Loss: 7.7385\n",
      "Epoch [6000/10000], Loss: 5.6447\n",
      "Epoch [7000/10000], Loss: 4.7265\n",
      "Epoch [8000/10000], Loss: 3.3700\n",
      "Epoch [9000/10000], Loss: 2.7273\n",
      "Epoch [10000/10000], Loss: 2.4124\n",
      "Epoch [1000/10000], Loss: 64.0622\n",
      "Epoch [2000/10000], Loss: 32.4199\n",
      "Epoch [3000/10000], Loss: 20.6345\n",
      "Epoch [4000/10000], Loss: 23.6905\n",
      "Epoch [5000/10000], Loss: 22.6421\n",
      "Epoch [6000/10000], Loss: 20.6158\n",
      "Epoch [7000/10000], Loss: 18.2528\n",
      "Epoch [8000/10000], Loss: 17.1028\n",
      "Epoch [9000/10000], Loss: 16.1278\n",
      "Epoch [10000/10000], Loss: 15.4341\n",
      "difference of optimizers: 13.02171015739441\n",
      "Done with run 8\n",
      "Epoch [1000/10000], Loss: 171.4623\n",
      "Epoch [2000/10000], Loss: 79.9484\n",
      "Epoch [3000/10000], Loss: 37.9804\n",
      "Epoch [4000/10000], Loss: 23.6491\n",
      "Epoch [5000/10000], Loss: 16.7130\n",
      "Epoch [6000/10000], Loss: 10.4665\n",
      "Epoch [7000/10000], Loss: 6.6839\n",
      "Epoch [8000/10000], Loss: 3.3111\n",
      "Epoch [9000/10000], Loss: 2.6986\n",
      "Epoch [10000/10000], Loss: 2.5594\n",
      "Epoch [1000/10000], Loss: 86.7552\n",
      "Epoch [2000/10000], Loss: 46.8365\n",
      "Epoch [3000/10000], Loss: 34.2831\n",
      "Epoch [4000/10000], Loss: 21.9123\n",
      "Epoch [5000/10000], Loss: 17.3084\n",
      "Epoch [6000/10000], Loss: 15.2049\n",
      "Epoch [7000/10000], Loss: 14.0611\n",
      "Epoch [8000/10000], Loss: 13.0073\n",
      "Epoch [9000/10000], Loss: 12.5925\n",
      "Epoch [10000/10000], Loss: 11.7694\n",
      "difference of optimizers: 9.209999561309814\n",
      "Done with run 9\n",
      "Epoch [1000/10000], Loss: 202.5909\n",
      "Epoch [2000/10000], Loss: 83.8303\n",
      "Epoch [3000/10000], Loss: 41.1399\n",
      "Epoch [4000/10000], Loss: 38.9749\n",
      "Epoch [5000/10000], Loss: 38.7979\n",
      "Epoch [6000/10000], Loss: 38.6406\n",
      "Epoch [7000/10000], Loss: 38.4661\n",
      "Epoch [8000/10000], Loss: 37.9423\n",
      "Epoch [9000/10000], Loss: 27.5883\n",
      "Epoch [10000/10000], Loss: 21.5889\n",
      "Epoch [1000/10000], Loss: 51.2685\n",
      "Epoch [2000/10000], Loss: 24.3067\n",
      "Epoch [3000/10000], Loss: 17.9350\n",
      "Epoch [4000/10000], Loss: 16.5116\n",
      "Epoch [5000/10000], Loss: 15.0423\n",
      "Epoch [6000/10000], Loss: 14.0932\n",
      "Epoch [7000/10000], Loss: 13.0174\n",
      "Epoch [8000/10000], Loss: 11.9112\n",
      "Epoch [9000/10000], Loss: 11.3980\n",
      "Epoch [10000/10000], Loss: 10.7669\n",
      "difference of optimizers: -10.821952819824219\n",
      "Done with run 10\n",
      "Epoch [1000/10000], Loss: 208.0809\n",
      "Epoch [2000/10000], Loss: 107.2689\n",
      "Epoch [3000/10000], Loss: 24.7168\n",
      "Epoch [4000/10000], Loss: 10.3439\n",
      "Epoch [5000/10000], Loss: 7.4619\n",
      "Epoch [6000/10000], Loss: 5.3661\n",
      "Epoch [7000/10000], Loss: 4.2611\n",
      "Epoch [8000/10000], Loss: 3.9652\n",
      "Epoch [9000/10000], Loss: 3.5657\n",
      "Epoch [10000/10000], Loss: 2.5690\n",
      "Epoch [1000/10000], Loss: 52.2870\n",
      "Epoch [2000/10000], Loss: 25.1129\n",
      "Epoch [3000/10000], Loss: 15.2483\n",
      "Epoch [4000/10000], Loss: 12.1577\n",
      "Epoch [5000/10000], Loss: 11.8447\n",
      "Epoch [6000/10000], Loss: 9.9325\n",
      "Epoch [7000/10000], Loss: 8.9602\n",
      "Epoch [8000/10000], Loss: 8.7475\n",
      "Epoch [9000/10000], Loss: 8.1949\n",
      "Epoch [10000/10000], Loss: 7.2841\n",
      "difference of optimizers: 4.715080499649048\n",
      "Done with run 11\n",
      "Epoch [1000/10000], Loss: 217.5699\n",
      "Epoch [2000/10000], Loss: 93.2075\n",
      "Epoch [3000/10000], Loss: 41.2855\n",
      "Epoch [4000/10000], Loss: 37.9683\n",
      "Epoch [5000/10000], Loss: 34.1675\n",
      "Epoch [6000/10000], Loss: 25.5651\n",
      "Epoch [7000/10000], Loss: 21.3633\n",
      "Epoch [8000/10000], Loss: 21.0929\n",
      "Epoch [9000/10000], Loss: 20.6714\n",
      "Epoch [10000/10000], Loss: 11.9904\n",
      "Epoch [1000/10000], Loss: 112.6060\n",
      "Epoch [2000/10000], Loss: 36.1165\n",
      "Epoch [3000/10000], Loss: 23.9434\n",
      "Epoch [4000/10000], Loss: 22.1213\n",
      "Epoch [5000/10000], Loss: 21.1611\n",
      "Epoch [6000/10000], Loss: 20.3107\n",
      "Epoch [7000/10000], Loss: 19.6855\n",
      "Epoch [8000/10000], Loss: 19.1576\n",
      "Epoch [9000/10000], Loss: 18.0481\n",
      "Epoch [10000/10000], Loss: 17.2815\n",
      "difference of optimizers: 5.291130065917969\n",
      "Done with run 12\n",
      "Epoch [1000/10000], Loss: 170.4127\n",
      "Epoch [2000/10000], Loss: 78.3192\n",
      "Epoch [3000/10000], Loss: 26.2091\n",
      "Epoch [4000/10000], Loss: 21.9998\n",
      "Epoch [5000/10000], Loss: 14.2220\n",
      "Epoch [6000/10000], Loss: 9.6999\n",
      "Epoch [7000/10000], Loss: 3.4266\n",
      "Epoch [8000/10000], Loss: 2.6375\n",
      "Epoch [9000/10000], Loss: 2.4622\n",
      "Epoch [10000/10000], Loss: 2.3909\n",
      "Epoch [1000/10000], Loss: 65.3399\n",
      "Epoch [2000/10000], Loss: 31.0473\n",
      "Epoch [3000/10000], Loss: 23.9063\n",
      "Epoch [4000/10000], Loss: 23.0777\n",
      "Epoch [5000/10000], Loss: 19.0515\n",
      "Epoch [6000/10000], Loss: 17.2266\n",
      "Epoch [7000/10000], Loss: 15.1794\n",
      "Epoch [8000/10000], Loss: 13.7206\n",
      "Epoch [9000/10000], Loss: 13.0537\n",
      "Epoch [10000/10000], Loss: 11.8522\n",
      "difference of optimizers: 9.46134614944458\n",
      "Done with run 13\n",
      "Epoch [1000/10000], Loss: 180.1859\n",
      "Epoch [2000/10000], Loss: 77.1239\n",
      "Epoch [3000/10000], Loss: 25.2843\n",
      "Epoch [4000/10000], Loss: 10.4489\n",
      "Epoch [5000/10000], Loss: 5.1127\n",
      "Epoch [6000/10000], Loss: 3.9585\n",
      "Epoch [7000/10000], Loss: 3.3553\n",
      "Epoch [8000/10000], Loss: 2.7541\n",
      "Epoch [9000/10000], Loss: 2.3899\n",
      "Epoch [10000/10000], Loss: 2.0770\n",
      "Epoch [1000/10000], Loss: 115.8612\n",
      "Epoch [2000/10000], Loss: 48.7001\n",
      "Epoch [3000/10000], Loss: 25.6662\n",
      "Epoch [4000/10000], Loss: 19.5837\n",
      "Epoch [5000/10000], Loss: 15.3933\n",
      "Epoch [6000/10000], Loss: 15.0361\n",
      "Epoch [7000/10000], Loss: 16.0202\n",
      "Epoch [8000/10000], Loss: 18.1135\n",
      "Epoch [9000/10000], Loss: 17.5874\n",
      "Epoch [10000/10000], Loss: 17.3410\n",
      "difference of optimizers: 15.26394772529602\n",
      "Done with run 14\n",
      "Epoch [1000/10000], Loss: 185.5896\n",
      "Epoch [2000/10000], Loss: 83.4687\n",
      "Epoch [3000/10000], Loss: 34.3101\n",
      "Epoch [4000/10000], Loss: 15.5807\n",
      "Epoch [5000/10000], Loss: 9.3601\n",
      "Epoch [6000/10000], Loss: 4.4721\n",
      "Epoch [7000/10000], Loss: 2.9181\n",
      "Epoch [8000/10000], Loss: 2.4997\n",
      "Epoch [9000/10000], Loss: 2.2243\n",
      "Epoch [10000/10000], Loss: 2.0250\n",
      "Epoch [1000/10000], Loss: 62.7507\n",
      "Epoch [2000/10000], Loss: 27.3820\n",
      "Epoch [3000/10000], Loss: 18.5532\n",
      "Epoch [4000/10000], Loss: 16.3999\n",
      "Epoch [5000/10000], Loss: 13.6205\n",
      "Epoch [6000/10000], Loss: 12.6905\n",
      "Epoch [7000/10000], Loss: 11.7134\n",
      "Epoch [8000/10000], Loss: 12.0739\n",
      "Epoch [9000/10000], Loss: 12.0353\n",
      "Epoch [10000/10000], Loss: 13.1181\n",
      "difference of optimizers: 11.093162775039673\n",
      "Done with run 15\n",
      "Epoch [1000/10000], Loss: 214.7541\n",
      "Epoch [2000/10000], Loss: 114.5924\n",
      "Epoch [3000/10000], Loss: 50.0040\n",
      "Epoch [4000/10000], Loss: 27.0915\n",
      "Epoch [5000/10000], Loss: 19.5687\n",
      "Epoch [6000/10000], Loss: 14.8267\n",
      "Epoch [7000/10000], Loss: 9.3278\n",
      "Epoch [8000/10000], Loss: 5.6409\n",
      "Epoch [9000/10000], Loss: 4.2933\n",
      "Epoch [10000/10000], Loss: 3.4339\n",
      "Epoch [1000/10000], Loss: 107.9060\n",
      "Epoch [2000/10000], Loss: 41.6255\n",
      "Epoch [3000/10000], Loss: 22.3092\n",
      "Epoch [4000/10000], Loss: 14.1470\n",
      "Epoch [5000/10000], Loss: 10.6861\n",
      "Epoch [6000/10000], Loss: 9.4529\n",
      "Epoch [7000/10000], Loss: 9.7765\n",
      "Epoch [8000/10000], Loss: 11.0894\n",
      "Epoch [9000/10000], Loss: 9.5673\n",
      "Epoch [10000/10000], Loss: 8.5106\n",
      "difference of optimizers: 5.076646089553833\n",
      "Done with run 16\n",
      "Epoch [1000/10000], Loss: 169.7626\n",
      "Epoch [2000/10000], Loss: 80.1737\n",
      "Epoch [3000/10000], Loss: 31.7005\n",
      "Epoch [4000/10000], Loss: 18.1727\n",
      "Epoch [5000/10000], Loss: 10.1176\n",
      "Epoch [6000/10000], Loss: 6.8054\n",
      "Epoch [7000/10000], Loss: 5.1062\n",
      "Epoch [8000/10000], Loss: 4.0734\n",
      "Epoch [9000/10000], Loss: 3.4055\n",
      "Epoch [10000/10000], Loss: 2.7189\n",
      "Epoch [1000/10000], Loss: 74.5366\n",
      "Epoch [2000/10000], Loss: 30.8727\n",
      "Epoch [3000/10000], Loss: 20.5891\n",
      "Epoch [4000/10000], Loss: 18.0497\n",
      "Epoch [5000/10000], Loss: 16.0872\n",
      "Epoch [6000/10000], Loss: 14.5327\n",
      "Epoch [7000/10000], Loss: 13.0435\n",
      "Epoch [8000/10000], Loss: 11.9866\n",
      "Epoch [9000/10000], Loss: 11.4575\n",
      "Epoch [10000/10000], Loss: 10.6696\n",
      "difference of optimizers: 7.950659513473511\n",
      "Done with run 17\n",
      "Epoch [1000/10000], Loss: 214.5383\n",
      "Epoch [2000/10000], Loss: 97.8404\n",
      "Epoch [3000/10000], Loss: 44.9830\n",
      "Epoch [4000/10000], Loss: 19.1804\n",
      "Epoch [5000/10000], Loss: 12.4542\n",
      "Epoch [6000/10000], Loss: 9.0958\n",
      "Epoch [7000/10000], Loss: 6.4222\n",
      "Epoch [8000/10000], Loss: 4.7932\n",
      "Epoch [9000/10000], Loss: 3.3993\n",
      "Epoch [10000/10000], Loss: 2.2843\n",
      "Epoch [1000/10000], Loss: 57.5812\n",
      "Epoch [2000/10000], Loss: 29.0893\n",
      "Epoch [3000/10000], Loss: 21.1167\n",
      "Epoch [4000/10000], Loss: 15.7825\n",
      "Epoch [5000/10000], Loss: 11.9386\n",
      "Epoch [6000/10000], Loss: 10.3589\n",
      "Epoch [7000/10000], Loss: 8.7533\n",
      "Epoch [8000/10000], Loss: 8.3963\n",
      "Epoch [9000/10000], Loss: 7.8009\n",
      "Epoch [10000/10000], Loss: 7.5193\n",
      "difference of optimizers: 5.234946012496948\n",
      "Done with run 18\n",
      "Epoch [1000/10000], Loss: 200.7242\n",
      "Epoch [2000/10000], Loss: 106.8654\n",
      "Epoch [3000/10000], Loss: 42.1965\n",
      "Epoch [4000/10000], Loss: 30.1627\n",
      "Epoch [5000/10000], Loss: 25.1658\n",
      "Epoch [6000/10000], Loss: 15.7568\n",
      "Epoch [7000/10000], Loss: 10.5526\n",
      "Epoch [8000/10000], Loss: 6.7113\n",
      "Epoch [9000/10000], Loss: 5.8329\n",
      "Epoch [10000/10000], Loss: 5.3041\n",
      "Epoch [1000/10000], Loss: 71.4175\n",
      "Epoch [2000/10000], Loss: 36.3902\n",
      "Epoch [3000/10000], Loss: 25.9243\n",
      "Epoch [4000/10000], Loss: 21.6508\n",
      "Epoch [5000/10000], Loss: 17.9394\n",
      "Epoch [6000/10000], Loss: 16.7333\n",
      "Epoch [7000/10000], Loss: 14.8853\n",
      "Epoch [8000/10000], Loss: 14.4868\n",
      "Epoch [9000/10000], Loss: 13.6567\n",
      "Epoch [10000/10000], Loss: 12.8931\n",
      "difference of optimizers: 7.5890421867370605\n",
      "Done with run 19\n",
      "--- 416.7980012893677 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Train the models with the new data using nn.Sequential\n",
    "losses_adam_seq = []\n",
    "losses_sgd_seq = []\n",
    "input_dim = X.shape[1]\n",
    "hidden_nodes = [8, 8]\n",
    "output_dim = 1\n",
    "\n",
    "# keeping time\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(20):\n",
    "    # build model with adam optimizer\n",
    "    model_adam_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_adam_seq.append(train_model_pytorch(model_adam_seq, X, y, optimizer_type='adam', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    # build model with sgd optimizer\n",
    "    model_sgd_seq = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_nodes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[0], hidden_nodes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_nodes[1], output_dim)\n",
    "    )\n",
    "    losses_sgd_seq.append(train_model_pytorch(model_sgd_seq, X, y, optimizer_type='sgd', num_epochs=10000, lr=0.001))\n",
    "\n",
    "    print(f'difference of optimizers: {losses_sgd_seq[-1][-1] - losses_adam_seq[-1][-1]}')\n",
    "    print(f'Done with run {_}')\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T08:02:04.944787900Z",
     "start_time": "2023-10-26T07:55:08.134799100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1500x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss evaluations for ADAM and SGD optimizers using the provided models\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Average loss for better visualization\n",
    "avg_losses_adam_seq = np.mean(losses_adam_seq, axis=0)\n",
    "avg_losses_sgd_seq = np.mean(losses_sgd_seq, axis=0)\n",
    "\n",
    "plt.plot(avg_losses_adam_seq, label='ADAM', color='blue')\n",
    "plt.plot(avg_losses_sgd_seq, label='SGD', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evaluation for ADAM vs. SGD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T08:02:05.234656700Z",
     "start_time": "2023-10-26T08:02:04.944787900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion\n",
    "\n",
    "<p>\n",
    "SGD’s Better Performance in this Specific Task:\n",
    "\n",
    "The graph suggests that SGD is performing better because the dataset is relatively small, and the model is not complex. SGD can be more effective in such scenarios because it may generalize better, avoiding overfitting which is a common concern in small datasets. In contrast, Adam, with its adaptive learning rates, might overfit or converge too quickly to suboptimal solutions in such settings.\n",
    "\n",
    " Adam is known for its efficiency in handling large datasets and its effectiveness in training complex models. It does this by adapting the learning rate for each parameter, which helps in navigating through the parameter space more efficiently, especially when dealing with sparse gradients or non-stationary objectives.\n",
    "Fast Convergence and Adaptability: Adam generally converges faster than traditional SGD because of its adaptive learning rate mechanism. This can be particularly beneficial when training deep networks or dealing with challenging optimization landscapes.\n",
    "\n",
    "In simpler scenarios or with smaller datasets, SGD's straightforward approach to optimization can sometimes lead to finding better (or more generalized) solutions. Since Adam adjusts learning rates based on recent gradient updates, it might miss certain nuances that a more steady, consistent update rule like SGD captures.\n",
    "Learning Rate, Noise, Regularization Factors: These factors can also influence the performance of the optimizers. For example, the inherent noise in SGD updates can help escape local minima, potentially leading to better solutions in some landscapes. Regularization factors might also interact differently with these optimizers, affecting their performance.\n",
    "\n",
    "While Adam is often a preferred choice for many deep learning tasks, it’s not universally superior. The choice of optimizer and its tuning should depend on the specific characteristics of the problem at hand.\n",
    "This emphasizes the importance of empirical testing and validation in choosing an optimizer. Different problems might benefit from different optimizers, and the best way to determine the right choice is often through experimentation and performance evaluations.\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
